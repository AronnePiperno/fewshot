[[36m2025-02-14 00:42:16,756[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
You are using a CUDA device ('NVIDIA GeForce RTX 4070 SUPER') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
No of videos in train is 214
Loading train Video Information ...
No of class 10
No of videos in validation is 203
Loading validation Video Information ...
No of class 10
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”“
â”ƒ[1;35m [0m[1;35m   [0m[1;35m [0mâ”ƒ[1;35m [0m[1;35mName                                                   [0m[1;35m [0mâ”ƒ[1;35m [0m[1;35mType                           [0m[1;35m [0mâ”ƒ[1;35m [0m[1;35mParams[0m[1;35m [0mâ”ƒ[1;35m [0m[1;35mMode [0m[1;35m [0mâ”ƒ
â”¡â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”©
â”‚[2m [0m[2m0  [0m[2m [0mâ”‚ net                                                     â”‚ T3ALNet                         â”‚  639 M â”‚ train â”‚
â”‚[2m [0m[2m1  [0m[2m [0mâ”‚ net.model                                               â”‚ CoCa                            â”‚  638 M â”‚ train â”‚
â”‚[2m [0m[2m2  [0m[2m [0mâ”‚ net.model.text                                          â”‚ TextTransformer                 â”‚  123 M â”‚ train â”‚
â”‚[2m [0m[2m3  [0m[2m [0mâ”‚ net.model.text.token_embedding                          â”‚ Embedding                       â”‚ 37.9 M â”‚ train â”‚
â”‚[2m [0m[2m4  [0m[2m [0mâ”‚ net.model.text.transformer                              â”‚ Transformer                     â”‚ 85.1 M â”‚ train â”‚
â”‚[2m [0m[2m5  [0m[2m [0mâ”‚ net.model.text.transformer.resblocks                    â”‚ ModuleList                      â”‚ 85.1 M â”‚ train â”‚
â”‚[2m [0m[2m6  [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m7  [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m8  [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m9  [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m10 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m11 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m12 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m13 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m14 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m15 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m16 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m17 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m18 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m19 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m20 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m21 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m22 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m23 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m24 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m25 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m26 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m27 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m28 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m29 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m30 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m31 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m32 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m33 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m34 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m35 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m36 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m37 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m38 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m39 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m40 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m41 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m42 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m43 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m44 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m45 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m46 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m47 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m48 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m49 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m50 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m51 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m52 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m53 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m54 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m55 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m56 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m57 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m58 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m59 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m60 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m61 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m62 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m63 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m64 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m65 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m66 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m67 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m68 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m69 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m70 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m71 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m72 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m73 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m74 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m75 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m76 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m77 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m78 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m79 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m80 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m81 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m82 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m83 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m84 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m85 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m86 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m87 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m88 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m89 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m90 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m91 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m92 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m93 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m94 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m95 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m96 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m97 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m98 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m99 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m100[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m101[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m102[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m103[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m104[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m105[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m106[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m107[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m108[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m109[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m110[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m111[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m112[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m113[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m114[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m115[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m116[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10                 â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m117[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.ln_1            â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m118[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.attn            â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m119[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.attn.out_proj   â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m120[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.ls_1            â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m121[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.ln_2            â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m122[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.mlp             â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m123[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.mlp.c_fc        â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m124[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.mlp.gelu        â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m125[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.mlp.c_proj      â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m126[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.ls_2            â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m127[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11                 â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m128[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.ln_1            â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m129[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.attn            â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m130[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.attn.out_proj   â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m131[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.ls_1            â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m132[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.ln_2            â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m133[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.mlp             â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m134[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.mlp.c_fc        â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m135[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.mlp.gelu        â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m136[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.mlp.c_proj      â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m137[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.ls_2            â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m138[0m[2m [0mâ”‚ net.model.text.ln_final                                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m139[0m[2m [0mâ”‚ net.model.visual                                        â”‚ VisionTransformer               â”‚  306 M â”‚ train â”‚
â”‚[2m [0m[2m140[0m[2m [0mâ”‚ net.model.visual.conv1                                  â”‚ Conv2d                          â”‚  602 K â”‚ train â”‚
â”‚[2m [0m[2m141[0m[2m [0mâ”‚ net.model.visual.patch_dropout                          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m142[0m[2m [0mâ”‚ net.model.visual.ln_pre                                 â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m143[0m[2m [0mâ”‚ net.model.visual.transformer                            â”‚ Transformer                     â”‚  302 M â”‚ train â”‚
â”‚[2m [0m[2m144[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks                  â”‚ ModuleList                      â”‚  302 M â”‚ train â”‚
â”‚[2m [0m[2m145[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m146[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m147[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m148[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m149[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m150[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m151[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m152[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m153[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m154[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m155[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m156[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m157[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m158[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m159[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m160[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m161[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m162[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m163[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m164[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m165[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m166[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m167[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m168[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m169[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m170[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m171[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m172[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m173[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m174[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m175[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m176[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m177[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m178[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m179[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m180[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m181[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m182[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m183[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m184[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m185[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m186[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m187[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m188[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m189[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m190[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m191[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m192[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m193[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m194[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m195[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m196[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m197[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m198[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m199[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m200[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m201[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m202[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m203[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m204[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m205[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m206[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m207[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m208[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m209[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m210[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m211[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m212[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m213[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m214[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m215[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m216[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m217[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m218[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m219[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m220[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m221[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m222[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m223[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m224[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m225[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m226[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m227[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m228[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m229[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m230[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m231[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m232[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m233[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m234[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m235[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m236[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m237[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m238[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m239[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m240[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m241[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m242[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m243[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m244[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m245[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m246[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m247[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m248[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m249[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m250[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m251[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m252[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m253[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m254[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m255[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m256[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m257[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m258[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m259[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m260[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m261[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m262[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m263[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m264[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m265[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m266[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m267[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m268[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m269[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m270[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m271[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m272[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m273[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m274[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m275[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m276[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m277[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m278[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m279[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m280[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m281[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m282[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m283[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m284[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m285[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m286[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m287[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m288[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m289[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m290[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m291[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m292[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m293[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m294[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m295[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m296[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m297[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m298[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m299[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m300[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m301[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m302[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m303[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m304[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m305[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m306[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m307[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m308[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m309[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m310[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m311[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m312[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m313[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m314[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m315[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m316[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m317[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m318[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m319[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m320[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m321[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m322[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m323[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m324[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m325[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m326[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m327[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m328[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m329[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m330[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m331[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m332[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m333[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m334[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m335[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m336[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m337[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m338[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m339[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m340[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m341[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m342[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m343[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m344[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m345[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m346[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m347[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m348[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m349[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m350[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m351[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m352[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m353[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m354[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m355[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m356[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m357[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m358[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m359[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m360[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m361[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m362[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m363[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m364[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m365[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m366[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m367[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m368[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m369[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m370[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m371[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m372[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m373[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m374[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m375[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m376[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m377[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m378[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m379[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m380[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m381[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m382[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m383[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m384[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m385[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m386[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m387[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m388[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m389[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m390[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m391[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m392[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m393[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m394[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m395[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m396[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m397[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m398[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m399[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m400[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m401[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m402[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m403[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m404[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m405[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m406[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m407[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m408[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m409[0m[2m [0mâ”‚ net.model.visual.attn_pool                              â”‚ AttentionalPooler               â”‚  3.0 M â”‚ train â”‚
â”‚[2m [0m[2m410[0m[2m [0mâ”‚ net.model.visual.attn_pool.attn                         â”‚ MultiheadAttention              â”‚  2.8 M â”‚ train â”‚
â”‚[2m [0m[2m411[0m[2m [0mâ”‚ net.model.visual.attn_pool.attn.out_proj                â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m412[0m[2m [0mâ”‚ net.model.visual.attn_pool.ln_q                         â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m413[0m[2m [0mâ”‚ net.model.visual.attn_pool.ln_k                         â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m414[0m[2m [0mâ”‚ net.model.visual.ln_post                                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m415[0m[2m [0mâ”‚ net.model.text_decoder                                  â”‚ MultimodalTransformer           â”‚  208 M â”‚ train â”‚
â”‚[2m [0m[2m416[0m[2m [0mâ”‚ net.model.text_decoder.resblocks                        â”‚ ModuleList                      â”‚ 85.1 M â”‚ train â”‚
â”‚[2m [0m[2m417[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m418[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m419[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m420[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m421[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m422[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m423[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m424[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m425[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m426[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m427[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m428[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m429[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m430[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m431[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m432[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m433[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m434[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m435[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m436[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m437[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m438[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m439[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m440[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m441[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m442[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m443[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m444[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m445[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m446[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m447[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m448[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m449[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m450[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m451[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m452[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m453[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m454[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m455[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m456[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m457[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m458[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m459[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m460[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m461[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m462[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m463[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m464[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m465[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m466[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m467[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m468[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m469[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m470[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m471[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m472[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m473[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m474[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m475[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m476[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m477[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m478[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m479[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m480[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m481[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m482[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m483[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m484[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m485[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m486[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m487[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m488[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m489[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m490[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m491[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m492[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m493[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m494[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m495[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m496[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m497[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m498[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m499[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m500[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m501[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m502[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m503[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m504[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m505[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m506[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m507[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m508[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m509[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m510[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m511[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m512[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m513[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m514[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m515[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m516[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m517[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m518[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m519[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m520[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m521[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m522[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m523[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m524[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m525[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m526[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m527[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m528[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m529[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m530[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m531[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m532[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m533[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m534[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m535[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m536[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m537[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m538[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m539[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m540[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m541[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m542[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m543[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m544[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m545[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m546[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m547[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m548[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m549[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn                       â”‚ ModuleList                      â”‚ 85.1 M â”‚ train â”‚
â”‚[2m [0m[2m550[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m551[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m552[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m553[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m554[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m555[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m556[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m557[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m558[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m559[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m560[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m561[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m562[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m563[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m564[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m565[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m566[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m567[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m568[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m569[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m570[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m571[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m572[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m573[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m574[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m575[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m576[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m577[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m578[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m579[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m580[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m581[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m582[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m583[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m584[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m585[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m586[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m587[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m588[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m589[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m590[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m591[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m592[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m593[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m594[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m595[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m596[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m597[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m598[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m599[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m600[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m601[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m602[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m603[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m604[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m605[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m606[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m607[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m608[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m609[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m610[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m611[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m612[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m613[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m614[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m615[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m616[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m617[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m618[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m619[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m620[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m621[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m622[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m623[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m624[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m625[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m626[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m627[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m628[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m629[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m630[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m631[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m632[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m633[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m634[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m635[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m636[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m637[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m638[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m639[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m640[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m641[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m642[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m643[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m644[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m645[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m646[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m647[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m648[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m649[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m650[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m651[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m652[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m653[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m654[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m655[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m656[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m657[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m658[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m659[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m660[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m661[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m662[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m663[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m664[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m665[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m666[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m667[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m668[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m669[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m670[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10                    â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m671[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.ln_1               â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m672[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.attn               â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m673[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.attn.out_proj      â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m674[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.ls_1               â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m675[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.ln_1_kv            â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m676[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.ln_2               â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m677[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.mlp                â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m678[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.mlp.c_fc           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m679[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.mlp.gelu           â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m680[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.mlp.c_proj         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m681[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.ls_2               â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m682[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11                    â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m683[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.ln_1               â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m684[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.attn               â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m685[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.attn.out_proj      â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m686[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.ls_1               â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m687[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.ln_1_kv            â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m688[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.ln_2               â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m689[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.mlp                â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m690[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.mlp.c_fc           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m691[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.mlp.gelu           â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m692[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.mlp.c_proj         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m693[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.ls_2               â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m694[0m[2m [0mâ”‚ net.model.text_decoder.ln_final                         â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m695[0m[2m [0mâ”‚ net.tta_loss                                            â”‚ ByolLoss                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m696[0m[2m [0mâ”‚ net.video_proj                                          â”‚ VideoProjector                  â”‚  591 K â”‚ train â”‚
â”‚[2m [0m[2m697[0m[2m [0mâ”‚ net.video_proj.transform                                â”‚ Sequential                      â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m698[0m[2m [0mâ”‚ net.video_proj.transform.0                              â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m699[0m[2m [0mâ”‚ net.video_proj.transform.1                              â”‚ Dropout                         â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m700[0m[2m [0mâ”‚ net.fusion                                              â”‚ Fusion                          â”‚  6.2 K â”‚ train â”‚
â”‚[2m [0m[2m701[0m[2m [0mâ”‚ net.fusion.attn                                         â”‚ Sequential                      â”‚  6.2 K â”‚ train â”‚
â”‚[2m [0m[2m702[0m[2m [0mâ”‚ net.fusion.attn.0                                       â”‚ Linear                          â”‚  6.1 K â”‚ train â”‚
â”‚[2m [0m[2m703[0m[2m [0mâ”‚ net.fusion.attn.1                                       â”‚ Linear                          â”‚     10 â”‚ train â”‚
â”‚[2m [0m[2m704[0m[2m [0mâ”‚ net.fusion.attn.2                                       â”‚ Softmax                         â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m705[0m[2m [0mâ”‚ binary_acc                                              â”‚ BinaryAccuracy                  â”‚      0 â”‚ train â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜
[1mTrainable params[0m: 1.2 M
[1mNon-trainable params[0m: 637 M
[1mTotal params[0m: 639 M
[1mTotal estimated model params size (MB)[0m: 2.6 K
[1mModules in train mode[0m: 706
[1mModules in eval mode[0m: 0
/home/def/miniforge3/envs/fewshot/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.
`Trainer.fit` stopped: `max_epochs=0` reached.
[[36m2025-02-14 00:42:18,429[0m][[34m__main__[0m][[32mINFO[0m] - Starting testing![0m
[[36m2025-02-14 00:42:18,429[0m][[34m__main__[0m][[33mWARNING[0m] - Best ckpt not found! Using current weights for testing...[0m
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/def/miniforge3/envs/fewshot/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.
[2KStart testing...
[2KAttention weights: tensor([[0.5582, 0.4418],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 0/203 [2m0:00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
        [0.5540, 0.4460],
        [0.5547, 0.4453],
        [0.5570, 0.4430],
        [0.5559, 0.4441],
        [0.5534, 0.4466],
        [0.5526, 0.4474],
        [0.5577, 0.4423],
        [0.5582, 0.4418],
[2K        [0.5555, 0.4445]], device='cuda:0', grad_fn=<SoftmaxBackward0>):00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2K/home/def/fewshot/src/models/components/tt_method.py:317: UserWarning: The use of `x.T` on tensors of dimension other than 2
to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of
matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at
../aten/src/ATen/native/TensorShape.cpp:3683.)
  dot_product = (x @ y.T)
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 0/203 [2m0:00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5939, 0.4061]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KStep 0 - TTA Loss: 0.6590â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 0/203 [2m0:00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.8330, 0.1670]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 7.9677e-11]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 7.3080e-18]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.1630e-23]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 7.9891e-28]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.1009e-31]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.6975e-34]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.2810e-36]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.8095e-38]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 6.3575e-40]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KStep 10 - TTA Loss: 1.4684â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 0/203 [2m0:00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.7744e-41]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 6.7248e-42]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.5849e-42]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 5.6472e-43]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.8026e-43]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.7796e-43]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.3593e-43]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.1771e-43]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.1210e-43]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[0.6017, 0.3983]], device='cuda:0')203 [2m0:00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.1854e-32],â”â”â”[0m 0/203 [2m0:00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
        [1.0000e+00, 3.2071e-21],
        [1.0000e+00, 4.1189e-28],
        [1.0000e+00, 2.3243e-28],
        [1.0000e+00, 4.2871e-37],
        [1.0000e+00, 9.9492e-44],
        [1.0000e+00, 5.3729e-38],
        [1.0000e+00, 1.0458e-31],
        [1.0000e+00, 8.8884e-30],
[2K        [1.0000e+00, 6.0601e-36]], device='cuda:0')â”â”â”â”[0m 0/203 [2m0:00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5582, 0.4418],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 1/203 [2m0:00:01 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
        [0.5540, 0.4460],
        [0.5548, 0.4452],
        [0.5570, 0.4430],
        [0.5560, 0.4440],
        [0.5534, 0.4466],
        [0.5527, 0.4473],
        [0.5578, 0.4422],
        [0.5582, 0.4418],
[2K        [0.5555, 0.4445]], device='cuda:0', grad_fn=<SoftmaxBackward0>):00:01 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 1/203 [2m0:00:01 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5913, 0.4087]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KStep 0 - TTA Loss: 0.8867â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 1/203 [2m0:00:01 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5913, 0.4087]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5953, 0.4047]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.6120, 0.3880]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.6476, 0.3524]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.7020, 0.2980]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.7620, 0.2380]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.8026, 0.1974]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.9472, 0.0528]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[9.9961e-01, 3.9108e-04]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[9.9999e-01, 9.6782e-06]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KStep 10 - TTA Loss: 1.5848â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 1/203 [2m0:00:01 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.9081e-07]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.0846e-09]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.4344e-11]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 6.1347e-13]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 8.7858e-15]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.3903e-16]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.5710e-18]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 5.8198e-20]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.6725e-21]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[0.5868, 0.4132]], device='cuda:0')203 [2m0:00:01 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.0659e-17],â”â”â”[0m 1/203 [2m0:00:01 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
        [1.0000e+00, 9.2735e-12],
        [1.0000e+00, 4.6072e-15],
        [1.0000e+00, 3.7190e-15],
        [1.0000e+00, 1.3157e-19],
        [1.0000e+00, 5.9610e-23],
        [1.0000e+00, 4.3487e-20],
        [1.0000e+00, 8.0897e-17],
        [1.0000e+00, 8.8325e-16],
[2K        [1.0000e+00, 4.7002e-19]], device='cuda:0')â”â”â”â”[0m 1/203 [2m0:00:01 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5581, 0.4419],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 2/203 [2m0:00:02 â€¢ 0:02:42[0m [2;4m1.25it/s[0m  
        [0.5539, 0.4461],
        [0.5547, 0.4453],
        [0.5570, 0.4430],
        [0.5559, 0.4441],
        [0.5533, 0.4467],
        [0.5525, 0.4475],
        [0.5577, 0.4423],
        [0.5581, 0.4419],
[2K        [0.5554, 0.4446]], device='cuda:0', grad_fn=<SoftmaxBackward0>):00:02 â€¢ 0:02:42[0m [2;4m1.25it/s[0m  
[2KClass label: TennisSwingâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 2/203 [2m0:00:02 â€¢ 0:02:42[0m [2;4m1.25it/s[0m  
[2KAttention weights: tensor([[0.6437, 0.3563]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m1.25it/s[0m  
[2KStep 0 - TTA Loss: 0.5263â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 2/203 [2m0:00:02 â€¢ 0:02:42[0m [2;4m1.25it/s[0m  
[2KAttention weights: tensor([[0.8938, 0.1062]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m1.25it/s[0m  
[2KAttention weights: tensor([[0.9968, 0.0032]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m1.25it/s[0m  
[2KAttention weights: tensor([[1.4646e-04, 9.9985e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)1.25it/s[0m  
[2KAttention weights: tensor([[4.2633e-10, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)1.25it/s[0m  
[2KAttention weights: tensor([[6.9833e-15, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)1.25it/s[0m  
[2KAttention weights: tensor([[5.8643e-19, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)1.25it/s[0m  
[2KAttention weights: tensor([[2.2418e-22, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)1.25it/s[0m  
[2KAttention weights: tensor([[3.3641e-25, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)1.25it/s[0m  
[2KAttention weights: tensor([[1.7021e-27, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)1.25it/s[0m  
[2KAttention weights: tensor([[2.5031e-29, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)1.25it/s[0m  
[2KStep 10 - TTA Loss: 0.9721â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 2/203 [2m0:00:02 â€¢ 0:02:42[0m [2;4m1.25it/s[0m  
[2KAttention weights: tensor([[9.2390e-31, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)1.25it/s[0m  
[2KAttention weights: tensor([[7.4485e-32, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)1.25it/s[0m  
[2KAttention weights: tensor([[1.1506e-32, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)1.25it/s[0m  
[2KAttention weights: tensor([[3.0219e-33, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)1.25it/s[0m  
[2KAttention weights: tensor([[1.2112e-33, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)1.25it/s[0m  
[2KAttention weights: tensor([[6.7247e-34, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)1.25it/s[0m  
[2KAttention weights: tensor([[4.7506e-34, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)1.25it/s[0m  
[2KAttention weights: tensor([[3.9671e-34, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)1.25it/s[0m  
[2KAttention weights: tensor([[3.6865e-34, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)1.25it/s[0m  
[2KAttention weights: tensor([[0.4932, 0.5068]], device='cuda:0')203 [2m0:00:02 â€¢ 0:02:42[0m [2;4m1.25it/s[0m  
[2KAttention weights: tensor([[8.3785e-20, 1.0000e+00],â”â”â”[0m 2/203 [2m0:00:02 â€¢ 0:02:42[0m [2;4m1.25it/s[0m  
        [1.2673e-15, 1.0000e+00],
        [6.0913e-12, 1.0000e+00],
        [1.8712e-21, 1.0000e+00],
        [3.8303e-15, 1.0000e+00],
        [8.5588e-14, 1.0000e+00],
        [5.8037e-08, 1.0000e+00],
        [4.5897e-12, 1.0000e+00],
        [3.5558e-34, 1.0000e+00],
[2K        [9.2523e-18, 1.0000e+00]], device='cuda:0')â”â”â”â”[0m 2/203 [2m0:00:02 â€¢ 0:02:42[0m [2;4m1.25it/s[0m  
[2KAttention weights: tensor([[0.5580, 0.4420],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 3/203 [2m0:00:02 â€¢ 0:02:33[0m [2;4m1.31it/s[0m  
        [0.5538, 0.4462],
        [0.5546, 0.4454],
        [0.5568, 0.4432],
        [0.5557, 0.4443],
        [0.5531, 0.4469],
        [0.5524, 0.4476],
        [0.5576, 0.4424],
        [0.5580, 0.4420],
[2K        [0.5553, 0.4447]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”[0m 3/203 [2m0:00:02 â€¢ 0:02:33[0m [2;4m1.31it/s[0m  
[2KClass label: HighJump[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 3/203 [2m0:00:02 â€¢ 0:02:33[0m [2;4m1.31it/s[0m  
[2KAttention weights: tensor([[0.5921, 0.4079]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:02 â€¢ 0:02:33[0m [2;4m1.31it/s[0m  
[2KStep 0 - TTA Loss: 0.5903[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 3/203 [2m0:00:02 â€¢ 0:02:33[0m [2;4m1.31it/s[0m  
[2KAttention weights: tensor([[0.5921, 0.4079]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:02 â€¢ 0:02:33[0m [2;4m1.31it/s[0m  
[2KAttention weights: tensor([[0.5923, 0.4077]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:02 â€¢ 0:02:33[0m [2;4m1.31it/s[0m  
[2KAttention weights: tensor([[0.5968, 0.4032]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:02 â€¢ 0:02:33[0m [2;4m1.31it/s[0m  
[2KAttention weights: tensor([[0.6150, 0.3850]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:02 â€¢ 0:02:33[0m [2;4m1.31it/s[0m  
[2KAttention weights: tensor([[0.6551, 0.3449]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:02 â€¢ 0:02:33[0m [2;4m1.31it/s[0m  
[2KAttention weights: tensor([[0.7134, 0.2866]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:02 â€¢ 0:02:33[0m [2;4m1.31it/s[0m  
[2KAttention weights: tensor([[0.7731, 0.2269]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:02 â€¢ 0:02:33[0m [2;4m1.31it/s[0m  
[2KAttention weights: tensor([[0.8966, 0.1034]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:02 â€¢ 0:02:33[0m [2;4m1.31it/s[0m  
[2KAttention weights: tensor([[0.9969, 0.0031]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:02 â€¢ 0:02:33[0m [2;4m1.31it/s[0m  
[2KAttention weights: tensor([[0.9987, 0.0013]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:02 â€¢ 0:02:33[0m [2;4m1.31it/s[0m  
[2KStep 10 - TTA Loss: 1.1527[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 3/203 [2m0:00:02 â€¢ 0:02:33[0m [2;4m1.31it/s[0m  
[2KAttention weights: tensor([[0.9945, 0.0055]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:02 â€¢ 0:02:33[0m [2;4m1.31it/s[0m  
[2KAttention weights: tensor([[0.9585, 0.0415]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:02 â€¢ 0:02:33[0m [2;4m1.31it/s[0m  
[2KAttention weights: tensor([[0.9887, 0.0113]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:02 â€¢ 0:02:33[0m [2;4m1.31it/s[0m  
[2KAttention weights: tensor([[9.9982e-01, 1.8204e-04]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:02:33[0m [2;4m1.31it/s[0m  
[2KAttention weights: tensor([[9.9999e-01, 8.2241e-06]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:02:33[0m [2;4m1.31it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.3560e-07]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:02:33[0m [2;4m1.31it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.6512e-08]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:02:33[0m [2;4m1.31it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.9006e-09]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:02:33[0m [2;4m1.31it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.6404e-10]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:02:33[0m [2;4m1.31it/s[0m  
[2KAttention weights: tensor([[0.5738, 0.4262]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 3/203 [2m0:00:02 â€¢ 0:02:33[0m [2;4m1.31it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 9.3833e-08],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 3/203 [2m0:00:02 â€¢ 0:02:33[0m [2;4m1.31it/s[0m  
        [9.9997e-01, 3.4927e-05],
        [1.0000e+00, 2.3217e-07],
        [1.0000e+00, 2.1707e-06],
        [1.0000e+00, 1.5708e-09],
        [1.0000e+00, 1.6884e-11],
        [1.0000e+00, 2.1317e-10],
        [1.0000e+00, 3.0248e-08],
        [9.9998e-01, 1.6758e-05],
[2K        [1.0000e+00, 5.1895e-09]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 3/203 [2m0:00:02 â€¢ 0:02:33[0m [2;4m1.31it/s[0m  
[2KAttention weights: tensor([[0.5580, 0.4420],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 4/203 [2m0:00:04 â€¢ 0:03:40[0m [2;4m0.91it/s[0m  
        [0.5538, 0.4462],
        [0.5546, 0.4454],
        [0.5568, 0.4432],
        [0.5557, 0.4443],
        [0.5531, 0.4469],
        [0.5524, 0.4476],
        [0.5576, 0.4424],
        [0.5580, 0.4420],
[2K        [0.5553, 0.4447]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”[0m 4/203 [2m0:00:04 â€¢ 0:03:40[0m [2;4m0.91it/s[0m  
[2KClass label: GolfSwing[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 4/203 [2m0:00:04 â€¢ 0:03:40[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6273, 0.3727]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:04 â€¢ 0:03:40[0m [2;4m0.91it/s[0m  
[2KStep 0 - TTA Loss: 0.8786[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 4/203 [2m0:00:04 â€¢ 0:03:40[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.8968, 0.1032]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:04 â€¢ 0:03:40[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.9600, 0.0400]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:04 â€¢ 0:03:40[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5557, 0.4443]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:04 â€¢ 0:03:40[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.0644, 0.9356]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:04 â€¢ 0:03:40[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.0057, 0.9943]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:04 â€¢ 0:03:40[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[7.7749e-04, 9.9922e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:03:40[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[1.5642e-04, 9.9984e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:03:40[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[4.3700e-05, 9.9996e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:03:40[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[1.6034e-05, 9.9998e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:03:40[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[7.3650e-06, 9.9999e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:03:40[0m [2;4m0.91it/s[0m  
[2KStep 10 - TTA Loss: 1.1175[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 4/203 [2m0:00:04 â€¢ 0:03:40[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[4.0663e-06, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:03:40[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[2.6196e-06, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:03:40[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[1.9104e-06, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:03:40[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[1.5324e-06, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:03:40[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[1.3257e-06, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:03:40[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[1.2123e-06, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:03:40[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[1.1528e-06, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:03:40[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[1.1244e-06, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:03:40[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[1.1137e-06, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:03:40[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5427, 0.4573]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 4/203 [2m0:00:04 â€¢ 0:03:40[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[3.8102e-04, 9.9962e-01],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 4/203 [2m0:00:04 â€¢ 0:03:40[0m [2;4m0.91it/s[0m  
        [1.5975e-01, 8.4025e-01],
        [1.5423e-01, 8.4577e-01],
        [1.0980e-06, 1.0000e+00],
        [5.1989e-02, 9.4801e-01],
        [9.3997e-01, 6.0031e-02],
        [7.4428e-01, 2.5572e-01],
        [5.7507e-01, 4.2493e-01],
        [2.7209e-03, 9.9728e-01],
[2K        [1.5394e-02, 9.8461e-01]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 4/203 [2m0:00:04 â€¢ 0:03:40[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5580, 0.4420],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 5/203 [2m0:00:05 â€¢ 0:03:16[0m [2;4m1.01it/s[0m  
        [0.5538, 0.4462],
        [0.5546, 0.4454],
        [0.5568, 0.4432],
        [0.5557, 0.4443],
        [0.5531, 0.4469],
        [0.5524, 0.4476],
        [0.5576, 0.4424],
        [0.5580, 0.4420],
[2K        [0.5553, 0.4447]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”[0m 5/203 [2m0:00:05 â€¢ 0:03:16[0m [2;4m1.01it/s[0m  
[2KClass label: HammerThrowm[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 5/203 [2m0:00:05 â€¢ 0:03:16[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.5816, 0.4184]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:03:16[0m [2;4m1.01it/s[0m  
[2KStep 0 - TTA Loss: 0.6472[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 5/203 [2m0:00:05 â€¢ 0:03:16[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.5816, 0.4184]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:03:16[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.5819, 0.4181]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:03:16[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.5836, 0.4164]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:03:16[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.5863, 0.4137]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:03:16[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.5881, 0.4119]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:03:16[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.5809, 0.4191]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:03:16[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.5537, 0.4463]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:03:16[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.5374, 0.4626]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:03:16[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.6698, 0.3302]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:03:16[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.9218, 0.0782]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:03:16[0m [2;4m1.01it/s[0m  
[2KStep 10 - TTA Loss: 1.1813[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 5/203 [2m0:00:05 â€¢ 0:03:16[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[9.9952e-01, 4.7854e-04]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:03:16[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[9.9997e-01, 2.6044e-05]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:03:16[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.6093e-06]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:03:16[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.0221e-07]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:03:16[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 6.8838e-09]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:03:16[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 5.1101e-10]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:03:16[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.3171e-11]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:03:16[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.2544e-12]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:03:16[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.9785e-13]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:03:16[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.5769, 0.4231]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 5/203 [2m0:00:05 â€¢ 0:03:16[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.4441e-10],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 5/203 [2m0:00:05 â€¢ 0:03:16[0m [2;4m1.01it/s[0m  
        [9.9999e-01, 9.7712e-06],
        [1.0000e+00, 5.7588e-09],
        [1.0000e+00, 9.9261e-09],
        [1.0000e+00, 6.8440e-14],
        [1.0000e+00, 6.7384e-10],
        [1.0000e+00, 1.9330e-09],
        [1.0000e+00, 9.1143e-09],
        [1.0000e+00, 1.3180e-07],
[2K        [1.0000e+00, 1.5612e-11]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 5/203 [2m0:00:05 â€¢ 0:03:16[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.5581, 0.4419],m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:06 â€¢ 0:03:08[0m [2;4m1.05it/s[0m  
        [0.5539, 0.4461],
        [0.5547, 0.4453],
        [0.5569, 0.4431],
        [0.5559, 0.4441],
        [0.5532, 0.4468],
        [0.5525, 0.4475],
        [0.5577, 0.4423],
        [0.5581, 0.4419],
[2K        [0.5554, 0.4446]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:06 â€¢ 0:03:08[0m [2;4m1.05it/s[0m  
[2KClass label: HammerThrowm[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:06 â€¢ 0:03:08[0m [2;4m1.05it/s[0m  
[2KAttention weights: tensor([[0.5845, 0.4155]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:06 â€¢ 0:03:08[0m [2;4m1.05it/s[0m  
[2KStep 0 - TTA Loss: 0.5510[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:06 â€¢ 0:03:08[0m [2;4m1.05it/s[0m  
[2KAttention weights: tensor([[0.9195, 0.0805]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:06 â€¢ 0:03:08[0m [2;4m1.05it/s[0m  
[2KAttention weights: tensor([[0.6782, 0.3218]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:06 â€¢ 0:03:08[0m [2;4m1.05it/s[0m  
[2KAttention weights: tensor([[0.3269, 0.6731]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:06 â€¢ 0:03:08[0m [2;4m1.05it/s[0m  
[2KAttention weights: tensor([[0.1195, 0.8805]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:06 â€¢ 0:03:08[0m [2;4m1.05it/s[0m  
[2KAttention weights: tensor([[0.0384, 0.9616]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:06 â€¢ 0:03:08[0m [2;4m1.05it/s[0m  
[2KAttention weights: tensor([[0.0131, 0.9869]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:06 â€¢ 0:03:08[0m [2;4m1.05it/s[0m  
[2KAttention weights: tensor([[0.0050, 0.9950]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:06 â€¢ 0:03:08[0m [2;4m1.05it/s[0m  
[2KAttention weights: tensor([[0.0023, 0.9977]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:06 â€¢ 0:03:08[0m [2;4m1.05it/s[0m  
[2KAttention weights: tensor([[0.0012, 0.9988]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:06 â€¢ 0:03:08[0m [2;4m1.05it/s[0m  
[2KAttention weights: tensor([[6.7654e-04, 9.9932e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:00:06 â€¢ 0:03:08[0m [2;4m1.05it/s[0m  
[2KStep 10 - TTA Loss: 0.7943[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:06 â€¢ 0:03:08[0m [2;4m1.05it/s[0m  
[2KAttention weights: tensor([[4.3452e-04, 9.9957e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:00:06 â€¢ 0:03:08[0m [2;4m1.05it/s[0m  
[2KAttention weights: tensor([[3.1082e-04, 9.9969e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:00:06 â€¢ 0:03:08[0m [2;4m1.05it/s[0m  
[2KAttention weights: tensor([[2.4290e-04, 9.9976e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:00:06 â€¢ 0:03:08[0m [2;4m1.05it/s[0m  
[2KAttention weights: tensor([[2.0496e-04, 9.9980e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:00:06 â€¢ 0:03:08[0m [2;4m1.05it/s[0m  
[2KAttention weights: tensor([[1.8263e-04, 9.9982e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:00:06 â€¢ 0:03:08[0m [2;4m1.05it/s[0m  
[2KAttention weights: tensor([[1.6975e-04, 9.9983e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:00:06 â€¢ 0:03:08[0m [2;4m1.05it/s[0m  
[2KAttention weights: tensor([[1.6289e-04, 9.9984e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:00:06 â€¢ 0:03:08[0m [2;4m1.05it/s[0m  
[2KAttention weights: tensor([[1.5964e-04, 9.9984e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:00:06 â€¢ 0:03:08[0m [2;4m1.05it/s[0m  
[2KAttention weights: tensor([[1.5837e-04, 9.9984e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:00:06 â€¢ 0:03:08[0m [2;4m1.05it/s[0m  
[2KAttention weights: tensor([[0.5460, 0.4540]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:06 â€¢ 0:03:08[0m [2;4m1.05it/s[0m  
[2KAttention weights: tensor([[1.9870e-03, 9.9801e-01],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:06 â€¢ 0:03:08[0m [2;4m1.05it/s[0m  
        [4.1420e-02, 9.5858e-01],
        [4.2749e-03, 9.9573e-01],
        [5.8789e-03, 9.9412e-01],
        [1.5707e-04, 9.9984e-01],
        [2.4736e-03, 9.9753e-01],
        [2.6554e-03, 9.9734e-01],
        [7.4572e-03, 9.9254e-01],
        [1.5568e-02, 9.8443e-01],
[2K        [9.9582e-04, 9.9900e-01]], device='cuda:0')mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:06 â€¢ 0:03:08[0m [2;4m1.05it/s[0m  
[2KAttention weights: tensor([[0.5582, 0.4418],m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:07 â€¢ 0:03:33[0m [2;4m0.92it/s[0m  
        [0.5540, 0.4460],
        [0.5548, 0.4452],
        [0.5570, 0.4430],
        [0.5561, 0.4439],
        [0.5533, 0.4467],
        [0.5526, 0.4474],
        [0.5578, 0.4422],
        [0.5582, 0.4418],
[2K        [0.5556, 0.4444]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:07 â€¢ 0:03:33[0m [2;4m0.92it/s[0m  
[2KClass label: Billiards[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:07 â€¢ 0:03:33[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5850, 0.4150]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:07 â€¢ 0:03:33[0m [2;4m0.92it/s[0m  
[2KStep 0 - TTA Loss: 0.4417[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:07 â€¢ 0:03:33[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5850, 0.4150]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:07 â€¢ 0:03:33[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5833, 0.4167]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:07 â€¢ 0:03:33[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5735, 0.4265]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:07 â€¢ 0:03:33[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5459, 0.4541]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:07 â€¢ 0:03:33[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.4818, 0.5182]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:07 â€¢ 0:03:33[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.3936, 0.6064]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:07 â€¢ 0:03:33[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.3552, 0.6448]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:07 â€¢ 0:03:33[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.3843, 0.6157]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:07 â€¢ 0:03:33[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.4890, 0.5110]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:07 â€¢ 0:03:33[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6497, 0.3503]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:07 â€¢ 0:03:33[0m [2;4m0.92it/s[0m  
[2KStep 10 - TTA Loss: 0.6403[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:07 â€¢ 0:03:33[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.8156, 0.1844]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:07 â€¢ 0:03:33[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.9232, 0.0768]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:07 â€¢ 0:03:33[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.9608, 0.0392]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:07 â€¢ 0:03:33[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.9377, 0.0623]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:07 â€¢ 0:03:33[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.8370, 0.1630]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:07 â€¢ 0:03:33[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5903, 0.4097]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:07 â€¢ 0:03:33[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.2968, 0.7032]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:07 â€¢ 0:03:33[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.1163, 0.8837]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:07 â€¢ 0:03:33[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.0426, 0.9574]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:07 â€¢ 0:03:33[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5503, 0.4497]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:07 â€¢ 0:03:33[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.2043, 0.7957],8;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:07 â€¢ 0:03:33[0m [2;4m0.92it/s[0m  
        [0.0164, 0.9836],
        [0.1925, 0.8075],
        [0.2369, 0.7631],
        [0.1578, 0.8422],
        [0.1816, 0.8184],
        [0.1639, 0.8361],
        [0.1973, 0.8027],
        [0.2236, 0.7764],
[2K        [0.2039, 0.7961]], device='cuda:0')38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:07 â€¢ 0:03:33[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5582, 0.4418],mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:08 â€¢ 0:03:26[0m [2;4m0.95it/s[0m  
        [0.5540, 0.4460],
        [0.5548, 0.4452],
        [0.5570, 0.4430],
        [0.5561, 0.4439],
        [0.5533, 0.4467],
        [0.5526, 0.4474],
        [0.5578, 0.4422],
        [0.5582, 0.4418],
[2K        [0.5556, 0.4444]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:08 â€¢ 0:03:26[0m [2;4m0.95it/s[0m  
[2KClass label: BaseballPitch[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:08 â€¢ 0:03:26[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6084, 0.3916]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:08 â€¢ 0:03:26[0m [2;4m0.95it/s[0m  
[2KStep 0 - TTA Loss: 0.7419[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:08 â€¢ 0:03:26[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.7206, 0.2794]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:08 â€¢ 0:03:26[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.7407, 0.2593]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:08 â€¢ 0:03:26[0m [2;4m0.95it/s[0m  
[2KTesting [38;2;98;6;224mâ”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:08 â€¢ 0:03:26[0m [2;4m0.95it/s[0m

Detected KeyboardInterrupt, attempting graceful shutdown ...
[?25h[[36m2025-02-14 00:42:28,303[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/def/fewshot/logs/train/runs/2025-02-14_00-42-08[0m
[[36m2025-02-14 00:42:28,304[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
