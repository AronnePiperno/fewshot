[[36m2025-02-15 11:47:48,951[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
You are using a CUDA device ('NVIDIA GeForce RTX 4070 SUPER') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
No of videos in train is 214
Loading train Video Information ...
No of class 10
No of videos in validation is 203
Loading validation Video Information ...
No of class 10
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”“
â”ƒ[1;35m [0m[1;35m   [0m[1;35m [0mâ”ƒ[1;35m [0m[1;35mName                                                   [0m[1;35m [0mâ”ƒ[1;35m [0m[1;35mType                           [0m[1;35m [0mâ”ƒ[1;35m [0m[1;35mParams[0m[1;35m [0mâ”ƒ[1;35m [0m[1;35mMode [0m[1;35m [0mâ”ƒ
â”¡â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”©
â”‚[2m [0m[2m0  [0m[2m [0mâ”‚ net                                                     â”‚ T3ALNet                         â”‚  639 M â”‚ train â”‚
â”‚[2m [0m[2m1  [0m[2m [0mâ”‚ net.model                                               â”‚ CoCa                            â”‚  638 M â”‚ train â”‚
â”‚[2m [0m[2m2  [0m[2m [0mâ”‚ net.model.text                                          â”‚ TextTransformer                 â”‚  123 M â”‚ train â”‚
â”‚[2m [0m[2m3  [0m[2m [0mâ”‚ net.model.text.token_embedding                          â”‚ Embedding                       â”‚ 37.9 M â”‚ train â”‚
â”‚[2m [0m[2m4  [0m[2m [0mâ”‚ net.model.text.transformer                              â”‚ Transformer                     â”‚ 85.1 M â”‚ train â”‚
â”‚[2m [0m[2m5  [0m[2m [0mâ”‚ net.model.text.transformer.resblocks                    â”‚ ModuleList                      â”‚ 85.1 M â”‚ train â”‚
â”‚[2m [0m[2m6  [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m7  [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m8  [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m9  [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m10 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m11 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m12 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m13 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m14 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m15 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m16 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m17 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m18 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m19 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m20 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m21 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m22 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m23 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m24 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m25 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m26 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m27 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m28 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m29 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m30 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m31 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m32 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m33 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m34 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m35 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m36 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m37 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m38 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m39 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m40 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m41 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m42 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m43 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m44 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m45 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m46 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m47 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m48 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m49 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m50 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m51 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m52 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m53 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m54 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m55 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m56 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m57 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m58 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m59 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m60 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m61 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m62 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m63 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m64 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m65 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m66 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m67 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m68 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m69 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m70 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m71 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m72 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m73 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m74 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m75 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m76 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m77 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m78 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m79 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m80 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m81 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m82 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m83 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m84 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m85 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m86 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m87 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m88 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m89 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m90 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m91 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m92 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m93 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m94 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m95 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m96 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m97 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m98 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m99 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m100[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m101[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m102[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m103[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m104[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m105[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m106[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m107[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m108[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m109[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m110[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m111[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m112[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m113[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m114[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m115[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m116[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10                 â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m117[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.ln_1            â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m118[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.attn            â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m119[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.attn.out_proj   â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m120[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.ls_1            â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m121[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.ln_2            â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m122[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.mlp             â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m123[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.mlp.c_fc        â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m124[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.mlp.gelu        â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m125[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.mlp.c_proj      â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m126[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.ls_2            â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m127[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11                 â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m128[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.ln_1            â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m129[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.attn            â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m130[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.attn.out_proj   â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m131[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.ls_1            â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m132[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.ln_2            â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m133[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.mlp             â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m134[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.mlp.c_fc        â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m135[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.mlp.gelu        â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m136[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.mlp.c_proj      â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m137[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.ls_2            â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m138[0m[2m [0mâ”‚ net.model.text.ln_final                                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m139[0m[2m [0mâ”‚ net.model.visual                                        â”‚ VisionTransformer               â”‚  306 M â”‚ train â”‚
â”‚[2m [0m[2m140[0m[2m [0mâ”‚ net.model.visual.conv1                                  â”‚ Conv2d                          â”‚  602 K â”‚ train â”‚
â”‚[2m [0m[2m141[0m[2m [0mâ”‚ net.model.visual.patch_dropout                          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m142[0m[2m [0mâ”‚ net.model.visual.ln_pre                                 â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m143[0m[2m [0mâ”‚ net.model.visual.transformer                            â”‚ Transformer                     â”‚  302 M â”‚ train â”‚
â”‚[2m [0m[2m144[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks                  â”‚ ModuleList                      â”‚  302 M â”‚ train â”‚
â”‚[2m [0m[2m145[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m146[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m147[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m148[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m149[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m150[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m151[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m152[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m153[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m154[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m155[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m156[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m157[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m158[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m159[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m160[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m161[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m162[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m163[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m164[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m165[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m166[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m167[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m168[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m169[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m170[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m171[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m172[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m173[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m174[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m175[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m176[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m177[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m178[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m179[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m180[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m181[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m182[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m183[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m184[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m185[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m186[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m187[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m188[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m189[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m190[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m191[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m192[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m193[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m194[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m195[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m196[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m197[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m198[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m199[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m200[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m201[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m202[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m203[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m204[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m205[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m206[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m207[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m208[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m209[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m210[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m211[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m212[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m213[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m214[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m215[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m216[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m217[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m218[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m219[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m220[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m221[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m222[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m223[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m224[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m225[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m226[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m227[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m228[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m229[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m230[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m231[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m232[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m233[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m234[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m235[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m236[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m237[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m238[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m239[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m240[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m241[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m242[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m243[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m244[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m245[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m246[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m247[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m248[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m249[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m250[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m251[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m252[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m253[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m254[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m255[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m256[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m257[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m258[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m259[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m260[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m261[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m262[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m263[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m264[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m265[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m266[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m267[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m268[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m269[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m270[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m271[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m272[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m273[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m274[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m275[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m276[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m277[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m278[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m279[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m280[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m281[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m282[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m283[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m284[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m285[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m286[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m287[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m288[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m289[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m290[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m291[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m292[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m293[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m294[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m295[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m296[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m297[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m298[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m299[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m300[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m301[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m302[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m303[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m304[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m305[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m306[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m307[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m308[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m309[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m310[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m311[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m312[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m313[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m314[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m315[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m316[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m317[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m318[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m319[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m320[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m321[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m322[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m323[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m324[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m325[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m326[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m327[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m328[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m329[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m330[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m331[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m332[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m333[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m334[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m335[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m336[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m337[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m338[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m339[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m340[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m341[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m342[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m343[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m344[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m345[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m346[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m347[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m348[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m349[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m350[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m351[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m352[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m353[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m354[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m355[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m356[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m357[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m358[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m359[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m360[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m361[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m362[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m363[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m364[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m365[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m366[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m367[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m368[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m369[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m370[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m371[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m372[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m373[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m374[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m375[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m376[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m377[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m378[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m379[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m380[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m381[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m382[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m383[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m384[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m385[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m386[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m387[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m388[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m389[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m390[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m391[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m392[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m393[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m394[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m395[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m396[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m397[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m398[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m399[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m400[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m401[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m402[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m403[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m404[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m405[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m406[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m407[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m408[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m409[0m[2m [0mâ”‚ net.model.visual.attn_pool                              â”‚ AttentionalPooler               â”‚  3.0 M â”‚ train â”‚
â”‚[2m [0m[2m410[0m[2m [0mâ”‚ net.model.visual.attn_pool.attn                         â”‚ MultiheadAttention              â”‚  2.8 M â”‚ train â”‚
â”‚[2m [0m[2m411[0m[2m [0mâ”‚ net.model.visual.attn_pool.attn.out_proj                â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m412[0m[2m [0mâ”‚ net.model.visual.attn_pool.ln_q                         â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m413[0m[2m [0mâ”‚ net.model.visual.attn_pool.ln_k                         â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m414[0m[2m [0mâ”‚ net.model.visual.ln_post                                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m415[0m[2m [0mâ”‚ net.model.text_decoder                                  â”‚ MultimodalTransformer           â”‚  208 M â”‚ train â”‚
â”‚[2m [0m[2m416[0m[2m [0mâ”‚ net.model.text_decoder.resblocks                        â”‚ ModuleList                      â”‚ 85.1 M â”‚ train â”‚
â”‚[2m [0m[2m417[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m418[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m419[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m420[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m421[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m422[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m423[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m424[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m425[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m426[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m427[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m428[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m429[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m430[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m431[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m432[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m433[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m434[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m435[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m436[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m437[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m438[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m439[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m440[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m441[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m442[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m443[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m444[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m445[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m446[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m447[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m448[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m449[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m450[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m451[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m452[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m453[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m454[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m455[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m456[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m457[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m458[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m459[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m460[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m461[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m462[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m463[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m464[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m465[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m466[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m467[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m468[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m469[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m470[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m471[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m472[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m473[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m474[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m475[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m476[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m477[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m478[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m479[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m480[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m481[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m482[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m483[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m484[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m485[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m486[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m487[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m488[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m489[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m490[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m491[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m492[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m493[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m494[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m495[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m496[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m497[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m498[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m499[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m500[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m501[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m502[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m503[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m504[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m505[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m506[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m507[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m508[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m509[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m510[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m511[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m512[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m513[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m514[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m515[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m516[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m517[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m518[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m519[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m520[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m521[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m522[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m523[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m524[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m525[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m526[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m527[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m528[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m529[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m530[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m531[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m532[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m533[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m534[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m535[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m536[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m537[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m538[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m539[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m540[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m541[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m542[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m543[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m544[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m545[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m546[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m547[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m548[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m549[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn                       â”‚ ModuleList                      â”‚ 85.1 M â”‚ train â”‚
â”‚[2m [0m[2m550[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m551[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m552[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m553[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m554[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m555[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m556[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m557[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m558[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m559[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m560[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m561[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m562[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m563[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m564[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m565[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m566[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m567[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m568[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m569[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m570[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m571[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m572[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m573[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m574[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m575[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m576[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m577[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m578[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m579[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m580[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m581[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m582[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m583[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m584[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m585[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m586[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m587[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m588[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m589[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m590[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m591[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m592[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m593[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m594[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m595[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m596[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m597[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m598[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m599[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m600[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m601[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m602[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m603[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m604[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m605[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m606[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m607[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m608[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m609[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m610[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m611[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m612[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m613[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m614[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m615[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m616[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m617[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m618[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m619[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m620[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m621[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m622[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m623[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m624[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m625[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m626[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m627[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m628[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m629[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m630[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m631[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m632[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m633[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m634[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m635[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m636[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m637[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m638[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m639[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m640[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m641[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m642[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m643[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m644[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m645[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m646[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m647[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m648[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m649[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m650[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m651[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m652[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m653[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m654[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m655[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m656[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m657[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m658[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m659[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m660[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m661[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m662[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m663[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m664[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m665[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m666[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m667[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m668[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m669[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m670[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10                    â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m671[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.ln_1               â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m672[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.attn               â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m673[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.attn.out_proj      â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m674[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.ls_1               â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m675[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.ln_1_kv            â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m676[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.ln_2               â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m677[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.mlp                â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m678[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.mlp.c_fc           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m679[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.mlp.gelu           â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m680[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.mlp.c_proj         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m681[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.ls_2               â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m682[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11                    â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m683[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.ln_1               â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m684[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.attn               â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m685[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.attn.out_proj      â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m686[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.ls_1               â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m687[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.ln_1_kv            â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m688[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.ln_2               â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m689[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.mlp                â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m690[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.mlp.c_fc           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m691[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.mlp.gelu           â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m692[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.mlp.c_proj         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m693[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.ls_2               â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m694[0m[2m [0mâ”‚ net.model.text_decoder.ln_final                         â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m695[0m[2m [0mâ”‚ net.tta_loss                                            â”‚ ByolLoss                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m696[0m[2m [0mâ”‚ net.video_proj                                          â”‚ VideoProjector                  â”‚  591 K â”‚ train â”‚
â”‚[2m [0m[2m697[0m[2m [0mâ”‚ net.video_proj.transform                                â”‚ Sequential                      â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m698[0m[2m [0mâ”‚ net.video_proj.transform.0                              â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m699[0m[2m [0mâ”‚ net.video_proj.transform.1                              â”‚ Dropout                         â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m700[0m[2m [0mâ”‚ net.fusion                                              â”‚ Fusion                          â”‚  6.2 K â”‚ train â”‚
â”‚[2m [0m[2m701[0m[2m [0mâ”‚ net.fusion.attn                                         â”‚ Sequential                      â”‚  6.2 K â”‚ train â”‚
â”‚[2m [0m[2m702[0m[2m [0mâ”‚ net.fusion.attn.0                                       â”‚ Linear                          â”‚  6.1 K â”‚ train â”‚
â”‚[2m [0m[2m703[0m[2m [0mâ”‚ net.fusion.attn.1                                       â”‚ Linear                          â”‚     10 â”‚ train â”‚
â”‚[2m [0m[2m704[0m[2m [0mâ”‚ net.fusion.attn.2                                       â”‚ Softmax                         â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m705[0m[2m [0mâ”‚ binary_acc                                              â”‚ BinaryAccuracy                  â”‚      0 â”‚ train â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜
[1mTrainable params[0m: 1.2 M
[1mNon-trainable params[0m: 637 M
[1mTotal params[0m: 639 M
[1mTotal estimated model params size (MB)[0m: 2.6 K
[1mModules in train mode[0m: 706
[1mModules in eval mode[0m: 0
/home/def/miniforge3/envs/fewshot/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.
`Trainer.fit` stopped: `max_epochs=0` reached.
[[36m2025-02-15 11:47:50,709[0m][[34m__main__[0m][[32mINFO[0m] - Starting testing![0m
[[36m2025-02-15 11:47:50,709[0m][[34m__main__[0m][[33mWARNING[0m] - Best ckpt not found! Using current weights for testing...[0m
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/def/miniforge3/envs/fewshot/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.
[2KStart testing...
[2KAttention weights: tensor([[0.5594, 0.4406],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 0/203 [2m0:00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
        [0.5544, 0.4456],
        [0.5548, 0.4452],
        [0.5578, 0.4422],
[2K        [0.5551, 0.4449]], device='cuda:0', grad_fn=<SoftmaxBackward0>):00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2K/home/def/fewshot/src/models/components/tt_method.py:317: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or
`x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3683.)
  dot_product = (x @ y.T)
[2KClass label: PoleVaultâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 0/203 [2m0:00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.6128, 0.3872]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KStep 0 - TTA Loss: 0.6066â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 0/203 [2m0:00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.6811, 0.3189]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[9.9999e-01, 1.0905e-05]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.9944e-09]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.0757e-11]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 6.7805e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 9.5316e-16]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.5105e-17]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.0985e-18]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 7.3146e-20]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 6.9343e-21]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KStep 10 - TTA Loss: 1.4507â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 0/203 [2m0:00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 8.8830e-22]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.4743e-22]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.0629e-23]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 7.7387e-24]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.3206e-24]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 8.0888e-25]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.2187e-25]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.4394e-25]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 7.1354e-26]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.8739e-26]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KStep 20 - TTA Loss: 1.4105â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 0/203 [2m0:00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.2789e-26]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.4389e-26]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 9.6703e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 6.8656e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 5.1153e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.9759e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.2069e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.6715e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.2891e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.0099e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KStep 30 - TTA Loss: 1.3805â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 0/203 [2m0:00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.8025e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.6462e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.5272e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.4358e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.3654e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.3110e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.2690e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.2367e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.2120e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.1933e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KStep 40 - TTA Loss: 1.4536â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 0/203 [2m0:00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.1794e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.1691e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.1618e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.1566e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.1532e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.1511e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.1498e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.1492e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.1489e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[0.5805, 0.4195]], device='cuda:0')203 [2m0:00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.8603e-18],â”â”â”[0m 0/203 [2m0:00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
        [1.0000e+00, 3.7767e-21],
        [1.0000e+00, 1.0936e-27],
        [1.0000e+00, 5.7871e-21],
[2K        [1.0000e+00, 7.3383e-18]], device='cuda:0')â”â”â”â”[0m 0/203 [2m0:00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5594, 0.4406],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 1/203 [2m0:00:02 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
        [0.5544, 0.4456],
        [0.5546, 0.4454],
        [0.5578, 0.4422],
[2K        [0.5550, 0.4450]], device='cuda:0', grad_fn=<SoftmaxBackward0>):00:02 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KClass label: HammerThrowâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 1/203 [2m0:00:02 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5414, 0.4586]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KStep 0 - TTA Loss: 0.9600â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 1/203 [2m0:00:02 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5414, 0.4586]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5416, 0.4584]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5428, 0.4572]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5457, 0.4543]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5512, 0.4488]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5597, 0.4403]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5688, 0.4312]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5722, 0.4278]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5647, 0.4353]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5519, 0.4481]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KStep 10 - TTA Loss: 1.1441â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 1/203 [2m0:00:02 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5672, 0.4328]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.6478, 0.3522]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.7847, 0.2153]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.9226, 0.0774]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.9805, 0.0195]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.9632, 0.0368]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.7984, 0.2016]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.3492, 0.6508]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.0682, 0.9318]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.0103, 0.9897]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KStep 20 - TTA Loss: 0.8087â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 1/203 [2m0:00:02 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.0016, 0.9984]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[2.4795e-04, 9.9975e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[4.2479e-05, 9.9996e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[7.8818e-06, 9.9999e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.5864e-06, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[3.4622e-07, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[8.2332e-08, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[2.1038e-08, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[5.8563e-09, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.7650e-09, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KStep 30 - TTA Loss: 0.8688â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 1/203 [2m0:00:02 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[5.7059e-10, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.9449e-10, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[6.9799e-11, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[2.6634e-11, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0678e-11, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[4.4364e-12, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.9323e-12, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[8.7497e-13, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[4.1070e-13, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.9938e-13, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KStep 40 - TTA Loss: 0.9265â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 1/203 [2m0:00:02 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[1.0003e-13, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[5.1540e-14, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[2.7871e-14, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.5832e-14, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[9.3705e-15, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[5.8113e-15, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[3.7842e-15, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[2.5774e-15, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.8097e-15, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[0.5362, 0.4638]], device='cuda:0')203 [2m0:00:02 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[2.6280e-09, 1.0000e+00],â”â”â”[0m 1/203 [2m0:00:02 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
        [1.3124e-15, 1.0000e+00],
        [5.7967e-08, 1.0000e+00],
        [1.7900e-09, 1.0000e+00],
[2K        [3.1082e-09, 1.0000e+00]], device='cuda:0')â”â”â”â”[0m 1/203 [2m0:00:02 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5594, 0.4406],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 2/203 [2m0:00:03 â€¢ 0:05:00[0m [2;4m0.67it/s[0m  
        [0.5543, 0.4457],
        [0.5546, 0.4454],
        [0.5577, 0.4423],
[2K        [0.5550, 0.4450]], device='cuda:0', grad_fn=<SoftmaxBackward0>):00:03 â€¢ 0:05:00[0m [2;4m0.67it/s[0m  
[2KClass label: SoccerPenaltyâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 2/203 [2m0:00:03 â€¢ 0:05:00[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[0.6355, 0.3645]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.67it/s[0m  
[2KStep 0 - TTA Loss: 1.1242â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 2/203 [2m0:00:03 â€¢ 0:05:00[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[0.6498, 0.3502]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[0.9658, 0.0342]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[9.9999e-01, 1.3682e-05]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.3542e-08]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.5607e-10]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.2930e-12]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.8055e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.0401e-16]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.3833e-17]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 6.9481e-19]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.67it/s[0m  
[2KStep 10 - TTA Loss: 1.7792â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 2/203 [2m0:00:03 â€¢ 0:05:00[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.9259e-20]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.7591e-21]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 6.0681e-22]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 9.9180e-23]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.0237e-23]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 5.0323e-24]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.4923e-24]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 5.1747e-25]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.0613e-25]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 9.2820e-26]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.67it/s[0m  
[2KStep 20 - TTA Loss: 1.6617â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 2/203 [2m0:00:03 â€¢ 0:05:00[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.6576e-26]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.5709e-26]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.5430e-26]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 9.9661e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 6.8633e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.9982e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.8212e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.0469e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.5195e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.1496e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.67it/s[0m  
[2KStep 30 - TTA Loss: 1.5443â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 2/203 [2m0:00:03 â€¢ 0:05:00[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.8839e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.6893e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.5447e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.4359e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.3536e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.2909e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.2432e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.2070e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.1796e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.1590e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.67it/s[0m  
[2KStep 40 - TTA Loss: 1.5730â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 2/203 [2m0:00:03 â€¢ 0:05:00[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.1438e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.1327e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.1249e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.1194e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.1158e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.1135e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.1122e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.1115e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.1113e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.67it/s[0m  
[2KAttention weights: tensor([[0.5803, 0.4197]], device='cuda:0')203 [2m0:00:03 â€¢ 0:05:00[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.9580e-14],â”â”â”[0m 2/203 [2m0:00:03 â€¢ 0:05:00[0m [2;4m0.67it/s[0m  
        [1.0000e+00, 3.6861e-11],
        [1.0000e+00, 5.2534e-11],
        [1.0000e+00, 1.1029e-27],
[2K        [1.0000e+00, 1.2790e-06]], device='cuda:0')â”â”â”â”[0m 2/203 [2m0:00:03 â€¢ 0:05:00[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[0.5591, 0.4409],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 3/203 [2m0:00:05 â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
        [0.5541, 0.4459],
        [0.5545, 0.4455],
        [0.5574, 0.4426],
[2K        [0.5548, 0.4452]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”[0m 3/203 [2m0:00:05 â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KClass label: HammerThrowm[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 3/203 [2m0:00:05 â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KAttention weights: tensor([[0.5394, 0.4606]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KStep 0 - TTA Loss: 0.6776[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 3/203 [2m0:00:05 â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KAttention weights: tensor([[0.5394, 0.4606]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KAttention weights: tensor([[0.5400, 0.4600]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KAttention weights: tensor([[0.5430, 0.4570]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KAttention weights: tensor([[0.5513, 0.4487]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KAttention weights: tensor([[0.5667, 0.4333]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KAttention weights: tensor([[0.5909, 0.4091]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KAttention weights: tensor([[0.6235, 0.3765]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KAttention weights: tensor([[0.6660, 0.3340]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KAttention weights: tensor([[0.7123, 0.2877]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KAttention weights: tensor([[0.7853, 0.2147]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KStep 10 - TTA Loss: 1.0528[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 3/203 [2m0:00:05 â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KAttention weights: tensor([[0.8983, 0.1017]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KAttention weights: tensor([[0.9883, 0.0117]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KAttention weights: tensor([[9.9961e-01, 3.9209e-04]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KAttention weights: tensor([[9.9998e-01, 1.9452e-05]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 8.9788e-07]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.8386e-08]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.5610e-09]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 6.2024e-11]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.4677e-12]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.0052e-13]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KStep 20 - TTA Loss: 1.4282[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 3/203 [2m0:00:05 â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.2753e-15]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.9316e-16]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 9.4085e-18]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 5.0025e-19]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.9334e-20]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.9128e-21]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.3959e-22]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.1452e-23]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.0597e-24]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.1077e-25]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KStep 30 - TTA Loss: 1.3859[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 3/203 [2m0:00:05 â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.3087e-26]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.7466e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.6291e-28]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.4533e-29]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 8.4634e-30]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.7983e-30]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.2556e-31]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.1167e-31]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.2351e-32]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.0297e-32]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KStep 40 - TTA Loss: 1.5010[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 3/203 [2m0:00:05 â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.5838e-33]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.3573e-33]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 5.5666e-34]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.4603e-34]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.1664e-34]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 5.9034e-35]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.1759e-35]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.8081e-35]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.0848e-35]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KAttention weights: tensor([[0.6075, 0.3925]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 3/203 [2m0:00:05 â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.3320e-21],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 3/203 [2m0:00:05 â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
        [1.0000e+00, 6.6960e-36],
        [1.0000e+00, 1.6386e-18],
        [1.0000e+00, 3.6667e-21],
[2K        [1.0000e+00, 1.4200e-21]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 3/203 [2m0:00:05 â€¢ 0:04:49[0m [2;4m0.69it/s[0m  
[2KAttention weights: tensor([[0.5594, 0.4406],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 4/203 [2m0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
        [0.5546, 0.4454],
        [0.5548, 0.4452],
        [0.5576, 0.4424],
[2K        [0.5552, 0.4448]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”[0m 4/203 [2m0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KClass label: BaseballPitch[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 4/203 [2m0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.6406, 0.3594]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KStep 0 - TTA Loss: 1.2570[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 4/203 [2m0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.8777, 0.1223]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.2769, 0.7231]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.2696, 0.7304]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.3322, 0.6678]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.4224, 0.5776]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.5249, 0.4751]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.6217, 0.3783]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.6945, 0.3055]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.7419, 0.2581]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.7636, 0.2364]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KStep 10 - TTA Loss: 1.2918[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 4/203 [2m0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.7633, 0.2367]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.7491, 0.2509]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.7113, 0.2887]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.6604, 0.3396]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.6065, 0.3935]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.5531, 0.4469]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.5043, 0.4957]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.4631, 0.5369]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.4290, 0.5710]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.4035, 0.5965]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KStep 20 - TTA Loss: 1.3397[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 4/203 [2m0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.3843, 0.6157]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.3699, 0.6301]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.3602, 0.6398]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.3562, 0.6438]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.3556, 0.6444]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.3568, 0.6432]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.3601, 0.6399]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.3645, 0.6355]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.3711, 0.6289]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.3781, 0.6219]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KStep 30 - TTA Loss: 1.3232[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 4/203 [2m0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.3855, 0.6145]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.3938, 0.6062]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.4018, 0.5982]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.4095, 0.5905]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.4161, 0.5839]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.4223, 0.5777]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.4279, 0.5721]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.4334, 0.5666]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.4382, 0.5618]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.4418, 0.5582]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KStep 40 - TTA Loss: 1.3750[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 4/203 [2m0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.4444, 0.5556]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.4465, 0.5535]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.4477, 0.5523]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.4487, 0.5513]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.4493, 0.5507]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.4497, 0.5503]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.4500, 0.5500]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.4502, 0.5498]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.4502, 0.5498]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.5561, 0.4439]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 4/203 [2m0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.4502, 0.5498],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 4/203 [2m0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
        [0.8762, 0.1238],
        [0.5638, 0.4362],
        [0.7475, 0.2525],
[2K        [0.7658, 0.2342]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 4/203 [2m0:00:07 â€¢ 0:06:09[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.5595, 0.4405],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 5/203 [2m0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
        [0.5548, 0.4452],
        [0.5549, 0.4451],
        [0.5577, 0.4423],
[2K        [0.5553, 0.4447]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”[0m 5/203 [2m0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KClass label: HammerThrowm[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 5/203 [2m0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5422, 0.4578]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KStep 0 - TTA Loss: 0.6397[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 5/203 [2m0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5422, 0.4578]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5425, 0.4575]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5439, 0.4561]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5478, 0.4522]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5562, 0.4438]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5700, 0.4300]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5884, 0.4116]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.6071, 0.3929]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.6262, 0.3738]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.6569, 0.3431]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KStep 10 - TTA Loss: 1.3632[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 5/203 [2m0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.7919, 0.2081]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.9529, 0.0471]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.9905, 0.0095]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.9673, 0.0327]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.8839, 0.1161]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.7710, 0.2290]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.6694, 0.3306]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5684, 0.4316]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.4632, 0.5368]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.3602, 0.6398]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KStep 20 - TTA Loss: 1.1005[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 5/203 [2m0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.2703, 0.7297]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.1997, 0.8003]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.1477, 0.8523]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.1079, 0.8921]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.0816, 0.9184]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.0620, 0.9380]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.0479, 0.9521]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.0369, 0.9631]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.0292, 0.9708]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.0228, 0.9772]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KStep 30 - TTA Loss: 1.1075[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 5/203 [2m0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.0177, 0.9823]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.0137, 0.9863]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.0107, 0.9893]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.0084, 0.9916]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.0067, 0.9933]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.0052, 0.9948]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.0041, 0.9959]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.0033, 0.9967]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.0026, 0.9974]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.0021, 0.9979]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KStep 40 - TTA Loss: 1.0275[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 5/203 [2m0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.0017, 0.9983]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.0015, 0.9985]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.0013, 0.9987]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.0012, 0.9988]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.0011, 0.9989]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[9.7719e-04, 9.9902e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[9.0275e-04, 9.9910e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[8.3918e-04, 9.9916e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[7.9657e-04, 9.9920e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5505, 0.4495]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 5/203 [2m0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[2.7874e-02, 9.7213e-01],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 5/203 [2m0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
        [7.6360e-04, 9.9924e-01],
        [3.4923e-02, 9.6508e-01],
        [2.2244e-02, 9.7776e-01],
[2K        [1.9305e-02, 9.8070e-01]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 5/203 [2m0:00:09 â€¢ 0:05:44[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5595, 0.4405],m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
        [0.5548, 0.4452],
        [0.5549, 0.4451],
        [0.5577, 0.4423],
[2K        [0.5554, 0.4446]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KClass label: BaseballPitch[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[0.6449, 0.3551]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KStep 0 - TTA Loss: 0.7051[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[0.8885, 0.1115]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 7.9180e-07]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.7655e-11]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.2065e-15]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.2826e-19]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.0881e-22]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.1980e-25]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.8119e-28]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.3070e-30]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.1243e-32]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KStep 10 - TTA Loss: 1.4674[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.6822e-34]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.1368e-36]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.5876e-37]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 9.0703e-39]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 7.3911e-40]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 8.2623e-41]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.2233e-41]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.3234e-42]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 5.5071e-43]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.5835e-43]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KStep 20 - TTA Loss: 1.4205[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 5.4651e-44]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.1019e-44]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 9.8091e-45]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 5.6052e-45]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.8026e-45]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.4013e-45]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.4013e-45]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.4013e-45]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KStep 30 - TTA Loss: 1.3781[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KStep 40 - TTA Loss: 1.3981[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[0.6054, 0.3946]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 0.0000e+00],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
        [1.0000e+00, 1.5533e-19],
        [1.0000e+00, 1.7166e-18],
        [1.0000e+00, 3.4692e-24],
[2K        [1.0000e+00, 1.0089e-12]], device='cuda:0')mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:10 â€¢ 0:05:32[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[0.5603, 0.4397],m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
        [0.5553, 0.4447],
        [0.5554, 0.4446],
        [0.5581, 0.4419],
[2K        [0.5558, 0.4442]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KClass label: SoccerPenalty[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.6360, 0.3640]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KStep 0 - TTA Loss: 0.9930[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.6360, 0.3640]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.6357, 0.3643]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.6341, 0.3659]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.6293, 0.3707]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.6186, 0.3814]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.5974, 0.4026]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.5588, 0.4412]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.4990, 0.5010]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.4344, 0.5656]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.3736, 0.6264]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KStep 10 - TTA Loss: 1.0585[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.3227, 0.6773]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.2823, 0.7177]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.2432, 0.7568]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.2070, 0.7930]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.1734, 0.8266]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.1434, 0.8566]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.1187, 0.8813]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.0976, 0.9024]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.0795, 0.9205]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.0644, 0.9356]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KStep 20 - TTA Loss: 0.6549[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.0521, 0.9479]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.0422, 0.9578]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.0345, 0.9655]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.0284, 0.9716]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.0235, 0.9765]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.0197, 0.9803]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.0167, 0.9833]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.0143, 0.9857]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.0124, 0.9876]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.0110, 0.9890]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KStep 30 - TTA Loss: 0.6152[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.0099, 0.9901]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.0089, 0.9911]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.0082, 0.9918]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.0076, 0.9924]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.0071, 0.9929]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.0068, 0.9932]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.0065, 0.9935]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.0062, 0.9938]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.0061, 0.9939]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.0060, 0.9940]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KStep 40 - TTA Loss: 0.6445[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.0059, 0.9941]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.0058, 0.9942]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.0058, 0.9942]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.0057, 0.9943]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.0057, 0.9943]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.0057, 0.9943]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.0057, 0.9943]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.0057, 0.9943]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.0058, 0.9942]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.5525, 0.4475]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.0892, 0.9108],8;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
        [0.0778, 0.9222],
        [0.1051, 0.8949],
        [0.0058, 0.9942],
[2K        [0.2604, 0.7396]], device='cuda:0')38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:13 â€¢ 0:05:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.5603, 0.4397],mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
        [0.5553, 0.4447],
        [0.5554, 0.4446],
        [0.5581, 0.4419],
[2K        [0.5558, 0.4442]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KClass label: BaseballPitch[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.6420, 0.3580]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KStep 0 - TTA Loss: 0.6102[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.9444, 0.0556]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.8591, 0.1409]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.3039e-08]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 6.3084e-15]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.5452e-20]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.6226e-25]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 6.2846e-30]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 7.8380e-34]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.7843e-37]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.5217e-40]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KStep 10 - TTA Loss: 1.5551[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 5.2689e-43]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.8026e-45]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KStep 20 - TTA Loss: 1.5560[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KStep 30 - TTA Loss: 1.5098[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KStep 40 - TTA Loss: 1.5213[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.6102, 0.3898]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 0.0000e+00],;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
        [1.0000e+00, 8.5806e-28],
        [1.0000e+00, 5.5325e-25],
        [1.0000e+00, 6.9982e-31],
[2K        [1.0000e+00, 8.2098e-18]], device='cuda:0')5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:14 â€¢ 0:05:50[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.5604, 0.4396],mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
        [0.5553, 0.4447],
        [0.5555, 0.4445],
        [0.5583, 0.4417],
[2K        [0.5559, 0.4441]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KClass label: BaseballPitch[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.6450, 0.3550]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KStep 0 - TTA Loss: 0.6325[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.6450, 0.3550]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.6455, 0.3545]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.6488, 0.3512]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.6580, 0.3420]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.6740, 0.3260]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.6979, 0.3021]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.7254, 0.2746]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.7584, 0.2416]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.7912, 0.2088]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.8246, 0.1754]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KStep 10 - TTA Loss: 0.8157[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.8731, 0.1269]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.9227, 0.0773]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.9779, 0.0221]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.9977, 0.0023]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.9963, 0.0037]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.8461, 0.1539]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.1563, 0.8437]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.0059, 0.9941]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1.8956e-04, 9.9981e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[6.1896e-06, 9.9999e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KStep 20 - TTA Loss: 1.0165[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[2.1675e-07, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[8.1731e-09, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[3.3661e-10, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1.5624e-11, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[8.1521e-13, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[4.8568e-14, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[3.3063e-15, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[2.5995e-16, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[2.3600e-17, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[2.4866e-18, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KStep 30 - TTA Loss: 0.8675[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[3.0173e-19, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[4.2241e-20, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[6.7763e-21, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1.2434e-21, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[2.5834e-22, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[6.1024e-23, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1.6377e-23, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[4.9178e-24, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1.6406e-24, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[6.0818e-25, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KStep 40 - TTA Loss: 0.8811[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[2.4610e-25, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1.0812e-25, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[5.1373e-26, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[2.6345e-26, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1.4480e-26, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[8.4041e-27, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[5.1786e-27, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[3.3682e-27, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[2.2995e-27, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.5236, 0.4764]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[1.6160e-27, 1.0000e+00],;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
        [2.2347e-13, 1.0000e+00],
        [1.3727e-11, 1.0000e+00],
        [4.6258e-14, 1.0000e+00],
[2K        [1.4369e-08, 1.0000e+00]], device='cuda:0')5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 9/203 [2m0:00:16 â€¢ 0:05:48[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.5604, 0.4396],mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
        [0.5553, 0.4447],
        [0.5555, 0.4445],
        [0.5583, 0.4417],
[2K        [0.5559, 0.4441]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KClass label: SoccerPenalty[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.6352, 0.3648]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KStep 0 - TTA Loss: 0.8542[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.2209, 0.7791]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.2091, 0.7909]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.2291, 0.7709]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.2470, 0.7530]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.2570, 0.7430]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.2606, 0.7394]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.2589, 0.7411]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.2446, 0.7554]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.2287, 0.7713]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.2101, 0.7899]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KStep 10 - TTA Loss: 0.7075[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.1895, 0.8105]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.1691, 0.8309]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.1492, 0.8508]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.1317, 0.8683]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.1172, 0.8828]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.1047, 0.8953]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.0936, 0.9064]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.0844, 0.9156]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.0767, 0.9233]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.0694, 0.9306]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KStep 20 - TTA Loss: 0.6278[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.0634, 0.9366]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.0585, 0.9415]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.0540, 0.9460]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.0503, 0.9497]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.0470, 0.9530]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.0444, 0.9556]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.0422, 0.9578]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.0405, 0.9595]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.0389, 0.9611]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.0377, 0.9623]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KStep 30 - TTA Loss: 0.7292[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.0367, 0.9633]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.0358, 0.9642]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.0350, 0.9650]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.0344, 0.9656]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.0339, 0.9661]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.0335, 0.9665]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.0332, 0.9668]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.0329, 0.9671]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.0327, 0.9673]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.0325, 0.9675]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KStep 40 - TTA Loss: 0.7203[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.0323, 0.9677]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.0322, 0.9678]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.0321, 0.9679]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.0321, 0.9679]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.0320, 0.9680]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.0320, 0.9680]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.0319, 0.9681]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.0319, 0.9681]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.0319, 0.9681]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.5538, 0.4462]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.0245, 0.9755],0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
        [0.0838, 0.9162],
        [0.1033, 0.8967],
        [0.0319, 0.9681],
[2K        [0.2289, 0.7711]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 10/203 [2m0:00:18 â€¢ 0:05:38[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.5604, 0.4396],0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m   
        [0.5553, 0.4447],
        [0.5555, 0.4445],
        [0.5583, 0.4417],
[2K        [0.5559, 0.4441]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KClass label: PoleVault[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.6179, 0.3821]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KStep 0 - TTA Loss: 0.9239m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.6179, 0.3821]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.6178, 0.3822]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.6175, 0.3825]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.6163, 0.3837]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.6138, 0.3862]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.6057, 0.3943]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5881, 0.4119]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5605, 0.4395]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5191, 0.4809]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.4737, 0.5263]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KStep 10 - TTA Loss: 1.4418[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.4508, 0.5492]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.4324, 0.5676]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.4255, 0.5745]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.4242, 0.5758]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.4383, 0.5617]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.4606, 0.5394]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.4797, 0.5203]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5005, 0.4995]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5194, 0.4806]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5347, 0.4653]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KStep 20 - TTA Loss: 0.9475[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5439, 0.4561]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5469, 0.4531]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5464, 0.4536]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5379, 0.4621]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5202, 0.4798]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.4943, 0.5057]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.4654, 0.5346]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.4350, 0.5650]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.4056, 0.5944]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.3741, 0.6259]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KStep 30 - TTA Loss: 1.0743[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.3443, 0.6557]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.3163, 0.6837]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.2883, 0.7117]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.2608, 0.7392]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.2358, 0.7642]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.2128, 0.7872]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.1907, 0.8093]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.1673, 0.8327]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.1450, 0.8550]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.1250, 0.8750]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KStep 40 - TTA Loss: 1.3058[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.1073, 0.8927]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.0897, 0.9103]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.0750, 0.9250]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.0637, 0.9363]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.0545, 0.9455]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.0468, 0.9532]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.0405, 0.9595]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.0354, 0.9646]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.0313, 0.9687]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5513, 0.4487]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.1441, 0.8559],38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
        [0.0946, 0.9054],
        [0.0282, 0.9718],
        [0.1305, 0.8695],
[2K        [0.1318, 0.8682]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 11/203 [2m0:00:19 â€¢ 0:05:30[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5604, 0.4396],0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 12/203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
        [0.5553, 0.4447],
        [0.5555, 0.4445],
        [0.5583, 0.4417],
[2K        [0.5559, 0.4441]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 12/203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KClass label: ThrowDiscus0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 12/203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[0.6172, 0.3828]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KStep 0 - TTA Loss: 1.0999m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 12/203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[0.9452, 0.0548]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[9.9959e-01, 4.1050e-04]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[0.0101, 0.9899]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1.5340e-07, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[7.6857e-12, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1.2153e-15, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[5.4845e-19, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[6.3217e-22, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1.7080e-24, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[9.7559e-27, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KStep 10 - TTA Loss: 0.7393[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 12/203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1.0705e-28, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[2.0855e-30, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[6.7326e-32, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[3.3911e-33, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[2.5259e-34, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[2.6511e-35, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[3.7566e-36, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[6.9321e-37, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1.6107e-37, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[4.5820e-38, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KStep 20 - TTA Loss: 0.7259[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 12/203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1.5562e-38, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[6.1628e-39, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[2.7935e-39, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1.4234e-39, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[8.0277e-40, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[4.9480e-40, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[3.2902e-40, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[2.3360e-40, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1.7542e-40, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1.3821e-40, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KStep 30 - TTA Loss: 0.7451[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 12/203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1.1347e-40, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[9.6446e-41, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[8.4445e-41, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[7.5813e-41, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[6.9901e-41, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[6.5532e-41, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[6.2271e-41, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[5.9845e-41, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[5.8045e-41, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[5.6712e-41, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KStep 40 - TTA Loss: 0.7355[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 12/203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[5.5737e-41, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[5.5036e-41, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[5.4546e-41, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[5.4209e-41, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[5.3992e-41, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[5.3855e-41, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[5.3778e-41, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[5.3740e-41, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[5.3724e-41, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[0.5136, 0.4864]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 12/203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[1.5971e-11, 1.0000e+00],mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 12/203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
        [9.7289e-22, 1.0000e+00],
        [4.6290e-17, 1.0000e+00],
        [6.9291e-11, 1.0000e+00],
[2K        [5.2519e-41, 1.0000e+00]], device='cuda:0')7mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 12/203 [2m0:00:20 â€¢ 0:05:23[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[0.5604, 0.4396],4mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
        [0.5553, 0.4447],
        [0.5555, 0.4445],
        [0.5583, 0.4417],
[2K        [0.5559, 0.4441]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KClass label: BaseballPitch[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.6458, 0.3542]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KStep 0 - TTA Loss: 0.6458m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.6458, 0.3542]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.6478, 0.3522]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.6587, 0.3413]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.6885, 0.3115]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.7446, 0.2554]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.8261, 0.1739]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.9091, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.9591, 0.0409]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.9828, 0.0172]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.9954, 0.0046]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KStep 10 - TTA Loss: 1.4679[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[9.9956e-01, 4.4480e-04]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[9.9997e-01, 2.5195e-05]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.1491e-06]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.4915e-08]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.5605e-09]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.9836e-11]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.5099e-12]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.4688e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.3271e-15]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.0513e-17]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KStep 20 - TTA Loss: 1.5814[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.2987e-18]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.4547e-20]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.6614e-21]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 6.8296e-23]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.1290e-24]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.6120e-25]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 9.4039e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 6.2424e-28]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.7309e-29]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.1005e-30]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KStep 30 - TTA Loss: 1.4751[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.0663e-31]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.6103e-32]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 5.9652e-33]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 8.7850e-34]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.4677e-34]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.7706e-35]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 5.8834e-36]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.3987e-36]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.7038e-37]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.0867e-37]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KStep 40 - TTA Loss: 1.4421[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.5141e-38]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.2456e-38]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.8132e-39]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.0170e-39]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 9.1175e-40]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.4234e-40]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.2919e-40]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.2621e-40]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 7.3540e-41]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.6091, 0.3909]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.4641e-41],5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
        [1.0000e+00, 7.6115e-21],
        [1.0000e+00, 5.5865e-18],
        [1.0000e+00, 3.2263e-22],
[2K        [1.0000e+00, 2.1465e-13]], device='cuda:0');5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 13/203 [2m0:00:22 â€¢ 0:05:16[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.5605, 0.4395],4mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
        [0.5554, 0.4446],
        [0.5555, 0.4445],
        [0.5583, 0.4417],
[2K        [0.5560, 0.4440]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KClass label: BaseballPitch[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.6414, 0.3586]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KStep 0 - TTA Loss: 1.0326m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.7404, 0.2596]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.5503, 0.4497]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.4877e-07]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.1921e-13]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.9613e-18]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 8.3173e-23]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.3520e-26]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 7.0233e-30]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 9.9738e-33]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.3860e-35]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KStep 10 - TTA Loss: 1.9008[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.4485e-37]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.4145e-39]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 8.4270e-41]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.4178e-42]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.1300e-43]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.9618e-44]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.8026e-45]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KStep 20 - TTA Loss: 1.9442[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KStep 30 - TTA Loss: 1.8586[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KStep 40 - TTA Loss: 1.9692[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.5762, 0.4238]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 0.0000e+00],5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
        [1.0000e+00, 2.6460e-27],
        [1.0000e+00, 4.6785e-23],
        [1.0000e+00, 4.3730e-28],
[2K        [1.0000e+00, 1.0695e-16]], device='cuda:0');5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:23 â€¢ 0:05:11[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.5605, 0.4395],4mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
        [0.5554, 0.4446],
        [0.5556, 0.4444],
        [0.5584, 0.4416],
[2K        [0.5560, 0.4440]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KClass label: PoleVault[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.6157, 0.3843]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KStep 0 - TTA Loss: 1.1514m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.6157, 0.3843]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.6159, 0.3841]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.6170, 0.3830]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.6200, 0.3800]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.6266, 0.3734]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.6388, 0.3612]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.6553, 0.3447]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.6756, 0.3244]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.6908, 0.3092]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.7003, 0.2997]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KStep 10 - TTA Loss: 1.2871[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.7270, 0.2730]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.8142, 0.1858]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.9477, 0.0523]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.9987, 0.0013]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.9987, 0.0013]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.9382, 0.0618]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.2817, 0.7183]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0102, 0.9898]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[2.8650e-04, 9.9971e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[8.7132e-06, 9.9999e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KStep 20 - TTA Loss: 1.4173[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[2.9365e-07, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[1.1084e-08, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[4.7660e-10, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[2.4242e-11, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[1.4193e-12, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[9.9632e-14, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[8.1843e-15, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[8.0575e-16, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[9.3400e-17, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[1.2539e-17, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KStep 30 - TTA Loss: 1.3710[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[1.9399e-18, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[3.4947e-19, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[7.1665e-20, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[1.6096e-20, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[4.0153e-21, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[1.0772e-21, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[3.1567e-22, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[1.0031e-22, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[3.4054e-23, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[1.2440e-23, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KStep 40 - TTA Loss: 1.4744[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[4.8884e-24, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[2.0883e-24, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[9.6829e-25, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[4.8147e-25, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[2.5157e-25, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[1.4015e-25, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[8.2215e-26, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[5.1351e-26, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[3.4211e-26, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.5157, 0.4843]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[1.2756e-14, 1.0000e+00],5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
        [1.2717e-17, 1.0000e+00],
        [2.2868e-26, 1.0000e+00],
        [4.9970e-17, 1.0000e+00],
[2K        [6.9615e-16, 1.0000e+00]], device='cuda:0');5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 15/203 [2m0:00:24 â€¢ 0:05:05[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.5605, 0.4395],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m   
        [0.5554, 0.4446],
        [0.5555, 0.4445],
        [0.5583, 0.4417],
[2K        [0.5559, 0.4441]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KClass label: PoleVaultâ”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.6145, 0.3855]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KStep 0 - TTA Loss: 0.80710m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.6041, 0.3959]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.4964, 0.5036]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.6490, 0.3510]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.7925, 0.2075]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.7800, 0.2200]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.6687, 0.3313]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.5131, 0.4869]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.3756, 0.6244]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.2758, 0.7242]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.2086, 0.7914]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KStep 10 - TTA Loss: 1.3154m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.1636, 0.8364]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.1307, 0.8693]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.1062, 0.8938]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0887, 0.9113]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0759, 0.9241]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0660, 0.9340]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0587, 0.9413]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0533, 0.9467]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0490, 0.9510]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0456, 0.9544]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KStep 20 - TTA Loss: 1.1974m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0430, 0.9570]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0409, 0.9591]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0391, 0.9609]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0378, 0.9622]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0368, 0.9632]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0360, 0.9640]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0354, 0.9646]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0349, 0.9651]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0346, 0.9654]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0344, 0.9656]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KStep 30 - TTA Loss: 1.2664m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0344, 0.9656]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0343, 0.9657]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0342, 0.9658]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0341, 0.9659]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0340, 0.9660]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0338, 0.9662]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0337, 0.9663]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0337, 0.9663]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0336, 0.9664]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0336, 0.9664]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KStep 40 - TTA Loss: 1.2560m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0335, 0.9665]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0335, 0.9665]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0335, 0.9665]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0334, 0.9666]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0334, 0.9666]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0334, 0.9666]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0334, 0.9666]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0334, 0.9666]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0334, 0.9666]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.5499, 0.4501]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.1662, 0.8338],[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
        [0.0813, 0.9187],
        [0.0334, 0.9666],
        [0.1300, 0.8700],
[2K        [0.1373, 0.8627]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 16/203 [2m0:00:26 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.5605, 0.4395],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 17/203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
        [0.5554, 0.4446],
        [0.5555, 0.4445],
        [0.5583, 0.4417],
[2K        [0.5559, 0.4441]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 17/203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KClass label: BaseballPitchm[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 17/203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.6454, 0.3546]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KStep 0 - TTA Loss: 1.04290m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 17/203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.6454, 0.3546]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.6453, 0.3547]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.6447, 0.3553]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.6433, 0.3567]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.6403, 0.3597]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.6356, 0.3644]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.6285, 0.3715]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.6205, 0.3795]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.6171, 0.3829]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.6371, 0.3629]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KStep 10 - TTA Loss: 1.5630m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 17/203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.7080, 0.2920]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.8167, 0.1833]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.8863, 0.1137]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.8896, 0.1104]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.8330, 0.1670]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.8799, 0.1201]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.9730, 0.0270]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.9436, 0.0564]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.7457, 0.2543]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.3234, 0.6766]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KStep 20 - TTA Loss: 1.4872m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 17/203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0745, 0.9255]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0141, 0.9859]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0026, 0.9974]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[4.9820e-04, 9.9950e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[9.7365e-05, 9.9990e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[2.0195e-05, 9.9998e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[4.5229e-06, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[1.1062e-06, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[2.9417e-07, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[8.5254e-08, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KStep 30 - TTA Loss: 1.5144m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 17/203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[2.7103e-08, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[9.2435e-09, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[3.3654e-09, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[1.2962e-09, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[5.2099e-10, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[2.2046e-10, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[9.9064e-11, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[4.7080e-11, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[2.3976e-11, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[1.3020e-11, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KStep 40 - TTA Loss: 1.5183m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 17/203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[7.4864e-12, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[4.5305e-12, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[2.8648e-12, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[1.8907e-12, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[1.2952e-12, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[9.1457e-13, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[6.6440e-13, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[4.9198e-13, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[3.6992e-13, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.5391, 0.4609]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 17/203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[2.8435e-13, 1.0000e+00],7mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 17/203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
        [2.9728e-07, 1.0000e+00],
        [1.0148e-05, 9.9999e-01],
        [2.2536e-07, 1.0000e+00],
[2K        [1.7884e-04, 9.9982e-01]], device='cuda:0')37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 17/203 [2m0:00:28 â€¢ 0:05:00[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.5604, 0.4396],24mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
        [0.5553, 0.4447],
        [0.5555, 0.4445],
        [0.5583, 0.4417],
[2K        [0.5559, 0.4441]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KClass label: ThrowDiscus[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.6141, 0.3859]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KStep 0 - TTA Loss: 0.75790m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.4167, 0.5833]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.3588, 0.6412]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.3974, 0.6026]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.4459, 0.5541]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.4573, 0.5427]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.4219, 0.5781]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.3636, 0.6364]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.2987, 0.7013]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.2414, 0.7586]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.1907, 0.8093]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KStep 10 - TTA Loss: 0.5794m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.1520, 0.8480]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.1216, 0.8784]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0991, 0.9009]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0826, 0.9174]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0705, 0.9295]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0613, 0.9387]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0544, 0.9456]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0487, 0.9513]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0437, 0.9563]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0398, 0.9602]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KStep 20 - TTA Loss: 0.6185m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0367, 0.9633]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0341, 0.9659]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0319, 0.9681]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0301, 0.9699]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0286, 0.9714]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0273, 0.9727]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0262, 0.9738]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0253, 0.9747]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0245, 0.9755]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0239, 0.9761]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KStep 30 - TTA Loss: 0.6158m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0234, 0.9766]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0230, 0.9770]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0226, 0.9774]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0223, 0.9777]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0220, 0.9780]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0218, 0.9782]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0216, 0.9784]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0215, 0.9785]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0213, 0.9787]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0212, 0.9788]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KStep 40 - TTA Loss: 0.6071m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0212, 0.9788]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0211, 0.9789]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0210, 0.9790]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0210, 0.9790]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0210, 0.9790]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0210, 0.9790]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0209, 0.9791]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0209, 0.9791]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0209, 0.9791]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.5505, 0.4495]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0605, 0.9395],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
        [0.0475, 0.9525],
        [0.1320, 0.8680],
        [0.1604, 0.8396],
[2K        [0.0209, 0.9791]], device='cuda:0')â•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 18/203 [2m0:00:29 â€¢ 0:04:57[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.5604, 0.4396],24mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
        [0.5553, 0.4447],
        [0.5555, 0.4445],
        [0.5583, 0.4417],
[2K        [0.5559, 0.4441]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KClass label: SoccerPenaltym[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6334, 0.3666]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KStep 0 - TTA Loss: 0.61360m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6334, 0.3666]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6336, 0.3664]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6346, 0.3654]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6374, 0.3626]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6426, 0.3574]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6502, 0.3498]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6579, 0.3421]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6641, 0.3359]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6746, 0.3254]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.7493, 0.2507]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KStep 10 - TTA Loss: 0.6728m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.8379, 0.1621]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.9231, 0.0769]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.9761, 0.0239]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.9940, 0.0060]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.9883, 0.0117]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.9973, 0.0027]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.9870, 0.0130]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.8891, 0.1109]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.5688, 0.4312]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.1983, 0.8017]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KStep 20 - TTA Loss: 0.8194m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.0459, 0.9541]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.0096, 0.9904]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.0020, 0.9980]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[4.4757e-04, 9.9955e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.0339e-04, 9.9990e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[2.5318e-05, 9.9997e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[6.5682e-06, 9.9999e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.8107e-06, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[5.3436e-07, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.6867e-07, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KStep 30 - TTA Loss: 0.6083m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[5.6926e-08, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[2.0462e-08, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[7.8356e-09, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[3.1928e-09, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.3572e-09, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[6.1256e-10, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[2.9303e-10, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.4823e-10, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[7.9103e-11, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[4.4415e-11, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KStep 40 - TTA Loss: 0.5297m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[2.6172e-11, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.6237e-11, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.0526e-11, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[7.0987e-12, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[4.9655e-12, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[3.5945e-12, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[2.6923e-12, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[2.0828e-12, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.6567e-12, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.5364, 0.4636]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[4.5120e-06, 1.0000e+00],;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
        [6.5294e-07, 1.0000e+00],
        [1.7123e-05, 9.9998e-01],
        [1.3486e-12, 1.0000e+00],
[2K        [8.6106e-04, 9.9914e-01]], device='cuda:0')8;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 19/203 [2m0:00:30 â€¢ 0:04:53[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.5604, 0.4396],24mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
        [0.5553, 0.4447],
        [0.5555, 0.4445],
        [0.5583, 0.4417],
[2K        [0.5559, 0.4441]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KClass label: PoleVaultâ”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6090, 0.3910]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KStep 0 - TTA Loss: 0.78550m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.8624, 0.1376]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.9723, 0.0277]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[9.9999e-01, 7.3735e-06]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.5454e-09]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 5.9984e-12]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.5885e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 7.9422e-17]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 7.0805e-19]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.0670e-20]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.5856e-22]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KStep 10 - TTA Loss: 1.4160m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 9.6223e-24]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 5.2696e-25]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.0833e-26]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.3183e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 6.0309e-28]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.0793e-28]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.4084e-29]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 6.5364e-30]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.1098e-30]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 7.9360e-31]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KStep 20 - TTA Loss: 1.4104m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.4162e-31]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.6555e-31]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 8.9003e-32]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 5.2393e-32]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.3380e-32]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.2779e-32]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.6498e-32]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.2579e-32]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.0025e-32]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 8.2983e-33]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KStep 30 - TTA Loss: 1.4705m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 7.0957e-33]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 6.2371e-33]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 5.6122e-33]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 5.1507e-33]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.8062e-33]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.5473e-33]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.3522e-33]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.2051e-33]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.0947e-33]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.0125e-33]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KStep 40 - TTA Loss: 1.3880m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.9520e-33]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.9081e-33]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.8770e-33]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.8557e-33]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.8415e-33]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.8327e-33]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.8276e-33]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.8250e-33]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.8239e-33]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6071, 0.3929]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.3980e-18],;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
        [1.0000e+00, 1.4607e-22],
        [1.0000e+00, 3.6202e-33],
        [1.0000e+00, 9.6726e-21],
[2K        [1.0000e+00, 9.3477e-21]], device='cuda:0')8;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 20/203 [2m0:00:32 â€¢ 0:04:51[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.5605, 0.4395],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m   
        [0.5555, 0.4445],
        [0.5558, 0.4442],
        [0.5584, 0.4416],
[2K        [0.5561, 0.4439]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KClass label: BaseballPitch0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.6427, 0.3573]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KStep 0 - TTA Loss: 0.8214[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.6427, 0.3573]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.6424, 0.3576]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.6405, 0.3595]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.6373, 0.3627]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.6287, 0.3713]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.6129, 0.3871]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.5835, 0.4165]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.5419, 0.4581]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.4911, 0.5089]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.4404, 0.5596]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KStep 10 - TTA Loss: 1.07180m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.3954, 0.6046]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.3585, 0.6415]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.3259, 0.6741]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.2981, 0.7019]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.2730, 0.7270]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.2483, 0.7517]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.2247, 0.7753]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.2027, 0.7973]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.1822, 0.8178]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.1631, 0.8369]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KStep 20 - TTA Loss: 0.72520m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.1465, 0.8535]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.1314, 0.8686]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.1190, 0.8810]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.1087, 0.8913]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0997, 0.9003]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0919, 0.9081]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0859, 0.9141]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0812, 0.9188]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0769, 0.9231]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0732, 0.9268]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KStep 30 - TTA Loss: 0.72150m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0703, 0.9297]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0679, 0.9321]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0658, 0.9342]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0638, 0.9362]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0622, 0.9378]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0609, 0.9391]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0598, 0.9402]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0589, 0.9411]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0580, 0.9420]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0571, 0.9429]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KStep 40 - TTA Loss: 0.76270m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0563, 0.9437]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0555, 0.9445]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0547, 0.9453]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0540, 0.9460]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0533, 0.9467]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0524, 0.9476]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0515, 0.9485]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0505, 0.9495]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0494, 0.9506]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.5553, 0.4447]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.0482, 0.9518],[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
        [0.1661, 0.8339],
        [0.2911, 0.7089],
        [0.2371, 0.7629],
[2K        [0.3602, 0.6398]], device='cuda:0')m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 21/203 [2m0:00:34 â€¢ 0:04:56[0m [2;4m0.62it/s[0m  
[2KAttention weights: tensor([[0.5605, 0.4395],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 22/203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
        [0.5555, 0.4445],
        [0.5558, 0.4442],
        [0.5584, 0.4416],
[2K        [0.5561, 0.4439]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 22/203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KClass label: ThrowDiscus[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 22/203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6185, 0.3815]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KStep 0 - TTA Loss: 1.2312[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 22/203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6401, 0.3599]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5279, 0.4721]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.9986, 0.0014]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 9.8240e-07]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.7776e-09]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 7.3570e-12]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 6.2417e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 9.8766e-16]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.6868e-17]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.1713e-18]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KStep 10 - TTA Loss: 2.07590m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 22/203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 7.6986e-20]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 7.2380e-21]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 9.2987e-22]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.5687e-22]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.3571e-23]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 8.8421e-24]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.7917e-24]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.0326e-24]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.3843e-25]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.0996e-25]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KStep 20 - TTA Loss: 2.06730m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 22/203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.1166e-25]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 6.5052e-26]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.1023e-26]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.7710e-26]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.9866e-26]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.4993e-26]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.1829e-26]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 9.6951e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 8.2107e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 7.1509e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KStep 30 - TTA Loss: 1.94640m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 22/203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 6.3782e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 5.8055e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 5.3755e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 5.0495e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.8007e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.6105e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.4652e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.3545e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.2706e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.2078e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KStep 40 - TTA Loss: 2.01510m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 22/203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.1614e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.1276e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.1035e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.0870e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.0760e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.0691e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.0652e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.0632e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.0623e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5624, 0.4376]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 22/203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.4871e-09],37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 22/203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
        [1.0000e+00, 7.5519e-17],
        [1.0000e+00, 5.8807e-13],
        [1.0000e+00, 9.3716e-10],
[2K        [1.0000e+00, 3.9993e-27]], device='cuda:0')237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 22/203 [2m0:00:35 â€¢ 0:04:42[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5600, 0.4400],224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
        [0.5547, 0.4453],
        [0.5552, 0.4448],
        [0.5580, 0.4420],
[2K        [0.5550, 0.4450]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KClass label: SoccerPenalty0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.6343, 0.3657]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KStep 0 - TTA Loss: 1.0841[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.6343, 0.3657]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.6343, 0.3657]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.6342, 0.3658]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.6339, 0.3661]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.6335, 0.3665]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.6304, 0.3696]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.6222, 0.3778]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.6108, 0.3892]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.6092, 0.3908]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.6340, 0.3660]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KStep 10 - TTA Loss: 1.30430m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.6930, 0.3070]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.7823, 0.2177]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.8689, 0.1311]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.9301, 0.0699]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.9623, 0.0377]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.9634, 0.0366]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.9264, 0.0736]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.8183, 0.1817]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.5943, 0.4057]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.3229, 0.6771]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KStep 20 - TTA Loss: 0.65660m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.1378, 0.8622]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0529, 0.9471]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0201, 0.9799]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0079, 0.9921]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0032, 0.9968]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0014, 0.9986]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[6.3863e-04, 9.9936e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[3.0590e-04, 9.9969e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.5398e-04, 9.9985e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[8.1027e-05, 9.9992e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KStep 30 - TTA Loss: 0.84130m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[4.4241e-05, 9.9996e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[2.5085e-05, 9.9997e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.4682e-05, 9.9999e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[8.6372e-06, 9.9999e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[5.2461e-06, 9.9999e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[3.2716e-06, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[2.0901e-06, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.3649e-06, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[9.1132e-07, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[6.0387e-07, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KStep 40 - TTA Loss: 0.91540m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[4.0875e-07, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[2.8426e-07, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[2.0242e-07, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.4774e-07, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.1145e-07, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[8.5550e-08, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[6.7488e-08, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[5.6087e-08, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[4.7852e-08, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.5460, 0.4540]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[3.5092e-04, 9.9965e-01],8;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
        [1.0877e-04, 9.9989e-01],
        [7.4601e-04, 9.9925e-01],
        [4.1914e-08, 1.0000e+00],
[2K        [1.4624e-02, 9.8538e-01]], device='cuda:0')38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 23/203 [2m0:00:37 â€¢ 0:04:39[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.5601, 0.4399],224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
        [0.5547, 0.4453],
        [0.5553, 0.4447],
        [0.5581, 0.4419],
[2K        [0.5550, 0.4450]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KClass label: SoccerPenalty0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.6323, 0.3677]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KStep 0 - TTA Loss: 1.4212[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.6017, 0.3983]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.5197, 0.4803]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.3782, 0.6218]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.2924, 0.7076]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.2267, 0.7733]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.1789, 0.8211]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.1447, 0.8553]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.1194, 0.8806]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.1007, 0.8993]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0867, 0.9133]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KStep 10 - TTA Loss: 0.93460m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0760, 0.9240]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0677, 0.9323]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0612, 0.9388]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0561, 0.9439]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0520, 0.9480]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0486, 0.9514]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0459, 0.9541]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0436, 0.9564]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0417, 0.9583]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0401, 0.9599]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KStep 20 - TTA Loss: 0.92550m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0388, 0.9612]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0377, 0.9623]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0368, 0.9632]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0360, 0.9640]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0353, 0.9647]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0348, 0.9652]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0344, 0.9656]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0340, 0.9660]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0337, 0.9663]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0335, 0.9665]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KStep 30 - TTA Loss: 0.92070m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0333, 0.9667]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0331, 0.9669]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0330, 0.9670]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0328, 0.9672]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0327, 0.9673]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0327, 0.9673]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0326, 0.9674]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0326, 0.9674]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0325, 0.9675]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0325, 0.9675]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KStep 40 - TTA Loss: 0.90690m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0325, 0.9675]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0325, 0.9675]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0325, 0.9675]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0325, 0.9675]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0325, 0.9675]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0324, 0.9676]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0324, 0.9676]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0324, 0.9676]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0324, 0.9676]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.5544, 0.4456]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.1408, 0.8592],â•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
        [0.1528, 0.8472],
        [0.2702, 0.7298],
        [0.0324, 0.9676],
[2K        [0.5560, 0.4440]], device='cuda:0')mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 24/203 [2m0:00:38 â€¢ 0:04:37[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.5601, 0.4399],224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
        [0.5547, 0.4453],
        [0.5553, 0.4447],
        [0.5581, 0.4419],
[2K        [0.5550, 0.4450]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KClass label: SoccerPenalty0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6380, 0.3620]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KStep 0 - TTA Loss: 1.2065[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6380, 0.3620]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6379, 0.3621]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6376, 0.3624]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6366, 0.3634]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6346, 0.3654]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6293, 0.3707]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6185, 0.3815]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5960, 0.4040]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5549, 0.4451]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.4870, 0.5130]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KStep 10 - TTA Loss: 1.19780m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.4129, 0.5871]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.3664, 0.6336]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.3513, 0.6487]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.3546, 0.6454]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.3675, 0.6325]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.3795, 0.6205]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.3839, 0.6161]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.3809, 0.6191]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.3710, 0.6290]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.3533, 0.6467]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KStep 20 - TTA Loss: 0.94100m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.3292, 0.6708]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.2989, 0.7011]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.2654, 0.7346]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.2318, 0.7682]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.1988, 0.8012]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.1692, 0.8308]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.1433, 0.8567]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.1211, 0.8789]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.1026, 0.8974]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.0874, 0.9126]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KStep 30 - TTA Loss: 1.05110m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.0751, 0.9249]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.0647, 0.9353]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.0562, 0.9438]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.0489, 0.9511]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.0426, 0.9574]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.0374, 0.9626]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.0330, 0.9670]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.0291, 0.9709]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.0258, 0.9742]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.0229, 0.9771]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KStep 40 - TTA Loss: 1.11570m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.0206, 0.9794]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.0186, 0.9814]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.0170, 0.9830]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.0157, 0.9843]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.0146, 0.9854]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.0136, 0.9864]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.0128, 0.9872]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.0122, 0.9878]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.0117, 0.9883]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5523, 0.4477]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.0980, 0.9020],â•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
        [0.0670, 0.9330],
        [0.1389, 0.8611],
        [0.0112, 0.9888],
[2K        [0.2579, 0.7421]], device='cuda:0')mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 25/203 [2m0:00:40 â€¢ 0:04:37[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5601, 0.4399],â•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 26/203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m   
        [0.5547, 0.4453],
        [0.5553, 0.4447],
        [0.5581, 0.4419],
[2K        [0.5550, 0.4450]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 26/203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KClass label: SoccerPenalty[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 26/203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[0.6379, 0.3621]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KStep 0 - TTA Loss: 0.7180[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 26/203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[0.7841, 0.2159]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[0.9988, 0.0012]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[9.9994e-01, 6.3390e-05]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.4697e-06]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.1515e-07]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 5.0212e-08]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 7.7233e-09]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.4741e-09]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.4135e-10]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 9.3958e-11]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KStep 10 - TTA Loss: 1.6421[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 26/203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.0179e-11]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.1125e-11]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.6358e-12]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.1544e-12]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.1030e-12]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 6.1540e-13]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.7044e-13]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.3848e-13]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.6289e-13]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.1721e-13]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KStep 20 - TTA Loss: 1.6147[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 26/203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 8.8294e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 6.9232e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 5.6222e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.7075e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.0479e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.5621e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.1978e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.9207e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.7075e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.5419e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KStep 30 - TTA Loss: 1.6327[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 26/203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.4124e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.3108e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.2309e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.1679e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.1185e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.0797e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.0496e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.0262e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.0084e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.9948e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KStep 40 - TTA Loss: 1.9492[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 26/203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.9848e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.9774e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.9722e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.9686e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.9662e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.9647e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.9638e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.9633e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.9632e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[0.5757, 0.4243]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 26/203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 9.9698e-08],237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 26/203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
        [1.0000e+00, 3.6372e-08],
        [1.0000e+00, 6.6874e-07],
        [1.0000e+00, 1.9556e-14],
[2K        [9.9986e-01, 1.3511e-04]], device='cuda:0');237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 26/203 [2m0:00:41 â€¢ 0:04:25[0m [2;4m0.67it/s[0m  
[2KAttention weights: tensor([[0.5602, 0.4398],â•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
        [0.5549, 0.4451],
        [0.5554, 0.4446],
        [0.5584, 0.4416],
[2K        [0.5552, 0.4448]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KClass label: SoccerPenalty[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.6318, 0.3682]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KStep 0 - TTA Loss: 1.3163[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.6318, 0.3682]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.6316, 0.3684]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.6306, 0.3694]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.6279, 0.3721]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.6229, 0.3771]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.6125, 0.3875]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.5955, 0.4045]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.5712, 0.4288]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.5403, 0.4597]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.5087, 0.4913]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KStep 10 - TTA Loss: 1.4438[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.4814, 0.5186]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.4656, 0.5344]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.4525, 0.5475]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.4405, 0.5595]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.4265, 0.5735]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.4108, 0.5892]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.3941, 0.6059]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.3764, 0.6236]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.3584, 0.6416]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.3399, 0.6601]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KStep 20 - TTA Loss: 0.7198[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.3219, 0.6781]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.3040, 0.6960]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.2867, 0.7133]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.2703, 0.7297]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.2549, 0.7451]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.2405, 0.7595]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.2269, 0.7731]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.2144, 0.7856]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.2029, 0.7971]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.1922, 0.8078]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KStep 30 - TTA Loss: 0.7303[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.1823, 0.8177]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.1734, 0.8266]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.1653, 0.8347]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.1577, 0.8423]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.1507, 0.8493]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.1443, 0.8557]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.1384, 0.8616]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.1329, 0.8671]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.1278, 0.8722]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.1231, 0.8769]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KStep 40 - TTA Loss: 0.7781[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.1189, 0.8811]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.1147, 0.8853]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.1108, 0.8892]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.1070, 0.8930]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.1035, 0.8965]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.1000, 0.9000]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.0969, 0.9031]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.0939, 0.9061]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.0912, 0.9088]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.5540, 0.4460]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.2649, 0.7351],m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
        [0.1988, 0.8012],
        [0.3026, 0.6974],
        [0.0886, 0.9114],
[2K        [0.4196, 0.5804]], device='cuda:0')0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 27/203 [2m0:00:43 â€¢ 0:04:28[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.5602, 0.4398],;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
        [0.5549, 0.4451],
        [0.5554, 0.4446],
        [0.5584, 0.4416],
[2K        [0.5552, 0.4448]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KClass label: PoleVaultâ”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.6166, 0.3834]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KStep 0 - TTA Loss: 0.9916[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.6989, 0.3011]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.8766, 0.1234]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.9689, 0.0311]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.9724, 0.0276]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.8937, 0.1063]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.7368, 0.2632]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.5135, 0.4865]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.2974, 0.7026]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.1548, 0.8452]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.0779, 0.9221]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KStep 10 - TTA Loss: 1.4015[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.0406, 0.9594]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.0229, 0.9771]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.0136, 0.9864]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.0085, 0.9915]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.0057, 0.9943]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.0041, 0.9959]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.0031, 0.9969]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.0025, 0.9975]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.0021, 0.9979]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.0018, 0.9982]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KStep 20 - TTA Loss: 1.4051[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.0016, 0.9984]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.0013, 0.9987]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.0012, 0.9988]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.0010, 0.9990]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[8.7261e-04, 9.9913e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[7.6511e-04, 9.9923e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[6.8981e-04, 9.9931e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[6.4024e-04, 9.9936e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[6.0547e-04, 9.9939e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[5.8066e-04, 9.9942e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KStep 30 - TTA Loss: 1.4597[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[5.6355e-04, 9.9944e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[5.5277e-04, 9.9945e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[5.4656e-04, 9.9945e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[5.4486e-04, 9.9946e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[5.4362e-04, 9.9946e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[5.4215e-04, 9.9946e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[5.3934e-04, 9.9946e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[5.3788e-04, 9.9946e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[5.3457e-04, 9.9947e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[5.3074e-04, 9.9947e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KStep 40 - TTA Loss: 1.4236[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[5.2730e-04, 9.9947e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[5.2404e-04, 9.9948e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[5.2201e-04, 9.9948e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[5.2106e-04, 9.9948e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[5.2060e-04, 9.9948e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[5.2033e-04, 9.9948e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[5.2018e-04, 9.9948e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[5.2006e-04, 9.9948e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[5.1998e-04, 9.9948e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.5451, 0.4549]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1.9348e-02, 9.8065e-01],38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
        [4.5658e-03, 9.9543e-01],
        [5.1694e-04, 9.9948e-01],
        [1.5902e-02, 9.8410e-01],
[2K        [1.2109e-02, 9.8789e-01]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 28/203 [2m0:00:45 â€¢ 0:04:24[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.5602, 0.4398],;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
        [0.5549, 0.4451],
        [0.5554, 0.4446],
        [0.5584, 0.4416],
[2K        [0.5552, 0.4448]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KClass label: SoccerPenalty[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.6396, 0.3604]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KStep 0 - TTA Loss: 1.1869[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.6396, 0.3604]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.6397, 0.3603]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.6400, 0.3600]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.6410, 0.3590]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.6434, 0.3566]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.6465, 0.3535]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.6501, 0.3499]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.6588, 0.3412]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.6862, 0.3138]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.7398, 0.2602]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KStep 10 - TTA Loss: 1.2236[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.8186, 0.1814]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.9026, 0.0974]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.9627, 0.0373]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.9896, 0.0104]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.9963, 0.0037]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.9953, 0.0047]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.9832, 0.0168]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.9328, 0.0672]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.7580, 0.2420]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.4138, 0.5862]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KStep 20 - TTA Loss: 0.4667[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.1408, 0.8592]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.0381, 0.9619]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.0099, 0.9901]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.0027, 0.9973]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[7.4826e-04, 9.9925e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[2.2297e-04, 9.9978e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[7.0536e-05, 9.9993e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[2.3789e-05, 9.9998e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[8.5312e-06, 9.9999e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[3.2532e-06, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KStep 30 - TTA Loss: 0.5105[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1.3164e-06, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[5.6591e-07, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[2.5779e-07, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1.2422e-07, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[6.2914e-08, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[3.3487e-08, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1.8686e-08, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1.0895e-08, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[6.6454e-09, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[4.2130e-09, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KStep 40 - TTA Loss: 0.5611[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[2.7623e-09, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1.8735e-09, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1.3136e-09, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[9.4615e-10, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[6.9828e-10, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[5.2710e-10, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[4.0865e-10, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[3.2393e-10, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[2.6149e-10, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.5412, 0.4588]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[8.5924e-06, 9.9999e-01],38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
        [2.7347e-06, 1.0000e+00],
        [3.8645e-05, 9.9996e-01],
        [2.1439e-10, 1.0000e+00],
[2K        [1.9056e-03, 9.9809e-01]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 29/203 [2m0:00:46 â€¢ 0:04:22[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.5602, 0.4398],;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
        [0.5549, 0.4451],
        [0.5554, 0.4446],
        [0.5584, 0.4416],
[2K        [0.5552, 0.4448]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KClass label: ThrowDiscusâ”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.6139, 0.3861]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KStep 0 - TTA Loss: 0.6557[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.2849, 0.7151]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.0930, 0.9070]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.0372, 0.9628]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.0168, 0.9832]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.0083, 0.9917]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.0044, 0.9956]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.0025, 0.9975]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.0014, 0.9986]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[8.7402e-04, 9.9913e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[5.5728e-04, 9.9944e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KStep 10 - TTA Loss: 0.6965[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[3.6988e-04, 9.9963e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[2.5437e-04, 9.9975e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1.8149e-04, 9.9982e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1.3420e-04, 9.9987e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1.0242e-04, 9.9990e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[8.0599e-05, 9.9992e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[6.5535e-05, 9.9993e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[5.4656e-05, 9.9995e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[4.6688e-05, 9.9995e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[4.0833e-05, 9.9996e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KStep 20 - TTA Loss: 0.7878[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[3.6368e-05, 9.9996e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[3.2982e-05, 9.9997e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[3.0378e-05, 9.9997e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[2.8386e-05, 9.9997e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[2.6851e-05, 9.9997e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[2.5674e-05, 9.9997e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[2.4762e-05, 9.9998e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[2.4049e-05, 9.9998e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[2.3465e-05, 9.9998e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[2.3028e-05, 9.9998e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KStep 30 - TTA Loss: 0.7873[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[2.2681e-05, 9.9998e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[2.2420e-05, 9.9998e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[2.2233e-05, 9.9998e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[2.2077e-05, 9.9998e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[2.1974e-05, 9.9998e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[2.1910e-05, 9.9998e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[2.1880e-05, 9.9998e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[2.1863e-05, 9.9998e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[2.1849e-05, 9.9998e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[2.1835e-05, 9.9998e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KStep 40 - TTA Loss: 0.7988[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[2.1832e-05, 9.9998e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[2.1832e-05, 9.9998e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[2.1836e-05, 9.9998e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[2.1842e-05, 9.9998e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[2.1853e-05, 9.9998e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[2.1860e-05, 9.9998e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[2.1865e-05, 9.9998e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[2.1867e-05, 9.9998e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[2.1868e-05, 9.9998e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.5432, 0.4568]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[3.1007e-02, 9.6899e-01],38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
        [1.0220e-03, 9.9898e-01],
        [8.2514e-03, 9.9175e-01],
        [2.0275e-02, 9.7973e-01],
[2K        [2.1811e-05, 9.9998e-01]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 30/203 [2m0:00:48 â€¢ 0:04:21[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.5602, 0.4398],mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 31/203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m   
        [0.5549, 0.4451],
        [0.5554, 0.4446],
        [0.5584, 0.4416],
[2K        [0.5551, 0.4449]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 31/203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KClass label: ThrowDiscusâ”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 31/203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.6150, 0.3850]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KStep 0 - TTA Loss: 0.9776â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 31/203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.6150, 0.3850]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.6152, 0.3848]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.6165, 0.3835]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.6202, 0.3798]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.6282, 0.3718]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.6426, 0.3574]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.6635, 0.3365]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.6923, 0.3077]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.7304, 0.2696]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.7681, 0.2319]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KStep 10 - TTA Loss: 0.8955[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 31/203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.8056, 0.1944]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.8349, 0.1651]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.8658, 0.1342]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.9014, 0.0986]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.9401, 0.0599]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.9785, 0.0215]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.9987, 0.0013]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[9.9995e-01, 5.3726e-05]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.0029e-06]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.7780e-07]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KStep 20 - TTA Loss: 1.9901[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 31/203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.1075e-08]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 7.3369e-10]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 5.2338e-11]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.0622e-12]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.4591e-13]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.2528e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.3943e-15]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.9434e-16]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 5.1109e-17]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 7.3952e-18]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KStep 30 - TTA Loss: 1.7863[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 31/203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.1944e-18]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.1505e-19]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.3084e-20]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 9.5801e-21]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.3570e-21]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 6.3937e-22]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.9050e-22]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 6.2086e-23]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.2040e-23]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 8.4847e-24]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KStep 40 - TTA Loss: 1.8054[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 31/203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.5266e-24]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.5756e-24]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 7.5337e-25]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.8385e-25]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.0753e-25]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.1858e-25]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 7.1315e-26]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.4978e-26]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.9640e-26]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.5888, 0.4112]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 31/203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.0445e-09],;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 31/203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
        [1.0000e+00, 7.1193e-16],
        [1.0000e+00, 2.9246e-12],
        [1.0000e+00, 1.7779e-08],
[2K        [1.0000e+00, 2.0057e-26]], device='cuda:0')5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 31/203 [2m0:00:49 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.5601, 0.4399],mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 32/203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
        [0.5547, 0.4453],
        [0.5553, 0.4447],
        [0.5583, 0.4417],
[2K        [0.5549, 0.4451]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 32/203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KClass label: SoccerPenalty[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 32/203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.6303, 0.3697]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KStep 0 - TTA Loss: 1.0548â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 32/203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.4219, 0.5781]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.7262, 0.2738]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.8071, 0.1929]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.7649, 0.2351]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.6269, 0.3731]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.4294, 0.5706]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.2479, 0.7521]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.1292, 0.8708]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.0655, 0.9345]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.0339, 0.9661]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KStep 10 - TTA Loss: 0.6678[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 32/203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.0183, 0.9817]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.0104, 0.9896]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.0063, 0.9937]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.0040, 0.9960]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.0027, 0.9973]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.0019, 0.9981]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.0014, 0.9986]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.0011, 0.9989]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[8.5064e-04, 9.9915e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[7.0349e-04, 9.9930e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KStep 20 - TTA Loss: 0.8088[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 32/203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[5.9920e-04, 9.9940e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[5.2378e-04, 9.9948e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[4.6834e-04, 9.9953e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[4.2709e-04, 9.9957e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[3.9598e-04, 9.9960e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[3.7229e-04, 9.9963e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[3.5415e-04, 9.9965e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[3.4022e-04, 9.9966e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[3.2952e-04, 9.9967e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[3.2132e-04, 9.9968e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KStep 30 - TTA Loss: 0.7870[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 32/203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[3.1507e-04, 9.9968e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[3.1027e-04, 9.9969e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[3.0663e-04, 9.9969e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[3.0391e-04, 9.9970e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[3.0184e-04, 9.9970e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[3.0030e-04, 9.9970e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[2.9918e-04, 9.9970e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[2.9840e-04, 9.9970e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[2.9787e-04, 9.9970e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[2.9750e-04, 9.9970e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KStep 40 - TTA Loss: 0.7949[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 32/203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[2.9724e-04, 9.9970e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[2.9708e-04, 9.9970e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[2.9699e-04, 9.9970e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[2.9694e-04, 9.9970e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[2.9692e-04, 9.9970e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[2.9691e-04, 9.9970e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[2.9690e-04, 9.9970e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[2.9691e-04, 9.9970e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[2.9691e-04, 9.9970e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.5505, 0.4495]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 32/203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[2.2097e-02, 9.7790e-01],;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 32/203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
        [2.6589e-02, 9.7341e-01],
        [6.2347e-02, 9.3765e-01],
        [2.9661e-04, 9.9970e-01],
[2K        [6.4991e-01, 3.5009e-01]], device='cuda:0')5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 32/203 [2m0:00:51 â€¢ 0:04:20[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.5600, 0.4400],6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
        [0.5546, 0.4454],
        [0.5552, 0.4448],
        [0.5582, 0.4418],
[2K        [0.5548, 0.4452]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KClass label: ThrowDiscusâ”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.6158, 0.3842]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KStep 0 - TTA Loss: 0.8584â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.6158, 0.3842]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.6159, 0.3841]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.6166, 0.3834]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.6187, 0.3813]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.6239, 0.3761]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.6309, 0.3691]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.6406, 0.3594]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.6492, 0.3508]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.6504, 0.3496]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.6434, 0.3566]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KStep 10 - TTA Loss: 0.8746[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.6325, 0.3675]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.6348, 0.3652]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.6694, 0.3306]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.7380, 0.2620]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.8311, 0.1689]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.9130, 0.0870]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.9607, 0.0393]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.9768, 0.0232]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.9678, 0.0322]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.9187, 0.0813]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KStep 20 - TTA Loss: 0.7865[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.7688, 0.2312]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.4834, 0.5166]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.2124, 0.7876]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0750, 0.9250]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0249, 0.9751]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0085, 0.9915]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0030, 0.9970]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0011, 0.9989]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[4.3758e-04, 9.9956e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.8188e-04, 9.9982e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KStep 30 - TTA Loss: 0.8692[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[7.9369e-05, 9.9992e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[3.6339e-05, 9.9996e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.7393e-05, 9.9998e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[8.7171e-06, 9.9999e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[4.5406e-06, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[2.4566e-06, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.3889e-06, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[8.1272e-07, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[4.9401e-07, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[3.0948e-07, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KStep 40 - TTA Loss: 0.9410[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.9979e-07, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.3345e-07, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[9.2100e-08, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[6.5359e-08, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[4.7722e-08, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[3.6019e-08, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[2.7912e-08, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[2.2228e-08, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.8059e-08, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.5395, 0.4605]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[5.2136e-03, 9.9479e-01],[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
        [2.4275e-05, 9.9998e-01],
        [4.9665e-04, 9.9950e-01],
        [1.1305e-02, 9.8869e-01],
[2K        [1.4973e-08, 1.0000e+00]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 33/203 [2m0:00:52 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.5600, 0.4400],6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
        [0.5546, 0.4454],
        [0.5552, 0.4448],
        [0.5582, 0.4418],
[2K        [0.5547, 0.4453]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.6131, 0.3869]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KStep 0 - TTA Loss: 0.7270â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.6852, 0.3148]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.7823, 0.2177]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.9661, 0.0339]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.9819, 0.0181]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.9235, 0.0765]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.6910, 0.3090]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.3317, 0.6683]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.1207, 0.8793]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0435, 0.9565]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0173, 0.9827]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KStep 10 - TTA Loss: 0.7208[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0076, 0.9924]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0037, 0.9963]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0019, 0.9981]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0011, 0.9989]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[6.6186e-04, 9.9934e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[4.2077e-04, 9.9958e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[2.8111e-04, 9.9972e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.9798e-04, 9.9980e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.4595e-04, 9.9985e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.1216e-04, 9.9989e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KStep 20 - TTA Loss: 0.7251[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[8.9486e-05, 9.9991e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[7.4272e-05, 9.9993e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[6.3548e-05, 9.9994e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[5.5815e-05, 9.9994e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[5.0167e-05, 9.9995e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[4.5943e-05, 9.9995e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[4.2906e-05, 9.9996e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[4.0623e-05, 9.9996e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[3.8860e-05, 9.9996e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[3.7562e-05, 9.9996e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KStep 30 - TTA Loss: 0.7736[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[3.6591e-05, 9.9996e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[3.5870e-05, 9.9996e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[3.5368e-05, 9.9996e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[3.4999e-05, 9.9996e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[3.4744e-05, 9.9997e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[3.4554e-05, 9.9997e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[3.4420e-05, 9.9997e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[3.4354e-05, 9.9997e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[3.4306e-05, 9.9997e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[3.4278e-05, 9.9997e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KStep 40 - TTA Loss: 0.7321[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[3.4262e-05, 9.9997e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[3.4267e-05, 9.9997e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[3.4269e-05, 9.9997e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[3.4273e-05, 9.9997e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[3.4278e-05, 9.9997e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[3.4285e-05, 9.9997e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[3.4291e-05, 9.9997e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[3.4295e-05, 9.9997e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[3.4298e-05, 9.9997e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.5420, 0.4580]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[2.9522e-03, 9.9705e-01],[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
        [4.8578e-04, 9.9951e-01],
        [3.4026e-05, 9.9997e-01],
        [2.7785e-03, 9.9722e-01],
[2K        [1.2386e-03, 9.9876e-01]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 34/203 [2m0:00:54 â€¢ 0:04:22[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.5600, 0.4400],6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
        [0.5545, 0.4455],
        [0.5552, 0.4448],
        [0.5582, 0.4418],
[2K        [0.5547, 0.4453]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.6138, 0.3862]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KStep 0 - TTA Loss: 0.9160â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.6138, 0.3862]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.6132, 0.3868]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.6100, 0.3900]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.6006, 0.3994]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.5813, 0.4187]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.5469, 0.4531]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.4948, 0.5052]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.4331, 0.5669]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.3809, 0.6191]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.3493, 0.6507]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KStep 10 - TTA Loss: 1.2796[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.3393, 0.6607]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.3454, 0.6546]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.3617, 0.6383]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.3804, 0.6196]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.3962, 0.6038]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.4056, 0.5944]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.4070, 0.5930]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.3993, 0.6007]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.3821, 0.6179]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.3565, 0.6435]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KStep 20 - TTA Loss: 0.3968[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.3239, 0.6761]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.2870, 0.7130]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.2480, 0.7520]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.2096, 0.7904]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.1737, 0.8263]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.1419, 0.8581]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.1148, 0.8852]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0925, 0.9075]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0748, 0.9252]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0608, 0.9392]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KStep 30 - TTA Loss: 0.5226[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0500, 0.9500]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0417, 0.9583]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0353, 0.9647]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0306, 0.9694]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0270, 0.9730]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0244, 0.9756]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0225, 0.9775]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0211, 0.9789]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0200, 0.9800]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0194, 0.9806]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KStep 40 - TTA Loss: 0.5865[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0189, 0.9811]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0187, 0.9813]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0186, 0.9814]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0187, 0.9813]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0188, 0.9812]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0190, 0.9810]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0191, 0.9809]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0192, 0.9808]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0193, 0.9807]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.5500, 0.4500]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.1155, 0.8845],4mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
        [0.0519, 0.9481],
        [0.0193, 0.9807],
        [0.1246, 0.8754],
[2K        [0.1150, 0.8850]], device='cuda:0')24mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 35/203 [2m0:00:55 â€¢ 0:04:21[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.5600, 0.4400],7mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 36/203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m   
        [0.5545, 0.4455],
        [0.5552, 0.4448],
        [0.5582, 0.4418],
[2K        [0.5547, 0.4453]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 36/203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KClass label: SoccerPenaltyâ”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 36/203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.6359, 0.3641]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KStep 0 - TTA Loss: 0.9507â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 36/203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.4331, 0.5669]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.2028, 0.7972]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0957, 0.9043]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0639, 0.9361]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0416, 0.9584]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0258, 0.9742]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0166, 0.9834]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0139, 0.9861]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0114, 0.9886]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0088, 0.9912]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KStep 10 - TTA Loss: 1.1532â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 36/203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0066, 0.9934]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0050, 0.9950]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0043, 0.9957]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0035, 0.9965]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0029, 0.9971]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0023, 0.9977]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0018, 0.9982]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0015, 0.9985]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.0012, 0.9988]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[9.4239e-04, 9.9906e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KStep 20 - TTA Loss: 1.2904â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 36/203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[7.7504e-04, 9.9922e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[7.2886e-04, 9.9927e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[6.8253e-04, 9.9932e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[6.4053e-04, 9.9936e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[6.1181e-04, 9.9939e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[6.3809e-04, 9.9936e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[6.5429e-04, 9.9935e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[7.0545e-04, 9.9929e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[7.4088e-04, 9.9926e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[7.6479e-04, 9.9924e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KStep 30 - TTA Loss: 1.0980â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 36/203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[7.8086e-04, 9.9922e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[7.8258e-04, 9.9922e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[7.7795e-04, 9.9922e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[7.6993e-04, 9.9923e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[7.5497e-04, 9.9925e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[7.3964e-04, 9.9926e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[7.2531e-04, 9.9927e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[7.0632e-04, 9.9929e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[6.8747e-04, 9.9931e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[6.7099e-04, 9.9933e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KStep 40 - TTA Loss: 1.5683â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 36/203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[6.6239e-04, 9.9934e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[6.5423e-04, 9.9935e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[6.4738e-04, 9.9935e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[6.4540e-04, 9.9935e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[6.4333e-04, 9.9936e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[6.4327e-04, 9.9936e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[6.4281e-04, 9.9936e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[6.4228e-04, 9.9936e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[6.4197e-04, 9.9936e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.5485, 0.4515]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 36/203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[2.3681e-02, 9.7632e-01],5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 36/203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
        [1.3258e-02, 9.8674e-01],
        [4.3886e-02, 9.5611e-01],
        [6.4129e-04, 9.9936e-01],
[2K        [1.3934e-01, 8.6066e-01]], device='cuda:0');5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 36/203 [2m0:00:57 â€¢ 0:04:16[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.5600, 0.4400],7mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
        [0.5545, 0.4455],
        [0.5552, 0.4448],
        [0.5582, 0.4418],
[2K        [0.5547, 0.4453]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KClass label: SoccerPenaltyâ”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.6323, 0.3677]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KStep 0 - TTA Loss: 0.9294â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.6323, 0.3677]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.6331, 0.3669]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.6385, 0.3615]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.6530, 0.3470]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.6823, 0.3177]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.7308, 0.2692]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.7955, 0.2045]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.8697, 0.1303]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.9328, 0.0672]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.9687, 0.0313]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KStep 10 - TTA Loss: 1.3733â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.9827, 0.0173]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.9923, 0.0077]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.9960, 0.0040]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.9842, 0.0158]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 5.5278e-09]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.2757e-15]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.2711e-22]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.5495e-29]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 5.4865e-36]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 9.3467e-43]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KStep 20 - TTA Loss: 1.7194â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KStep 30 - TTA Loss: 1.7211â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KStep 40 - TTA Loss: 1.7222â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.6379, 0.3621]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[1., 0.],5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
        [1., 0.],
        [1., 0.],
        [1., 0.],
[2K        [1., 0.]], device='cuda:0');5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:58 â€¢ 0:04:14[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.5603, 0.4397],7mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 38/203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
        [0.5548, 0.4452],
        [0.5554, 0.4446],
        [0.5588, 0.4412],
[2K        [0.5549, 0.4451]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 38/203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 38/203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.6117, 0.3883]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KStep 0 - TTA Loss: 0.7666â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 38/203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.9078, 0.0922]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.9920, 0.0080]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 7.8976e-07]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.9460e-10]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.1187e-13]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.3990e-16]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.5616e-19]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.7337e-21]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.5213e-23]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.2774e-25]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KStep 10 - TTA Loss: 1.5529â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 38/203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 5.5258e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.0721e-28]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.1494e-29]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 9.0574e-31]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 9.7712e-32]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.3951e-32]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.5562e-33]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 5.8448e-34]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.6259e-34]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 5.3786e-35]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KStep 20 - TTA Loss: 1.5615â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 38/203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.0726e-35]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 9.1323e-36]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.5258e-36]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.4854e-36]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.4927e-36]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 9.6898e-37]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 6.7283e-37]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.9517e-37]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.8313e-37]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.0944e-37]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KStep 30 - TTA Loss: 1.5847â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 38/203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.5926e-37]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.2412e-37]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.9893e-37]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.8055e-37]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.6698e-37]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.5686e-37]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.4928e-37]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.4361e-37]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.3936e-37]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.3621e-37]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KStep 40 - TTA Loss: 1.5228â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 38/203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.3389e-37]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.3222e-37]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.3103e-37]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.3022e-37]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.2968e-37]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.2935e-37]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.2915e-37]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.2905e-37]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.2902e-37]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.6156, 0.3844]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 38/203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 5.0314e-24],5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 38/203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
        [1.0000e+00, 5.3640e-28],
        [1.0000e+00, 1.2124e-37],
        [1.0000e+00, 6.7271e-26],
[2K        [1.0000e+00, 5.9307e-23]], device='cuda:0');5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 38/203 [2m0:01:00 â€¢ 0:04:13[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.5602, 0.4398],;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
        [0.5547, 0.4453],
        [0.5553, 0.4447],
        [0.5587, 0.4413],
[2K        [0.5549, 0.4451]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6188, 0.3812]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KStep 0 - TTA Loss: 0.9109â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6188, 0.3812]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6187, 0.3813]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6179, 0.3821]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6155, 0.3845]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6112, 0.3888]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6037, 0.3963]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5913, 0.4087]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5703, 0.4297]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5427, 0.4573]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5080, 0.4920]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KStep 10 - TTA Loss: 1.1537â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.4675, 0.5325]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.4256, 0.5744]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.3986, 0.6014]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.3827, 0.6173]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.3767, 0.6233]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.3760, 0.6240]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.3754, 0.6246]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.3688, 0.6312]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.3565, 0.6435]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.3428, 0.6572]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KStep 20 - TTA Loss: 1.6366â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.3340, 0.6660]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.3384, 0.6616]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.3511, 0.6489]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.3639, 0.6361]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.3726, 0.6274]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.3766, 0.6234]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.3746, 0.6254]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.3685, 0.6315]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.3583, 0.6417]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.3466, 0.6534]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KStep 30 - TTA Loss: 1.5935â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.3375, 0.6625]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.3341, 0.6659]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.3450, 0.6550]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.3620, 0.6380]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.3757, 0.6243]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.3860, 0.6140]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.3929, 0.6071]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.3947, 0.6053]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.3905, 0.6095]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.3823, 0.6177]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KStep 40 - TTA Loss: 1.5710â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.3758, 0.6242]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.3807, 0.6193]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.3912, 0.6088]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.4060, 0.5940]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.4148, 0.5852]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.4284, 0.5716]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.4405, 0.5595]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.4529, 0.5471]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.4559, 0.5441]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5524, 0.4476]], device='cuda:0')mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6008, 0.3992],24mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
        [0.4561, 0.5439],
        [0.5774, 0.4226],
        [0.6075, 0.3925],
[2K        [0.4519, 0.5481]], device='cuda:0')224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 39/203 [2m0:01:01 â€¢ 0:04:15[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5602, 0.4398],;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
        [0.5547, 0.4453],
        [0.5553, 0.4447],
        [0.5587, 0.4413],
[2K        [0.5549, 0.4451]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.6127, 0.3873]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KStep 0 - TTA Loss: 0.6278â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.6047, 0.3953]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.5233, 0.4767]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.5367, 0.4633]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.5450, 0.4550]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.5418, 0.4582]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.5215, 0.4785]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.4888, 0.5112]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.4511, 0.5489]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.4123, 0.5877]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.3747, 0.6253]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KStep 10 - TTA Loss: 0.6382â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.3427, 0.6573]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.3156, 0.6844]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.2940, 0.7060]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.2755, 0.7245]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.2608, 0.7392]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.2498, 0.7502]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.2412, 0.7588]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.2348, 0.7652]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.2302, 0.7698]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.2267, 0.7733]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KStep 20 - TTA Loss: 0.6903â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.2246, 0.7754]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.2234, 0.7766]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.2228, 0.7772]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.2225, 0.7775]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.2225, 0.7775]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.2229, 0.7771]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.2236, 0.7764]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.2245, 0.7755]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.2255, 0.7745]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.2263, 0.7737]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KStep 30 - TTA Loss: 0.7478â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.2272, 0.7728]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.2282, 0.7718]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.2291, 0.7709]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.2299, 0.7701]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.2306, 0.7694]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.2312, 0.7688]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.2316, 0.7684]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.2320, 0.7680]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.2323, 0.7677]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.2326, 0.7674]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KStep 40 - TTA Loss: 0.7857â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.2328, 0.7672]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.2330, 0.7670]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.2331, 0.7669]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.2332, 0.7668]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.2332, 0.7668]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.2333, 0.7667]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.2333, 0.7667]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.2333, 0.7667]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.2333, 0.7667]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.5525, 0.4475]], device='cuda:0')mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.4036, 0.5964],24mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
        [0.2624, 0.7376],
        [0.2331, 0.7669],
        [0.2794, 0.7206],
[2K        [0.3775, 0.6225]], device='cuda:0')224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 40/203 [2m0:01:03 â€¢ 0:04:09[0m [2;4m0.66it/s[0m  
[2KAttention weights: tensor([[0.5602, 0.4398],37mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 41/203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m   
        [0.5547, 0.4453],
        [0.5553, 0.4447],
        [0.5587, 0.4413],
[2K        [0.5549, 0.4451]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 41/203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 41/203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6127, 0.3873]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KStep 0 - TTA Loss: 0.7382â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 41/203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6127, 0.3873]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6128, 0.3872]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6135, 0.3865]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6157, 0.3843]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6203, 0.3797]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6286, 0.3714]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6397, 0.3603]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6542, 0.3458]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6706, 0.3294]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6918, 0.3082]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KStep 10 - TTA Loss: 0.7843â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 41/203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.7175, 0.2825]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.7441, 0.2559]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.7743, 0.2257]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.8213, 0.1787]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.8894, 0.1106]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.9617, 0.0383]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.9931, 0.0069]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[9.9903e-01, 9.7377e-04]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[9.9982e-01, 1.7538e-04]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[9.9997e-01, 3.2622e-05]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KStep 20 - TTA Loss: 1.4783â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 41/203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[9.9999e-01, 6.2964e-06]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.2629e-06]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.6543e-07]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 5.8791e-08]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.3788e-08]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.4363e-09]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 9.1238e-10]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.5851e-10]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 7.8233e-11]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.5292e-11]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KStep 30 - TTA Loss: 1.4737â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 41/203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 8.7312e-12]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.2157e-12]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.2618e-12]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 5.2659e-13]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.3328e-13]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.0946e-13]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 5.4265e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.8354e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.5575e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 8.9699e-15]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KStep 40 - TTA Loss: 1.5217â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 41/203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 5.4019e-15]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.3928e-15]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.2166e-15]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.5026e-15]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.0542e-15]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 7.6365e-16]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 5.6984e-16]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.3706e-16]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.4382e-16]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5867, 0.4133]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 41/203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.0492e-09],;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 41/203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
        [1.0000e+00, 2.4328e-11],
        [1.0000e+00, 2.6951e-16],
        [1.0000e+00, 4.1949e-09],
[2K        [1.0000e+00, 5.7487e-10]], device='cuda:0')8;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 41/203 [2m0:01:05 â€¢ 0:04:12[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5602, 0.4398],37mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 42/203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
        [0.5547, 0.4453],
        [0.5552, 0.4448],
        [0.5587, 0.4413],
[2K        [0.5549, 0.4451]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 42/203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 42/203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6182, 0.3818]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KStep 0 - TTA Loss: 0.9821â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 42/203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.7731, 0.2269]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.9222, 0.0778]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.3546, 0.6454]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.0458, 0.9542]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.0022, 0.9978]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.5720e-04, 9.9984e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.5802e-05, 9.9998e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[2.1575e-06, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[3.8589e-07, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[8.6432e-08, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KStep 10 - TTA Loss: 1.0081â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 42/203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[2.3649e-08, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[7.6722e-09, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[2.8947e-09, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.2417e-09, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[5.9699e-10, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[3.1648e-10, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.8335e-10, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.1443e-10, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[7.6155e-11, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[5.3814e-11, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KStep 20 - TTA Loss: 0.9569â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 42/203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[4.0013e-11, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[3.1059e-11, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[2.5021e-11, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[2.0822e-11, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.7803e-11, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.5577e-11, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.3919e-11, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.2683e-11, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.1733e-11, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.1005e-11, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KStep 30 - TTA Loss: 1.0481â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 42/203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0440e-11, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0001e-11, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[9.6504e-12, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[9.3833e-12, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[9.1769e-12, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[9.0118e-12, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[8.8873e-12, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[8.7911e-12, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[8.7169e-12, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[8.6610e-12, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KStep 40 - TTA Loss: 1.0536â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 42/203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[8.6178e-12, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[8.5860e-12, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[8.5623e-12, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[8.5456e-12, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[8.5350e-12, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[8.5284e-12, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[8.5249e-12, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[8.5231e-12, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[8.5224e-12, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5455, 0.4545]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 42/203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.3383e-03, 9.9866e-01],;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 42/203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
        [4.9638e-07, 1.0000e+00],
        [8.3213e-05, 9.9992e-01],
        [4.2275e-03, 9.9577e-01],
[2K        [8.4670e-12, 1.0000e+00]], device='cuda:0')8;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 42/203 [2m0:01:06 â€¢ 0:04:11[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5601, 0.4399],37mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 43/203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
        [0.5547, 0.4453],
        [0.5552, 0.4448],
        [0.5587, 0.4413],
[2K        [0.5549, 0.4451]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 43/203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 43/203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6135, 0.3865]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KStep 0 - TTA Loss: 0.7208â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 43/203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6135, 0.3865]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6137, 0.3863]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6146, 0.3854]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6169, 0.3831]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6214, 0.3786]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6286, 0.3714]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6392, 0.3608]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6529, 0.3471]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6692, 0.3308]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6902, 0.3098]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KStep 10 - TTA Loss: 0.6634â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 43/203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.7163, 0.2837]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.7533, 0.2467]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.8050, 0.1950]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.8747, 0.1253]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.9494, 0.0506]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.9943, 0.0057]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.9972, 0.0028]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.9989, 0.0011]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[9.9974e-01, 2.6366e-04]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[9.9993e-01, 7.4191e-05]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KStep 20 - TTA Loss: 1.6386â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 43/203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[9.9997e-01, 2.6186e-05]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[9.9999e-01, 1.2183e-05]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[9.9999e-01, 5.8081e-06]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.8911e-06]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.4814e-06]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 7.8112e-07]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.2408e-07]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.3701e-07]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.3690e-07]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 8.1612e-08]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KStep 30 - TTA Loss: 1.5281â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 43/203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 5.0098e-08]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.1676e-08]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.0607e-08]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.3784e-08]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 9.4724e-09]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 6.6813e-09]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.8322e-09]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.5798e-09]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.7134e-09]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.1017e-09]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KStep 40 - TTA Loss: 1.5391â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 43/203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.6615e-09]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.3391e-09]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.0990e-09]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 9.1745e-10]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 7.7812e-10]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 6.6977e-10]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 5.8447e-10]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 5.1657e-10]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.6195e-10]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5696, 0.4304]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 43/203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[9.9941e-01, 5.9031e-04],;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 43/203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
        [1.0000e+00, 3.1058e-06],
        [9.9994e-01, 5.9146e-05],
        [9.9836e-01, 1.6397e-03],
[2K        [1.0000e+00, 4.1551e-10]], device='cuda:0')8;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 43/203 [2m0:01:08 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5601, 0.4399],8;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
        [0.5546, 0.4454],
        [0.5552, 0.4448],
        [0.5587, 0.4413],
[2K        [0.5548, 0.4452]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.6128, 0.3872]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KStep 0 - TTA Loss: 0.6173â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.7926, 0.2074]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.9442, 0.0558]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.9972, 0.0028]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[9.9985e-01, 1.4855e-04]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[9.9999e-01, 8.3050e-06]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 7.0108e-07]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 7.8497e-08]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.1287e-08]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.0299e-09]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.4624e-10]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KStep 10 - TTA Loss: 1.4825â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.1741e-10]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.6279e-11]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.2938e-11]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 5.2431e-12]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.3805e-12]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.1956e-12]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 6.5660e-13]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.9028e-13]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.4875e-13]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.6860e-13]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KStep 20 - TTA Loss: 1.4829â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.2061e-13]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 9.0471e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 7.0733e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 5.7336e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.7962e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.1230e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.6291e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.2602e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.9805e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.7661e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KStep 30 - TTA Loss: 1.4840â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.6003e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.4713e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.3705e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.2916e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.2299e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.1817e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.1443e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.1155e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.0934e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.0768e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KStep 40 - TTA Loss: 1.4793â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.0644e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.0553e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.0489e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.0444e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.0415e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.0396e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.0386e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.0380e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.0378e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.5955, 0.4045]], device='cuda:0')7mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 7.9460e-09],m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
        [1.0000e+00, 3.1532e-10],
        [1.0000e+00, 1.9903e-14],
        [1.0000e+00, 2.3892e-08],
[2K        [1.0000e+00, 5.8189e-09]], device='cuda:0')0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 44/203 [2m0:01:09 â€¢ 0:04:05[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.5602, 0.4398],8;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 45/203 [2m0:01:11 â€¢ 0:04:06[0m [2;4m0.64it/s[0m  
        [0.5547, 0.4453],
        [0.5553, 0.4447],
        [0.5587, 0.4413],
[2K        [0.5549, 0.4451]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 45/203 [2m0:01:11 â€¢ 0:04:06[0m [2;4m0.64it/s[0m  
[2KClass label: BaseballPitchâ”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 45/203 [2m0:01:11 â€¢ 0:04:06[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6450, 0.3550]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:01:11 â€¢ 0:04:06[0m [2;4m0.64it/s[0m  
[2KStep 0 - TTA Loss: 1.2356â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 45/203 [2m0:01:11 â€¢ 0:04:06[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6450, 0.3550]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:01:11 â€¢ 0:04:06[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6448, 0.3552]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:01:11 â€¢ 0:04:06[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6440, 0.3560]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:01:11 â€¢ 0:04:06[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6405, 0.3595]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:01:11 â€¢ 0:04:06[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6329, 0.3671]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:01:11 â€¢ 0:04:06[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6208, 0.3792]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:01:11 â€¢ 0:04:06[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6007, 0.3993]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:01:11 â€¢ 0:04:06[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5714, 0.4286]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:01:11 â€¢ 0:04:06[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5330, 0.4670]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:01:11 â€¢ 0:04:06[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.4926, 0.5074]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:01:11 â€¢ 0:04:06[0m [2;4m0.64it/s[0m  
[2KStep 10 - TTA Loss: 1.2229â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 45/203 [2m0:01:11 â€¢ 0:04:06[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.4555, 0.5445]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:01:11 â€¢ 0:04:06[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.4238, 0.5762]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:01:11 â€¢ 0:04:06[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.3967, 0.6033]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:01:11 â€¢ 0:04:06[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.3744, 0.6256]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:01:11 â€¢ 0:04:06[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.3549, 0.6451]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:01:11 â€¢ 0:04:06[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.3378, 0.6622]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:01:11 â€¢ 0:04:06[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.3196, 0.6804]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:01:11 â€¢ 0:04:06[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.3027, 0.6973]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:01:11 â€¢ 0:04:06[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.2842, 0.7158]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:01:11 â€¢ 0:04:06[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.2656, 0.7344]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:01:11 â€¢ 0:04:06[0m [2;4m0.64it/s[0m  
[2KStep 20 - TTA Loss: 0.7656â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 45/203 [2m0:01:11 â€¢ 0:04:06[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.2491, 0.7509]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:01:11 â€¢ 0:04:06[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.2337, 0.7663]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:01:11 â€¢ 0:04:06[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.2191, 0.7809]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:01:11 â€¢ 0:04:06[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.2049, 0.7951]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:01:11 â€¢ 0:04:06[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.1918, 0.8082]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:01:11 â€¢ 0:04:06[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.1798, 0.8202]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:01:11 â€¢ 0:04:06[0m [2;4m0.64it/s[0m  
[2KTesting [38;2;98;6;224mâ”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 45/203 [2m0:01:12 â€¢ 0:04:06[0m [2;4m0.64it/s[0m

Detected KeyboardInterrupt, attempting graceful shutdown ...
[?25h[[36m2025-02-15 11:49:04,347[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/def/fewshot/logs/train/runs/2025-02-15_11-47-39[0m
[[36m2025-02-15 11:49:04,347[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
