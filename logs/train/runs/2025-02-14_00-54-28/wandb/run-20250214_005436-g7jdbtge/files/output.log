[[36m2025-02-14 00:54:37,595[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
You are using a CUDA device ('NVIDIA GeForce RTX 4070 SUPER') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
No of videos in train is 214
Loading train Video Information ...
No of class 10
No of videos in validation is 203
Loading validation Video Information ...
No of class 10
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”“
â”ƒ[1;35m [0m[1;35m   [0m[1;35m [0mâ”ƒ[1;35m [0m[1;35mName                                                   [0m[1;35m [0mâ”ƒ[1;35m [0m[1;35mType                           [0m[1;35m [0mâ”ƒ[1;35m [0m[1;35mParams[0m[1;35m [0mâ”ƒ[1;35m [0m[1;35mMode [0m[1;35m [0mâ”ƒ
â”¡â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”©
â”‚[2m [0m[2m0  [0m[2m [0mâ”‚ net                                                     â”‚ T3ALNet                         â”‚  639 M â”‚ train â”‚
â”‚[2m [0m[2m1  [0m[2m [0mâ”‚ net.model                                               â”‚ CoCa                            â”‚  638 M â”‚ train â”‚
â”‚[2m [0m[2m2  [0m[2m [0mâ”‚ net.model.text                                          â”‚ TextTransformer                 â”‚  123 M â”‚ train â”‚
â”‚[2m [0m[2m3  [0m[2m [0mâ”‚ net.model.text.token_embedding                          â”‚ Embedding                       â”‚ 37.9 M â”‚ train â”‚
â”‚[2m [0m[2m4  [0m[2m [0mâ”‚ net.model.text.transformer                              â”‚ Transformer                     â”‚ 85.1 M â”‚ train â”‚
â”‚[2m [0m[2m5  [0m[2m [0mâ”‚ net.model.text.transformer.resblocks                    â”‚ ModuleList                      â”‚ 85.1 M â”‚ train â”‚
â”‚[2m [0m[2m6  [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m7  [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m8  [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m9  [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m10 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m11 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m12 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m13 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m14 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m15 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m16 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m17 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m18 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m19 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m20 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m21 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m22 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m23 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m24 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m25 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m26 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m27 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m28 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m29 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m30 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m31 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m32 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m33 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m34 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m35 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m36 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m37 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m38 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m39 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m40 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m41 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m42 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m43 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m44 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m45 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m46 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m47 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m48 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m49 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m50 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m51 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m52 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m53 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m54 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m55 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m56 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m57 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m58 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m59 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m60 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m61 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m62 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m63 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m64 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m65 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m66 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m67 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m68 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m69 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m70 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m71 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m72 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m73 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m74 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m75 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m76 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m77 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m78 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m79 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m80 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m81 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m82 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m83 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m84 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m85 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m86 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m87 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m88 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m89 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m90 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m91 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m92 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m93 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m94 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m95 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m96 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m97 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m98 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m99 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m100[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m101[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m102[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m103[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m104[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m105[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m106[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m107[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m108[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m109[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m110[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m111[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m112[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m113[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m114[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m115[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m116[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10                 â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m117[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.ln_1            â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m118[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.attn            â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m119[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.attn.out_proj   â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m120[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.ls_1            â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m121[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.ln_2            â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m122[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.mlp             â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m123[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.mlp.c_fc        â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m124[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.mlp.gelu        â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m125[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.mlp.c_proj      â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m126[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.ls_2            â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m127[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11                 â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m128[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.ln_1            â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m129[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.attn            â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m130[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.attn.out_proj   â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m131[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.ls_1            â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m132[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.ln_2            â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m133[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.mlp             â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m134[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.mlp.c_fc        â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m135[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.mlp.gelu        â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m136[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.mlp.c_proj      â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m137[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.ls_2            â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m138[0m[2m [0mâ”‚ net.model.text.ln_final                                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m139[0m[2m [0mâ”‚ net.model.visual                                        â”‚ VisionTransformer               â”‚  306 M â”‚ train â”‚
â”‚[2m [0m[2m140[0m[2m [0mâ”‚ net.model.visual.conv1                                  â”‚ Conv2d                          â”‚  602 K â”‚ train â”‚
â”‚[2m [0m[2m141[0m[2m [0mâ”‚ net.model.visual.patch_dropout                          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m142[0m[2m [0mâ”‚ net.model.visual.ln_pre                                 â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m143[0m[2m [0mâ”‚ net.model.visual.transformer                            â”‚ Transformer                     â”‚  302 M â”‚ train â”‚
â”‚[2m [0m[2m144[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks                  â”‚ ModuleList                      â”‚  302 M â”‚ train â”‚
â”‚[2m [0m[2m145[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m146[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m147[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m148[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m149[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m150[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m151[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m152[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m153[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m154[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m155[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m156[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m157[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m158[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m159[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m160[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m161[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m162[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m163[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m164[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m165[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m166[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m167[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m168[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m169[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m170[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m171[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m172[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m173[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m174[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m175[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m176[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m177[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m178[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m179[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m180[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m181[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m182[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m183[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m184[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m185[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m186[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m187[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m188[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m189[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m190[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m191[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m192[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m193[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m194[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m195[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m196[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m197[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m198[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m199[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m200[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m201[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m202[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m203[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m204[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m205[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m206[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m207[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m208[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m209[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m210[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m211[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m212[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m213[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m214[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m215[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m216[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m217[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m218[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m219[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m220[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m221[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m222[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m223[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m224[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m225[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m226[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m227[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m228[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m229[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m230[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m231[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m232[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m233[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m234[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m235[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m236[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m237[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m238[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m239[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m240[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m241[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m242[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m243[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m244[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m245[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m246[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m247[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m248[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m249[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m250[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m251[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m252[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m253[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m254[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m255[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m256[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m257[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m258[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m259[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m260[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m261[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m262[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m263[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m264[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m265[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m266[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m267[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m268[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m269[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m270[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m271[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m272[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m273[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m274[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m275[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m276[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m277[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m278[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m279[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m280[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m281[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m282[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m283[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m284[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m285[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m286[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m287[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m288[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m289[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m290[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m291[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m292[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m293[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m294[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m295[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m296[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m297[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m298[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m299[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m300[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m301[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m302[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m303[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m304[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m305[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m306[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m307[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m308[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m309[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m310[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m311[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m312[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m313[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m314[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m315[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m316[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m317[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m318[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m319[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m320[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m321[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m322[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m323[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m324[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m325[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m326[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m327[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m328[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m329[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m330[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m331[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m332[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m333[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m334[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m335[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m336[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m337[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m338[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m339[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m340[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m341[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m342[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m343[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m344[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m345[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m346[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m347[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m348[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m349[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m350[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m351[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m352[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m353[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m354[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m355[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m356[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m357[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m358[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m359[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m360[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m361[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m362[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m363[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m364[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m365[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m366[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m367[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m368[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m369[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m370[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m371[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m372[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m373[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m374[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m375[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m376[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m377[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m378[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m379[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m380[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m381[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m382[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m383[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m384[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m385[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m386[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m387[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m388[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m389[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m390[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m391[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m392[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m393[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m394[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m395[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m396[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m397[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m398[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m399[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m400[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m401[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m402[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m403[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m404[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m405[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m406[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m407[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m408[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m409[0m[2m [0mâ”‚ net.model.visual.attn_pool                              â”‚ AttentionalPooler               â”‚  3.0 M â”‚ train â”‚
â”‚[2m [0m[2m410[0m[2m [0mâ”‚ net.model.visual.attn_pool.attn                         â”‚ MultiheadAttention              â”‚  2.8 M â”‚ train â”‚
â”‚[2m [0m[2m411[0m[2m [0mâ”‚ net.model.visual.attn_pool.attn.out_proj                â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m412[0m[2m [0mâ”‚ net.model.visual.attn_pool.ln_q                         â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m413[0m[2m [0mâ”‚ net.model.visual.attn_pool.ln_k                         â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m414[0m[2m [0mâ”‚ net.model.visual.ln_post                                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m415[0m[2m [0mâ”‚ net.model.text_decoder                                  â”‚ MultimodalTransformer           â”‚  208 M â”‚ train â”‚
â”‚[2m [0m[2m416[0m[2m [0mâ”‚ net.model.text_decoder.resblocks                        â”‚ ModuleList                      â”‚ 85.1 M â”‚ train â”‚
â”‚[2m [0m[2m417[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m418[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m419[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m420[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m421[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m422[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m423[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m424[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m425[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m426[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m427[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m428[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m429[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m430[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m431[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m432[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m433[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m434[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m435[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m436[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m437[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m438[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m439[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m440[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m441[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m442[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m443[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m444[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m445[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m446[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m447[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m448[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m449[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m450[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m451[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m452[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m453[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m454[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m455[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m456[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m457[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m458[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m459[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m460[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m461[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m462[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m463[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m464[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m465[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m466[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m467[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m468[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m469[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m470[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m471[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m472[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m473[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m474[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m475[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m476[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m477[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m478[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m479[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m480[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m481[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m482[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m483[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m484[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m485[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m486[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m487[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m488[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m489[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m490[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m491[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m492[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m493[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m494[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m495[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m496[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m497[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m498[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m499[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m500[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m501[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m502[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m503[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m504[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m505[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m506[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m507[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m508[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m509[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m510[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m511[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m512[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m513[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m514[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m515[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m516[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m517[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m518[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m519[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m520[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m521[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m522[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m523[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m524[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m525[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m526[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m527[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m528[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m529[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m530[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m531[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m532[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m533[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m534[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m535[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m536[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m537[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m538[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m539[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m540[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m541[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m542[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m543[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m544[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m545[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m546[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m547[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m548[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m549[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn                       â”‚ ModuleList                      â”‚ 85.1 M â”‚ train â”‚
â”‚[2m [0m[2m550[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m551[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m552[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m553[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m554[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m555[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m556[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m557[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m558[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m559[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m560[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m561[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m562[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m563[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m564[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m565[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m566[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m567[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m568[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m569[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m570[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m571[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m572[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m573[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m574[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m575[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m576[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m577[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m578[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m579[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m580[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m581[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m582[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m583[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m584[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m585[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m586[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m587[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m588[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m589[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m590[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m591[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m592[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m593[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m594[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m595[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m596[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m597[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m598[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m599[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m600[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m601[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m602[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m603[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m604[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m605[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m606[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m607[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m608[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m609[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m610[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m611[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m612[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m613[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m614[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m615[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m616[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m617[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m618[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m619[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m620[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m621[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m622[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m623[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m624[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m625[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m626[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m627[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m628[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m629[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m630[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m631[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m632[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m633[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m634[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m635[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m636[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m637[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m638[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m639[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m640[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m641[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m642[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m643[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m644[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m645[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m646[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m647[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m648[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m649[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m650[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m651[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m652[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m653[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m654[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m655[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m656[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m657[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m658[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m659[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m660[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m661[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m662[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m663[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m664[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m665[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m666[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m667[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m668[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m669[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m670[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10                    â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m671[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.ln_1               â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m672[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.attn               â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m673[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.attn.out_proj      â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m674[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.ls_1               â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m675[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.ln_1_kv            â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m676[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.ln_2               â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m677[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.mlp                â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m678[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.mlp.c_fc           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m679[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.mlp.gelu           â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m680[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.mlp.c_proj         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m681[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.ls_2               â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m682[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11                    â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m683[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.ln_1               â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m684[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.attn               â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m685[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.attn.out_proj      â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m686[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.ls_1               â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m687[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.ln_1_kv            â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m688[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.ln_2               â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m689[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.mlp                â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m690[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.mlp.c_fc           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m691[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.mlp.gelu           â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m692[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.mlp.c_proj         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m693[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.ls_2               â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m694[0m[2m [0mâ”‚ net.model.text_decoder.ln_final                         â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m695[0m[2m [0mâ”‚ net.tta_loss                                            â”‚ ByolLoss                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m696[0m[2m [0mâ”‚ net.video_proj                                          â”‚ VideoProjector                  â”‚  591 K â”‚ train â”‚
â”‚[2m [0m[2m697[0m[2m [0mâ”‚ net.video_proj.transform                                â”‚ Sequential                      â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m698[0m[2m [0mâ”‚ net.video_proj.transform.0                              â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m699[0m[2m [0mâ”‚ net.video_proj.transform.1                              â”‚ Dropout                         â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m700[0m[2m [0mâ”‚ net.fusion                                              â”‚ Fusion                          â”‚  6.2 K â”‚ train â”‚
â”‚[2m [0m[2m701[0m[2m [0mâ”‚ net.fusion.attn                                         â”‚ Sequential                      â”‚  6.2 K â”‚ train â”‚
â”‚[2m [0m[2m702[0m[2m [0mâ”‚ net.fusion.attn.0                                       â”‚ Linear                          â”‚  6.1 K â”‚ train â”‚
â”‚[2m [0m[2m703[0m[2m [0mâ”‚ net.fusion.attn.1                                       â”‚ Linear                          â”‚     10 â”‚ train â”‚
â”‚[2m [0m[2m704[0m[2m [0mâ”‚ net.fusion.attn.2                                       â”‚ Softmax                         â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m705[0m[2m [0mâ”‚ binary_acc                                              â”‚ BinaryAccuracy                  â”‚      0 â”‚ train â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜
[1mTrainable params[0m: 1.2 M
[1mNon-trainable params[0m: 637 M
[1mTotal params[0m: 639 M
[1mTotal estimated model params size (MB)[0m: 2.6 K
[1mModules in train mode[0m: 706
[1mModules in eval mode[0m: 0
/home/def/miniforge3/envs/fewshot/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.
`Trainer.fit` stopped: `max_epochs=0` reached.
[[36m2025-02-14 00:54:39,198[0m][[34m__main__[0m][[32mINFO[0m] - Starting testing![0m
[[36m2025-02-14 00:54:39,198[0m][[34m__main__[0m][[33mWARNING[0m] - Best ckpt not found! Using current weights for testing...[0m
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/def/miniforge3/envs/fewshot/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.
[2KStart testing...
[2KAttention weights: tensor([[0.5582, 0.4418],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 0/203 [2m0:00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
        [0.5540, 0.4460],
        [0.5547, 0.4453],
        [0.5570, 0.4430],
        [0.5559, 0.4441],
        [0.5534, 0.4466],
        [0.5526, 0.4474],
        [0.5577, 0.4423],
        [0.5582, 0.4418],
[2K        [0.5555, 0.4445]], device='cuda:0', grad_fn=<SoftmaxBackward0>):00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2K/home/def/fewshot/src/models/components/tt_method.py:317: UserWarning: The use of `x.T` on tensors of dimension other than 2
to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of
matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at
../aten/src/ATen/native/TensorShape.cpp:3683.)
  dot_product = (x @ y.T)
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 0/203 [2m0:00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5939, 0.4061]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KStep 0 - TTA Loss: 0.6590â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 0/203 [2m0:00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5942, 0.4058]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5942, 0.4058]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5943, 0.4057]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5944, 0.4056]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5946, 0.4054]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5948, 0.4052]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5950, 0.4050]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5951, 0.4049]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5953, 0.4047]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5955, 0.4045]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KStep 10 - TTA Loss: 0.6285â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 0/203 [2m0:00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5956, 0.4044]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5958, 0.4042]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5959, 0.4041]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5961, 0.4039]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5962, 0.4038]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5963, 0.4037]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5964, 0.4036]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5965, 0.4035]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5967, 0.4033]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5968, 0.4032]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KStep 20 - TTA Loss: 0.5399â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 0/203 [2m0:00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5968, 0.4032]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5969, 0.4031]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5970, 0.4030]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5970, 0.4030]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5971, 0.4029]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5971, 0.4029]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5971, 0.4029]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5971, 0.4029]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5971, 0.4029]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5535, 0.4465]], device='cuda:0')203 [2m0:00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.6173, 0.3827],â”â”â”â”â”â”â”â”â”â”â”[0m 0/203 [2m0:00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
        [0.5860, 0.4140],
        [0.5667, 0.4333],
        [0.6301, 0.3699],
        [0.5866, 0.4134],
        [0.5972, 0.4028],
        [0.5501, 0.4499],
        [0.6387, 0.3613],
        [0.6482, 0.3518],
[2K        [0.6368, 0.3632]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”[0m 0/203 [2m0:00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5581, 0.4419],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 1/203 [2m0:00:01 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
        [0.5539, 0.4461],
        [0.5546, 0.4454],
        [0.5569, 0.4431],
        [0.5558, 0.4442],
        [0.5532, 0.4468],
        [0.5525, 0.4475],
        [0.5577, 0.4423],
        [0.5581, 0.4419],
[2K        [0.5554, 0.4446]], device='cuda:0', grad_fn=<SoftmaxBackward0>):00:01 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 1/203 [2m0:00:01 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5931, 0.4069]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KStep 0 - TTA Loss: 0.8401â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 1/203 [2m0:00:01 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5931, 0.4069]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5931, 0.4069]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5931, 0.4069]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5932, 0.4068]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5932, 0.4068]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5932, 0.4068]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5933, 0.4067]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5934, 0.4066]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5934, 0.4066]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5936, 0.4064]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KStep 10 - TTA Loss: 0.8533â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 1/203 [2m0:00:01 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5937, 0.4063]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5939, 0.4061]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5941, 0.4059]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5944, 0.4056]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5946, 0.4054]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5950, 0.4050]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5953, 0.4047]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5957, 0.4043]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5962, 0.4038]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5967, 0.4033]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KStep 20 - TTA Loss: 0.8573â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 1/203 [2m0:00:01 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5973, 0.4027]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5979, 0.4021]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5985, 0.4015]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5992, 0.4008]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5999, 0.4001]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.6007, 0.3993]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.6015, 0.3985]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.6023, 0.3977]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.6031, 0.3969]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5536, 0.4464]], device='cuda:0')203 [2m0:00:01 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.6199, 0.3801],â”â”â”â”â”â”â”â”â”â”â”[0m 1/203 [2m0:00:01 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
        [0.5847, 0.4153],
        [0.5708, 0.4292],
        [0.6351, 0.3649],
        [0.5923, 0.4077],
        [0.6041, 0.3959],
        [0.5567, 0.4433],
        [0.6443, 0.3557],
        [0.6522, 0.3478],
[2K        [0.6410, 0.3590]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”[0m 1/203 [2m0:00:01 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5580, 0.4420],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 2/203 [2m0:00:02 â€¢ 0:03:28[0m [2;4m0.97it/s[0m  
        [0.5538, 0.4462],
        [0.5545, 0.4455],
        [0.5568, 0.4432],
        [0.5557, 0.4443],
        [0.5530, 0.4470],
        [0.5523, 0.4477],
        [0.5576, 0.4424],
        [0.5580, 0.4420],
[2K        [0.5553, 0.4447]], device='cuda:0', grad_fn=<SoftmaxBackward0>):00:02 â€¢ 0:03:28[0m [2;4m0.97it/s[0m  
[2KClass label: TennisSwingâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 2/203 [2m0:00:02 â€¢ 0:03:28[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6464, 0.3536]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.97it/s[0m  
[2KStep 0 - TTA Loss: 0.6042â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 2/203 [2m0:00:02 â€¢ 0:03:28[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6469, 0.3531]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6472, 0.3528]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6475, 0.3525]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6477, 0.3523]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6479, 0.3521]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6480, 0.3520]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6481, 0.3519]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6481, 0.3519]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6482, 0.3518]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6482, 0.3518]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.97it/s[0m  
[2KStep 10 - TTA Loss: 0.5349â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 2/203 [2m0:00:02 â€¢ 0:03:28[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6483, 0.3517]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6483, 0.3517]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6484, 0.3516]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6484, 0.3516]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6484, 0.3516]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6484, 0.3516]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6485, 0.3515]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6485, 0.3515]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6485, 0.3515]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6485, 0.3515]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.97it/s[0m  
[2KStep 20 - TTA Loss: 0.5276â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 2/203 [2m0:00:02 â€¢ 0:03:28[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6486, 0.3514]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6486, 0.3514]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6486, 0.3514]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6486, 0.3514]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6487, 0.3513]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6487, 0.3513]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6487, 0.3513]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6487, 0.3513]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6487, 0.3513]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.5577, 0.4423]], device='cuda:0')203 [2m0:00:02 â€¢ 0:03:28[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6201, 0.3799],â”â”â”â”â”â”â”â”â”â”â”[0m 2/203 [2m0:00:02 â€¢ 0:03:28[0m [2;4m0.97it/s[0m  
        [0.5842, 0.4158],
        [0.5719, 0.4281],
        [0.6325, 0.3675],
        [0.5898, 0.4102],
        [0.6008, 0.3992],
        [0.5523, 0.4477],
        [0.6425, 0.3575],
        [0.6487, 0.3513],
[2K        [0.6395, 0.3605]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”[0m 2/203 [2m0:00:02 â€¢ 0:03:28[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.5576, 0.4424],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 3/203 [2m0:00:03 â€¢ 0:03:18[0m [2;4m1.01it/s[0m  
        [0.5535, 0.4465],
        [0.5542, 0.4458],
        [0.5564, 0.4436],
        [0.5553, 0.4447],
        [0.5527, 0.4473],
        [0.5520, 0.4480],
        [0.5572, 0.4428],
        [0.5575, 0.4425],
[2K        [0.5550, 0.4450]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”[0m 3/203 [2m0:00:03 â€¢ 0:03:18[0m [2;4m1.01it/s[0m  
[2KClass label: HighJump[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 3/203 [2m0:00:03 â€¢ 0:03:18[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.5940, 0.4060]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:03 â€¢ 0:03:18[0m [2;4m1.01it/s[0m  
[2KStep 0 - TTA Loss: 0.5411[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 3/203 [2m0:00:03 â€¢ 0:03:18[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.5940, 0.4060]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:03 â€¢ 0:03:18[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.5940, 0.4060]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:03 â€¢ 0:03:18[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.5940, 0.4060]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:03 â€¢ 0:03:18[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.5940, 0.4060]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:03 â€¢ 0:03:18[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.5941, 0.4059]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:03 â€¢ 0:03:18[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.5941, 0.4059]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:03 â€¢ 0:03:18[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.5942, 0.4058]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:03 â€¢ 0:03:18[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.5943, 0.4057]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:03 â€¢ 0:03:18[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.5945, 0.4055]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:03 â€¢ 0:03:18[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.5947, 0.4053]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:03 â€¢ 0:03:18[0m [2;4m1.01it/s[0m  
[2KStep 10 - TTA Loss: 0.5005[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 3/203 [2m0:00:03 â€¢ 0:03:18[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.5950, 0.4050]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:03 â€¢ 0:03:18[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.5953, 0.4047]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:03 â€¢ 0:03:18[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.5958, 0.4042]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:03 â€¢ 0:03:18[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.5963, 0.4037]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:03 â€¢ 0:03:18[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.5968, 0.4032]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:03 â€¢ 0:03:18[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.5975, 0.4025]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:03 â€¢ 0:03:18[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.5982, 0.4018]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:03 â€¢ 0:03:18[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.5990, 0.4010]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:03 â€¢ 0:03:18[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.5999, 0.4001]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:03 â€¢ 0:03:18[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.6009, 0.3991]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:03 â€¢ 0:03:18[0m [2;4m1.01it/s[0m  
[2KStep 20 - TTA Loss: 0.5154[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 3/203 [2m0:00:03 â€¢ 0:03:18[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.6020, 0.3980]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:03 â€¢ 0:03:18[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.6031, 0.3969]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:03 â€¢ 0:03:18[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.6043, 0.3957]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:03 â€¢ 0:03:18[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.6056, 0.3944]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:03 â€¢ 0:03:18[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.6070, 0.3930]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:03 â€¢ 0:03:18[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.6084, 0.3916]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:03 â€¢ 0:03:18[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.6099, 0.3901]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:03 â€¢ 0:03:18[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.6114, 0.3886]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:03 â€¢ 0:03:18[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.6130, 0.3870]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:03 â€¢ 0:03:18[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.5541, 0.4459]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 3/203 [2m0:00:03 â€¢ 0:03:18[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.6316, 0.3684],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 3/203 [2m0:00:03 â€¢ 0:03:18[0m [2;4m1.01it/s[0m  
        [0.5920, 0.4080],
        [0.5826, 0.4174],
        [0.6434, 0.3566],
        [0.6005, 0.3995],
        [0.6147, 0.3853],
        [0.5674, 0.4326],
        [0.6527, 0.3473],
        [0.6593, 0.3407],
[2K        [0.6502, 0.3498]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 3/203 [2m0:00:03 â€¢ 0:03:18[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.5578, 0.4422],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 4/203 [2m0:00:05 â€¢ 0:04:17[0m [2;4m0.78it/s[0m  
        [0.5537, 0.4463],
        [0.5544, 0.4456],
        [0.5566, 0.4434],
        [0.5555, 0.4445],
        [0.5530, 0.4470],
        [0.5522, 0.4478],
        [0.5574, 0.4426],
        [0.5576, 0.4424],
[2K        [0.5551, 0.4449]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”[0m 4/203 [2m0:00:05 â€¢ 0:04:17[0m [2;4m0.78it/s[0m  
[2KClass label: GolfSwing[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 4/203 [2m0:00:05 â€¢ 0:04:17[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6325, 0.3675]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:17[0m [2;4m0.78it/s[0m  
[2KStep 0 - TTA Loss: 0.8861[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 4/203 [2m0:00:05 â€¢ 0:04:17[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6335, 0.3665]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:17[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6344, 0.3656]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:17[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6353, 0.3647]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:17[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6363, 0.3637]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:17[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6372, 0.3628]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:17[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6380, 0.3620]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:17[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6388, 0.3612]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:17[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6396, 0.3604]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:17[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6404, 0.3596]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:17[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6411, 0.3589]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:17[0m [2;4m0.78it/s[0m  
[2KStep 10 - TTA Loss: 0.8850[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 4/203 [2m0:00:05 â€¢ 0:04:17[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6418, 0.3582]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:17[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6424, 0.3576]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:17[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6430, 0.3570]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:17[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6436, 0.3564]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:17[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6441, 0.3559]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:17[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6446, 0.3554]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:17[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6450, 0.3550]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:17[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6454, 0.3546]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:17[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6457, 0.3543]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:17[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6460, 0.3540]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:17[0m [2;4m0.78it/s[0m  
[2KStep 20 - TTA Loss: 0.9173[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 4/203 [2m0:00:05 â€¢ 0:04:17[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6462, 0.3538]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:17[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6464, 0.3536]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:17[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6465, 0.3535]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:17[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6466, 0.3534]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:17[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6467, 0.3533]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:17[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6468, 0.3532]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:17[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6468, 0.3532]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:17[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6469, 0.3531]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:17[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6469, 0.3531]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:17[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.5578, 0.4422]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 4/203 [2m0:00:05 â€¢ 0:04:17[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6315, 0.3685],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 4/203 [2m0:00:05 â€¢ 0:04:17[0m [2;4m0.78it/s[0m  
        [0.5905, 0.4095],
        [0.5789, 0.4211],
        [0.6469, 0.3531],
        [0.6018, 0.3982],
        [0.6110, 0.3890],
        [0.5639, 0.4361],
        [0.6500, 0.3500],
        [0.6586, 0.3414],
[2K        [0.6520, 0.3480]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 4/203 [2m0:00:05 â€¢ 0:04:17[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.5580, 0.4420],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 5/203 [2m0:00:06 â€¢ 0:03:54[0m [2;4m0.85it/s[0m  
        [0.5538, 0.4462],
        [0.5546, 0.4454],
        [0.5568, 0.4432],
        [0.5558, 0.4442],
        [0.5534, 0.4466],
        [0.5525, 0.4475],
        [0.5576, 0.4424],
        [0.5578, 0.4422],
[2K        [0.5554, 0.4446]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”[0m 5/203 [2m0:00:06 â€¢ 0:03:54[0m [2;4m0.85it/s[0m  
[2KClass label: HammerThrowm[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 5/203 [2m0:00:06 â€¢ 0:03:54[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5821, 0.4179]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:06 â€¢ 0:03:54[0m [2;4m0.85it/s[0m  
[2KStep 0 - TTA Loss: 0.6397[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 5/203 [2m0:00:06 â€¢ 0:03:54[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5821, 0.4179]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:06 â€¢ 0:03:54[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5821, 0.4179]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:06 â€¢ 0:03:54[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5821, 0.4179]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:06 â€¢ 0:03:54[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5821, 0.4179]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:06 â€¢ 0:03:54[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5822, 0.4178]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:06 â€¢ 0:03:54[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5822, 0.4178]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:06 â€¢ 0:03:54[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5822, 0.4178]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:06 â€¢ 0:03:54[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5823, 0.4177]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:06 â€¢ 0:03:54[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5823, 0.4177]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:06 â€¢ 0:03:54[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5824, 0.4176]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:06 â€¢ 0:03:54[0m [2;4m0.85it/s[0m  
[2KStep 10 - TTA Loss: 0.6165[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 5/203 [2m0:00:06 â€¢ 0:03:54[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5824, 0.4176]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:06 â€¢ 0:03:54[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5825, 0.4175]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:06 â€¢ 0:03:54[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5826, 0.4174]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:06 â€¢ 0:03:54[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5827, 0.4173]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:06 â€¢ 0:03:54[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5828, 0.4172]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:06 â€¢ 0:03:54[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5829, 0.4171]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:06 â€¢ 0:03:54[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5830, 0.4170]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:06 â€¢ 0:03:54[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5831, 0.4169]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:06 â€¢ 0:03:54[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5833, 0.4167]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:06 â€¢ 0:03:54[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5835, 0.4165]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:06 â€¢ 0:03:54[0m [2;4m0.85it/s[0m  
[2KStep 20 - TTA Loss: 0.6181[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 5/203 [2m0:00:06 â€¢ 0:03:54[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5837, 0.4163]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:06 â€¢ 0:03:54[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5840, 0.4160]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:06 â€¢ 0:03:54[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5841, 0.4159]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:06 â€¢ 0:03:54[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5844, 0.4156]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:06 â€¢ 0:03:54[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5846, 0.4154]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:06 â€¢ 0:03:54[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5848, 0.4152]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:06 â€¢ 0:03:54[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5851, 0.4149]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:06 â€¢ 0:03:54[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5854, 0.4146]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:06 â€¢ 0:03:54[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5857, 0.4143]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:06 â€¢ 0:03:54[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5568, 0.4432]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 5/203 [2m0:00:06 â€¢ 0:03:54[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6152, 0.3848],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 5/203 [2m0:00:06 â€¢ 0:03:54[0m [2;4m0.85it/s[0m  
        [0.5809, 0.4191],
        [0.5682, 0.4318],
        [0.6307, 0.3693],
        [0.5861, 0.4139],
        [0.5948, 0.4052],
        [0.5485, 0.4515],
        [0.6386, 0.3614],
        [0.6474, 0.3526],
[2K        [0.6362, 0.3638]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 5/203 [2m0:00:06 â€¢ 0:03:54[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5585, 0.4415],m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:07 â€¢ 0:03:53[0m [2;4m0.85it/s[0m  
        [0.5542, 0.4458],
        [0.5551, 0.4449],
        [0.5572, 0.4428],
        [0.5566, 0.4434],
        [0.5538, 0.4462],
        [0.5530, 0.4470],
        [0.5580, 0.4420],
        [0.5582, 0.4418],
[2K        [0.5560, 0.4440]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:07 â€¢ 0:03:53[0m [2;4m0.85it/s[0m  
[2KClass label: HammerThrowm[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:07 â€¢ 0:03:53[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5900, 0.4100]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:07 â€¢ 0:03:53[0m [2;4m0.85it/s[0m  
[2KStep 0 - TTA Loss: 0.5797[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:07 â€¢ 0:03:53[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5903, 0.4097]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:07 â€¢ 0:03:53[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5907, 0.4093]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:07 â€¢ 0:03:53[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5912, 0.4088]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:07 â€¢ 0:03:53[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5916, 0.4084]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:07 â€¢ 0:03:53[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5921, 0.4079]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:07 â€¢ 0:03:53[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5926, 0.4074]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:07 â€¢ 0:03:53[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5930, 0.4070]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:07 â€¢ 0:03:53[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5935, 0.4065]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:07 â€¢ 0:03:53[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5940, 0.4060]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:07 â€¢ 0:03:53[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5945, 0.4055]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:07 â€¢ 0:03:53[0m [2;4m0.85it/s[0m  
[2KStep 10 - TTA Loss: 0.5680[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:07 â€¢ 0:03:53[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5950, 0.4050]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:07 â€¢ 0:03:53[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5955, 0.4045]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:07 â€¢ 0:03:53[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5960, 0.4040]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:07 â€¢ 0:03:53[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5964, 0.4036]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:07 â€¢ 0:03:53[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5968, 0.4032]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:07 â€¢ 0:03:53[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5971, 0.4029]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:07 â€¢ 0:03:53[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5975, 0.4025]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:07 â€¢ 0:03:53[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5978, 0.4022]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:07 â€¢ 0:03:53[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5980, 0.4020]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:07 â€¢ 0:03:53[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5983, 0.4017]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:07 â€¢ 0:03:53[0m [2;4m0.85it/s[0m  
[2KStep 20 - TTA Loss: 0.5552[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:07 â€¢ 0:03:53[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5985, 0.4015]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:07 â€¢ 0:03:53[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5987, 0.4013]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:07 â€¢ 0:03:53[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5988, 0.4012]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:07 â€¢ 0:03:53[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5989, 0.4011]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:07 â€¢ 0:03:53[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5990, 0.4010]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:07 â€¢ 0:03:53[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5990, 0.4010]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:07 â€¢ 0:03:53[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5991, 0.4009]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:07 â€¢ 0:03:53[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5991, 0.4009]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:07 â€¢ 0:03:53[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5991, 0.4009]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:07 â€¢ 0:03:53[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5582, 0.4418]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:07 â€¢ 0:03:53[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6249, 0.3751],8;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:07 â€¢ 0:03:53[0m [2;4m0.85it/s[0m  
        [0.5872, 0.4128],
        [0.5781, 0.4219],
        [0.6379, 0.3621],
        [0.5991, 0.4009],
        [0.6041, 0.3959],
        [0.5584, 0.4416],
        [0.6465, 0.3535],
        [0.6536, 0.3464],
[2K        [0.6456, 0.3544]], device='cuda:0')38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:07 â€¢ 0:03:53[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5591, 0.4409],m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:09 â€¢ 0:04:20[0m [2;4m0.76it/s[0m  
        [0.5545, 0.4455],
        [0.5557, 0.4443],
        [0.5576, 0.4424],
        [0.5575, 0.4425],
        [0.5543, 0.4457],
        [0.5536, 0.4464],
        [0.5584, 0.4416],
        [0.5587, 0.4413],
[2K        [0.5567, 0.4433]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:09 â€¢ 0:04:20[0m [2;4m0.76it/s[0m  
[2KClass label: Billiards[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:09 â€¢ 0:04:20[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.5855, 0.4145]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:09 â€¢ 0:04:20[0m [2;4m0.76it/s[0m  
[2KStep 0 - TTA Loss: 0.4442[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:09 â€¢ 0:04:20[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.5855, 0.4145]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:09 â€¢ 0:04:20[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.5855, 0.4145]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:09 â€¢ 0:04:20[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.5855, 0.4145]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:09 â€¢ 0:04:20[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.5855, 0.4145]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:09 â€¢ 0:04:20[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.5855, 0.4145]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:09 â€¢ 0:04:20[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.5855, 0.4145]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:09 â€¢ 0:04:20[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.5855, 0.4145]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:09 â€¢ 0:04:20[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.5854, 0.4146]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:09 â€¢ 0:04:20[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.5853, 0.4147]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:09 â€¢ 0:04:20[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.5852, 0.4148]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:09 â€¢ 0:04:20[0m [2;4m0.76it/s[0m  
[2KStep 10 - TTA Loss: 0.4404[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:09 â€¢ 0:04:20[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.5851, 0.4149]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:09 â€¢ 0:04:20[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.5850, 0.4150]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:09 â€¢ 0:04:20[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.5848, 0.4152]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:09 â€¢ 0:04:20[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.5846, 0.4154]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:09 â€¢ 0:04:20[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.5843, 0.4157]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:09 â€¢ 0:04:20[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.5840, 0.4160]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:09 â€¢ 0:04:20[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.5837, 0.4163]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:09 â€¢ 0:04:20[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.5834, 0.4166]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:09 â€¢ 0:04:20[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.5830, 0.4170]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:09 â€¢ 0:04:20[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.5825, 0.4175]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:09 â€¢ 0:04:20[0m [2;4m0.76it/s[0m  
[2KStep 20 - TTA Loss: 0.4168[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:09 â€¢ 0:04:20[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.5820, 0.4180]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:09 â€¢ 0:04:20[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.5815, 0.4185]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:09 â€¢ 0:04:20[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.5810, 0.4190]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:09 â€¢ 0:04:20[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.5804, 0.4196]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:09 â€¢ 0:04:20[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.5799, 0.4201]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:09 â€¢ 0:04:20[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.5793, 0.4207]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:09 â€¢ 0:04:20[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.5787, 0.4213]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:09 â€¢ 0:04:20[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.5781, 0.4219]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:09 â€¢ 0:04:20[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.5775, 0.4225]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:09 â€¢ 0:04:20[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.5541, 0.4459]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:09 â€¢ 0:04:20[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.6127, 0.3873],8;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:09 â€¢ 0:04:20[0m [2;4m0.76it/s[0m  
        [0.5769, 0.4231],
        [0.5680, 0.4320],
        [0.6289, 0.3711],
        [0.5838, 0.4162],
        [0.5917, 0.4083],
        [0.5455, 0.4545],
        [0.6361, 0.3639],
        [0.6459, 0.3541],
[2K        [0.6334, 0.3666]], device='cuda:0')38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:09 â€¢ 0:04:20[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.5589, 0.4411],mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:10 â€¢ 0:04:12[0m [2;4m0.78it/s[0m  
        [0.5541, 0.4459],
        [0.5556, 0.4444],
        [0.5574, 0.4426],
        [0.5575, 0.4425],
        [0.5542, 0.4458],
        [0.5535, 0.4465],
        [0.5583, 0.4417],
        [0.5585, 0.4415],
[2K        [0.5567, 0.4433]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:10 â€¢ 0:04:12[0m [2;4m0.78it/s[0m  
[2KClass label: BaseballPitch[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:10 â€¢ 0:04:12[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6161, 0.3839]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:10 â€¢ 0:04:12[0m [2;4m0.78it/s[0m  
[2KStep 0 - TTA Loss: 0.7422[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:10 â€¢ 0:04:12[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6162, 0.3838]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:10 â€¢ 0:04:12[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6164, 0.3836]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:10 â€¢ 0:04:12[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6167, 0.3833]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:10 â€¢ 0:04:12[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6172, 0.3828]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:10 â€¢ 0:04:12[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6178, 0.3822]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:10 â€¢ 0:04:12[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6185, 0.3815]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:10 â€¢ 0:04:12[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6193, 0.3807]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:10 â€¢ 0:04:12[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6202, 0.3798]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:10 â€¢ 0:04:12[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6211, 0.3789]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:10 â€¢ 0:04:12[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6220, 0.3780]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:10 â€¢ 0:04:12[0m [2;4m0.78it/s[0m  
[2KStep 10 - TTA Loss: 0.7389[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:10 â€¢ 0:04:12[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6228, 0.3772]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:10 â€¢ 0:04:12[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6237, 0.3763]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:10 â€¢ 0:04:12[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6246, 0.3754]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:10 â€¢ 0:04:12[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6254, 0.3746]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:10 â€¢ 0:04:12[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6263, 0.3737]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:10 â€¢ 0:04:12[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6270, 0.3730]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:10 â€¢ 0:04:12[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6277, 0.3723]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:10 â€¢ 0:04:12[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6284, 0.3716]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:10 â€¢ 0:04:12[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6290, 0.3710]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:10 â€¢ 0:04:12[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6295, 0.3705]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:10 â€¢ 0:04:12[0m [2;4m0.78it/s[0m  
[2KStep 20 - TTA Loss: 0.7164[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:10 â€¢ 0:04:12[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6299, 0.3701]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:10 â€¢ 0:04:12[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6303, 0.3697]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:10 â€¢ 0:04:12[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6306, 0.3694]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:10 â€¢ 0:04:12[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6308, 0.3692]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:10 â€¢ 0:04:12[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6310, 0.3690]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:10 â€¢ 0:04:12[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6311, 0.3689]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:10 â€¢ 0:04:12[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6312, 0.3688]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:10 â€¢ 0:04:12[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6313, 0.3687]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:10 â€¢ 0:04:12[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6313, 0.3687]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:10 â€¢ 0:04:12[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.5599, 0.4401]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:10 â€¢ 0:04:12[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.6313, 0.3687],0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:10 â€¢ 0:04:12[0m [2;4m0.78it/s[0m  
        [0.5814, 0.4186],
        [0.5749, 0.4251],
        [0.6398, 0.3602],
        [0.5960, 0.4040],
        [0.6032, 0.3968],
        [0.5551, 0.4449],
        [0.6440, 0.3560],
        [0.6547, 0.3453],
[2K        [0.6456, 0.3544]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:10 â€¢ 0:04:12[0m [2;4m0.78it/s[0m  
[2KAttention weights: tensor([[0.5592, 0.4408],mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 9/203 [2m0:00:11 â€¢ 0:04:03[0m [2;4m0.80it/s[0m  
        [0.5541, 0.4459],
        [0.5557, 0.4443],
        [0.5575, 0.4425],
        [0.5576, 0.4424],
        [0.5543, 0.4457],
        [0.5536, 0.4464],
        [0.5584, 0.4416],
        [0.5586, 0.4414],
[2K        [0.5568, 0.4432]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 9/203 [2m0:00:11 â€¢ 0:04:03[0m [2;4m0.80it/s[0m  
[2KClass label: BaseballPitch[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 9/203 [2m0:00:11 â€¢ 0:04:03[0m [2;4m0.80it/s[0m  
[2KAttention weights: tensor([[0.6153, 0.3847]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:11 â€¢ 0:04:03[0m [2;4m0.80it/s[0m  
[2KStep 0 - TTA Loss: 0.5477[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 9/203 [2m0:00:11 â€¢ 0:04:03[0m [2;4m0.80it/s[0m  
[2KAttention weights: tensor([[0.6153, 0.3847]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:11 â€¢ 0:04:03[0m [2;4m0.80it/s[0m  
[2KAttention weights: tensor([[0.6153, 0.3847]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:11 â€¢ 0:04:03[0m [2;4m0.80it/s[0m  
[2KAttention weights: tensor([[0.6153, 0.3847]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:11 â€¢ 0:04:03[0m [2;4m0.80it/s[0m  
[2KAttention weights: tensor([[0.6154, 0.3846]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:11 â€¢ 0:04:03[0m [2;4m0.80it/s[0m  
[2KAttention weights: tensor([[0.6154, 0.3846]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:11 â€¢ 0:04:03[0m [2;4m0.80it/s[0m  
[2KAttention weights: tensor([[0.6156, 0.3844]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:11 â€¢ 0:04:03[0m [2;4m0.80it/s[0m  
[2KAttention weights: tensor([[0.6158, 0.3842]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:11 â€¢ 0:04:03[0m [2;4m0.80it/s[0m  
[2KAttention weights: tensor([[0.6160, 0.3840]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:11 â€¢ 0:04:03[0m [2;4m0.80it/s[0m  
[2KAttention weights: tensor([[0.6164, 0.3836]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:11 â€¢ 0:04:03[0m [2;4m0.80it/s[0m  
[2KAttention weights: tensor([[0.6168, 0.3832]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:11 â€¢ 0:04:03[0m [2;4m0.80it/s[0m  
[2KStep 10 - TTA Loss: 0.5197[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 9/203 [2m0:00:11 â€¢ 0:04:03[0m [2;4m0.80it/s[0m  
[2KAttention weights: tensor([[0.6173, 0.3827]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:11 â€¢ 0:04:03[0m [2;4m0.80it/s[0m  
[2KAttention weights: tensor([[0.6178, 0.3822]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:11 â€¢ 0:04:03[0m [2;4m0.80it/s[0m  
[2KAttention weights: tensor([[0.6185, 0.3815]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:11 â€¢ 0:04:03[0m [2;4m0.80it/s[0m  
[2KAttention weights: tensor([[0.6193, 0.3807]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:11 â€¢ 0:04:03[0m [2;4m0.80it/s[0m  
[2KAttention weights: tensor([[0.6201, 0.3799]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:11 â€¢ 0:04:03[0m [2;4m0.80it/s[0m  
[2KAttention weights: tensor([[0.6210, 0.3790]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:11 â€¢ 0:04:03[0m [2;4m0.80it/s[0m  
[2KAttention weights: tensor([[0.6220, 0.3780]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:11 â€¢ 0:04:03[0m [2;4m0.80it/s[0m  
[2KAttention weights: tensor([[0.6231, 0.3769]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:11 â€¢ 0:04:03[0m [2;4m0.80it/s[0m  
[2KAttention weights: tensor([[0.6244, 0.3756]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:11 â€¢ 0:04:03[0m [2;4m0.80it/s[0m  
[2KAttention weights: tensor([[0.6257, 0.3743]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:11 â€¢ 0:04:03[0m [2;4m0.80it/s[0m  
[2KStep 20 - TTA Loss: 0.5485[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 9/203 [2m0:00:11 â€¢ 0:04:03[0m [2;4m0.80it/s[0m  
[2KAttention weights: tensor([[0.6271, 0.3729]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:11 â€¢ 0:04:03[0m [2;4m0.80it/s[0m  
[2KAttention weights: tensor([[0.6285, 0.3715]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:11 â€¢ 0:04:03[0m [2;4m0.80it/s[0m  
[2KAttention weights: tensor([[0.6300, 0.3700]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:11 â€¢ 0:04:03[0m [2;4m0.80it/s[0m  
[2KAttention weights: tensor([[0.6316, 0.3684]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:11 â€¢ 0:04:03[0m [2;4m0.80it/s[0m  
[2KAttention weights: tensor([[0.6333, 0.3667]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:11 â€¢ 0:04:03[0m [2;4m0.80it/s[0m  
[2KAttention weights: tensor([[0.6350, 0.3650]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:11 â€¢ 0:04:03[0m [2;4m0.80it/s[0m  
[2KAttention weights: tensor([[0.6367, 0.3633]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:11 â€¢ 0:04:03[0m [2;4m0.80it/s[0m  
[2KAttention weights: tensor([[0.6385, 0.3615]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:11 â€¢ 0:04:03[0m [2;4m0.80it/s[0m  
[2KAttention weights: tensor([[0.6403, 0.3597]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:11 â€¢ 0:04:03[0m [2;4m0.80it/s[0m  
[2KAttention weights: tensor([[0.5607, 0.4393]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 9/203 [2m0:00:11 â€¢ 0:04:03[0m [2;4m0.80it/s[0m  
[2KAttention weights: tensor([[0.6421, 0.3579],0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 9/203 [2m0:00:11 â€¢ 0:04:03[0m [2;4m0.80it/s[0m  
        [0.5870, 0.4130],
        [0.5770, 0.4230],
        [0.6455, 0.3545],
        [0.6030, 0.3970],
        [0.6084, 0.3916],
        [0.5611, 0.4389],
        [0.6490, 0.3510],
        [0.6595, 0.3405],
[2K        [0.6521, 0.3479]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 9/203 [2m0:00:11 â€¢ 0:04:03[0m [2;4m0.80it/s[0m  
[2KAttention weights: tensor([[0.5596, 0.4404],mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 10/203 [2m0:00:12 â€¢ 0:03:54[0m [2;4m0.83it/s[0m  
        [0.5543, 0.4457],
        [0.5559, 0.4441],
        [0.5578, 0.4422],
        [0.5578, 0.4422],
        [0.5545, 0.4455],
        [0.5538, 0.4462],
        [0.5587, 0.4413],
        [0.5589, 0.4411],
[2K        [0.5570, 0.4430]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 10/203 [2m0:00:12 â€¢ 0:03:54[0m [2;4m0.83it/s[0m  
[2KClass label: TennisSwingm[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 10/203 [2m0:00:12 â€¢ 0:03:54[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6461, 0.3539]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:12 â€¢ 0:03:54[0m [2;4m0.83it/s[0m  
[2KStep 0 - TTA Loss: 0.4680[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 10/203 [2m0:00:12 â€¢ 0:03:54[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6470, 0.3530]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:12 â€¢ 0:03:54[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6477, 0.3523]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:12 â€¢ 0:03:54[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6482, 0.3518]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:12 â€¢ 0:03:54[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6486, 0.3514]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:12 â€¢ 0:03:54[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6490, 0.3510]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:12 â€¢ 0:03:54[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6494, 0.3506]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:12 â€¢ 0:03:54[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6497, 0.3503]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:12 â€¢ 0:03:54[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6499, 0.3501]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:12 â€¢ 0:03:54[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6502, 0.3498]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:12 â€¢ 0:03:54[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6504, 0.3496]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:12 â€¢ 0:03:54[0m [2;4m0.83it/s[0m  
[2KStep 10 - TTA Loss: 0.4945[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 10/203 [2m0:00:12 â€¢ 0:03:54[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6506, 0.3494]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:12 â€¢ 0:03:54[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6509, 0.3491]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:12 â€¢ 0:03:54[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6511, 0.3489]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:12 â€¢ 0:03:54[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6513, 0.3487]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:12 â€¢ 0:03:54[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6514, 0.3486]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:12 â€¢ 0:03:54[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6514, 0.3486]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:12 â€¢ 0:03:54[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6515, 0.3485]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:12 â€¢ 0:03:54[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6516, 0.3484]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:12 â€¢ 0:03:54[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6517, 0.3483]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:12 â€¢ 0:03:54[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6517, 0.3483]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:12 â€¢ 0:03:54[0m [2;4m0.83it/s[0m  
[2KStep 20 - TTA Loss: 0.4089[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 10/203 [2m0:00:12 â€¢ 0:03:54[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6517, 0.3483]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:12 â€¢ 0:03:54[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6517, 0.3483]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:12 â€¢ 0:03:54[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6518, 0.3482]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:12 â€¢ 0:03:54[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6518, 0.3482]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:12 â€¢ 0:03:54[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6518, 0.3482]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:12 â€¢ 0:03:54[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6518, 0.3482]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:12 â€¢ 0:03:54[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6518, 0.3482]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:12 â€¢ 0:03:54[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6518, 0.3482]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:12 â€¢ 0:03:54[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6518, 0.3482]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:12 â€¢ 0:03:54[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.5594, 0.4406]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 10/203 [2m0:00:12 â€¢ 0:03:54[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6299, 0.3701],0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 10/203 [2m0:00:12 â€¢ 0:03:54[0m [2;4m0.83it/s[0m  
        [0.5858, 0.4142],
        [0.5770, 0.4230],
        [0.6391, 0.3609],
        [0.5975, 0.4025],
        [0.6059, 0.3941],
        [0.5590, 0.4410],
        [0.6448, 0.3552],
        [0.6518, 0.3482],
[2K        [0.6469, 0.3531]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 10/203 [2m0:00:12 â€¢ 0:03:54[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.5598, 0.4402],0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 11/203 [2m0:00:13 â€¢ 0:03:47[0m [2;4m0.85it/s[0m   
        [0.5545, 0.4455],
        [0.5560, 0.4440],
        [0.5580, 0.4420],
        [0.5580, 0.4420],
        [0.5547, 0.4453],
        [0.5540, 0.4460],
        [0.5589, 0.4411],
        [0.5591, 0.4409],
[2K        [0.5572, 0.4428]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 11/203 [2m0:00:13 â€¢ 0:03:47[0m [2;4m0.85it/s[0m  
[2KClass label: HammerThrow0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 11/203 [2m0:00:13 â€¢ 0:03:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5833, 0.4167]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:13 â€¢ 0:03:47[0m [2;4m0.85it/s[0m  
[2KStep 0 - TTA Loss: 0.5777m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 11/203 [2m0:00:13 â€¢ 0:03:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5833, 0.4167]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:13 â€¢ 0:03:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5833, 0.4167]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:13 â€¢ 0:03:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5833, 0.4167]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:13 â€¢ 0:03:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5834, 0.4166]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:13 â€¢ 0:03:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5834, 0.4166]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:13 â€¢ 0:03:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5835, 0.4165]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:13 â€¢ 0:03:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5836, 0.4164]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:13 â€¢ 0:03:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5837, 0.4163]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:13 â€¢ 0:03:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5840, 0.4160]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:13 â€¢ 0:03:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5843, 0.4157]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:13 â€¢ 0:03:47[0m [2;4m0.85it/s[0m  
[2KStep 10 - TTA Loss: 0.5739[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 11/203 [2m0:00:13 â€¢ 0:03:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5846, 0.4154]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:13 â€¢ 0:03:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5851, 0.4149]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:13 â€¢ 0:03:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5856, 0.4144]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:13 â€¢ 0:03:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5862, 0.4138]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:13 â€¢ 0:03:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5870, 0.4130]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:13 â€¢ 0:03:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5878, 0.4122]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:13 â€¢ 0:03:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5888, 0.4112]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:13 â€¢ 0:03:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5898, 0.4102]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:13 â€¢ 0:03:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5910, 0.4090]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:13 â€¢ 0:03:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5922, 0.4078]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:13 â€¢ 0:03:47[0m [2;4m0.85it/s[0m  
[2KStep 20 - TTA Loss: 0.5703[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 11/203 [2m0:00:13 â€¢ 0:03:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5936, 0.4064]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:13 â€¢ 0:03:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5951, 0.4049]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:13 â€¢ 0:03:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5966, 0.4034]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:13 â€¢ 0:03:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5983, 0.4017]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:13 â€¢ 0:03:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6000, 0.4000]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:13 â€¢ 0:03:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6018, 0.3982]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:13 â€¢ 0:03:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6036, 0.3964]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:13 â€¢ 0:03:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6055, 0.3945]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:13 â€¢ 0:03:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6075, 0.3925]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:13 â€¢ 0:03:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5599, 0.4401]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 11/203 [2m0:00:13 â€¢ 0:03:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6305, 0.3695],38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 11/203 [2m0:00:13 â€¢ 0:03:47[0m [2;4m0.85it/s[0m  
        [0.5893, 0.4107],
        [0.5849, 0.4151],
        [0.6429, 0.3571],
        [0.6095, 0.3905],
        [0.6109, 0.3891],
        [0.5651, 0.4349],
        [0.6492, 0.3508],
        [0.6565, 0.3435],
[2K        [0.6533, 0.3467]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 11/203 [2m0:00:13 â€¢ 0:03:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5603, 0.4397],0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 12/203 [2m0:00:14 â€¢ 0:03:42[0m [2;4m0.86it/s[0m  
        [0.5548, 0.4452],
        [0.5565, 0.4435],
        [0.5583, 0.4417],
        [0.5588, 0.4412],
        [0.5552, 0.4448],
        [0.5546, 0.4454],
        [0.5593, 0.4407],
        [0.5595, 0.4405],
[2K        [0.5578, 0.4422]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 12/203 [2m0:00:14 â€¢ 0:03:42[0m [2;4m0.86it/s[0m  
[2KClass label: CleanAndJerkm[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 12/203 [2m0:00:14 â€¢ 0:03:42[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5729, 0.4271]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:14 â€¢ 0:03:42[0m [2;4m0.86it/s[0m  
[2KStep 0 - TTA Loss: 0.6138m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 12/203 [2m0:00:14 â€¢ 0:03:42[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5744, 0.4256]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:14 â€¢ 0:03:42[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5759, 0.4241]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:14 â€¢ 0:03:42[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5776, 0.4224]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:14 â€¢ 0:03:42[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5793, 0.4207]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:14 â€¢ 0:03:42[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5810, 0.4190]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:14 â€¢ 0:03:42[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5828, 0.4172]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:14 â€¢ 0:03:42[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5845, 0.4155]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:14 â€¢ 0:03:42[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5862, 0.4138]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:14 â€¢ 0:03:42[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5879, 0.4121]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:14 â€¢ 0:03:42[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5896, 0.4104]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:14 â€¢ 0:03:42[0m [2;4m0.86it/s[0m  
[2KStep 10 - TTA Loss: 0.5924[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 12/203 [2m0:00:14 â€¢ 0:03:42[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5912, 0.4088]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:14 â€¢ 0:03:42[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5927, 0.4073]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:14 â€¢ 0:03:42[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5941, 0.4059]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:14 â€¢ 0:03:42[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5955, 0.4045]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:14 â€¢ 0:03:42[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5967, 0.4033]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:14 â€¢ 0:03:42[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5979, 0.4021]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:14 â€¢ 0:03:42[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5990, 0.4010]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:14 â€¢ 0:03:42[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5999, 0.4001]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:14 â€¢ 0:03:42[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6007, 0.3993]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:14 â€¢ 0:03:42[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6014, 0.3986]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:14 â€¢ 0:03:42[0m [2;4m0.86it/s[0m  
[2KStep 20 - TTA Loss: 0.5589[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 12/203 [2m0:00:14 â€¢ 0:03:42[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6021, 0.3979]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:14 â€¢ 0:03:42[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6026, 0.3974]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:14 â€¢ 0:03:42[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6030, 0.3970]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:14 â€¢ 0:03:42[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6033, 0.3967]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:14 â€¢ 0:03:42[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6036, 0.3964]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:14 â€¢ 0:03:42[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6037, 0.3963]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:14 â€¢ 0:03:42[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6038, 0.3962]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:14 â€¢ 0:03:42[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6039, 0.3961]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:14 â€¢ 0:03:42[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6039, 0.3961]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:14 â€¢ 0:03:42[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5586, 0.4414]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 12/203 [2m0:00:14 â€¢ 0:03:42[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6341, 0.3659],38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 12/203 [2m0:00:14 â€¢ 0:03:42[0m [2;4m0.86it/s[0m  
        [0.5943, 0.4057],
        [0.6040, 0.3960],
        [0.6494, 0.3506],
        [0.6133, 0.3867],
        [0.6187, 0.3813],
        [0.5718, 0.4282],
        [0.6531, 0.3469],
        [0.6647, 0.3353],
[2K        [0.6579, 0.3421]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 12/203 [2m0:00:14 â€¢ 0:03:42[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5607, 0.4393],4mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 13/203 [2m0:00:15 â€¢ 0:03:36[0m [2;4m0.88it/s[0m  
        [0.5553, 0.4447],
        [0.5574, 0.4426],
        [0.5588, 0.4412],
        [0.5596, 0.4404],
        [0.5558, 0.4442],
        [0.5552, 0.4448],
        [0.5598, 0.4402],
        [0.5600, 0.4400],
[2K        [0.5586, 0.4414]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 13/203 [2m0:00:15 â€¢ 0:03:36[0m [2;4m0.88it/s[0m  
[2KClass label: BaseballPitch[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 13/203 [2m0:00:15 â€¢ 0:03:36[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6158, 0.3842]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:15 â€¢ 0:03:36[0m [2;4m0.88it/s[0m  
[2KStep 0 - TTA Loss: 0.8630m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 13/203 [2m0:00:15 â€¢ 0:03:36[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6158, 0.3842]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:15 â€¢ 0:03:36[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6158, 0.3842]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:15 â€¢ 0:03:36[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6158, 0.3842]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:15 â€¢ 0:03:36[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6159, 0.3841]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:15 â€¢ 0:03:36[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6160, 0.3840]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:15 â€¢ 0:03:36[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6162, 0.3838]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:15 â€¢ 0:03:36[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6165, 0.3835]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:15 â€¢ 0:03:36[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6169, 0.3831]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:15 â€¢ 0:03:36[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6175, 0.3825]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:15 â€¢ 0:03:36[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6182, 0.3818]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:15 â€¢ 0:03:36[0m [2;4m0.88it/s[0m  
[2KStep 10 - TTA Loss: 0.7826[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 13/203 [2m0:00:15 â€¢ 0:03:36[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6192, 0.3808]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:15 â€¢ 0:03:36[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6203, 0.3797]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:15 â€¢ 0:03:36[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6216, 0.3784]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:15 â€¢ 0:03:36[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6232, 0.3768]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:15 â€¢ 0:03:36[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6250, 0.3750]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:15 â€¢ 0:03:36[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6271, 0.3729]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:15 â€¢ 0:03:36[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6294, 0.3706]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:15 â€¢ 0:03:36[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6319, 0.3681]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:15 â€¢ 0:03:36[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6347, 0.3653]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:15 â€¢ 0:03:36[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6378, 0.3622]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:15 â€¢ 0:03:36[0m [2;4m0.88it/s[0m  
[2KStep 20 - TTA Loss: 0.8641[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 13/203 [2m0:00:15 â€¢ 0:03:36[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6411, 0.3589]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:15 â€¢ 0:03:36[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6446, 0.3554]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:15 â€¢ 0:03:36[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6483, 0.3517]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:15 â€¢ 0:03:36[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6523, 0.3477]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:15 â€¢ 0:03:36[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6565, 0.3435]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:15 â€¢ 0:03:36[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6608, 0.3392]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:15 â€¢ 0:03:36[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6653, 0.3347]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:15 â€¢ 0:03:36[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6699, 0.3301]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:15 â€¢ 0:03:36[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6746, 0.3254]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:15 â€¢ 0:03:36[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.5647, 0.4353]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 13/203 [2m0:00:15 â€¢ 0:03:36[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6795, 0.3205],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 13/203 [2m0:00:15 â€¢ 0:03:36[0m [2;4m0.88it/s[0m  
        [0.6099, 0.3901],
        [0.5992, 0.4008],
        [0.6738, 0.3262],
        [0.6304, 0.3696],
        [0.6351, 0.3649],
        [0.5861, 0.4139],
        [0.6699, 0.3301],
        [0.6850, 0.3150],
[2K        [0.6807, 0.3193]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 13/203 [2m0:00:15 â€¢ 0:03:36[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.5621, 0.4379],4mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:16 â€¢ 0:03:37[0m [2;4m0.87it/s[0m  
        [0.5560, 0.4440],
        [0.5581, 0.4419],
        [0.5596, 0.4404],
        [0.5604, 0.4396],
        [0.5565, 0.4435],
        [0.5560, 0.4440],
        [0.5606, 0.4394],
        [0.5608, 0.4392],
[2K        [0.5594, 0.4406]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:16 â€¢ 0:03:37[0m [2;4m0.87it/s[0m  
[2KClass label: GolfSwing[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:16 â€¢ 0:03:37[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6302, 0.3698]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:16 â€¢ 0:03:37[0m [2;4m0.87it/s[0m  
[2KStep 0 - TTA Loss: 0.8623m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:16 â€¢ 0:03:37[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6333, 0.3667]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:16 â€¢ 0:03:37[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6362, 0.3638]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:16 â€¢ 0:03:37[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6387, 0.3613]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:16 â€¢ 0:03:37[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6409, 0.3591]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:16 â€¢ 0:03:37[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6429, 0.3571]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:16 â€¢ 0:03:37[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6446, 0.3554]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:16 â€¢ 0:03:37[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6461, 0.3539]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:16 â€¢ 0:03:37[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6474, 0.3526]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:16 â€¢ 0:03:37[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6485, 0.3515]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:16 â€¢ 0:03:37[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6495, 0.3505]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:16 â€¢ 0:03:37[0m [2;4m0.87it/s[0m  
[2KStep 10 - TTA Loss: 0.8633[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:16 â€¢ 0:03:37[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6503, 0.3497]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:16 â€¢ 0:03:37[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6510, 0.3490]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:16 â€¢ 0:03:37[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6516, 0.3484]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:16 â€¢ 0:03:37[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6521, 0.3479]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:16 â€¢ 0:03:37[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6525, 0.3475]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:16 â€¢ 0:03:37[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6528, 0.3472]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:16 â€¢ 0:03:37[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6531, 0.3469]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:16 â€¢ 0:03:37[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6533, 0.3467]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:16 â€¢ 0:03:37[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6535, 0.3465]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:16 â€¢ 0:03:37[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6536, 0.3464]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:16 â€¢ 0:03:37[0m [2;4m0.87it/s[0m  
[2KStep 20 - TTA Loss: 0.8549[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:16 â€¢ 0:03:37[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6538, 0.3462]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:16 â€¢ 0:03:37[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6538, 0.3462]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:16 â€¢ 0:03:37[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6539, 0.3461]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:16 â€¢ 0:03:37[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6540, 0.3460]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:16 â€¢ 0:03:37[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6540, 0.3460]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:16 â€¢ 0:03:37[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6540, 0.3460]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:16 â€¢ 0:03:37[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6540, 0.3460]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:16 â€¢ 0:03:37[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6540, 0.3460]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:16 â€¢ 0:03:37[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6540, 0.3460]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:16 â€¢ 0:03:37[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5614, 0.4386]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:16 â€¢ 0:03:37[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6534, 0.3466],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:16 â€¢ 0:03:37[0m [2;4m0.87it/s[0m  
        [0.5941, 0.4059],
        [0.5852, 0.4148],
        [0.6541, 0.3459],
        [0.6118, 0.3882],
        [0.6183, 0.3817],
        [0.5717, 0.4283],
        [0.6589, 0.3411],
        [0.6679, 0.3321],
[2K        [0.6618, 0.3382]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:16 â€¢ 0:03:37[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5629, 0.4371],4mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 15/203 [2m0:00:17 â€¢ 0:03:33[0m [2;4m0.89it/s[0m  
        [0.5564, 0.4436],
        [0.5585, 0.4415],
        [0.5602, 0.4398],
        [0.5609, 0.4391],
        [0.5570, 0.4430],
        [0.5564, 0.4436],
        [0.5612, 0.4388],
        [0.5613, 0.4387],
[2K        [0.5599, 0.4401]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 15/203 [2m0:00:17 â€¢ 0:03:33[0m [2;4m0.89it/s[0m  
[2KClass label: HighJumpâ”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 15/203 [2m0:00:17 â€¢ 0:03:33[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6022, 0.3978]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:17 â€¢ 0:03:33[0m [2;4m0.89it/s[0m  
[2KStep 0 - TTA Loss: 1.0135m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 15/203 [2m0:00:17 â€¢ 0:03:33[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6022, 0.3978]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:17 â€¢ 0:03:33[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6022, 0.3978]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:17 â€¢ 0:03:33[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6022, 0.3978]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:17 â€¢ 0:03:33[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6022, 0.3978]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:17 â€¢ 0:03:33[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6022, 0.3978]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:17 â€¢ 0:03:33[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6022, 0.3978]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:17 â€¢ 0:03:33[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6023, 0.3977]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:17 â€¢ 0:03:33[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6024, 0.3976]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:17 â€¢ 0:03:33[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6025, 0.3975]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:17 â€¢ 0:03:33[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6027, 0.3973]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:17 â€¢ 0:03:33[0m [2;4m0.89it/s[0m  
[2KStep 10 - TTA Loss: 0.9067[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 15/203 [2m0:00:17 â€¢ 0:03:33[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6028, 0.3972]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:17 â€¢ 0:03:33[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6031, 0.3969]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:17 â€¢ 0:03:33[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6034, 0.3966]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:17 â€¢ 0:03:33[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6037, 0.3963]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:17 â€¢ 0:03:33[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6041, 0.3959]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:17 â€¢ 0:03:33[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6046, 0.3954]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:17 â€¢ 0:03:33[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6051, 0.3949]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:17 â€¢ 0:03:33[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6056, 0.3944]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:17 â€¢ 0:03:33[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6063, 0.3937]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:17 â€¢ 0:03:33[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6069, 0.3931]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:17 â€¢ 0:03:33[0m [2;4m0.89it/s[0m  
[2KStep 20 - TTA Loss: 1.0763[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 15/203 [2m0:00:17 â€¢ 0:03:33[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6077, 0.3923]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:17 â€¢ 0:03:33[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6085, 0.3915]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:17 â€¢ 0:03:33[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6094, 0.3906]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:17 â€¢ 0:03:33[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6103, 0.3897]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:17 â€¢ 0:03:33[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6113, 0.3887]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:17 â€¢ 0:03:33[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6123, 0.3877]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:17 â€¢ 0:03:33[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6134, 0.3866]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:17 â€¢ 0:03:33[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6145, 0.3855]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:17 â€¢ 0:03:33[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6156, 0.3844]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:17 â€¢ 0:03:33[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.5581, 0.4419]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 15/203 [2m0:00:17 â€¢ 0:03:33[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6322, 0.3678],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 15/203 [2m0:00:17 â€¢ 0:03:33[0m [2;4m0.89it/s[0m  
        [0.5873, 0.4127],
        [0.5811, 0.4189],
        [0.6437, 0.3563],
        [0.6036, 0.3964],
        [0.6168, 0.3832],
        [0.5678, 0.4322],
        [0.6526, 0.3474],
        [0.6607, 0.3393],
[2K        [0.6524, 0.3476]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 15/203 [2m0:00:17 â€¢ 0:03:33[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.5630, 0.4370],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 16/203 [2m0:00:18 â€¢ 0:03:29[0m [2;4m0.90it/s[0m   
        [0.5565, 0.4435],
        [0.5586, 0.4414],
        [0.5603, 0.4397],
        [0.5610, 0.4390],
        [0.5572, 0.4428],
        [0.5566, 0.4434],
        [0.5613, 0.4387],
        [0.5614, 0.4386],
[2K        [0.5600, 0.4400]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 16/203 [2m0:00:18 â€¢ 0:03:29[0m [2;4m0.90it/s[0m  
[2KClass label: HighJumpâ”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 16/203 [2m0:00:18 â€¢ 0:03:29[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5954, 0.4046]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:18 â€¢ 0:03:29[0m [2;4m0.90it/s[0m  
[2KStep 0 - TTA Loss: 0.74410m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 16/203 [2m0:00:18 â€¢ 0:03:29[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5964, 0.4036]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:18 â€¢ 0:03:29[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5974, 0.4026]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:18 â€¢ 0:03:29[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5982, 0.4018]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:18 â€¢ 0:03:29[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5990, 0.4010]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:18 â€¢ 0:03:29[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5997, 0.4003]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:18 â€¢ 0:03:29[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6003, 0.3997]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:18 â€¢ 0:03:29[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6009, 0.3991]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:18 â€¢ 0:03:29[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6014, 0.3986]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:18 â€¢ 0:03:29[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6019, 0.3981]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:18 â€¢ 0:03:29[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6024, 0.3976]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:18 â€¢ 0:03:29[0m [2;4m0.90it/s[0m  
[2KStep 10 - TTA Loss: 0.7777m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 16/203 [2m0:00:18 â€¢ 0:03:29[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6028, 0.3972]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:18 â€¢ 0:03:29[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6031, 0.3969]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:18 â€¢ 0:03:29[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6034, 0.3966]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:18 â€¢ 0:03:29[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6037, 0.3963]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:18 â€¢ 0:03:29[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6039, 0.3961]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:18 â€¢ 0:03:29[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6041, 0.3959]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:18 â€¢ 0:03:29[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6043, 0.3957]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:18 â€¢ 0:03:29[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6045, 0.3955]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:18 â€¢ 0:03:29[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6046, 0.3954]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:18 â€¢ 0:03:29[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6047, 0.3953]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:18 â€¢ 0:03:29[0m [2;4m0.90it/s[0m  
[2KStep 20 - TTA Loss: 0.7862m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 16/203 [2m0:00:18 â€¢ 0:03:29[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6048, 0.3952]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:18 â€¢ 0:03:29[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6049, 0.3951]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:18 â€¢ 0:03:29[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6050, 0.3950]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:18 â€¢ 0:03:29[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6050, 0.3950]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:18 â€¢ 0:03:29[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6051, 0.3949]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:18 â€¢ 0:03:29[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6051, 0.3949]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:18 â€¢ 0:03:29[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6051, 0.3949]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:18 â€¢ 0:03:29[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6051, 0.3949]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:18 â€¢ 0:03:29[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6051, 0.3949]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:18 â€¢ 0:03:29[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5578, 0.4422]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 16/203 [2m0:00:18 â€¢ 0:03:29[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6207, 0.3793],[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 16/203 [2m0:00:18 â€¢ 0:03:29[0m [2;4m0.90it/s[0m  
        [0.5854, 0.4146],
        [0.5740, 0.4260],
        [0.6335, 0.3665],
        [0.5927, 0.4073],
        [0.6052, 0.3948],
        [0.5592, 0.4408],
        [0.6454, 0.3546],
        [0.6495, 0.3505],
[2K        [0.6411, 0.3589]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 16/203 [2m0:00:18 â€¢ 0:03:29[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5630, 0.4370],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 17/203 [2m0:00:19 â€¢ 0:03:26[0m [2;4m0.90it/s[0m  
        [0.5565, 0.4435],
        [0.5587, 0.4413],
        [0.5604, 0.4396],
        [0.5611, 0.4389],
        [0.5573, 0.4427],
        [0.5567, 0.4433],
        [0.5613, 0.4387],
        [0.5615, 0.4385],
[2K        [0.5601, 0.4399]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 17/203 [2m0:00:19 â€¢ 0:03:26[0m [2;4m0.90it/s[0m  
[2KClass label: GolfSwingâ”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 17/203 [2m0:00:19 â€¢ 0:03:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6310, 0.3690]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:19 â€¢ 0:03:26[0m [2;4m0.90it/s[0m  
[2KStep 0 - TTA Loss: 0.68140m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 17/203 [2m0:00:19 â€¢ 0:03:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6310, 0.3690]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:19 â€¢ 0:03:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6311, 0.3689]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:19 â€¢ 0:03:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6311, 0.3689]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:19 â€¢ 0:03:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6311, 0.3689]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:19 â€¢ 0:03:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6311, 0.3689]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:19 â€¢ 0:03:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6311, 0.3689]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:19 â€¢ 0:03:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6311, 0.3689]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:19 â€¢ 0:03:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6311, 0.3689]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:19 â€¢ 0:03:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6311, 0.3689]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:19 â€¢ 0:03:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6311, 0.3689]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:19 â€¢ 0:03:26[0m [2;4m0.90it/s[0m  
[2KStep 10 - TTA Loss: 0.6897m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 17/203 [2m0:00:19 â€¢ 0:03:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6311, 0.3689]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:19 â€¢ 0:03:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6312, 0.3688]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:19 â€¢ 0:03:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6312, 0.3688]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:19 â€¢ 0:03:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6312, 0.3688]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:19 â€¢ 0:03:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6312, 0.3688]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:19 â€¢ 0:03:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6313, 0.3687]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:19 â€¢ 0:03:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6313, 0.3687]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:19 â€¢ 0:03:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6313, 0.3687]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:19 â€¢ 0:03:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6314, 0.3686]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:19 â€¢ 0:03:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6314, 0.3686]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:19 â€¢ 0:03:26[0m [2;4m0.90it/s[0m  
[2KStep 20 - TTA Loss: 0.6995m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 17/203 [2m0:00:19 â€¢ 0:03:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6315, 0.3685]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:19 â€¢ 0:03:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6315, 0.3685]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:19 â€¢ 0:03:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6316, 0.3684]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:19 â€¢ 0:03:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6317, 0.3683]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:19 â€¢ 0:03:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6317, 0.3683]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:19 â€¢ 0:03:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6318, 0.3682]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:19 â€¢ 0:03:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6319, 0.3681]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:19 â€¢ 0:03:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6320, 0.3680]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:19 â€¢ 0:03:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6321, 0.3679]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:19 â€¢ 0:03:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5603, 0.4397]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 17/203 [2m0:00:19 â€¢ 0:03:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6207, 0.3793],[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 17/203 [2m0:00:19 â€¢ 0:03:26[0m [2;4m0.90it/s[0m  
        [0.5830, 0.4170],
        [0.5716, 0.4284],
        [0.6323, 0.3677],
        [0.5898, 0.4102],
        [0.5980, 0.4020],
        [0.5505, 0.4495],
        [0.6382, 0.3618],
        [0.6503, 0.3497],
[2K        [0.6404, 0.3596]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 17/203 [2m0:00:19 â€¢ 0:03:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5628, 0.4372],24mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 18/203 [2m0:00:20 â€¢ 0:03:24[0m [2;4m0.91it/s[0m  
        [0.5563, 0.4437],
        [0.5585, 0.4415],
        [0.5600, 0.4400],
        [0.5609, 0.4391],
        [0.5572, 0.4428],
        [0.5565, 0.4435],
        [0.5611, 0.4389],
        [0.5612, 0.4388],
[2K        [0.5599, 0.4401]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 18/203 [2m0:00:20 â€¢ 0:03:24[0m [2;4m0.91it/s[0m  
[2KClass label: ThrowDiscus[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 18/203 [2m0:00:20 â€¢ 0:03:24[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6385, 0.3615]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:20 â€¢ 0:03:24[0m [2;4m0.91it/s[0m  
[2KStep 0 - TTA Loss: 0.47900m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 18/203 [2m0:00:20 â€¢ 0:03:24[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6385, 0.3615]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:20 â€¢ 0:03:24[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6383, 0.3617]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:20 â€¢ 0:03:24[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6382, 0.3618]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:20 â€¢ 0:03:24[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6380, 0.3620]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:20 â€¢ 0:03:24[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6377, 0.3623]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:20 â€¢ 0:03:24[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6374, 0.3626]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:20 â€¢ 0:03:24[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6370, 0.3630]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:20 â€¢ 0:03:24[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6366, 0.3634]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:20 â€¢ 0:03:24[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6362, 0.3638]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:20 â€¢ 0:03:24[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6360, 0.3640]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:20 â€¢ 0:03:24[0m [2;4m0.91it/s[0m  
[2KStep 10 - TTA Loss: 0.4878m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 18/203 [2m0:00:20 â€¢ 0:03:24[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6358, 0.3642]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:20 â€¢ 0:03:24[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6356, 0.3644]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:20 â€¢ 0:03:24[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6355, 0.3645]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:20 â€¢ 0:03:24[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6353, 0.3647]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:20 â€¢ 0:03:24[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6351, 0.3649]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:20 â€¢ 0:03:24[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6349, 0.3651]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:20 â€¢ 0:03:24[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6347, 0.3653]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:20 â€¢ 0:03:24[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6345, 0.3655]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:20 â€¢ 0:03:24[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6344, 0.3656]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:20 â€¢ 0:03:24[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6342, 0.3658]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:20 â€¢ 0:03:24[0m [2;4m0.91it/s[0m  
[2KStep 20 - TTA Loss: 0.4548m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 18/203 [2m0:00:20 â€¢ 0:03:24[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6341, 0.3659]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:20 â€¢ 0:03:24[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6340, 0.3660]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:20 â€¢ 0:03:24[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6340, 0.3660]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:20 â€¢ 0:03:24[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6339, 0.3661]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:20 â€¢ 0:03:24[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6339, 0.3661]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:20 â€¢ 0:03:24[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6339, 0.3661]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:20 â€¢ 0:03:24[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6339, 0.3661]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:20 â€¢ 0:03:24[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6339, 0.3661]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:20 â€¢ 0:03:24[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6339, 0.3661]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:20 â€¢ 0:03:24[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5597, 0.4403]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 18/203 [2m0:00:20 â€¢ 0:03:24[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6151, 0.3849],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 18/203 [2m0:00:20 â€¢ 0:03:24[0m [2;4m0.91it/s[0m  
        [0.5823, 0.4177],
        [0.5693, 0.4307],
        [0.6291, 0.3709],
        [0.5843, 0.4157],
        [0.5930, 0.4070],
        [0.5471, 0.4529],
        [0.6359, 0.3641],
        [0.6458, 0.3542],
[2K        [0.6339, 0.3661]], device='cuda:0')â•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 18/203 [2m0:00:20 â€¢ 0:03:24[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5624, 0.4376],24mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 19/203 [2m0:00:21 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
        [0.5561, 0.4439],
        [0.5582, 0.4418],
        [0.5596, 0.4404],
        [0.5605, 0.4395],
        [0.5569, 0.4431],
        [0.5562, 0.4438],
        [0.5609, 0.4391],
        [0.5609, 0.4391],
[2K        [0.5594, 0.4406]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 19/203 [2m0:00:21 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KClass label: SoccerPenaltym[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 19/203 [2m0:00:21 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6398, 0.3602]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:21 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KStep 0 - TTA Loss: 0.48620m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 19/203 [2m0:00:21 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6398, 0.3602]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:21 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6398, 0.3602]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:21 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6398, 0.3602]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:21 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6398, 0.3602]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:21 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6399, 0.3601]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:21 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6399, 0.3601]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:21 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6401, 0.3599]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:21 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6402, 0.3598]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:21 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6404, 0.3596]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:21 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6407, 0.3593]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:21 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KStep 10 - TTA Loss: 0.4886m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 19/203 [2m0:00:21 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6411, 0.3589]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:21 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6415, 0.3585]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:21 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6420, 0.3580]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:21 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6427, 0.3573]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:21 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6434, 0.3566]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:21 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6443, 0.3557]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:21 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6453, 0.3547]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:21 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6464, 0.3536]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:21 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6476, 0.3524]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:21 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6490, 0.3510]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:21 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KStep 20 - TTA Loss: 0.4627m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 19/203 [2m0:00:21 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6504, 0.3496]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:21 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6520, 0.3480]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:21 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6537, 0.3463]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:21 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6555, 0.3445]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:21 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6573, 0.3427]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:21 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6593, 0.3407]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:21 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6614, 0.3386]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:21 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6636, 0.3364]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:21 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6658, 0.3342]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:21 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5623, 0.4377]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 19/203 [2m0:00:21 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6279, 0.3721],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 19/203 [2m0:00:21 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
        [0.5903, 0.4097],
        [0.5746, 0.4254],
        [0.6381, 0.3619],
        [0.5997, 0.4003],
        [0.6092, 0.3908],
        [0.5645, 0.4355],
        [0.6681, 0.3319],
        [0.6539, 0.3461],
[2K        [0.6461, 0.3539]], device='cuda:0')â•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 19/203 [2m0:00:21 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5626, 0.4374],24mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 20/203 [2m0:00:22 â€¢ 0:03:19[0m [2;4m0.92it/s[0m  
        [0.5563, 0.4437],
        [0.5583, 0.4417],
        [0.5598, 0.4402],
        [0.5606, 0.4394],
        [0.5570, 0.4430],
        [0.5563, 0.4437],
        [0.5611, 0.4389],
        [0.5610, 0.4390],
[2K        [0.5595, 0.4405]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 20/203 [2m0:00:22 â€¢ 0:03:19[0m [2;4m0.92it/s[0m  
[2KClass label: HighJumpâ”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 20/203 [2m0:00:22 â€¢ 0:03:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5972, 0.4028]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:22 â€¢ 0:03:19[0m [2;4m0.92it/s[0m  
[2KStep 0 - TTA Loss: 0.62590m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 20/203 [2m0:00:22 â€¢ 0:03:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5985, 0.4015]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:22 â€¢ 0:03:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6000, 0.4000]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:22 â€¢ 0:03:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6016, 0.3984]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:22 â€¢ 0:03:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6033, 0.3967]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:22 â€¢ 0:03:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6050, 0.3950]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:22 â€¢ 0:03:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6069, 0.3931]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:22 â€¢ 0:03:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6087, 0.3913]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:22 â€¢ 0:03:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6106, 0.3894]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:22 â€¢ 0:03:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6124, 0.3876]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:22 â€¢ 0:03:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6143, 0.3857]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:22 â€¢ 0:03:19[0m [2;4m0.92it/s[0m  
[2KStep 10 - TTA Loss: 0.5811m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 20/203 [2m0:00:22 â€¢ 0:03:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6161, 0.3839]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:22 â€¢ 0:03:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6178, 0.3822]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:22 â€¢ 0:03:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6194, 0.3806]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:22 â€¢ 0:03:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6210, 0.3790]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:22 â€¢ 0:03:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6224, 0.3776]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:22 â€¢ 0:03:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6238, 0.3762]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:22 â€¢ 0:03:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6250, 0.3750]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:22 â€¢ 0:03:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6261, 0.3739]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:22 â€¢ 0:03:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6271, 0.3729]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:22 â€¢ 0:03:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6280, 0.3720]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:22 â€¢ 0:03:19[0m [2;4m0.92it/s[0m  
[2KStep 20 - TTA Loss: 0.5861m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 20/203 [2m0:00:22 â€¢ 0:03:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6287, 0.3713]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:22 â€¢ 0:03:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6293, 0.3707]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:22 â€¢ 0:03:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6298, 0.3702]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:22 â€¢ 0:03:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6302, 0.3698]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:22 â€¢ 0:03:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6305, 0.3695]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:22 â€¢ 0:03:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6307, 0.3693]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:22 â€¢ 0:03:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6308, 0.3692]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:22 â€¢ 0:03:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6309, 0.3691]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:22 â€¢ 0:03:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6310, 0.3690]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:22 â€¢ 0:03:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5595, 0.4405]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 20/203 [2m0:00:22 â€¢ 0:03:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6395, 0.3605],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 20/203 [2m0:00:22 â€¢ 0:03:19[0m [2;4m0.92it/s[0m  
        [0.5999, 0.4001],
        [0.5902, 0.4098],
        [0.6507, 0.3493],
        [0.6142, 0.3858],
        [0.6310, 0.3690],
        [0.5798, 0.4202],
        [0.6725, 0.3275],
        [0.6710, 0.3290],
[2K        [0.6616, 0.3384]], device='cuda:0')â•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 20/203 [2m0:00:22 â€¢ 0:03:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5631, 0.4369],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 21/203 [2m0:00:23 â€¢ 0:03:22[0m [2;4m0.91it/s[0m   
        [0.5568, 0.4432],
        [0.5589, 0.4411],
        [0.5603, 0.4397],
        [0.5612, 0.4388],
        [0.5581, 0.4419],
        [0.5572, 0.4428],
        [0.5618, 0.4382],
        [0.5616, 0.4384],
[2K        [0.5601, 0.4399]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 21/203 [2m0:00:23 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KClass label: BaseballPitch0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 21/203 [2m0:00:23 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6229, 0.3771]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:23 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KStep 0 - TTA Loss: 0.5678[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 21/203 [2m0:00:23 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6229, 0.3771]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:23 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6229, 0.3771]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:23 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6229, 0.3771]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:23 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6230, 0.3770]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:23 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6231, 0.3769]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:23 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6232, 0.3768]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:23 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6234, 0.3766]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:23 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6236, 0.3764]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:23 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6239, 0.3761]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:23 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6243, 0.3757]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:23 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KStep 10 - TTA Loss: 0.58820m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 21/203 [2m0:00:23 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6247, 0.3753]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:23 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6252, 0.3748]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:23 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6257, 0.3743]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:23 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6264, 0.3736]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:23 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6271, 0.3729]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:23 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6277, 0.3723]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:23 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6285, 0.3715]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:23 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6295, 0.3705]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:23 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6304, 0.3696]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:23 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6314, 0.3686]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:23 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KStep 20 - TTA Loss: 0.53930m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 21/203 [2m0:00:23 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6325, 0.3675]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:23 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6334, 0.3666]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:23 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6346, 0.3654]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:23 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6358, 0.3642]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:23 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6370, 0.3630]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:23 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6383, 0.3617]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:23 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6398, 0.3602]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:23 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6412, 0.3588]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:23 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6425, 0.3575]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:23 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5644, 0.4356]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 21/203 [2m0:00:23 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6439, 0.3561],[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 21/203 [2m0:00:23 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
        [0.5981, 0.4019],
        [0.5877, 0.4123],
        [0.6519, 0.3481],
        [0.6108, 0.3892],
        [0.6211, 0.3789],
        [0.5720, 0.4280],
        [0.6564, 0.3436],
        [0.6687, 0.3313],
[2K        [0.6599, 0.3401]], device='cuda:0')m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 21/203 [2m0:00:23 â€¢ 0:03:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5633, 0.4367],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 22/203 [2m0:00:24 â€¢ 0:03:20[0m [2;4m0.91it/s[0m  
        [0.5569, 0.4431],
        [0.5591, 0.4409],
        [0.5605, 0.4395],
        [0.5614, 0.4386],
        [0.5583, 0.4417],
        [0.5574, 0.4426],
        [0.5619, 0.4381],
        [0.5617, 0.4383],
[2K        [0.5603, 0.4397]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 22/203 [2m0:00:24 â€¢ 0:03:20[0m [2;4m0.91it/s[0m  
[2KClass label: CleanAndJerk[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 22/203 [2m0:00:24 â€¢ 0:03:20[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5668, 0.4332]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:24 â€¢ 0:03:20[0m [2;4m0.91it/s[0m  
[2KStep 0 - TTA Loss: 0.8423[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 22/203 [2m0:00:24 â€¢ 0:03:20[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5673, 0.4327]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:24 â€¢ 0:03:20[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5679, 0.4321]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:24 â€¢ 0:03:20[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5684, 0.4316]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:24 â€¢ 0:03:20[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5689, 0.4311]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:24 â€¢ 0:03:20[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5693, 0.4307]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:24 â€¢ 0:03:20[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5698, 0.4302]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:24 â€¢ 0:03:20[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5703, 0.4297]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:24 â€¢ 0:03:20[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5707, 0.4293]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:24 â€¢ 0:03:20[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5711, 0.4289]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:24 â€¢ 0:03:20[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5715, 0.4285]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:24 â€¢ 0:03:20[0m [2;4m0.91it/s[0m  
[2KStep 10 - TTA Loss: 0.80640m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 22/203 [2m0:00:24 â€¢ 0:03:20[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5719, 0.4281]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:24 â€¢ 0:03:20[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5722, 0.4278]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:24 â€¢ 0:03:20[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5725, 0.4275]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:24 â€¢ 0:03:20[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5728, 0.4272]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:24 â€¢ 0:03:20[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5731, 0.4269]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:24 â€¢ 0:03:20[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5734, 0.4266]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:24 â€¢ 0:03:20[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5736, 0.4264]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:24 â€¢ 0:03:20[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5739, 0.4261]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:24 â€¢ 0:03:20[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5741, 0.4259]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:24 â€¢ 0:03:20[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5743, 0.4257]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:24 â€¢ 0:03:20[0m [2;4m0.91it/s[0m  
[2KStep 20 - TTA Loss: 0.79490m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 22/203 [2m0:00:24 â€¢ 0:03:20[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5744, 0.4256]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:24 â€¢ 0:03:20[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5745, 0.4255]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:24 â€¢ 0:03:20[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5746, 0.4254]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:24 â€¢ 0:03:20[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5747, 0.4253]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:24 â€¢ 0:03:20[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5748, 0.4252]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:24 â€¢ 0:03:20[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5748, 0.4252]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:24 â€¢ 0:03:20[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5748, 0.4252]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:24 â€¢ 0:03:20[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5748, 0.4252]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:24 â€¢ 0:03:20[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5749, 0.4251]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:24 â€¢ 0:03:20[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5591, 0.4409]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 22/203 [2m0:00:24 â€¢ 0:03:20[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6291, 0.3709],[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 22/203 [2m0:00:24 â€¢ 0:03:20[0m [2;4m0.91it/s[0m  
        [0.5870, 0.4130],
        [0.5749, 0.4251],
        [0.6391, 0.3609],
        [0.5969, 0.4031],
        [0.6059, 0.3941],
        [0.5597, 0.4403],
        [0.6475, 0.3525],
        [0.6536, 0.3464],
[2K        [0.6462, 0.3538]], device='cuda:0')m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 22/203 [2m0:00:24 â€¢ 0:03:20[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5633, 0.4367],224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 23/203 [2m0:00:25 â€¢ 0:03:17[0m [2;4m0.92it/s[0m  
        [0.5570, 0.4430],
        [0.5590, 0.4410],
        [0.5605, 0.4395],
        [0.5614, 0.4386],
        [0.5584, 0.4416],
        [0.5574, 0.4426],
        [0.5619, 0.4381],
        [0.5617, 0.4383],
[2K        [0.5603, 0.4397]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 23/203 [2m0:00:25 â€¢ 0:03:17[0m [2;4m0.92it/s[0m  
[2KClass label: TennisSwing[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 23/203 [2m0:00:25 â€¢ 0:03:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6536, 0.3464]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:25 â€¢ 0:03:17[0m [2;4m0.92it/s[0m  
[2KStep 0 - TTA Loss: 0.6713[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 23/203 [2m0:00:25 â€¢ 0:03:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6536, 0.3464]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:25 â€¢ 0:03:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6536, 0.3464]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:25 â€¢ 0:03:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6536, 0.3464]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:25 â€¢ 0:03:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6536, 0.3464]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:25 â€¢ 0:03:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6536, 0.3464]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:25 â€¢ 0:03:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6536, 0.3464]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:25 â€¢ 0:03:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6536, 0.3464]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:25 â€¢ 0:03:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6536, 0.3464]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:25 â€¢ 0:03:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6535, 0.3465]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:25 â€¢ 0:03:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6535, 0.3465]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:25 â€¢ 0:03:17[0m [2;4m0.92it/s[0m  
[2KStep 10 - TTA Loss: 0.74410m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 23/203 [2m0:00:25 â€¢ 0:03:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6534, 0.3466]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:25 â€¢ 0:03:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6533, 0.3467]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:25 â€¢ 0:03:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6532, 0.3468]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:25 â€¢ 0:03:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6531, 0.3469]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:25 â€¢ 0:03:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6529, 0.3471]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:25 â€¢ 0:03:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6527, 0.3473]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:25 â€¢ 0:03:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6525, 0.3475]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:25 â€¢ 0:03:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6523, 0.3477]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:25 â€¢ 0:03:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6521, 0.3479]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:25 â€¢ 0:03:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6519, 0.3481]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:25 â€¢ 0:03:17[0m [2;4m0.92it/s[0m  
[2KStep 20 - TTA Loss: 0.75470m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 23/203 [2m0:00:25 â€¢ 0:03:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6517, 0.3483]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:25 â€¢ 0:03:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6514, 0.3486]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:25 â€¢ 0:03:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6512, 0.3488]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:25 â€¢ 0:03:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6510, 0.3490]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:25 â€¢ 0:03:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6508, 0.3492]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:25 â€¢ 0:03:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6506, 0.3494]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:25 â€¢ 0:03:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6505, 0.3495]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:25 â€¢ 0:03:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6505, 0.3495]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:25 â€¢ 0:03:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6505, 0.3495]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:25 â€¢ 0:03:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5613, 0.4387]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 23/203 [2m0:00:25 â€¢ 0:03:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6194, 0.3806],â•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 23/203 [2m0:00:25 â€¢ 0:03:17[0m [2;4m0.92it/s[0m  
        [0.5885, 0.4115],
        [0.5755, 0.4245],
        [0.6346, 0.3654],
        [0.5918, 0.4082],
        [0.6019, 0.3981],
        [0.5556, 0.4444],
        [0.6427, 0.3573],
        [0.6507, 0.3493],
[2K        [0.6420, 0.3580]], device='cuda:0')mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 23/203 [2m0:00:25 â€¢ 0:03:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5629, 0.4371],224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 24/203 [2m0:00:26 â€¢ 0:03:16[0m [2;4m0.92it/s[0m  
        [0.5566, 0.4434],
        [0.5587, 0.4413],
        [0.5600, 0.4400],
        [0.5610, 0.4390],
        [0.5580, 0.4420],
        [0.5571, 0.4429],
        [0.5615, 0.4385],
        [0.5611, 0.4389],
[2K        [0.5600, 0.4400]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 24/203 [2m0:00:26 â€¢ 0:03:16[0m [2;4m0.92it/s[0m  
[2KClass label: Billiardsâ”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 24/203 [2m0:00:26 â€¢ 0:03:16[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5757, 0.4243]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:26 â€¢ 0:03:16[0m [2;4m0.92it/s[0m  
[2KStep 0 - TTA Loss: 0.9548[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 24/203 [2m0:00:26 â€¢ 0:03:16[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5757, 0.4243]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:26 â€¢ 0:03:16[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5758, 0.4242]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:26 â€¢ 0:03:16[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5758, 0.4242]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:26 â€¢ 0:03:16[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5758, 0.4242]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:26 â€¢ 0:03:16[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5758, 0.4242]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:26 â€¢ 0:03:16[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5759, 0.4241]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:26 â€¢ 0:03:16[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5759, 0.4241]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:26 â€¢ 0:03:16[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5759, 0.4241]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:26 â€¢ 0:03:16[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5760, 0.4240]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:26 â€¢ 0:03:16[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5760, 0.4240]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:26 â€¢ 0:03:16[0m [2;4m0.92it/s[0m  
[2KStep 10 - TTA Loss: 0.94170m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 24/203 [2m0:00:26 â€¢ 0:03:16[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5760, 0.4240]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:26 â€¢ 0:03:16[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5761, 0.4239]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:26 â€¢ 0:03:16[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5761, 0.4239]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:26 â€¢ 0:03:16[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5762, 0.4238]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:26 â€¢ 0:03:16[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5762, 0.4238]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:26 â€¢ 0:03:16[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5762, 0.4238]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:26 â€¢ 0:03:16[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5763, 0.4237]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:26 â€¢ 0:03:16[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5763, 0.4237]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:26 â€¢ 0:03:16[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5763, 0.4237]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:26 â€¢ 0:03:16[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5764, 0.4236]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:26 â€¢ 0:03:16[0m [2;4m0.92it/s[0m  
[2KStep 20 - TTA Loss: 0.92610m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 24/203 [2m0:00:26 â€¢ 0:03:16[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5764, 0.4236]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:26 â€¢ 0:03:16[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5764, 0.4236]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:26 â€¢ 0:03:16[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5764, 0.4236]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:26 â€¢ 0:03:16[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5764, 0.4236]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:26 â€¢ 0:03:16[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5764, 0.4236]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:26 â€¢ 0:03:16[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5764, 0.4236]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:26 â€¢ 0:03:16[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5764, 0.4236]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:26 â€¢ 0:03:16[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5764, 0.4236]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:26 â€¢ 0:03:16[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5764, 0.4236]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:26 â€¢ 0:03:16[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5562, 0.4438]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 24/203 [2m0:00:26 â€¢ 0:03:16[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6163, 0.3837],â•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 24/203 [2m0:00:26 â€¢ 0:03:16[0m [2;4m0.92it/s[0m  
        [0.5765, 0.4235],
        [0.5649, 0.4351],
        [0.6297, 0.3703],
        [0.5846, 0.4154],
        [0.5956, 0.4044],
        [0.5489, 0.4511],
        [0.6356, 0.3644],
        [0.6479, 0.3521],
[2K        [0.6345, 0.3655]], device='cuda:0')mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 24/203 [2m0:00:26 â€¢ 0:03:16[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5627, 0.4373],224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 25/203 [2m0:00:27 â€¢ 0:03:14[0m [2;4m0.92it/s[0m  
        [0.5564, 0.4436],
        [0.5586, 0.4414],
        [0.5598, 0.4402],
        [0.5609, 0.4391],
        [0.5579, 0.4421],
        [0.5569, 0.4431],
        [0.5613, 0.4387],
        [0.5608, 0.4392],
[2K        [0.5598, 0.4402]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 25/203 [2m0:00:27 â€¢ 0:03:14[0m [2;4m0.92it/s[0m  
[2KClass label: GolfSwingâ”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 25/203 [2m0:00:27 â€¢ 0:03:14[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6274, 0.3726]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:27 â€¢ 0:03:14[0m [2;4m0.92it/s[0m  
[2KStep 0 - TTA Loss: 0.6888[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 25/203 [2m0:00:27 â€¢ 0:03:14[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6274, 0.3726]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:27 â€¢ 0:03:14[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6274, 0.3726]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:27 â€¢ 0:03:14[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6274, 0.3726]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:27 â€¢ 0:03:14[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6274, 0.3726]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:27 â€¢ 0:03:14[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6274, 0.3726]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:27 â€¢ 0:03:14[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6274, 0.3726]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:27 â€¢ 0:03:14[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6275, 0.3725]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:27 â€¢ 0:03:14[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6276, 0.3724]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:27 â€¢ 0:03:14[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6277, 0.3723]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:27 â€¢ 0:03:14[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6278, 0.3722]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:27 â€¢ 0:03:14[0m [2;4m0.92it/s[0m  
[2KStep 10 - TTA Loss: 0.66120m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 25/203 [2m0:00:27 â€¢ 0:03:14[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6280, 0.3720]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:27 â€¢ 0:03:14[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6282, 0.3718]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:27 â€¢ 0:03:14[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6284, 0.3716]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:27 â€¢ 0:03:14[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6287, 0.3713]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:27 â€¢ 0:03:14[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6291, 0.3709]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:27 â€¢ 0:03:14[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6295, 0.3705]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:27 â€¢ 0:03:14[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6299, 0.3701]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:27 â€¢ 0:03:14[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6304, 0.3696]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:27 â€¢ 0:03:14[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6309, 0.3691]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:27 â€¢ 0:03:14[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6315, 0.3685]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:27 â€¢ 0:03:14[0m [2;4m0.92it/s[0m  
[2KStep 20 - TTA Loss: 0.65880m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 25/203 [2m0:00:27 â€¢ 0:03:14[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6321, 0.3679]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:27 â€¢ 0:03:14[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6327, 0.3673]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:27 â€¢ 0:03:14[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6334, 0.3666]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:27 â€¢ 0:03:14[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6342, 0.3658]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:27 â€¢ 0:03:14[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6350, 0.3650]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:27 â€¢ 0:03:14[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6359, 0.3641]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:27 â€¢ 0:03:14[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6368, 0.3632]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:27 â€¢ 0:03:14[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6378, 0.3622]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:27 â€¢ 0:03:14[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6387, 0.3613]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:27 â€¢ 0:03:14[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5609, 0.4391]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 25/203 [2m0:00:27 â€¢ 0:03:14[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6245, 0.3755],â•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 25/203 [2m0:00:27 â€¢ 0:03:14[0m [2;4m0.92it/s[0m  
        [0.5849, 0.4151],
        [0.5741, 0.4259],
        [0.6398, 0.3602],
        [0.5927, 0.4073],
        [0.6030, 0.3970],
        [0.5546, 0.4454],
        [0.6420, 0.3580],
        [0.6552, 0.3448],
[2K        [0.6435, 0.3565]], device='cuda:0')mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 25/203 [2m0:00:27 â€¢ 0:03:14[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5630, 0.4370],â•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 26/203 [2m0:00:28 â€¢ 0:03:12[0m [2;4m0.92it/s[0m   
        [0.5567, 0.4433],
        [0.5588, 0.4412],
        [0.5603, 0.4397],
        [0.5611, 0.4389],
        [0.5581, 0.4419],
        [0.5572, 0.4428],
        [0.5617, 0.4383],
        [0.5612, 0.4388],
[2K        [0.5601, 0.4399]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 26/203 [2m0:00:28 â€¢ 0:03:12[0m [2;4m0.92it/s[0m  
[2KClass label: HighJumpâ”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 26/203 [2m0:00:28 â€¢ 0:03:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5986, 0.4014]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:28 â€¢ 0:03:12[0m [2;4m0.92it/s[0m  
[2KStep 0 - TTA Loss: 0.6282[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 26/203 [2m0:00:28 â€¢ 0:03:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5995, 0.4005]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:28 â€¢ 0:03:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6005, 0.3995]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:28 â€¢ 0:03:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6018, 0.3982]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:28 â€¢ 0:03:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6033, 0.3967]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:28 â€¢ 0:03:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6048, 0.3952]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:28 â€¢ 0:03:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6064, 0.3936]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:28 â€¢ 0:03:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6081, 0.3919]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:28 â€¢ 0:03:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6097, 0.3903]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:28 â€¢ 0:03:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6114, 0.3886]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:28 â€¢ 0:03:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6131, 0.3869]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:28 â€¢ 0:03:12[0m [2;4m0.92it/s[0m  
[2KStep 10 - TTA Loss: 0.6022[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 26/203 [2m0:00:28 â€¢ 0:03:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6147, 0.3853]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:28 â€¢ 0:03:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6163, 0.3837]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:28 â€¢ 0:03:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6178, 0.3822]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:28 â€¢ 0:03:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6192, 0.3808]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:28 â€¢ 0:03:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6205, 0.3795]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:28 â€¢ 0:03:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6217, 0.3783]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:28 â€¢ 0:03:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6229, 0.3771]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:28 â€¢ 0:03:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6239, 0.3761]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:28 â€¢ 0:03:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6248, 0.3752]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:28 â€¢ 0:03:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6256, 0.3744]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:28 â€¢ 0:03:12[0m [2;4m0.92it/s[0m  
[2KStep 20 - TTA Loss: 0.7316[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 26/203 [2m0:00:28 â€¢ 0:03:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6263, 0.3737]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:28 â€¢ 0:03:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6268, 0.3732]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:28 â€¢ 0:03:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6273, 0.3727]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:28 â€¢ 0:03:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6276, 0.3724]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:28 â€¢ 0:03:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6279, 0.3721]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:28 â€¢ 0:03:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6281, 0.3719]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:28 â€¢ 0:03:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6282, 0.3718]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:28 â€¢ 0:03:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6283, 0.3717]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:28 â€¢ 0:03:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6283, 0.3717]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:28 â€¢ 0:03:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5602, 0.4398]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 26/203 [2m0:00:28 â€¢ 0:03:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6406, 0.3594],m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 26/203 [2m0:00:28 â€¢ 0:03:12[0m [2;4m0.92it/s[0m  
        [0.5944, 0.4056],
        [0.5912, 0.4088],
        [0.6515, 0.3485],
        [0.6139, 0.3861],
        [0.6284, 0.3716],
        [0.5769, 0.4231],
        [0.6610, 0.3390],
        [0.6687, 0.3313],
[2K        [0.6618, 0.3382]], device='cuda:0')0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 26/203 [2m0:00:28 â€¢ 0:03:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5635, 0.4365],â•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 27/203 [2m0:00:30 â€¢ 0:03:15[0m [2;4m0.91it/s[0m  
        [0.5571, 0.4429],
        [0.5593, 0.4407],
        [0.5609, 0.4391],
        [0.5616, 0.4384],
        [0.5589, 0.4411],
        [0.5578, 0.4422],
        [0.5622, 0.4378],
        [0.5618, 0.4382],
[2K        [0.5605, 0.4395]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 27/203 [2m0:00:30 â€¢ 0:03:15[0m [2;4m0.91it/s[0m  
[2KClass label: GolfSwingâ”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 27/203 [2m0:00:30 â€¢ 0:03:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6342, 0.3658]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:30 â€¢ 0:03:15[0m [2;4m0.91it/s[0m  
[2KStep 0 - TTA Loss: 0.7321[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 27/203 [2m0:00:30 â€¢ 0:03:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6342, 0.3658]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:30 â€¢ 0:03:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6342, 0.3658]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:30 â€¢ 0:03:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6342, 0.3658]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:30 â€¢ 0:03:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6342, 0.3658]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:30 â€¢ 0:03:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6343, 0.3657]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:30 â€¢ 0:03:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6344, 0.3656]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:30 â€¢ 0:03:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6346, 0.3654]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:30 â€¢ 0:03:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6348, 0.3652]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:30 â€¢ 0:03:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6351, 0.3649]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:30 â€¢ 0:03:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6355, 0.3645]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:30 â€¢ 0:03:15[0m [2;4m0.91it/s[0m  
[2KStep 10 - TTA Loss: 0.6849[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 27/203 [2m0:00:30 â€¢ 0:03:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6359, 0.3641]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:30 â€¢ 0:03:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6364, 0.3636]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:30 â€¢ 0:03:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6370, 0.3630]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:30 â€¢ 0:03:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6377, 0.3623]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:30 â€¢ 0:03:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6385, 0.3615]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:30 â€¢ 0:03:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6394, 0.3606]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:30 â€¢ 0:03:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6404, 0.3596]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:30 â€¢ 0:03:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6414, 0.3586]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:30 â€¢ 0:03:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6426, 0.3574]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:30 â€¢ 0:03:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6439, 0.3561]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:30 â€¢ 0:03:15[0m [2;4m0.91it/s[0m  
[2KStep 20 - TTA Loss: 0.6639[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 27/203 [2m0:00:30 â€¢ 0:03:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6452, 0.3548]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:30 â€¢ 0:03:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6467, 0.3533]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:30 â€¢ 0:03:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6482, 0.3518]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:30 â€¢ 0:03:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6499, 0.3501]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:30 â€¢ 0:03:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6516, 0.3484]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:30 â€¢ 0:03:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6534, 0.3466]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:30 â€¢ 0:03:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6554, 0.3446]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:30 â€¢ 0:03:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6573, 0.3427]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:30 â€¢ 0:03:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6593, 0.3407]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:30 â€¢ 0:03:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5630, 0.4370]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 27/203 [2m0:00:30 â€¢ 0:03:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6406, 0.3594],m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 27/203 [2m0:00:30 â€¢ 0:03:15[0m [2;4m0.91it/s[0m  
        [0.5961, 0.4039],
        [0.5872, 0.4128],
        [0.6615, 0.3385],
        [0.6105, 0.3895],
        [0.6193, 0.3807],
        [0.5712, 0.4288],
        [0.6553, 0.3447],
        [0.6709, 0.3291],
[2K        [0.6599, 0.3401]], device='cuda:0')0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 27/203 [2m0:00:30 â€¢ 0:03:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5640, 0.4360],;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 28/203 [2m0:00:31 â€¢ 0:03:15[0m [2;4m0.90it/s[0m  
        [0.5576, 0.4424],
        [0.5596, 0.4404],
        [0.5616, 0.4384],
        [0.5620, 0.4380],
        [0.5594, 0.4406],
        [0.5582, 0.4418],
        [0.5626, 0.4374],
        [0.5623, 0.4377],
[2K        [0.5609, 0.4391]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 28/203 [2m0:00:31 â€¢ 0:03:15[0m [2;4m0.90it/s[0m  
[2KClass label: CleanAndJerk[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 28/203 [2m0:00:31 â€¢ 0:03:15[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5767, 0.4233]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:31 â€¢ 0:03:15[0m [2;4m0.90it/s[0m  
[2KStep 0 - TTA Loss: 0.8465[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 28/203 [2m0:00:31 â€¢ 0:03:15[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5782, 0.4218]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:31 â€¢ 0:03:15[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5798, 0.4202]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:31 â€¢ 0:03:15[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5816, 0.4184]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:31 â€¢ 0:03:15[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5835, 0.4165]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:31 â€¢ 0:03:15[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5855, 0.4145]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:31 â€¢ 0:03:15[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5877, 0.4123]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:31 â€¢ 0:03:15[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5898, 0.4102]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:31 â€¢ 0:03:15[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5920, 0.4080]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:31 â€¢ 0:03:15[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5941, 0.4059]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:31 â€¢ 0:03:15[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5962, 0.4038]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:31 â€¢ 0:03:15[0m [2;4m0.90it/s[0m  
[2KStep 10 - TTA Loss: 0.8915[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 28/203 [2m0:00:31 â€¢ 0:03:15[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5982, 0.4018]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:31 â€¢ 0:03:15[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6001, 0.3999]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:31 â€¢ 0:03:15[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6020, 0.3980]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:31 â€¢ 0:03:15[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6037, 0.3963]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:31 â€¢ 0:03:15[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6053, 0.3947]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:31 â€¢ 0:03:15[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6068, 0.3932]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:31 â€¢ 0:03:15[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6082, 0.3918]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:31 â€¢ 0:03:15[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6094, 0.3906]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:31 â€¢ 0:03:15[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6105, 0.3895]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:31 â€¢ 0:03:15[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6114, 0.3886]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:31 â€¢ 0:03:15[0m [2;4m0.90it/s[0m  
[2KStep 20 - TTA Loss: 0.8303[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 28/203 [2m0:00:31 â€¢ 0:03:15[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6122, 0.3878]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:31 â€¢ 0:03:15[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6129, 0.3871]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:31 â€¢ 0:03:15[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6134, 0.3866]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:31 â€¢ 0:03:15[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6138, 0.3862]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:31 â€¢ 0:03:15[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6142, 0.3858]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:31 â€¢ 0:03:15[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6144, 0.3856]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:31 â€¢ 0:03:15[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6145, 0.3855]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:31 â€¢ 0:03:15[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6146, 0.3854]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:31 â€¢ 0:03:15[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6146, 0.3854]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:31 â€¢ 0:03:15[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5621, 0.4379]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 28/203 [2m0:00:31 â€¢ 0:03:15[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6442, 0.3558],mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 28/203 [2m0:00:31 â€¢ 0:03:15[0m [2;4m0.90it/s[0m  
        [0.6020, 0.3980],
        [0.6147, 0.3853],
        [0.6590, 0.3410],
        [0.6196, 0.3804],
        [0.6266, 0.3734],
        [0.5783, 0.4217],
        [0.6583, 0.3417],
        [0.6705, 0.3295],
[2K        [0.6653, 0.3347]], device='cuda:0')4mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 28/203 [2m0:00:31 â€¢ 0:03:15[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5645, 0.4355],;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 29/203 [2m0:00:32 â€¢ 0:03:13[0m [2;4m0.90it/s[0m  
        [0.5581, 0.4419],
        [0.5606, 0.4394],
        [0.5623, 0.4377],
        [0.5626, 0.4374],
        [0.5600, 0.4400],
        [0.5588, 0.4412],
        [0.5631, 0.4369],
        [0.5630, 0.4370],
[2K        [0.5616, 0.4384]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 29/203 [2m0:00:32 â€¢ 0:03:13[0m [2;4m0.90it/s[0m  
[2KClass label: Billiardsâ”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 29/203 [2m0:00:32 â€¢ 0:03:13[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5794, 0.4206]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:32 â€¢ 0:03:13[0m [2;4m0.90it/s[0m  
[2KStep 0 - TTA Loss: 0.5512[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 29/203 [2m0:00:32 â€¢ 0:03:13[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5794, 0.4206]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:32 â€¢ 0:03:13[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5794, 0.4206]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:32 â€¢ 0:03:13[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5794, 0.4206]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:32 â€¢ 0:03:13[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5795, 0.4205]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:32 â€¢ 0:03:13[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5795, 0.4205]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:32 â€¢ 0:03:13[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5797, 0.4203]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:32 â€¢ 0:03:13[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5798, 0.4202]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:32 â€¢ 0:03:13[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5801, 0.4199]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:32 â€¢ 0:03:13[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5804, 0.4196]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:32 â€¢ 0:03:13[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5808, 0.4192]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:32 â€¢ 0:03:13[0m [2;4m0.90it/s[0m  
[2KStep 10 - TTA Loss: 0.5432[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 29/203 [2m0:00:32 â€¢ 0:03:13[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5812, 0.4188]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:32 â€¢ 0:03:13[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5818, 0.4182]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:32 â€¢ 0:03:13[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5825, 0.4175]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:32 â€¢ 0:03:13[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5833, 0.4167]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:32 â€¢ 0:03:13[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5842, 0.4158]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:32 â€¢ 0:03:13[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5853, 0.4147]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:32 â€¢ 0:03:13[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5864, 0.4136]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:32 â€¢ 0:03:13[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5877, 0.4123]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:32 â€¢ 0:03:13[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5890, 0.4110]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:32 â€¢ 0:03:13[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5905, 0.4095]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:32 â€¢ 0:03:13[0m [2;4m0.90it/s[0m  
[2KStep 20 - TTA Loss: 0.5251[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 29/203 [2m0:00:32 â€¢ 0:03:13[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5921, 0.4079]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:32 â€¢ 0:03:13[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5939, 0.4061]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:32 â€¢ 0:03:13[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5957, 0.4043]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:32 â€¢ 0:03:13[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5977, 0.4023]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:32 â€¢ 0:03:13[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5997, 0.4003]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:32 â€¢ 0:03:13[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6019, 0.3981]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:32 â€¢ 0:03:13[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6041, 0.3959]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:32 â€¢ 0:03:13[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6063, 0.3937]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:32 â€¢ 0:03:13[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6086, 0.3914]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:32 â€¢ 0:03:13[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5599, 0.4401]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 29/203 [2m0:00:32 â€¢ 0:03:13[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6275, 0.3725],mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 29/203 [2m0:00:32 â€¢ 0:03:13[0m [2;4m0.90it/s[0m  
        [0.6110, 0.3890],
        [0.5827, 0.4173],
        [0.6408, 0.3592],
        [0.5995, 0.4005],
        [0.6100, 0.3900],
        [0.5615, 0.4385],
        [0.6520, 0.3480],
        [0.6605, 0.3395],
[2K        [0.6473, 0.3527]], device='cuda:0')4mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 29/203 [2m0:00:32 â€¢ 0:03:13[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5648, 0.4352],;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 30/203 [2m0:00:33 â€¢ 0:03:12[0m [2;4m0.90it/s[0m  
        [0.5588, 0.4412],
        [0.5610, 0.4390],
        [0.5626, 0.4374],
        [0.5629, 0.4371],
        [0.5603, 0.4397],
        [0.5592, 0.4408],
        [0.5635, 0.4365],
        [0.5633, 0.4367],
[2K        [0.5619, 0.4381]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 30/203 [2m0:00:33 â€¢ 0:03:12[0m [2;4m0.90it/s[0m  
[2KClass label: CleanAndJerk[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 30/203 [2m0:00:33 â€¢ 0:03:12[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5681, 0.4319]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:33 â€¢ 0:03:12[0m [2;4m0.90it/s[0m  
[2KStep 0 - TTA Loss: 0.6032[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 30/203 [2m0:00:33 â€¢ 0:03:12[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5688, 0.4312]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:33 â€¢ 0:03:12[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5696, 0.4304]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:33 â€¢ 0:03:12[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5703, 0.4297]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:33 â€¢ 0:03:12[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5710, 0.4290]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:33 â€¢ 0:03:12[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5717, 0.4283]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:33 â€¢ 0:03:12[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5724, 0.4276]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:33 â€¢ 0:03:12[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5730, 0.4270]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:33 â€¢ 0:03:12[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5736, 0.4264]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:33 â€¢ 0:03:12[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5741, 0.4259]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:33 â€¢ 0:03:12[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5747, 0.4253]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:33 â€¢ 0:03:12[0m [2;4m0.90it/s[0m  
[2KStep 10 - TTA Loss: 0.5824[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 30/203 [2m0:00:33 â€¢ 0:03:12[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5752, 0.4248]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:33 â€¢ 0:03:12[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5757, 0.4243]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:33 â€¢ 0:03:12[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5762, 0.4238]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:33 â€¢ 0:03:12[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5767, 0.4233]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:33 â€¢ 0:03:12[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5771, 0.4229]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:33 â€¢ 0:03:12[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5775, 0.4225]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:33 â€¢ 0:03:12[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5778, 0.4222]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:33 â€¢ 0:03:12[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5781, 0.4219]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:33 â€¢ 0:03:12[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5784, 0.4216]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:33 â€¢ 0:03:12[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5786, 0.4214]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:33 â€¢ 0:03:12[0m [2;4m0.90it/s[0m  
[2KStep 20 - TTA Loss: 0.6001[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 30/203 [2m0:00:33 â€¢ 0:03:12[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5788, 0.4212]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:33 â€¢ 0:03:12[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5790, 0.4210]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:33 â€¢ 0:03:12[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5791, 0.4209]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:33 â€¢ 0:03:12[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5792, 0.4208]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:33 â€¢ 0:03:12[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5793, 0.4207]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:33 â€¢ 0:03:12[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5793, 0.4207]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:33 â€¢ 0:03:12[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5794, 0.4206]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:33 â€¢ 0:03:12[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5794, 0.4206]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:33 â€¢ 0:03:12[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5794, 0.4206]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:33 â€¢ 0:03:12[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5613, 0.4387]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 30/203 [2m0:00:33 â€¢ 0:03:12[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6250, 0.3750],mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 30/203 [2m0:00:33 â€¢ 0:03:12[0m [2;4m0.90it/s[0m  
        [0.6023, 0.3977],
        [0.5794, 0.4206],
        [0.6365, 0.3635],
        [0.5949, 0.4051],
        [0.6038, 0.3962],
        [0.5585, 0.4415],
        [0.6456, 0.3544],
        [0.6532, 0.3468],
[2K        [0.6439, 0.3561]], device='cuda:0')4mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 30/203 [2m0:00:33 â€¢ 0:03:12[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5649, 0.4351],mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 31/203 [2m0:00:34 â€¢ 0:03:05[0m [2;4m0.93it/s[0m   
        [0.5591, 0.4409],
        [0.5611, 0.4389],
        [0.5627, 0.4373],
        [0.5630, 0.4370],
        [0.5604, 0.4396],
        [0.5593, 0.4407],
        [0.5636, 0.4364],
        [0.5634, 0.4366],
[2K        [0.5620, 0.4380]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 31/203 [2m0:00:34 â€¢ 0:03:05[0m [2;4m0.93it/s[0m  
[2KClass label: CleanAndJerkâ”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 31/203 [2m0:00:34 â€¢ 0:03:05[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5757, 0.4243]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:34 â€¢ 0:03:05[0m [2;4m0.93it/s[0m  
[2KStep 0 - TTA Loss: 0.6132â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 31/203 [2m0:00:34 â€¢ 0:03:05[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5757, 0.4243]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:34 â€¢ 0:03:05[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5757, 0.4243]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:34 â€¢ 0:03:05[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5757, 0.4243]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:34 â€¢ 0:03:05[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5758, 0.4242]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:34 â€¢ 0:03:05[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5758, 0.4242]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:34 â€¢ 0:03:05[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5759, 0.4241]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:34 â€¢ 0:03:05[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5761, 0.4239]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:34 â€¢ 0:03:05[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5763, 0.4237]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:34 â€¢ 0:03:05[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5766, 0.4234]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:34 â€¢ 0:03:05[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5770, 0.4230]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:34 â€¢ 0:03:05[0m [2;4m0.93it/s[0m  
[2KStep 10 - TTA Loss: 0.5705[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 31/203 [2m0:00:34 â€¢ 0:03:05[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5775, 0.4225]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:34 â€¢ 0:03:05[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5781, 0.4219]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:34 â€¢ 0:03:05[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5788, 0.4212]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:34 â€¢ 0:03:05[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5797, 0.4203]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:34 â€¢ 0:03:05[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5806, 0.4194]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:34 â€¢ 0:03:05[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5817, 0.4183]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:34 â€¢ 0:03:05[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5829, 0.4171]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:34 â€¢ 0:03:05[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5842, 0.4158]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:34 â€¢ 0:03:05[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5856, 0.4144]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:34 â€¢ 0:03:05[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5872, 0.4128]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:34 â€¢ 0:03:05[0m [2;4m0.93it/s[0m  
[2KStep 20 - TTA Loss: 0.5671[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 31/203 [2m0:00:34 â€¢ 0:03:05[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5889, 0.4111]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:34 â€¢ 0:03:05[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5907, 0.4093]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:34 â€¢ 0:03:05[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5926, 0.4074]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:34 â€¢ 0:03:05[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5947, 0.4053]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:34 â€¢ 0:03:05[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5968, 0.4032]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:34 â€¢ 0:03:05[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5990, 0.4010]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:34 â€¢ 0:03:05[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6013, 0.3987]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:34 â€¢ 0:03:05[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6036, 0.3964]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:34 â€¢ 0:03:05[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6061, 0.3939]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:34 â€¢ 0:03:05[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5631, 0.4369]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 31/203 [2m0:00:34 â€¢ 0:03:05[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6356, 0.3644],0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 31/203 [2m0:00:34 â€¢ 0:03:05[0m [2;4m0.93it/s[0m  
        [0.5987, 0.4013],
        [0.6086, 0.3914],
        [0.6496, 0.3504],
        [0.6129, 0.3871],
        [0.6235, 0.3765],
        [0.5761, 0.4239],
        [0.6595, 0.3405],
        [0.6678, 0.3322],
[2K        [0.6604, 0.3396]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 31/203 [2m0:00:34 â€¢ 0:03:05[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5652, 0.4348],mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 32/203 [2m0:00:35 â€¢ 0:03:05[0m [2;4m0.92it/s[0m  
        [0.5594, 0.4406],
        [0.5618, 0.4382],
        [0.5630, 0.4370],
        [0.5635, 0.4365],
        [0.5608, 0.4392],
        [0.5596, 0.4404],
        [0.5639, 0.4361],
        [0.5638, 0.4362],
[2K        [0.5625, 0.4375]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 32/203 [2m0:00:35 â€¢ 0:03:05[0m [2;4m0.92it/s[0m  
[2KClass label: TennisSwingâ”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 32/203 [2m0:00:35 â€¢ 0:03:05[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6550, 0.3450]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:35 â€¢ 0:03:05[0m [2;4m0.92it/s[0m  
[2KStep 0 - TTA Loss: 0.6449â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 32/203 [2m0:00:35 â€¢ 0:03:05[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6558, 0.3442]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:35 â€¢ 0:03:05[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6564, 0.3436]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:35 â€¢ 0:03:05[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6569, 0.3431]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:35 â€¢ 0:03:05[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6572, 0.3428]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:35 â€¢ 0:03:05[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6575, 0.3425]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:35 â€¢ 0:03:05[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6576, 0.3424]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:35 â€¢ 0:03:05[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6577, 0.3423]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:35 â€¢ 0:03:05[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6577, 0.3423]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:35 â€¢ 0:03:05[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6577, 0.3423]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:35 â€¢ 0:03:05[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6577, 0.3423]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:35 â€¢ 0:03:05[0m [2;4m0.92it/s[0m  
[2KStep 10 - TTA Loss: 0.6507[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 32/203 [2m0:00:35 â€¢ 0:03:05[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6576, 0.3424]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:35 â€¢ 0:03:05[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6575, 0.3425]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:35 â€¢ 0:03:05[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6573, 0.3427]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:35 â€¢ 0:03:05[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6572, 0.3428]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:35 â€¢ 0:03:05[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6571, 0.3429]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:35 â€¢ 0:03:05[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6570, 0.3430]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:35 â€¢ 0:03:05[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6569, 0.3431]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:35 â€¢ 0:03:05[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6568, 0.3432]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:35 â€¢ 0:03:05[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6567, 0.3433]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:35 â€¢ 0:03:05[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6566, 0.3434]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:35 â€¢ 0:03:05[0m [2;4m0.92it/s[0m  
[2KStep 20 - TTA Loss: 0.6577[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 32/203 [2m0:00:35 â€¢ 0:03:05[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6566, 0.3434]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:35 â€¢ 0:03:05[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6565, 0.3435]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:35 â€¢ 0:03:05[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6565, 0.3435]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:35 â€¢ 0:03:05[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6565, 0.3435]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:35 â€¢ 0:03:05[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6564, 0.3436]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:35 â€¢ 0:03:05[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6564, 0.3436]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:35 â€¢ 0:03:05[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6564, 0.3436]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:35 â€¢ 0:03:05[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6564, 0.3436]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:35 â€¢ 0:03:05[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6564, 0.3436]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:35 â€¢ 0:03:05[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5638, 0.4362]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 32/203 [2m0:00:35 â€¢ 0:03:05[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6258, 0.3742],0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 32/203 [2m0:00:35 â€¢ 0:03:05[0m [2;4m0.92it/s[0m  
        [0.5883, 0.4117],
        [0.5905, 0.4095],
        [0.6414, 0.3586],
        [0.6009, 0.3991],
        [0.6099, 0.3901],
        [0.5627, 0.4373],
        [0.6461, 0.3539],
        [0.6564, 0.3436],
[2K        [0.6496, 0.3504]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 32/203 [2m0:00:35 â€¢ 0:03:05[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5650, 0.4350],6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 33/203 [2m0:00:36 â€¢ 0:03:03[0m [2;4m0.93it/s[0m  
        [0.5594, 0.4406],
        [0.5620, 0.4380],
        [0.5628, 0.4372],
        [0.5634, 0.4366],
        [0.5607, 0.4393],
        [0.5596, 0.4404],
        [0.5638, 0.4362],
        [0.5635, 0.4365],
[2K        [0.5625, 0.4375]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 33/203 [2m0:00:36 â€¢ 0:03:03[0m [2;4m0.93it/s[0m  
[2KClass label: CleanAndJerkâ”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 33/203 [2m0:00:36 â€¢ 0:03:03[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5735, 0.4265]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:36 â€¢ 0:03:03[0m [2;4m0.93it/s[0m  
[2KStep 0 - TTA Loss: 0.4749â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 33/203 [2m0:00:36 â€¢ 0:03:03[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5735, 0.4265]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:36 â€¢ 0:03:03[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5735, 0.4265]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:36 â€¢ 0:03:03[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5735, 0.4265]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:36 â€¢ 0:03:03[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5735, 0.4265]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:36 â€¢ 0:03:03[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5735, 0.4265]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:36 â€¢ 0:03:03[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5736, 0.4264]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:36 â€¢ 0:03:03[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5738, 0.4262]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:36 â€¢ 0:03:03[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5739, 0.4261]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:36 â€¢ 0:03:03[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5742, 0.4258]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:36 â€¢ 0:03:03[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5745, 0.4255]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:36 â€¢ 0:03:03[0m [2;4m0.93it/s[0m  
[2KStep 10 - TTA Loss: 0.6916[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 33/203 [2m0:00:36 â€¢ 0:03:03[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5750, 0.4250]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:36 â€¢ 0:03:03[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5754, 0.4246]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:36 â€¢ 0:03:03[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5761, 0.4239]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:36 â€¢ 0:03:03[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5768, 0.4232]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:36 â€¢ 0:03:03[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5776, 0.4224]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:36 â€¢ 0:03:03[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5785, 0.4215]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:36 â€¢ 0:03:03[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5796, 0.4204]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:36 â€¢ 0:03:03[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5808, 0.4192]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:36 â€¢ 0:03:03[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5821, 0.4179]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:36 â€¢ 0:03:03[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5836, 0.4164]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:36 â€¢ 0:03:03[0m [2;4m0.93it/s[0m  
[2KStep 20 - TTA Loss: 0.6911[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 33/203 [2m0:00:36 â€¢ 0:03:03[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5852, 0.4148]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:36 â€¢ 0:03:03[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5870, 0.4130]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:36 â€¢ 0:03:03[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5888, 0.4112]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:36 â€¢ 0:03:03[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5907, 0.4093]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:36 â€¢ 0:03:03[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5928, 0.4072]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:36 â€¢ 0:03:03[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5950, 0.4050]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:36 â€¢ 0:03:03[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5973, 0.4027]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:36 â€¢ 0:03:03[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5997, 0.4003]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:36 â€¢ 0:03:03[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6021, 0.3979]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:36 â€¢ 0:03:03[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5635, 0.4365]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 33/203 [2m0:00:36 â€¢ 0:03:03[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6328, 0.3672],4mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 33/203 [2m0:00:36 â€¢ 0:03:03[0m [2;4m0.93it/s[0m  
        [0.5957, 0.4043],
        [0.6046, 0.3954],
        [0.6481, 0.3519],
        [0.6094, 0.3906],
        [0.6202, 0.3798],
        [0.5716, 0.4284],
        [0.6524, 0.3476],
        [0.6632, 0.3368],
[2K        [0.6568, 0.3432]], device='cuda:0')24mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 33/203 [2m0:00:36 â€¢ 0:03:03[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5651, 0.4349],6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 34/203 [2m0:00:37 â€¢ 0:03:02[0m [2;4m0.93it/s[0m  
        [0.5595, 0.4405],
        [0.5624, 0.4376],
        [0.5629, 0.4371],
        [0.5636, 0.4364],
        [0.5609, 0.4391],
        [0.5598, 0.4402],
        [0.5640, 0.4360],
        [0.5636, 0.4364],
[2K        [0.5627, 0.4373]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 34/203 [2m0:00:37 â€¢ 0:03:02[0m [2;4m0.93it/s[0m  
[2KClass label: HammerThrowâ”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 34/203 [2m0:00:37 â€¢ 0:03:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5938, 0.4062]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:37 â€¢ 0:03:02[0m [2;4m0.93it/s[0m  
[2KStep 0 - TTA Loss: 0.6202â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 34/203 [2m0:00:37 â€¢ 0:03:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5952, 0.4048]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:37 â€¢ 0:03:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5966, 0.4034]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:37 â€¢ 0:03:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5981, 0.4019]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:37 â€¢ 0:03:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5995, 0.4005]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:37 â€¢ 0:03:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6010, 0.3990]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:37 â€¢ 0:03:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6024, 0.3976]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:37 â€¢ 0:03:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6038, 0.3962]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:37 â€¢ 0:03:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6052, 0.3948]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:37 â€¢ 0:03:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6066, 0.3934]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:37 â€¢ 0:03:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6079, 0.3921]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:37 â€¢ 0:03:02[0m [2;4m0.93it/s[0m  
[2KStep 10 - TTA Loss: 0.5444[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 34/203 [2m0:00:37 â€¢ 0:03:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6092, 0.3908]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:37 â€¢ 0:03:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6104, 0.3896]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:37 â€¢ 0:03:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6115, 0.3885]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:37 â€¢ 0:03:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6126, 0.3874]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:37 â€¢ 0:03:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6136, 0.3864]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:37 â€¢ 0:03:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6145, 0.3855]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:37 â€¢ 0:03:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6154, 0.3846]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:37 â€¢ 0:03:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6161, 0.3839]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:37 â€¢ 0:03:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6168, 0.3832]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:37 â€¢ 0:03:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6173, 0.3827]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:37 â€¢ 0:03:02[0m [2;4m0.93it/s[0m  
[2KStep 20 - TTA Loss: 0.5811[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 34/203 [2m0:00:37 â€¢ 0:03:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6178, 0.3822]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:37 â€¢ 0:03:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6182, 0.3818]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:37 â€¢ 0:03:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6185, 0.3815]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:37 â€¢ 0:03:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6188, 0.3812]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:37 â€¢ 0:03:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6190, 0.3810]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:37 â€¢ 0:03:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6191, 0.3809]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:37 â€¢ 0:03:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6192, 0.3808]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:37 â€¢ 0:03:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6192, 0.3808]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:37 â€¢ 0:03:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6193, 0.3807]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:37 â€¢ 0:03:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5654, 0.4346]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 34/203 [2m0:00:37 â€¢ 0:03:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6397, 0.3603],4mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 34/203 [2m0:00:37 â€¢ 0:03:02[0m [2;4m0.93it/s[0m  
        [0.5997, 0.4003],
        [0.6027, 0.3973],
        [0.6520, 0.3480],
        [0.6193, 0.3807],
        [0.6234, 0.3766],
        [0.5754, 0.4246],
        [0.6560, 0.3440],
        [0.6664, 0.3336],
[2K        [0.6647, 0.3353]], device='cuda:0')24mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 34/203 [2m0:00:37 â€¢ 0:03:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5655, 0.4345],6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 35/203 [2m0:00:38 â€¢ 0:02:54[0m [2;4m0.97it/s[0m  
        [0.5599, 0.4401],
        [0.5629, 0.4371],
        [0.5632, 0.4368],
        [0.5643, 0.4357],
        [0.5613, 0.4387],
        [0.5602, 0.4398],
        [0.5643, 0.4357],
        [0.5640, 0.4360],
[2K        [0.5632, 0.4368]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 35/203 [2m0:00:38 â€¢ 0:02:54[0m [2;4m0.97it/s[0m  
[2KClass label: GolfSwingâ”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 35/203 [2m0:00:38 â€¢ 0:02:54[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6361, 0.3639]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:38 â€¢ 0:02:54[0m [2;4m0.97it/s[0m  
[2KStep 0 - TTA Loss: 1.0804â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 35/203 [2m0:00:38 â€¢ 0:02:54[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6361, 0.3639]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:38 â€¢ 0:02:54[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6361, 0.3639]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:38 â€¢ 0:02:54[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6361, 0.3639]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:38 â€¢ 0:02:54[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6362, 0.3638]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:38 â€¢ 0:02:54[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6362, 0.3638]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:38 â€¢ 0:02:54[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6363, 0.3637]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:38 â€¢ 0:02:54[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6364, 0.3636]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:38 â€¢ 0:02:54[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6366, 0.3634]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:38 â€¢ 0:02:54[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6368, 0.3632]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:38 â€¢ 0:02:54[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6371, 0.3629]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:38 â€¢ 0:02:54[0m [2;4m0.97it/s[0m  
[2KStep 10 - TTA Loss: 1.0404[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 35/203 [2m0:00:38 â€¢ 0:02:54[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6374, 0.3626]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:38 â€¢ 0:02:54[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6378, 0.3622]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:38 â€¢ 0:02:54[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6383, 0.3617]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:38 â€¢ 0:02:54[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6389, 0.3611]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:38 â€¢ 0:02:54[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6396, 0.3604]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:38 â€¢ 0:02:54[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6403, 0.3597]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:38 â€¢ 0:02:54[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6411, 0.3589]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:38 â€¢ 0:02:54[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6419, 0.3581]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:38 â€¢ 0:02:54[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6429, 0.3571]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:38 â€¢ 0:02:54[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6441, 0.3559]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:38 â€¢ 0:02:54[0m [2;4m0.97it/s[0m  
[2KStep 20 - TTA Loss: 1.0083[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 35/203 [2m0:00:38 â€¢ 0:02:54[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6453, 0.3547]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:38 â€¢ 0:02:54[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6466, 0.3534]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:38 â€¢ 0:02:54[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6481, 0.3519]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:38 â€¢ 0:02:54[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6496, 0.3504]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:38 â€¢ 0:02:54[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6510, 0.3490]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:38 â€¢ 0:02:54[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6526, 0.3474]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:38 â€¢ 0:02:54[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6542, 0.3458]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:38 â€¢ 0:02:54[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6557, 0.3443]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:38 â€¢ 0:02:54[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6574, 0.3426]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:38 â€¢ 0:02:54[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.5648, 0.4352]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 35/203 [2m0:00:38 â€¢ 0:02:54[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6419, 0.3581],4mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 35/203 [2m0:00:38 â€¢ 0:02:54[0m [2;4m0.97it/s[0m  
        [0.5955, 0.4045],
        [0.5917, 0.4083],
        [0.6594, 0.3406],
        [0.6143, 0.3857],
        [0.6216, 0.3784],
        [0.5738, 0.4262],
        [0.6587, 0.3413],
        [0.6694, 0.3306],
[2K        [0.6628, 0.3372]], device='cuda:0')24mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 35/203 [2m0:00:38 â€¢ 0:02:54[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.5657, 0.4343],7mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 36/203 [2m0:00:39 â€¢ 0:02:52[0m [2;4m0.97it/s[0m   
        [0.5601, 0.4399],
        [0.5630, 0.4370],
        [0.5635, 0.4365],
        [0.5644, 0.4356],
        [0.5615, 0.4385],
        [0.5604, 0.4396],
        [0.5645, 0.4355],
        [0.5642, 0.4358],
[2K        [0.5634, 0.4366]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 36/203 [2m0:00:39 â€¢ 0:02:52[0m [2;4m0.97it/s[0m  
[2KClass label: TennisSwingâ”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 36/203 [2m0:00:39 â€¢ 0:02:52[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6545, 0.3455]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:39 â€¢ 0:02:52[0m [2;4m0.97it/s[0m  
[2KStep 0 - TTA Loss: 0.7000â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 36/203 [2m0:00:39 â€¢ 0:02:52[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6560, 0.3440]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:39 â€¢ 0:02:52[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6573, 0.3427]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:39 â€¢ 0:02:52[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6588, 0.3412]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:39 â€¢ 0:02:52[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6603, 0.3397]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:39 â€¢ 0:02:52[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6618, 0.3382]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:39 â€¢ 0:02:52[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6631, 0.3369]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:39 â€¢ 0:02:52[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6645, 0.3355]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:39 â€¢ 0:02:52[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6659, 0.3341]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:39 â€¢ 0:02:52[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6673, 0.3327]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:39 â€¢ 0:02:52[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6686, 0.3314]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:39 â€¢ 0:02:52[0m [2;4m0.97it/s[0m  
[2KStep 10 - TTA Loss: 0.8138â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 36/203 [2m0:00:39 â€¢ 0:02:52[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6699, 0.3301]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:39 â€¢ 0:02:52[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6712, 0.3288]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:39 â€¢ 0:02:52[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6724, 0.3276]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:39 â€¢ 0:02:52[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6734, 0.3266]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:39 â€¢ 0:02:52[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6744, 0.3256]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:39 â€¢ 0:02:52[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6752, 0.3248]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:39 â€¢ 0:02:52[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6760, 0.3240]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:39 â€¢ 0:02:52[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6767, 0.3233]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:39 â€¢ 0:02:52[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6774, 0.3226]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:39 â€¢ 0:02:52[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6779, 0.3221]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:39 â€¢ 0:02:52[0m [2;4m0.97it/s[0m  
[2KStep 20 - TTA Loss: 0.7007â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 36/203 [2m0:00:39 â€¢ 0:02:52[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6784, 0.3216]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:39 â€¢ 0:02:52[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6787, 0.3213]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:39 â€¢ 0:02:52[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6790, 0.3210]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:39 â€¢ 0:02:52[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6792, 0.3208]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:39 â€¢ 0:02:52[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6794, 0.3206]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:39 â€¢ 0:02:52[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6795, 0.3205]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:39 â€¢ 0:02:52[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6796, 0.3204]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:39 â€¢ 0:02:52[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6796, 0.3204]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:39 â€¢ 0:02:52[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6796, 0.3204]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:39 â€¢ 0:02:52[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.5657, 0.4343]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 36/203 [2m0:00:39 â€¢ 0:02:52[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6426, 0.3574],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 36/203 [2m0:00:39 â€¢ 0:02:52[0m [2;4m0.97it/s[0m  
        [0.5994, 0.4006],
        [0.5890, 0.4110],
        [0.6602, 0.3398],
        [0.6141, 0.3859],
        [0.6220, 0.3780],
        [0.5707, 0.4293],
        [0.6589, 0.3411],
        [0.6796, 0.3204],
[2K        [0.6621, 0.3379]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 36/203 [2m0:00:39 â€¢ 0:02:52[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.5659, 0.4341],7mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:40 â€¢ 0:02:50[0m [2;4m0.98it/s[0m  
        [0.5603, 0.4397],
        [0.5632, 0.4368],
        [0.5638, 0.4362],
        [0.5646, 0.4354],
        [0.5617, 0.4383],
        [0.5606, 0.4394],
        [0.5647, 0.4353],
        [0.5645, 0.4355],
[2K        [0.5636, 0.4364]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:40 â€¢ 0:02:50[0m [2;4m0.98it/s[0m  
[2KClass label: SoccerPenaltyâ”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:40 â€¢ 0:02:50[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6434, 0.3566]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:00:40 â€¢ 0:02:50[0m [2;4m0.98it/s[0m  
[2KStep 0 - TTA Loss: 1.0461â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:40 â€¢ 0:02:50[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6434, 0.3566]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:00:40 â€¢ 0:02:50[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6434, 0.3566]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:00:40 â€¢ 0:02:50[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6434, 0.3566]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:00:40 â€¢ 0:02:50[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6434, 0.3566]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:00:40 â€¢ 0:02:50[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6435, 0.3565]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:00:40 â€¢ 0:02:50[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6437, 0.3563]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:00:40 â€¢ 0:02:50[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6439, 0.3561]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:00:40 â€¢ 0:02:50[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6443, 0.3557]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:00:40 â€¢ 0:02:50[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6447, 0.3553]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:00:40 â€¢ 0:02:50[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6453, 0.3547]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:00:40 â€¢ 0:02:50[0m [2;4m0.98it/s[0m  
[2KStep 10 - TTA Loss: 1.0301â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:40 â€¢ 0:02:50[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6460, 0.3540]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:00:40 â€¢ 0:02:50[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6470, 0.3530]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:00:40 â€¢ 0:02:50[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6480, 0.3520]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:00:40 â€¢ 0:02:50[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6493, 0.3507]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:00:40 â€¢ 0:02:50[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6508, 0.3492]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:00:40 â€¢ 0:02:50[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6525, 0.3475]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:00:40 â€¢ 0:02:50[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6544, 0.3456]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:00:40 â€¢ 0:02:50[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6565, 0.3435]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:00:40 â€¢ 0:02:50[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6589, 0.3411]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:00:40 â€¢ 0:02:50[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6615, 0.3385]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:00:40 â€¢ 0:02:50[0m [2;4m0.98it/s[0m  
[2KStep 20 - TTA Loss: 0.9312â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:40 â€¢ 0:02:50[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6643, 0.3357]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:00:40 â€¢ 0:02:50[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6672, 0.3328]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:00:40 â€¢ 0:02:50[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6705, 0.3295]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:00:40 â€¢ 0:02:50[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6739, 0.3261]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:00:40 â€¢ 0:02:50[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6775, 0.3225]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:00:40 â€¢ 0:02:50[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6813, 0.3187]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:00:40 â€¢ 0:02:50[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6851, 0.3149]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:00:40 â€¢ 0:02:50[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6891, 0.3109]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:00:40 â€¢ 0:02:50[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6931, 0.3069]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:00:40 â€¢ 0:02:50[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.5681, 0.4319]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:40 â€¢ 0:02:50[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6479, 0.3521],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:40 â€¢ 0:02:50[0m [2;4m0.98it/s[0m  
        [0.6089, 0.3911],
        [0.5926, 0.4074],
        [0.6570, 0.3430],
        [0.6194, 0.3806],
        [0.6298, 0.3702],
        [0.5832, 0.4168],
        [0.6972, 0.3028],
        [0.6752, 0.3248],
[2K        [0.6651, 0.3349]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:40 â€¢ 0:02:50[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.5665, 0.4335],7mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 38/203 [2m0:00:41 â€¢ 0:02:49[0m [2;4m0.98it/s[0m  
        [0.5609, 0.4391],
        [0.5637, 0.4363],
        [0.5644, 0.4356],
        [0.5651, 0.4349],
        [0.5623, 0.4377],
        [0.5611, 0.4389],
        [0.5658, 0.4342],
        [0.5651, 0.4349],
[2K        [0.5641, 0.4359]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 38/203 [2m0:00:41 â€¢ 0:02:49[0m [2;4m0.98it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 38/203 [2m0:00:41 â€¢ 0:02:49[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6395, 0.3605]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:00:41 â€¢ 0:02:49[0m [2;4m0.98it/s[0m  
[2KStep 0 - TTA Loss: 0.8851â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 38/203 [2m0:00:41 â€¢ 0:02:49[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6415, 0.3585]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:00:41 â€¢ 0:02:49[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6437, 0.3563]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:00:41 â€¢ 0:02:49[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6459, 0.3541]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:00:41 â€¢ 0:02:49[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6482, 0.3518]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:00:41 â€¢ 0:02:49[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6506, 0.3494]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:00:41 â€¢ 0:02:49[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6530, 0.3470]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:00:41 â€¢ 0:02:49[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6554, 0.3446]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:00:41 â€¢ 0:02:49[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6578, 0.3422]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:00:41 â€¢ 0:02:49[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6601, 0.3399]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:00:41 â€¢ 0:02:49[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6625, 0.3375]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:00:41 â€¢ 0:02:49[0m [2;4m0.98it/s[0m  
[2KStep 10 - TTA Loss: 0.9737â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 38/203 [2m0:00:41 â€¢ 0:02:49[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6647, 0.3353]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:00:41 â€¢ 0:02:49[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6669, 0.3331]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:00:41 â€¢ 0:02:49[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6690, 0.3310]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:00:41 â€¢ 0:02:49[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6710, 0.3290]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:00:41 â€¢ 0:02:49[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6728, 0.3272]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:00:41 â€¢ 0:02:49[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6745, 0.3255]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:00:41 â€¢ 0:02:49[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6760, 0.3240]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:00:41 â€¢ 0:02:49[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6775, 0.3225]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:00:41 â€¢ 0:02:49[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6787, 0.3213]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:00:41 â€¢ 0:02:49[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6798, 0.3202]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:00:41 â€¢ 0:02:49[0m [2;4m0.98it/s[0m  
[2KStep 20 - TTA Loss: 0.8255â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 38/203 [2m0:00:41 â€¢ 0:02:49[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6807, 0.3193]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:00:41 â€¢ 0:02:49[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6815, 0.3185]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:00:41 â€¢ 0:02:49[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6821, 0.3179]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:00:41 â€¢ 0:02:49[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6826, 0.3174]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:00:41 â€¢ 0:02:49[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6830, 0.3170]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:00:41 â€¢ 0:02:49[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6833, 0.3167]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:00:41 â€¢ 0:02:49[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6834, 0.3166]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:00:41 â€¢ 0:02:49[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6835, 0.3165]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:00:41 â€¢ 0:02:49[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6836, 0.3164]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:00:41 â€¢ 0:02:49[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.5673, 0.4327]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 38/203 [2m0:00:41 â€¢ 0:02:49[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6597, 0.3403],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 38/203 [2m0:00:41 â€¢ 0:02:49[0m [2;4m0.98it/s[0m  
        [0.6107, 0.3893],
        [0.6025, 0.3975],
        [0.6655, 0.3345],
        [0.6358, 0.3642],
        [0.6418, 0.3582],
        [0.5932, 0.4068],
        [0.6901, 0.3099],
        [0.6818, 0.3182],
[2K        [0.6837, 0.3163]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 38/203 [2m0:00:41 â€¢ 0:02:49[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.5673, 0.4327],;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 39/203 [2m0:00:42 â€¢ 0:02:49[0m [2;4m0.97it/s[0m  
        [0.5615, 0.4385],
        [0.5645, 0.4355],
        [0.5650, 0.4350],
        [0.5660, 0.4340],
        [0.5631, 0.4369],
        [0.5619, 0.4381],
        [0.5667, 0.4333],
        [0.5659, 0.4341],
[2K        [0.5653, 0.4347]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 39/203 [2m0:00:42 â€¢ 0:02:49[0m [2;4m0.97it/s[0m  
[2KClass label: GolfSwingâ”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 39/203 [2m0:00:42 â€¢ 0:02:49[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6361, 0.3639]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:00:42 â€¢ 0:02:49[0m [2;4m0.97it/s[0m  
[2KStep 0 - TTA Loss: 0.6580â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 39/203 [2m0:00:42 â€¢ 0:02:49[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6361, 0.3639]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:00:42 â€¢ 0:02:49[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6361, 0.3639]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:00:42 â€¢ 0:02:49[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6362, 0.3638]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:00:42 â€¢ 0:02:49[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6362, 0.3638]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:00:42 â€¢ 0:02:49[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6363, 0.3637]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:00:42 â€¢ 0:02:49[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6364, 0.3636]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:00:42 â€¢ 0:02:49[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6366, 0.3634]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:00:42 â€¢ 0:02:49[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6368, 0.3632]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:00:42 â€¢ 0:02:49[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6371, 0.3629]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:00:42 â€¢ 0:02:49[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6374, 0.3626]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:00:42 â€¢ 0:02:49[0m [2;4m0.97it/s[0m  
[2KStep 10 - TTA Loss: 0.6221â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 39/203 [2m0:00:42 â€¢ 0:02:49[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6377, 0.3623]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:00:42 â€¢ 0:02:49[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6381, 0.3619]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:00:42 â€¢ 0:02:49[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6385, 0.3615]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:00:42 â€¢ 0:02:49[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6389, 0.3611]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:00:42 â€¢ 0:02:49[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6394, 0.3606]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:00:42 â€¢ 0:02:49[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6399, 0.3601]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:00:42 â€¢ 0:02:49[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6404, 0.3596]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:00:42 â€¢ 0:02:49[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6408, 0.3592]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:00:42 â€¢ 0:02:49[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6414, 0.3586]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:00:42 â€¢ 0:02:49[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6419, 0.3581]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:00:42 â€¢ 0:02:49[0m [2;4m0.97it/s[0m  
[2KStep 20 - TTA Loss: 0.6247â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 39/203 [2m0:00:42 â€¢ 0:02:49[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6425, 0.3575]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:00:42 â€¢ 0:02:49[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6431, 0.3569]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:00:42 â€¢ 0:02:49[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6438, 0.3562]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:00:42 â€¢ 0:02:49[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6445, 0.3555]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:00:42 â€¢ 0:02:49[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6452, 0.3548]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:00:42 â€¢ 0:02:49[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6459, 0.3541]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:00:42 â€¢ 0:02:49[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6465, 0.3535]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:00:42 â€¢ 0:02:49[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6471, 0.3529]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:00:42 â€¢ 0:02:49[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6478, 0.3522]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:00:42 â€¢ 0:02:49[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.5659, 0.4341]], device='cuda:0')mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 39/203 [2m0:00:42 â€¢ 0:02:49[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6336, 0.3664],24mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 39/203 [2m0:00:42 â€¢ 0:02:49[0m [2;4m0.97it/s[0m  
        [0.5946, 0.4054],
        [0.5830, 0.4170],
        [0.6484, 0.3516],
        [0.6044, 0.3956],
        [0.6133, 0.3867],
        [0.5668, 0.4332],
        [0.6520, 0.3480],
        [0.6625, 0.3375],
[2K        [0.6552, 0.3448]], device='cuda:0')224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 39/203 [2m0:00:42 â€¢ 0:02:49[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.5674, 0.4326],;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 40/203 [2m0:00:43 â€¢ 0:02:50[0m [2;4m0.96it/s[0m  
        [0.5616, 0.4384],
        [0.5646, 0.4354],
        [0.5652, 0.4348],
        [0.5662, 0.4338],
        [0.5632, 0.4368],
        [0.5620, 0.4380],
        [0.5669, 0.4331],
        [0.5660, 0.4340],
[2K        [0.5655, 0.4345]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 40/203 [2m0:00:43 â€¢ 0:02:50[0m [2;4m0.96it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 40/203 [2m0:00:43 â€¢ 0:02:50[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.5561, 0.4439]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:00:43 â€¢ 0:02:50[0m [2;4m0.96it/s[0m  
[2KStep 0 - TTA Loss: 0.7698â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 40/203 [2m0:00:43 â€¢ 0:02:50[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.5565, 0.4435]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:00:43 â€¢ 0:02:50[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.5569, 0.4431]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:00:43 â€¢ 0:02:50[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.5572, 0.4428]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:00:43 â€¢ 0:02:50[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.5575, 0.4425]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:00:43 â€¢ 0:02:50[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.5577, 0.4423]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:00:43 â€¢ 0:02:50[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.5580, 0.4420]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:00:43 â€¢ 0:02:50[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.5582, 0.4418]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:00:43 â€¢ 0:02:50[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.5584, 0.4416]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:00:43 â€¢ 0:02:50[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.5587, 0.4413]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:00:43 â€¢ 0:02:50[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.5589, 0.4411]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:00:43 â€¢ 0:02:50[0m [2;4m0.96it/s[0m  
[2KStep 10 - TTA Loss: 0.7097â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 40/203 [2m0:00:43 â€¢ 0:02:50[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.5591, 0.4409]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:00:43 â€¢ 0:02:50[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.5593, 0.4407]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:00:43 â€¢ 0:02:50[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.5594, 0.4406]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:00:43 â€¢ 0:02:50[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.5596, 0.4404]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:00:43 â€¢ 0:02:50[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.5598, 0.4402]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:00:43 â€¢ 0:02:50[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.5599, 0.4401]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:00:43 â€¢ 0:02:50[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.5601, 0.4399]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:00:43 â€¢ 0:02:50[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.5602, 0.4398]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:00:43 â€¢ 0:02:50[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.5604, 0.4396]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:00:43 â€¢ 0:02:50[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.5606, 0.4394]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:00:43 â€¢ 0:02:50[0m [2;4m0.96it/s[0m  
[2KStep 20 - TTA Loss: 0.7097â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 40/203 [2m0:00:43 â€¢ 0:02:50[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.5607, 0.4393]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:00:43 â€¢ 0:02:50[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.5608, 0.4392]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:00:43 â€¢ 0:02:50[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.5609, 0.4391]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:00:43 â€¢ 0:02:50[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.5610, 0.4390]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:00:43 â€¢ 0:02:50[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.5611, 0.4389]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:00:43 â€¢ 0:02:50[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.5612, 0.4388]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:00:43 â€¢ 0:02:50[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.5612, 0.4388]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:00:43 â€¢ 0:02:50[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.5612, 0.4388]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:00:43 â€¢ 0:02:50[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.5612, 0.4388]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:00:43 â€¢ 0:02:50[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.5621, 0.4379]], device='cuda:0')mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 40/203 [2m0:00:43 â€¢ 0:02:50[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6270, 0.3730],24mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 40/203 [2m0:00:43 â€¢ 0:02:50[0m [2;4m0.96it/s[0m  
        [0.5865, 0.4135],
        [0.5805, 0.4195],
        [0.6400, 0.3600],
        [0.5992, 0.4008],
        [0.6073, 0.3927],
        [0.5612, 0.4388],
        [0.6487, 0.3513],
        [0.6561, 0.3439],
[2K        [0.6476, 0.3524]], device='cuda:0')224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 40/203 [2m0:00:43 â€¢ 0:02:50[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.5673, 0.4327],37mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 41/203 [2m0:00:44 â€¢ 0:02:51[0m [2;4m0.95it/s[0m   
        [0.5616, 0.4384],
        [0.5645, 0.4355],
        [0.5652, 0.4348],
        [0.5660, 0.4340],
        [0.5631, 0.4369],
        [0.5619, 0.4381],
        [0.5668, 0.4332],
        [0.5659, 0.4341],
[2K        [0.5655, 0.4345]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 41/203 [2m0:00:44 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 41/203 [2m0:00:44 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6403, 0.3597]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:00:44 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KStep 0 - TTA Loss: 0.5860â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 41/203 [2m0:00:44 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6403, 0.3597]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:00:44 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6403, 0.3597]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:00:44 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6403, 0.3597]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:00:44 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6403, 0.3597]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:00:44 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6404, 0.3596]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:00:44 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6405, 0.3595]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:00:44 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6407, 0.3593]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:00:44 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6409, 0.3591]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:00:44 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6412, 0.3588]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:00:44 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6416, 0.3584]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:00:44 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KStep 10 - TTA Loss: 0.6465â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 41/203 [2m0:00:44 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6421, 0.3579]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:00:44 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6427, 0.3573]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:00:44 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6435, 0.3565]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:00:44 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6444, 0.3556]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:00:44 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6454, 0.3546]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:00:44 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6465, 0.3535]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:00:44 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6478, 0.3522]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:00:44 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6493, 0.3507]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:00:44 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6510, 0.3490]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:00:44 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6528, 0.3472]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:00:44 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KStep 20 - TTA Loss: 0.6480â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 41/203 [2m0:00:44 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6547, 0.3453]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:00:44 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6569, 0.3431]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:00:44 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6592, 0.3408]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:00:44 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6617, 0.3383]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:00:44 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6643, 0.3357]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:00:44 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6672, 0.3328]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:00:44 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6701, 0.3299]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:00:44 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6731, 0.3269]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:00:44 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6762, 0.3238]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:00:44 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.5680, 0.4320]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 41/203 [2m0:00:44 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6524, 0.3476],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 41/203 [2m0:00:44 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
        [0.5984, 0.4016],
        [0.6011, 0.3989],
        [0.6639, 0.3361],
        [0.6295, 0.3705],
        [0.6348, 0.3652],
        [0.5863, 0.4137],
        [0.6649, 0.3351],
        [0.6779, 0.3221],
[2K        [0.6795, 0.3205]], device='cuda:0')â•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 41/203 [2m0:00:44 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.5676, 0.4324],37mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 42/203 [2m0:00:45 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
        [0.5618, 0.4382],
        [0.5649, 0.4351],
        [0.5654, 0.4346],
        [0.5664, 0.4336],
        [0.5634, 0.4366],
        [0.5622, 0.4378],
        [0.5671, 0.4329],
        [0.5662, 0.4338],
[2K        [0.5661, 0.4339]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 42/203 [2m0:00:45 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 42/203 [2m0:00:45 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6492, 0.3508]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:00:45 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KStep 0 - TTA Loss: 0.8548â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 42/203 [2m0:00:45 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6523, 0.3477]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:00:45 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6552, 0.3448]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:00:45 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6578, 0.3422]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:00:45 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6603, 0.3397]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:00:45 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6625, 0.3375]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:00:45 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6646, 0.3354]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:00:45 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6665, 0.3335]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:00:45 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6682, 0.3318]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:00:45 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6698, 0.3302]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:00:45 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6712, 0.3288]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:00:45 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KStep 10 - TTA Loss: 0.8799â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 42/203 [2m0:00:45 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6725, 0.3275]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:00:45 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6736, 0.3264]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:00:45 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6747, 0.3253]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:00:45 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6756, 0.3244]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:00:45 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6764, 0.3236]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:00:45 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6771, 0.3229]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:00:45 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6777, 0.3223]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:00:45 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6782, 0.3218]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:00:45 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6787, 0.3213]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:00:45 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6790, 0.3210]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:00:45 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KStep 20 - TTA Loss: 0.8879â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 42/203 [2m0:00:45 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6794, 0.3206]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:00:45 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6796, 0.3204]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:00:45 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6798, 0.3202]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:00:45 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6800, 0.3200]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:00:45 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6801, 0.3199]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:00:45 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6802, 0.3198]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:00:45 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6802, 0.3198]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:00:45 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6803, 0.3197]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:00:45 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6803, 0.3197]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:00:45 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.5687, 0.4313]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 42/203 [2m0:00:45 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6541, 0.3459],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 42/203 [2m0:00:45 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
        [0.6062, 0.3938],
        [0.6026, 0.3974],
        [0.6645, 0.3355],
        [0.6301, 0.3699],
        [0.6355, 0.3645],
        [0.5888, 0.4112],
        [0.6696, 0.3304],
        [0.6797, 0.3203],
[2K        [0.6804, 0.3196]], device='cuda:0')â•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 42/203 [2m0:00:45 â€¢ 0:02:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.5680, 0.4320],37mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 43/203 [2m0:00:46 â€¢ 0:02:50[0m [2;4m0.95it/s[0m  
        [0.5621, 0.4379],
        [0.5653, 0.4347],
        [0.5657, 0.4343],
        [0.5669, 0.4331],
        [0.5637, 0.4363],
        [0.5625, 0.4375],
        [0.5674, 0.4326],
        [0.5665, 0.4335],
[2K        [0.5669, 0.4331]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 43/203 [2m0:00:46 â€¢ 0:02:50[0m [2;4m0.95it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 43/203 [2m0:00:46 â€¢ 0:02:50[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6452, 0.3548]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:00:46 â€¢ 0:02:50[0m [2;4m0.95it/s[0m  
[2KStep 0 - TTA Loss: 0.7000â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 43/203 [2m0:00:46 â€¢ 0:02:50[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6452, 0.3548]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:00:46 â€¢ 0:02:50[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6452, 0.3548]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:00:46 â€¢ 0:02:50[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6452, 0.3548]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:00:46 â€¢ 0:02:50[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6452, 0.3548]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:00:46 â€¢ 0:02:50[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6453, 0.3547]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:00:46 â€¢ 0:02:50[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6454, 0.3546]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:00:46 â€¢ 0:02:50[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6455, 0.3545]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:00:46 â€¢ 0:02:50[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6457, 0.3543]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:00:46 â€¢ 0:02:50[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6460, 0.3540]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:00:46 â€¢ 0:02:50[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6463, 0.3537]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:00:46 â€¢ 0:02:50[0m [2;4m0.95it/s[0m  
[2KStep 10 - TTA Loss: 0.7061â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 43/203 [2m0:00:46 â€¢ 0:02:50[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6468, 0.3532]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:00:46 â€¢ 0:02:50[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6473, 0.3527]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:00:46 â€¢ 0:02:50[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6479, 0.3521]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:00:46 â€¢ 0:02:50[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6486, 0.3514]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:00:46 â€¢ 0:02:50[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6493, 0.3507]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:00:46 â€¢ 0:02:50[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6502, 0.3498]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:00:46 â€¢ 0:02:50[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6511, 0.3489]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:00:46 â€¢ 0:02:50[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6521, 0.3479]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:00:46 â€¢ 0:02:50[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6533, 0.3467]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:00:46 â€¢ 0:02:50[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6545, 0.3455]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:00:46 â€¢ 0:02:50[0m [2;4m0.95it/s[0m  
[2KStep 20 - TTA Loss: 0.7255â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 43/203 [2m0:00:46 â€¢ 0:02:50[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6559, 0.3441]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:00:46 â€¢ 0:02:50[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6574, 0.3426]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:00:46 â€¢ 0:02:50[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6589, 0.3411]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:00:46 â€¢ 0:02:50[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6606, 0.3394]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:00:46 â€¢ 0:02:50[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6624, 0.3376]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:00:46 â€¢ 0:02:50[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6643, 0.3357]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:00:46 â€¢ 0:02:50[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6662, 0.3338]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:00:46 â€¢ 0:02:50[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6683, 0.3317]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:00:46 â€¢ 0:02:50[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6705, 0.3295]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:00:46 â€¢ 0:02:50[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.5695, 0.4305]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 43/203 [2m0:00:46 â€¢ 0:02:50[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6467, 0.3533],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 43/203 [2m0:00:46 â€¢ 0:02:50[0m [2;4m0.95it/s[0m  
        [0.5984, 0.4016],
        [0.5970, 0.4030],
        [0.6589, 0.3411],
        [0.6210, 0.3790],
        [0.6292, 0.3708],
        [0.5807, 0.4193],
        [0.6625, 0.3375],
        [0.6759, 0.3241],
[2K        [0.6729, 0.3271]], device='cuda:0')â•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 43/203 [2m0:00:46 â€¢ 0:02:50[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.5685, 0.4315],8;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 44/203 [2m0:00:47 â€¢ 0:02:49[0m [2;4m0.94it/s[0m  
        [0.5625, 0.4375],
        [0.5660, 0.4340],
        [0.5661, 0.4339],
        [0.5677, 0.4323],
        [0.5643, 0.4357],
        [0.5631, 0.4369],
        [0.5678, 0.4322],
        [0.5670, 0.4330],
[2K        [0.5680, 0.4320]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 44/203 [2m0:00:47 â€¢ 0:02:49[0m [2;4m0.94it/s[0m  
[2KClass label: HighJumpâ”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 44/203 [2m0:00:47 â€¢ 0:02:49[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6015, 0.3985]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:00:47 â€¢ 0:02:49[0m [2;4m0.94it/s[0m  
[2KStep 0 - TTA Loss: 0.4909â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 44/203 [2m0:00:47 â€¢ 0:02:49[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6037, 0.3963]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:00:47 â€¢ 0:02:49[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6060, 0.3940]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:00:47 â€¢ 0:02:49[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6083, 0.3917]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:00:47 â€¢ 0:02:49[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6108, 0.3892]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:00:47 â€¢ 0:02:49[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6133, 0.3867]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:00:47 â€¢ 0:02:49[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6158, 0.3842]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:00:47 â€¢ 0:02:49[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6183, 0.3817]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:00:47 â€¢ 0:02:49[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6208, 0.3792]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:00:47 â€¢ 0:02:49[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6232, 0.3768]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:00:47 â€¢ 0:02:49[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6255, 0.3745]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:00:47 â€¢ 0:02:49[0m [2;4m0.94it/s[0m  
[2KStep 10 - TTA Loss: 0.4694â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 44/203 [2m0:00:47 â€¢ 0:02:49[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6277, 0.3723]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:00:47 â€¢ 0:02:49[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6298, 0.3702]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:00:47 â€¢ 0:02:49[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6319, 0.3681]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:00:47 â€¢ 0:02:49[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6338, 0.3662]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:00:47 â€¢ 0:02:49[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6355, 0.3645]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:00:47 â€¢ 0:02:49[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6371, 0.3629]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:00:47 â€¢ 0:02:49[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6386, 0.3614]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:00:47 â€¢ 0:02:49[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6399, 0.3601]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:00:47 â€¢ 0:02:49[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6410, 0.3590]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:00:47 â€¢ 0:02:49[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6420, 0.3580]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:00:47 â€¢ 0:02:49[0m [2;4m0.94it/s[0m  
[2KStep 20 - TTA Loss: 0.4321â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 44/203 [2m0:00:47 â€¢ 0:02:49[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6429, 0.3571]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:00:47 â€¢ 0:02:49[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6436, 0.3564]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:00:47 â€¢ 0:02:49[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6442, 0.3558]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:00:47 â€¢ 0:02:49[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6446, 0.3554]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:00:47 â€¢ 0:02:49[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6450, 0.3550]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:00:47 â€¢ 0:02:49[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6452, 0.3548]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:00:47 â€¢ 0:02:49[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6453, 0.3547]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:00:47 â€¢ 0:02:49[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6454, 0.3546]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:00:47 â€¢ 0:02:49[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6455, 0.3545]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:00:47 â€¢ 0:02:49[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5669, 0.4331]], device='cuda:0')7mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 44/203 [2m0:00:47 â€¢ 0:02:49[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6546, 0.3454],224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 44/203 [2m0:00:47 â€¢ 0:02:49[0m [2;4m0.94it/s[0m  
        [0.6053, 0.3947],
        [0.6080, 0.3920],
        [0.6621, 0.3379],
        [0.6330, 0.3670],
        [0.6455, 0.3545],
        [0.5937, 0.4063],
        [0.6708, 0.3292],
        [0.6803, 0.3197],
[2K        [0.6813, 0.3187]], device='cuda:0');224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 44/203 [2m0:00:47 â€¢ 0:02:49[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5689, 0.4311],8;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 45/203 [2m0:00:49 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
        [0.5629, 0.4371],
        [0.5666, 0.4334],
        [0.5665, 0.4335],
        [0.5684, 0.4316],
        [0.5652, 0.4348],
        [0.5639, 0.4361],
        [0.5684, 0.4316],
        [0.5675, 0.4325],
[2K        [0.5688, 0.4312]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 45/203 [2m0:00:49 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KClass label: GolfSwingâ”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 45/203 [2m0:00:49 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6384, 0.3616]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:00:49 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KStep 0 - TTA Loss: 0.7657â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 45/203 [2m0:00:49 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6384, 0.3616]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:00:49 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6384, 0.3616]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:00:49 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6384, 0.3616]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:00:49 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6384, 0.3616]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:00:49 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6386, 0.3614]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:00:49 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6387, 0.3613]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:00:49 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6390, 0.3610]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:00:49 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6394, 0.3606]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:00:49 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6398, 0.3602]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:00:49 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6404, 0.3596]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:00:49 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KStep 10 - TTA Loss: 0.7636â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 45/203 [2m0:00:49 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6412, 0.3588]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:00:49 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6420, 0.3580]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:00:49 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6431, 0.3569]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:00:49 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6443, 0.3557]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:00:49 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6456, 0.3544]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:00:49 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6472, 0.3528]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:00:49 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6489, 0.3511]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:00:49 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6508, 0.3492]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:00:49 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6529, 0.3471]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:00:49 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6552, 0.3448]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:00:49 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KStep 20 - TTA Loss: 0.7539â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 45/203 [2m0:00:49 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6576, 0.3424]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:00:49 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6602, 0.3398]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:00:49 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6630, 0.3370]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:00:49 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6659, 0.3341]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:00:49 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6690, 0.3310]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:00:49 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6721, 0.3279]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:00:49 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6754, 0.3246]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:00:49 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6788, 0.3212]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:00:49 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6822, 0.3178]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:00:49 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5695, 0.4305]], device='cuda:0')7mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 45/203 [2m0:00:49 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6642, 0.3358],224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 45/203 [2m0:00:49 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
        [0.6138, 0.3862],
        [0.6047, 0.3953],
        [0.6858, 0.3142],
        [0.6334, 0.3666],
        [0.6400, 0.3600],
        [0.5913, 0.4087],
        [0.6726, 0.3274],
        [0.6899, 0.3101],
[2K        [0.6834, 0.3166]], device='cuda:0');224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 45/203 [2m0:00:49 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5695, 0.4305],237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 46/203 [2m0:00:50 â€¢ 0:02:52[0m [2;4m0.92it/s[0m   
        [0.5635, 0.4365],
        [0.5670, 0.4330],
        [0.5673, 0.4327],
        [0.5688, 0.4312],
        [0.5658, 0.4342],
        [0.5644, 0.4356],
        [0.5689, 0.4311],
        [0.5681, 0.4319],
[2K        [0.5693, 0.4307]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 46/203 [2m0:00:50 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KClass label: GolfSwingâ”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 46/203 [2m0:00:50 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6377, 0.3623]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 46/203 [2m0:00:50 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KStep 0 - TTA Loss: 0.7669â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 46/203 [2m0:00:50 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6413, 0.3587]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 46/203 [2m0:00:50 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6447, 0.3553]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 46/203 [2m0:00:50 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6481, 0.3519]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 46/203 [2m0:00:50 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6513, 0.3487]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 46/203 [2m0:00:50 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6545, 0.3455]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 46/203 [2m0:00:50 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6576, 0.3424]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 46/203 [2m0:00:50 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6605, 0.3395]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 46/203 [2m0:00:50 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6634, 0.3366]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 46/203 [2m0:00:50 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6661, 0.3339]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 46/203 [2m0:00:50 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6685, 0.3315]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 46/203 [2m0:00:50 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KStep 10 - TTA Loss: 0.7007â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 46/203 [2m0:00:50 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6708, 0.3292]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 46/203 [2m0:00:50 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6730, 0.3270]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 46/203 [2m0:00:50 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6750, 0.3250]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 46/203 [2m0:00:50 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6769, 0.3231]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 46/203 [2m0:00:50 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6786, 0.3214]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 46/203 [2m0:00:50 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6802, 0.3198]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 46/203 [2m0:00:50 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6816, 0.3184]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 46/203 [2m0:00:50 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6828, 0.3172]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 46/203 [2m0:00:50 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6839, 0.3161]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 46/203 [2m0:00:50 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6849, 0.3151]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 46/203 [2m0:00:50 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KStep 20 - TTA Loss: 0.7430â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 46/203 [2m0:00:50 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6856, 0.3144]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 46/203 [2m0:00:50 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6863, 0.3137]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 46/203 [2m0:00:50 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6868, 0.3132]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 46/203 [2m0:00:50 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6872, 0.3128]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 46/203 [2m0:00:50 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6876, 0.3124]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 46/203 [2m0:00:50 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6878, 0.3122]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 46/203 [2m0:00:50 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6879, 0.3121]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 46/203 [2m0:00:50 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6880, 0.3120]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 46/203 [2m0:00:50 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6880, 0.3120]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 46/203 [2m0:00:50 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5703, 0.4297]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 46/203 [2m0:00:50 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6656, 0.3344],â•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 46/203 [2m0:00:50 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
        [0.6102, 0.3898],
        [0.6067, 0.3933],
        [0.6881, 0.3119],
        [0.6336, 0.3664],
        [0.6383, 0.3617],
        [0.5911, 0.4089],
        [0.6711, 0.3289],
        [0.6922, 0.3078],
[2K        [0.6835, 0.3165]], device='cuda:0')mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 46/203 [2m0:00:50 â€¢ 0:02:52[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5700, 0.4300],237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 47/203 [2m0:00:51 â€¢ 0:02:51[0m [2;4m0.91it/s[0m  
        [0.5640, 0.4360],
        [0.5674, 0.4326],
        [0.5681, 0.4319],
        [0.5692, 0.4308],
        [0.5662, 0.4338],
        [0.5648, 0.4352],
        [0.5693, 0.4307],
        [0.5687, 0.4313],
[2K        [0.5696, 0.4304]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 47/203 [2m0:00:51 â€¢ 0:02:51[0m [2;4m0.91it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 47/203 [2m0:00:51 â€¢ 0:02:51[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5580, 0.4420]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 47/203 [2m0:00:51 â€¢ 0:02:51[0m [2;4m0.91it/s[0m  
[2KStep 0 - TTA Loss: 0.7393â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 47/203 [2m0:00:51 â€¢ 0:02:51[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5580, 0.4420]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 47/203 [2m0:00:51 â€¢ 0:02:51[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5580, 0.4420]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 47/203 [2m0:00:51 â€¢ 0:02:51[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5580, 0.4420]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 47/203 [2m0:00:51 â€¢ 0:02:51[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5581, 0.4419]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 47/203 [2m0:00:51 â€¢ 0:02:51[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5582, 0.4418]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 47/203 [2m0:00:51 â€¢ 0:02:51[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5584, 0.4416]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 47/203 [2m0:00:51 â€¢ 0:02:51[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5587, 0.4413]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 47/203 [2m0:00:51 â€¢ 0:02:51[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5592, 0.4408]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 47/203 [2m0:00:51 â€¢ 0:02:51[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5598, 0.4402]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 47/203 [2m0:00:51 â€¢ 0:02:51[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5605, 0.4395]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 47/203 [2m0:00:51 â€¢ 0:02:51[0m [2;4m0.91it/s[0m  
[2KStep 10 - TTA Loss: 0.7208â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 47/203 [2m0:00:51 â€¢ 0:02:51[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5614, 0.4386]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 47/203 [2m0:00:51 â€¢ 0:02:51[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5625, 0.4375]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 47/203 [2m0:00:51 â€¢ 0:02:51[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5638, 0.4362]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 47/203 [2m0:00:51 â€¢ 0:02:51[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5654, 0.4346]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 47/203 [2m0:00:51 â€¢ 0:02:51[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5671, 0.4329]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 47/203 [2m0:00:51 â€¢ 0:02:51[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5691, 0.4309]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 47/203 [2m0:00:51 â€¢ 0:02:51[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5712, 0.4288]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 47/203 [2m0:00:51 â€¢ 0:02:51[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5737, 0.4263]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 47/203 [2m0:00:51 â€¢ 0:02:51[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5763, 0.4237]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 47/203 [2m0:00:51 â€¢ 0:02:51[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5792, 0.4208]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 47/203 [2m0:00:51 â€¢ 0:02:51[0m [2;4m0.91it/s[0m  
[2KStep 20 - TTA Loss: 0.7428â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 47/203 [2m0:00:51 â€¢ 0:02:51[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5824, 0.4176]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 47/203 [2m0:00:51 â€¢ 0:02:51[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5857, 0.4143]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 47/203 [2m0:00:51 â€¢ 0:02:51[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5893, 0.4107]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 47/203 [2m0:00:51 â€¢ 0:02:51[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5931, 0.4069]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 47/203 [2m0:00:51 â€¢ 0:02:51[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5971, 0.4029]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 47/203 [2m0:00:51 â€¢ 0:02:51[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6013, 0.3987]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 47/203 [2m0:00:51 â€¢ 0:02:51[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6056, 0.3944]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 47/203 [2m0:00:51 â€¢ 0:02:51[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6101, 0.3899]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 47/203 [2m0:00:51 â€¢ 0:02:51[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6147, 0.3853]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 47/203 [2m0:00:51 â€¢ 0:02:51[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5680, 0.4320]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 47/203 [2m0:00:51 â€¢ 0:02:51[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6602, 0.3398],â•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 47/203 [2m0:00:51 â€¢ 0:02:51[0m [2;4m0.91it/s[0m  
        [0.6132, 0.3868],
        [0.6125, 0.3875],
        [0.6713, 0.3287],
        [0.6404, 0.3596],
        [0.6530, 0.3470],
        [0.6194, 0.3806],
        [0.6777, 0.3223],
        [0.6796, 0.3204],
[2K        [0.6859, 0.3141]], device='cuda:0')mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 47/203 [2m0:00:51 â€¢ 0:02:51[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5704, 0.4296],237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 48/203 [2m0:00:53 â€¢ 0:02:49[0m [2;4m0.92it/s[0m  
        [0.5644, 0.4356],
        [0.5678, 0.4322],
        [0.5685, 0.4315],
        [0.5696, 0.4304],
        [0.5668, 0.4332],
        [0.5657, 0.4343],
        [0.5697, 0.4303],
        [0.5691, 0.4309],
[2K        [0.5700, 0.4300]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 48/203 [2m0:00:53 â€¢ 0:02:49[0m [2;4m0.92it/s[0m  
[2KClass label: TennisSwingâ”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 48/203 [2m0:00:53 â€¢ 0:02:49[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6539, 0.3461]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 48/203 [2m0:00:53 â€¢ 0:02:49[0m [2;4m0.92it/s[0m  
[2KStep 0 - TTA Loss: 0.7528â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 48/203 [2m0:00:53 â€¢ 0:02:49[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6557, 0.3443]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 48/203 [2m0:00:53 â€¢ 0:02:49[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6574, 0.3426]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 48/203 [2m0:00:53 â€¢ 0:02:49[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6589, 0.3411]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 48/203 [2m0:00:53 â€¢ 0:02:49[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6604, 0.3396]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 48/203 [2m0:00:53 â€¢ 0:02:49[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6617, 0.3383]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 48/203 [2m0:00:53 â€¢ 0:02:49[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6629, 0.3371]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 48/203 [2m0:00:53 â€¢ 0:02:49[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6640, 0.3360]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 48/203 [2m0:00:53 â€¢ 0:02:49[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6650, 0.3350]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 48/203 [2m0:00:53 â€¢ 0:02:49[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6659, 0.3341]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 48/203 [2m0:00:53 â€¢ 0:02:49[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6668, 0.3332]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 48/203 [2m0:00:53 â€¢ 0:02:49[0m [2;4m0.92it/s[0m  
[2KStep 10 - TTA Loss: 0.7420â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 48/203 [2m0:00:53 â€¢ 0:02:49[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6675, 0.3325]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 48/203 [2m0:00:53 â€¢ 0:02:49[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6682, 0.3318]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 48/203 [2m0:00:53 â€¢ 0:02:49[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6689, 0.3311]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 48/203 [2m0:00:53 â€¢ 0:02:49[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6694, 0.3306]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 48/203 [2m0:00:53 â€¢ 0:02:49[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6699, 0.3301]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 48/203 [2m0:00:53 â€¢ 0:02:49[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6703, 0.3297]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 48/203 [2m0:00:53 â€¢ 0:02:49[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6707, 0.3293]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 48/203 [2m0:00:53 â€¢ 0:02:49[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6710, 0.3290]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 48/203 [2m0:00:53 â€¢ 0:02:49[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6713, 0.3287]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 48/203 [2m0:00:53 â€¢ 0:02:49[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6715, 0.3285]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 48/203 [2m0:00:53 â€¢ 0:02:49[0m [2;4m0.92it/s[0m  
[2KStep 20 - TTA Loss: 0.7450â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 48/203 [2m0:00:53 â€¢ 0:02:49[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6717, 0.3283]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 48/203 [2m0:00:53 â€¢ 0:02:49[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6719, 0.3281]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 48/203 [2m0:00:53 â€¢ 0:02:49[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6720, 0.3280]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 48/203 [2m0:00:53 â€¢ 0:02:49[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6721, 0.3279]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 48/203 [2m0:00:53 â€¢ 0:02:49[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6722, 0.3278]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 48/203 [2m0:00:53 â€¢ 0:02:49[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6722, 0.3278]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 48/203 [2m0:00:53 â€¢ 0:02:49[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6723, 0.3277]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 48/203 [2m0:00:53 â€¢ 0:02:49[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6723, 0.3277]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 48/203 [2m0:00:53 â€¢ 0:02:49[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6723, 0.3277]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 48/203 [2m0:00:53 â€¢ 0:02:49[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5702, 0.4298]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 48/203 [2m0:00:53 â€¢ 0:02:49[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6423, 0.3577],â•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 48/203 [2m0:00:53 â€¢ 0:02:49[0m [2;4m0.92it/s[0m  
        [0.6035, 0.3965],
        [0.5972, 0.4028],
        [0.6570, 0.3430],
        [0.6208, 0.3792],
        [0.6335, 0.3665],
        [0.5953, 0.4047],
        [0.6671, 0.3329],
        [0.6723, 0.3277],
[2K        [0.6691, 0.3309]], device='cuda:0')mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 48/203 [2m0:00:53 â€¢ 0:02:49[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5706, 0.4294],98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 49/203 [2m0:00:53 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
        [0.5647, 0.4353],
        [0.5680, 0.4320],
        [0.5687, 0.4313],
        [0.5699, 0.4301],
        [0.5672, 0.4328],
        [0.5663, 0.4337],
        [0.5700, 0.4300],
        [0.5694, 0.4306],
[2K        [0.5702, 0.4298]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 49/203 [2m0:00:53 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KClass label: CleanAndJerkâ”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 49/203 [2m0:00:53 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5792, 0.4208]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 49/203 [2m0:00:53 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KStep 0 - TTA Loss: 0.4898â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 49/203 [2m0:00:53 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5792, 0.4208]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 49/203 [2m0:00:53 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5792, 0.4208]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 49/203 [2m0:00:53 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5792, 0.4208]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 49/203 [2m0:00:53 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5792, 0.4208]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 49/203 [2m0:00:53 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5793, 0.4207]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 49/203 [2m0:00:53 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5794, 0.4206]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 49/203 [2m0:00:53 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5796, 0.4204]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 49/203 [2m0:00:53 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5799, 0.4201]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 49/203 [2m0:00:53 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5802, 0.4198]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 49/203 [2m0:00:53 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5807, 0.4193]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 49/203 [2m0:00:53 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KStep 10 - TTA Loss: 0.5534â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 49/203 [2m0:00:53 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5813, 0.4187]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 49/203 [2m0:00:53 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5820, 0.4180]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 49/203 [2m0:00:53 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5828, 0.4172]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 49/203 [2m0:00:53 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5839, 0.4161]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 49/203 [2m0:00:53 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5851, 0.4149]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 49/203 [2m0:00:53 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5865, 0.4135]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 49/203 [2m0:00:53 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5881, 0.4119]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 49/203 [2m0:00:53 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5898, 0.4102]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 49/203 [2m0:00:53 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5918, 0.4082]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 49/203 [2m0:00:53 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5939, 0.4061]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 49/203 [2m0:00:53 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KStep 20 - TTA Loss: 0.5348â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 49/203 [2m0:00:53 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5961, 0.4039]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 49/203 [2m0:00:53 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5985, 0.4015]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 49/203 [2m0:00:53 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6011, 0.3989]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 49/203 [2m0:00:53 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6039, 0.3961]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 49/203 [2m0:00:53 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6067, 0.3933]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 49/203 [2m0:00:53 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6097, 0.3903]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 49/203 [2m0:00:53 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6128, 0.3872]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 49/203 [2m0:00:53 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6161, 0.3839]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 49/203 [2m0:00:53 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6194, 0.3806]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 49/203 [2m0:00:53 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5703, 0.4297]], device='cuda:0')37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 49/203 [2m0:00:53 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6396, 0.3604],;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 49/203 [2m0:00:53 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
        [0.6034, 0.3966],
        [0.6228, 0.3772],
        [0.6541, 0.3459],
        [0.6197, 0.3803],
        [0.6275, 0.3725],
        [0.5797, 0.4203],
        [0.6573, 0.3427],
        [0.6706, 0.3294],
[2K        [0.6672, 0.3328]], device='cuda:0')6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 49/203 [2m0:00:53 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5709, 0.4291],98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 50/203 [2m0:00:55 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
        [0.5651, 0.4349],
        [0.5688, 0.4312],
        [0.5690, 0.4310],
        [0.5703, 0.4297],
        [0.5676, 0.4324],
        [0.5666, 0.4334],
        [0.5703, 0.4297],
        [0.5698, 0.4302],
[2K        [0.5707, 0.4293]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 50/203 [2m0:00:55 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KClass label: SoccerPenaltyâ”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 50/203 [2m0:00:55 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6472, 0.3528]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 50/203 [2m0:00:55 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KStep 0 - TTA Loss: 0.6831â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 50/203 [2m0:00:55 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6483, 0.3517]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 50/203 [2m0:00:55 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6494, 0.3506]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 50/203 [2m0:00:55 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6507, 0.3493]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 50/203 [2m0:00:55 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6520, 0.3480]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 50/203 [2m0:00:55 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6534, 0.3466]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 50/203 [2m0:00:55 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6549, 0.3451]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 50/203 [2m0:00:55 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6563, 0.3437]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 50/203 [2m0:00:55 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6577, 0.3423]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 50/203 [2m0:00:55 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6591, 0.3409]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 50/203 [2m0:00:55 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6605, 0.3395]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 50/203 [2m0:00:55 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KStep 10 - TTA Loss: 0.5807â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 50/203 [2m0:00:55 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6619, 0.3381]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 50/203 [2m0:00:55 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6633, 0.3367]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 50/203 [2m0:00:55 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6645, 0.3355]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 50/203 [2m0:00:55 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6658, 0.3342]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 50/203 [2m0:00:55 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6669, 0.3331]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 50/203 [2m0:00:55 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6680, 0.3320]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 50/203 [2m0:00:55 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6690, 0.3310]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 50/203 [2m0:00:55 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6699, 0.3301]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 50/203 [2m0:00:55 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6707, 0.3293]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 50/203 [2m0:00:55 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6714, 0.3286]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 50/203 [2m0:00:55 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KStep 20 - TTA Loss: 0.5545â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 50/203 [2m0:00:55 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6720, 0.3280]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 50/203 [2m0:00:55 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6725, 0.3275]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 50/203 [2m0:00:55 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6730, 0.3270]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 50/203 [2m0:00:55 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6733, 0.3267]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 50/203 [2m0:00:55 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6735, 0.3265]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 50/203 [2m0:00:55 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6737, 0.3263]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 50/203 [2m0:00:55 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6738, 0.3262]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 50/203 [2m0:00:55 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6739, 0.3261]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 50/203 [2m0:00:55 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6739, 0.3261]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 50/203 [2m0:00:55 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5715, 0.4285]], device='cuda:0')37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 50/203 [2m0:00:55 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6445, 0.3555],;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 50/203 [2m0:00:55 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
        [0.6063, 0.3937],
        [0.6117, 0.3883],
        [0.6559, 0.3441],
        [0.6207, 0.3793],
        [0.6303, 0.3697],
        [0.5838, 0.4162],
        [0.6740, 0.3260],
        [0.6729, 0.3271],
[2K        [0.6685, 0.3315]], device='cuda:0')6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 50/203 [2m0:00:55 â€¢ 0:02:48[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5711, 0.4289],;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 51/203 [2m0:00:56 â€¢ 0:02:46[0m [2;4m0.92it/s[0m   
        [0.5653, 0.4347],
        [0.5692, 0.4308],
        [0.5692, 0.4308],
        [0.5705, 0.4295],
        [0.5678, 0.4322],
        [0.5668, 0.4332],
        [0.5704, 0.4296],
        [0.5700, 0.4300],
[2K        [0.5709, 0.4291]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 51/203 [2m0:00:56 â€¢ 0:02:46[0m [2;4m0.92it/s[0m  
[2KClass label: BaseballPitchâ”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 51/203 [2m0:00:56 â€¢ 0:02:46[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6264, 0.3736]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 51/203 [2m0:00:56 â€¢ 0:02:46[0m [2;4m0.92it/s[0m  
[2KStep 0 - TTA Loss: 1.0806â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 51/203 [2m0:00:56 â€¢ 0:02:46[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6264, 0.3736]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 51/203 [2m0:00:56 â€¢ 0:02:46[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6264, 0.3736]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 51/203 [2m0:00:56 â€¢ 0:02:46[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6264, 0.3736]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 51/203 [2m0:00:56 â€¢ 0:02:46[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6265, 0.3735]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 51/203 [2m0:00:56 â€¢ 0:02:46[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6265, 0.3735]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 51/203 [2m0:00:56 â€¢ 0:02:46[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6266, 0.3734]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 51/203 [2m0:00:56 â€¢ 0:02:46[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6266, 0.3734]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 51/203 [2m0:00:56 â€¢ 0:02:46[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6268, 0.3732]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 51/203 [2m0:00:56 â€¢ 0:02:46[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6269, 0.3731]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 51/203 [2m0:00:56 â€¢ 0:02:46[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6270, 0.3730]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 51/203 [2m0:00:56 â€¢ 0:02:46[0m [2;4m0.92it/s[0m  
[2KStep 10 - TTA Loss: 0.9518â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 51/203 [2m0:00:56 â€¢ 0:02:46[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6272, 0.3728]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 51/203 [2m0:00:56 â€¢ 0:02:46[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6275, 0.3725]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 51/203 [2m0:00:56 â€¢ 0:02:46[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6277, 0.3723]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 51/203 [2m0:00:56 â€¢ 0:02:46[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6279, 0.3721]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 51/203 [2m0:00:56 â€¢ 0:02:46[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6282, 0.3718]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 51/203 [2m0:00:56 â€¢ 0:02:46[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6285, 0.3715]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 51/203 [2m0:00:56 â€¢ 0:02:46[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6288, 0.3712]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 51/203 [2m0:00:56 â€¢ 0:02:46[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6292, 0.3708]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 51/203 [2m0:00:56 â€¢ 0:02:46[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6295, 0.3705]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 51/203 [2m0:00:56 â€¢ 0:02:46[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6299, 0.3701]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 51/203 [2m0:00:56 â€¢ 0:02:46[0m [2;4m0.92it/s[0m  
[2KStep 20 - TTA Loss: 1.0279â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 51/203 [2m0:00:56 â€¢ 0:02:46[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6302, 0.3698]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 51/203 [2m0:00:56 â€¢ 0:02:46[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6306, 0.3694]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 51/203 [2m0:00:56 â€¢ 0:02:46[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6310, 0.3690]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 51/203 [2m0:00:56 â€¢ 0:02:46[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6314, 0.3686]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 51/203 [2m0:00:56 â€¢ 0:02:46[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6318, 0.3682]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 51/203 [2m0:00:56 â€¢ 0:02:46[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6323, 0.3677]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 51/203 [2m0:00:56 â€¢ 0:02:46[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6327, 0.3673]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 51/203 [2m0:00:56 â€¢ 0:02:46[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6331, 0.3669]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 51/203 [2m0:00:56 â€¢ 0:02:46[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6336, 0.3664]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 51/203 [2m0:00:56 â€¢ 0:02:46[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5712, 0.4288]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 51/203 [2m0:00:56 â€¢ 0:02:46[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6342, 0.3658],mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 51/203 [2m0:00:56 â€¢ 0:02:46[0m [2;4m0.92it/s[0m  
        [0.5940, 0.4060],
        [0.5848, 0.4152],
        [0.6456, 0.3544],
        [0.6036, 0.3964],
        [0.6141, 0.3859],
        [0.5682, 0.4318],
        [0.6579, 0.3421],
        [0.6614, 0.3386],
[2K        [0.6544, 0.3456]], device='cuda:0')7mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 51/203 [2m0:00:56 â€¢ 0:02:46[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5708, 0.4292],;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 52/203 [2m0:00:56 â€¢ 0:02:44[0m [2;4m0.92it/s[0m  
        [0.5652, 0.4348],
        [0.5690, 0.4310],
        [0.5691, 0.4309],
        [0.5703, 0.4297],
        [0.5676, 0.4324],
        [0.5667, 0.4333],
        [0.5702, 0.4298],
        [0.5698, 0.4302],
[2K        [0.5708, 0.4292]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 52/203 [2m0:00:56 â€¢ 0:02:44[0m [2;4m0.92it/s[0m  
[2KClass label: CleanAndJerkâ”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 52/203 [2m0:00:56 â€¢ 0:02:44[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5809, 0.4191]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 52/203 [2m0:00:56 â€¢ 0:02:44[0m [2;4m0.92it/s[0m  
[2KStep 0 - TTA Loss: 0.6897â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 52/203 [2m0:00:56 â€¢ 0:02:44[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5813, 0.4187]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 52/203 [2m0:00:56 â€¢ 0:02:44[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5821, 0.4179]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 52/203 [2m0:00:56 â€¢ 0:02:44[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5832, 0.4168]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 52/203 [2m0:00:56 â€¢ 0:02:44[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5844, 0.4156]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 52/203 [2m0:00:56 â€¢ 0:02:44[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5859, 0.4141]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 52/203 [2m0:00:56 â€¢ 0:02:44[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5875, 0.4125]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 52/203 [2m0:00:56 â€¢ 0:02:44[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5893, 0.4107]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 52/203 [2m0:00:56 â€¢ 0:02:44[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5912, 0.4088]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 52/203 [2m0:00:56 â€¢ 0:02:44[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5933, 0.4067]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 52/203 [2m0:00:56 â€¢ 0:02:44[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5953, 0.4047]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 52/203 [2m0:00:56 â€¢ 0:02:44[0m [2;4m0.92it/s[0m  
[2KStep 10 - TTA Loss: 0.6142â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 52/203 [2m0:00:56 â€¢ 0:02:44[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5974, 0.4026]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 52/203 [2m0:00:56 â€¢ 0:02:44[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5994, 0.4006]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 52/203 [2m0:00:56 â€¢ 0:02:44[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6013, 0.3987]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 52/203 [2m0:00:56 â€¢ 0:02:44[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6031, 0.3969]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 52/203 [2m0:00:56 â€¢ 0:02:44[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6049, 0.3951]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 52/203 [2m0:00:56 â€¢ 0:02:44[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6065, 0.3935]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 52/203 [2m0:00:56 â€¢ 0:02:44[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6081, 0.3919]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 52/203 [2m0:00:56 â€¢ 0:02:44[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6094, 0.3906]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 52/203 [2m0:00:56 â€¢ 0:02:44[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6107, 0.3893]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 52/203 [2m0:00:56 â€¢ 0:02:44[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6118, 0.3882]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 52/203 [2m0:00:56 â€¢ 0:02:44[0m [2;4m0.92it/s[0m  
[2KStep 20 - TTA Loss: 0.5988â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 52/203 [2m0:00:56 â€¢ 0:02:44[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6127, 0.3873]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 52/203 [2m0:00:56 â€¢ 0:02:44[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6135, 0.3865]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 52/203 [2m0:00:56 â€¢ 0:02:44[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6141, 0.3859]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 52/203 [2m0:00:56 â€¢ 0:02:44[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6146, 0.3854]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 52/203 [2m0:00:56 â€¢ 0:02:44[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6150, 0.3850]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 52/203 [2m0:00:56 â€¢ 0:02:44[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6152, 0.3848]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 52/203 [2m0:00:56 â€¢ 0:02:44[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6154, 0.3846]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 52/203 [2m0:00:56 â€¢ 0:02:44[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6155, 0.3845]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 52/203 [2m0:00:56 â€¢ 0:02:44[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6156, 0.3844]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 52/203 [2m0:00:56 â€¢ 0:02:44[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5708, 0.4292]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 52/203 [2m0:00:56 â€¢ 0:02:44[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6388, 0.3612],mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 52/203 [2m0:00:56 â€¢ 0:02:44[0m [2;4m0.92it/s[0m  
        [0.6038, 0.3962],
        [0.6156, 0.3844],
        [0.6535, 0.3465],
        [0.6179, 0.3821],
        [0.6282, 0.3718],
        [0.5799, 0.4201],
        [0.6584, 0.3416],
        [0.6701, 0.3299],
[2K        [0.6651, 0.3349]], device='cuda:0')7mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 52/203 [2m0:00:56 â€¢ 0:02:44[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5709, 0.4291],;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 53/203 [2m0:00:57 â€¢ 0:02:43[0m [2;4m0.93it/s[0m  
        [0.5654, 0.4346],
        [0.5696, 0.4304],
        [0.5692, 0.4308],
        [0.5706, 0.4294],
        [0.5679, 0.4321],
        [0.5669, 0.4331],
        [0.5704, 0.4296],
        [0.5700, 0.4300],
[2K        [0.5710, 0.4290]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 53/203 [2m0:00:57 â€¢ 0:02:43[0m [2;4m0.93it/s[0m  
[2KClass label: Billiardsâ”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 53/203 [2m0:00:57 â€¢ 0:02:43[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5915, 0.4085]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 53/203 [2m0:00:57 â€¢ 0:02:43[0m [2;4m0.93it/s[0m  
[2KStep 0 - TTA Loss: 0.9136â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 53/203 [2m0:00:57 â€¢ 0:02:43[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5915, 0.4085]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 53/203 [2m0:00:57 â€¢ 0:02:43[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5915, 0.4085]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 53/203 [2m0:00:57 â€¢ 0:02:43[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5915, 0.4085]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 53/203 [2m0:00:57 â€¢ 0:02:43[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5916, 0.4084]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 53/203 [2m0:00:57 â€¢ 0:02:43[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5916, 0.4084]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 53/203 [2m0:00:57 â€¢ 0:02:43[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5917, 0.4083]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 53/203 [2m0:00:57 â€¢ 0:02:43[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5919, 0.4081]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 53/203 [2m0:00:57 â€¢ 0:02:43[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5921, 0.4079]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 53/203 [2m0:00:57 â€¢ 0:02:43[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5923, 0.4077]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 53/203 [2m0:00:57 â€¢ 0:02:43[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5926, 0.4074]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 53/203 [2m0:00:57 â€¢ 0:02:43[0m [2;4m0.93it/s[0m  
[2KStep 10 - TTA Loss: 0.8709â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 53/203 [2m0:00:57 â€¢ 0:02:43[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5930, 0.4070]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 53/203 [2m0:00:57 â€¢ 0:02:43[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5934, 0.4066]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 53/203 [2m0:00:57 â€¢ 0:02:43[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5940, 0.4060]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 53/203 [2m0:00:57 â€¢ 0:02:43[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5946, 0.4054]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 53/203 [2m0:00:57 â€¢ 0:02:43[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5952, 0.4048]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 53/203 [2m0:00:57 â€¢ 0:02:43[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5960, 0.4040]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 53/203 [2m0:00:57 â€¢ 0:02:43[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5968, 0.4032]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 53/203 [2m0:00:57 â€¢ 0:02:43[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5977, 0.4023]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 53/203 [2m0:00:57 â€¢ 0:02:43[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5987, 0.4013]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 53/203 [2m0:00:57 â€¢ 0:02:43[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5998, 0.4002]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 53/203 [2m0:00:57 â€¢ 0:02:43[0m [2;4m0.93it/s[0m  
[2KStep 20 - TTA Loss: 0.8762â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 53/203 [2m0:00:57 â€¢ 0:02:43[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6010, 0.3990]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 53/203 [2m0:00:57 â€¢ 0:02:43[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6024, 0.3976]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 53/203 [2m0:00:57 â€¢ 0:02:43[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6037, 0.3963]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 53/203 [2m0:00:57 â€¢ 0:02:43[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6052, 0.3948]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 53/203 [2m0:00:57 â€¢ 0:02:43[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6068, 0.3932]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 53/203 [2m0:00:57 â€¢ 0:02:43[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6085, 0.3915]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 53/203 [2m0:00:57 â€¢ 0:02:43[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6102, 0.3898]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 53/203 [2m0:00:57 â€¢ 0:02:43[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6120, 0.3880]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 53/203 [2m0:00:57 â€¢ 0:02:43[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6138, 0.3862]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 53/203 [2m0:00:57 â€¢ 0:02:43[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5667, 0.4333]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 53/203 [2m0:00:57 â€¢ 0:02:43[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6380, 0.3620],mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 53/203 [2m0:00:57 â€¢ 0:02:43[0m [2;4m0.93it/s[0m  
        [0.6158, 0.3842],
        [0.6007, 0.3993],
        [0.6505, 0.3495],
        [0.6133, 0.3867],
        [0.6221, 0.3779],
        [0.5747, 0.4253],
        [0.6618, 0.3382],
        [0.6683, 0.3317],
[2K        [0.6614, 0.3386]], device='cuda:0')7mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 53/203 [2m0:00:57 â€¢ 0:02:43[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5710, 0.4290],;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 54/203 [2m0:00:58 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
        [0.5657, 0.4343],
        [0.5698, 0.4302],
        [0.5694, 0.4306],
        [0.5708, 0.4292],
        [0.5681, 0.4319],
        [0.5671, 0.4329],
        [0.5706, 0.4294],
        [0.5702, 0.4298],
[2K        [0.5712, 0.4288]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 54/203 [2m0:00:58 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KClass label: SoccerPenaltyâ”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 54/203 [2m0:00:58 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6494, 0.3506]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 54/203 [2m0:00:58 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KStep 0 - TTA Loss: 0.4224â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 54/203 [2m0:00:58 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6505, 0.3495]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 54/203 [2m0:00:58 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6521, 0.3479]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 54/203 [2m0:00:58 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6540, 0.3460]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 54/203 [2m0:00:58 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6562, 0.3438]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 54/203 [2m0:00:58 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6587, 0.3413]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 54/203 [2m0:00:58 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6614, 0.3386]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 54/203 [2m0:00:58 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6642, 0.3358]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 54/203 [2m0:00:58 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6672, 0.3328]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 54/203 [2m0:00:58 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6701, 0.3299]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 54/203 [2m0:00:58 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6731, 0.3269]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 54/203 [2m0:00:58 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KStep 10 - TTA Loss: 0.4144â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 54/203 [2m0:00:58 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6761, 0.3239]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 54/203 [2m0:00:58 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6790, 0.3210]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 54/203 [2m0:00:58 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6817, 0.3183]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 54/203 [2m0:00:58 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6844, 0.3156]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 54/203 [2m0:00:58 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6868, 0.3132]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 54/203 [2m0:00:58 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6891, 0.3109]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 54/203 [2m0:00:58 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6912, 0.3088]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 54/203 [2m0:00:58 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6931, 0.3069]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 54/203 [2m0:00:58 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6949, 0.3051]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 54/203 [2m0:00:58 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6963, 0.3037]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 54/203 [2m0:00:58 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KStep 20 - TTA Loss: 0.3927â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 54/203 [2m0:00:58 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6976, 0.3024]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 54/203 [2m0:00:58 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6987, 0.3013]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 54/203 [2m0:00:58 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6995, 0.3005]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 54/203 [2m0:00:58 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.7002, 0.2998]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 54/203 [2m0:00:58 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.7007, 0.2993]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 54/203 [2m0:00:58 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.7011, 0.2989]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 54/203 [2m0:00:58 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.7013, 0.2987]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 54/203 [2m0:00:58 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.7015, 0.2985]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 54/203 [2m0:00:58 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.7015, 0.2985]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 54/203 [2m0:00:58 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5733, 0.4267]], device='cuda:0')237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 54/203 [2m0:00:58 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6556, 0.3444],6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 54/203 [2m0:00:58 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
        [0.6225, 0.3775],
        [0.6019, 0.3981],
        [0.6613, 0.3387],
        [0.6286, 0.3714],
        [0.6380, 0.3620],
        [0.5903, 0.4097],
        [0.7016, 0.2984],
        [0.6814, 0.3186],
[2K        [0.6734, 0.3266]], device='cuda:0');6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 54/203 [2m0:00:58 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5713, 0.4287],;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 55/203 [2m0:01:00 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
        [0.5660, 0.4340],
        [0.5700, 0.4300],
        [0.5696, 0.4304],
        [0.5710, 0.4290],
        [0.5683, 0.4317],
        [0.5673, 0.4327],
        [0.5711, 0.4289],
        [0.5705, 0.4295],
[2K        [0.5714, 0.4286]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 55/203 [2m0:01:00 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KClass label: HammerThrowâ”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 55/203 [2m0:01:00 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6023, 0.3977]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 55/203 [2m0:01:00 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KStep 0 - TTA Loss: 0.5911â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 55/203 [2m0:01:00 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6023, 0.3977]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 55/203 [2m0:01:00 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6024, 0.3976]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 55/203 [2m0:01:00 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6024, 0.3976]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 55/203 [2m0:01:00 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6025, 0.3975]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 55/203 [2m0:01:00 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6026, 0.3974]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 55/203 [2m0:01:00 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6029, 0.3971]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 55/203 [2m0:01:00 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6032, 0.3968]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 55/203 [2m0:01:00 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6037, 0.3963]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 55/203 [2m0:01:00 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6043, 0.3957]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 55/203 [2m0:01:00 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6051, 0.3949]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 55/203 [2m0:01:00 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KStep 10 - TTA Loss: 0.6972â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 55/203 [2m0:01:00 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6060, 0.3940]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 55/203 [2m0:01:00 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6071, 0.3929]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 55/203 [2m0:01:00 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6084, 0.3916]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 55/203 [2m0:01:00 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6100, 0.3900]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 55/203 [2m0:01:00 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6117, 0.3883]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 55/203 [2m0:01:00 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6137, 0.3863]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 55/203 [2m0:01:00 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6159, 0.3841]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 55/203 [2m0:01:00 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6183, 0.3817]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 55/203 [2m0:01:00 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6210, 0.3790]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 55/203 [2m0:01:00 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6239, 0.3761]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 55/203 [2m0:01:00 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KStep 20 - TTA Loss: 0.6728â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 55/203 [2m0:01:00 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6270, 0.3730]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 55/203 [2m0:01:00 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6302, 0.3698]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 55/203 [2m0:01:00 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6336, 0.3664]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 55/203 [2m0:01:00 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6373, 0.3627]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 55/203 [2m0:01:00 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6412, 0.3588]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 55/203 [2m0:01:00 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6452, 0.3548]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 55/203 [2m0:01:00 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6495, 0.3505]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 55/203 [2m0:01:00 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6538, 0.3462]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 55/203 [2m0:01:00 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6582, 0.3418]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 55/203 [2m0:01:00 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5748, 0.4252]], device='cuda:0')237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 55/203 [2m0:01:00 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6752, 0.3248],6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 55/203 [2m0:01:00 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
        [0.6238, 0.3762],
        [0.6248, 0.3752],
        [0.6832, 0.3168],
        [0.6628, 0.3372],
        [0.6597, 0.3403],
        [0.6126, 0.3874],
        [0.6948, 0.3052],
        [0.6940, 0.3060],
[2K        [0.7008, 0.2992]], device='cuda:0');6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 55/203 [2m0:01:00 â€¢ 0:02:39[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5720, 0.4280],5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 56/203 [2m0:01:01 â€¢ 0:02:36[0m [2;4m0.94it/s[0m   
        [0.5666, 0.4334],
        [0.5708, 0.4292],
        [0.5702, 0.4298],
        [0.5722, 0.4278],
        [0.5691, 0.4309],
        [0.5682, 0.4318],
        [0.5717, 0.4283],
        [0.5711, 0.4289],
[2K        [0.5723, 0.4277]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 56/203 [2m0:01:01 â€¢ 0:02:36[0m [2;4m0.94it/s[0m  
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 56/203 [2m0:01:01 â€¢ 0:02:36[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6140, 0.3860]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 56/203 [2m0:01:01 â€¢ 0:02:36[0m [2;4m0.94it/s[0m  
[2KStep 0 - TTA Loss: 1.4046â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 56/203 [2m0:01:01 â€¢ 0:02:36[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6173, 0.3827]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 56/203 [2m0:01:01 â€¢ 0:02:36[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6203, 0.3797]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 56/203 [2m0:01:01 â€¢ 0:02:36[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6230, 0.3770]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 56/203 [2m0:01:01 â€¢ 0:02:36[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6256, 0.3744]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 56/203 [2m0:01:01 â€¢ 0:02:36[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6278, 0.3722]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 56/203 [2m0:01:01 â€¢ 0:02:36[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6299, 0.3701]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 56/203 [2m0:01:01 â€¢ 0:02:36[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6318, 0.3682]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 56/203 [2m0:01:01 â€¢ 0:02:36[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6334, 0.3666]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 56/203 [2m0:01:01 â€¢ 0:02:36[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6350, 0.3650]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 56/203 [2m0:01:01 â€¢ 0:02:36[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6364, 0.3636]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 56/203 [2m0:01:01 â€¢ 0:02:36[0m [2;4m0.94it/s[0m  
[2KStep 10 - TTA Loss: 1.2889â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 56/203 [2m0:01:01 â€¢ 0:02:36[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6377, 0.3623]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 56/203 [2m0:01:01 â€¢ 0:02:36[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6388, 0.3612]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 56/203 [2m0:01:01 â€¢ 0:02:36[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6399, 0.3601]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 56/203 [2m0:01:01 â€¢ 0:02:36[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6408, 0.3592]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 56/203 [2m0:01:01 â€¢ 0:02:36[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6417, 0.3583]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 56/203 [2m0:01:01 â€¢ 0:02:36[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6425, 0.3575]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 56/203 [2m0:01:01 â€¢ 0:02:36[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6432, 0.3568]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 56/203 [2m0:01:01 â€¢ 0:02:36[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6437, 0.3563]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 56/203 [2m0:01:01 â€¢ 0:02:36[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6442, 0.3558]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 56/203 [2m0:01:01 â€¢ 0:02:36[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6446, 0.3554]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 56/203 [2m0:01:01 â€¢ 0:02:36[0m [2;4m0.94it/s[0m  
[2KStep 20 - TTA Loss: 1.0721â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 56/203 [2m0:01:01 â€¢ 0:02:36[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6449, 0.3551]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 56/203 [2m0:01:01 â€¢ 0:02:36[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6452, 0.3548]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 56/203 [2m0:01:01 â€¢ 0:02:36[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6454, 0.3546]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 56/203 [2m0:01:01 â€¢ 0:02:36[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6455, 0.3545]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 56/203 [2m0:01:01 â€¢ 0:02:36[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6456, 0.3544]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 56/203 [2m0:01:01 â€¢ 0:02:36[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6457, 0.3543]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 56/203 [2m0:01:01 â€¢ 0:02:36[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6457, 0.3543]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 56/203 [2m0:01:01 â€¢ 0:02:36[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6458, 0.3542]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 56/203 [2m0:01:01 â€¢ 0:02:36[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6458, 0.3542]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 56/203 [2m0:01:01 â€¢ 0:02:36[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5713, 0.4287]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 56/203 [2m0:01:01 â€¢ 0:02:36[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6616, 0.3384],7mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 56/203 [2m0:01:01 â€¢ 0:02:36[0m [2;4m0.94it/s[0m  
        [0.6147, 0.3853],
        [0.6132, 0.3868],
        [0.6713, 0.3287],
        [0.6410, 0.3590],
        [0.6459, 0.3541],
        [0.5975, 0.4025],
        [0.6780, 0.3220],
        [0.6849, 0.3151],
[2K        [0.6847, 0.3153]], device='cuda:0')37mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 56/203 [2m0:01:01 â€¢ 0:02:36[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5725, 0.4275],5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 57/203 [2m0:01:02 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
        [0.5670, 0.4330],
        [0.5713, 0.4287],
        [0.5706, 0.4294],
        [0.5729, 0.4271],
        [0.5698, 0.4302],
        [0.5688, 0.4312],
        [0.5721, 0.4279],
        [0.5716, 0.4284],
[2K        [0.5729, 0.4271]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 57/203 [2m0:01:02 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 57/203 [2m0:01:02 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6093, 0.3907]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 57/203 [2m0:01:02 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KStep 0 - TTA Loss: 0.5546â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 57/203 [2m0:01:02 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6093, 0.3907]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 57/203 [2m0:01:02 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6093, 0.3907]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 57/203 [2m0:01:02 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6093, 0.3907]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 57/203 [2m0:01:02 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6093, 0.3907]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 57/203 [2m0:01:02 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6094, 0.3906]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 57/203 [2m0:01:02 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6095, 0.3905]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 57/203 [2m0:01:02 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6096, 0.3904]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 57/203 [2m0:01:02 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6098, 0.3902]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 57/203 [2m0:01:02 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6101, 0.3899]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 57/203 [2m0:01:02 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6104, 0.3896]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 57/203 [2m0:01:02 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KStep 10 - TTA Loss: 0.5348â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 57/203 [2m0:01:02 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6108, 0.3892]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 57/203 [2m0:01:02 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6112, 0.3888]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 57/203 [2m0:01:02 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6117, 0.3883]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 57/203 [2m0:01:02 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6124, 0.3876]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 57/203 [2m0:01:02 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6131, 0.3869]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 57/203 [2m0:01:02 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6139, 0.3861]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 57/203 [2m0:01:02 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6149, 0.3851]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 57/203 [2m0:01:02 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6159, 0.3841]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 57/203 [2m0:01:02 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6170, 0.3830]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 57/203 [2m0:01:02 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6182, 0.3818]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 57/203 [2m0:01:02 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KStep 20 - TTA Loss: 0.5091â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 57/203 [2m0:01:02 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6195, 0.3805]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 57/203 [2m0:01:02 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6209, 0.3791]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 57/203 [2m0:01:02 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6224, 0.3776]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 57/203 [2m0:01:02 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6240, 0.3760]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 57/203 [2m0:01:02 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6257, 0.3743]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 57/203 [2m0:01:02 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6275, 0.3725]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 57/203 [2m0:01:02 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6294, 0.3706]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 57/203 [2m0:01:02 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6313, 0.3687]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 57/203 [2m0:01:02 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6332, 0.3668]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 57/203 [2m0:01:02 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5711, 0.4289]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 57/203 [2m0:01:02 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6436, 0.3564],7mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 57/203 [2m0:01:02 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
        [0.6030, 0.3970],
        [0.5958, 0.4042],
        [0.6551, 0.3449],
        [0.6198, 0.3802],
        [0.6353, 0.3647],
        [0.5849, 0.4151],
        [0.6674, 0.3326],
        [0.6714, 0.3286],
[2K        [0.6678, 0.3322]], device='cuda:0')37mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 57/203 [2m0:01:02 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5725, 0.4275],5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 58/203 [2m0:01:03 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
        [0.5671, 0.4329],
        [0.5714, 0.4286],
        [0.5707, 0.4293],
        [0.5730, 0.4270],
        [0.5700, 0.4300],
        [0.5691, 0.4309],
        [0.5722, 0.4278],
        [0.5717, 0.4283],
[2K        [0.5729, 0.4271]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 58/203 [2m0:01:03 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KClass label: SoccerPenaltyâ”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 58/203 [2m0:01:03 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6500, 0.3500]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 58/203 [2m0:01:03 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KStep 0 - TTA Loss: 0.7685â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 58/203 [2m0:01:03 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6518, 0.3482]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 58/203 [2m0:01:03 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6542, 0.3458]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 58/203 [2m0:01:03 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6569, 0.3431]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 58/203 [2m0:01:03 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6601, 0.3399]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 58/203 [2m0:01:03 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6635, 0.3365]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 58/203 [2m0:01:03 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6671, 0.3329]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 58/203 [2m0:01:03 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6708, 0.3292]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 58/203 [2m0:01:03 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6747, 0.3253]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 58/203 [2m0:01:03 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6786, 0.3214]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 58/203 [2m0:01:03 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6825, 0.3175]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 58/203 [2m0:01:03 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KStep 10 - TTA Loss: 0.7410â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 58/203 [2m0:01:03 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6863, 0.3137]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 58/203 [2m0:01:03 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6900, 0.3100]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 58/203 [2m0:01:03 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6936, 0.3064]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 58/203 [2m0:01:03 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6971, 0.3029]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 58/203 [2m0:01:03 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.7003, 0.2997]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 58/203 [2m0:01:03 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.7033, 0.2967]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 58/203 [2m0:01:03 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.7060, 0.2940]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 58/203 [2m0:01:03 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.7085, 0.2915]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 58/203 [2m0:01:03 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.7107, 0.2893]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 58/203 [2m0:01:03 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.7127, 0.2873]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 58/203 [2m0:01:03 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KStep 20 - TTA Loss: 0.6864â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 58/203 [2m0:01:03 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.7143, 0.2857]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 58/203 [2m0:01:03 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.7157, 0.2843]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 58/203 [2m0:01:03 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.7169, 0.2831]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 58/203 [2m0:01:03 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.7177, 0.2823]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 58/203 [2m0:01:03 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.7184, 0.2816]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 58/203 [2m0:01:03 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.7189, 0.2811]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 58/203 [2m0:01:03 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.7192, 0.2808]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 58/203 [2m0:01:03 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.7194, 0.2806]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 58/203 [2m0:01:03 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.7195, 0.2805]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 58/203 [2m0:01:03 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5761, 0.4239]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 58/203 [2m0:01:03 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6673, 0.3327],7mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 58/203 [2m0:01:03 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
        [0.6268, 0.3732],
        [0.6113, 0.3887],
        [0.6726, 0.3274],
        [0.6424, 0.3576],
        [0.6559, 0.3441],
        [0.6093, 0.3907],
        [0.7195, 0.2805],
        [0.6899, 0.3101],
[2K        [0.6868, 0.3132]], device='cuda:0')37mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 58/203 [2m0:01:03 â€¢ 0:02:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5731, 0.4269],2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 59/203 [2m0:01:04 â€¢ 0:02:33[0m [2;4m0.94it/s[0m  
        [0.5677, 0.4323],
        [0.5718, 0.4282],
        [0.5712, 0.4288],
        [0.5733, 0.4267],
        [0.5705, 0.4295],
        [0.5696, 0.4304],
        [0.5732, 0.4268],
        [0.5722, 0.4278],
[2K        [0.5733, 0.4267]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 59/203 [2m0:01:04 â€¢ 0:02:33[0m [2;4m0.94it/s[0m  
[2KClass label: Billiardsâ”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 59/203 [2m0:01:04 â€¢ 0:02:33[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5928, 0.4072]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 59/203 [2m0:01:04 â€¢ 0:02:33[0m [2;4m0.94it/s[0m  
[2KStep 0 - TTA Loss: 0.8157â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 59/203 [2m0:01:04 â€¢ 0:02:33[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5928, 0.4072]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 59/203 [2m0:01:04 â€¢ 0:02:33[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5928, 0.4072]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 59/203 [2m0:01:04 â€¢ 0:02:33[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5928, 0.4072]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 59/203 [2m0:01:04 â€¢ 0:02:33[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5929, 0.4071]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 59/203 [2m0:01:04 â€¢ 0:02:33[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5930, 0.4070]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 59/203 [2m0:01:04 â€¢ 0:02:33[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5932, 0.4068]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 59/203 [2m0:01:04 â€¢ 0:02:33[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5934, 0.4066]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 59/203 [2m0:01:04 â€¢ 0:02:33[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5938, 0.4062]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 59/203 [2m0:01:04 â€¢ 0:02:33[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5942, 0.4058]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 59/203 [2m0:01:04 â€¢ 0:02:33[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5948, 0.4052]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 59/203 [2m0:01:04 â€¢ 0:02:33[0m [2;4m0.94it/s[0m  
[2KStep 10 - TTA Loss: 0.8022â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 59/203 [2m0:01:04 â€¢ 0:02:33[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5954, 0.4046]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 59/203 [2m0:01:04 â€¢ 0:02:33[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5962, 0.4038]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 59/203 [2m0:01:04 â€¢ 0:02:33[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5971, 0.4029]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 59/203 [2m0:01:04 â€¢ 0:02:33[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5982, 0.4018]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 59/203 [2m0:01:04 â€¢ 0:02:33[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5993, 0.4007]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 59/203 [2m0:01:04 â€¢ 0:02:33[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6006, 0.3994]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 59/203 [2m0:01:04 â€¢ 0:02:33[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6021, 0.3979]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 59/203 [2m0:01:04 â€¢ 0:02:33[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6037, 0.3963]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 59/203 [2m0:01:04 â€¢ 0:02:33[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6054, 0.3946]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 59/203 [2m0:01:04 â€¢ 0:02:33[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6072, 0.3928]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 59/203 [2m0:01:04 â€¢ 0:02:33[0m [2;4m0.94it/s[0m  
[2KStep 20 - TTA Loss: 0.7963â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 59/203 [2m0:01:04 â€¢ 0:02:33[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6091, 0.3909]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 59/203 [2m0:01:04 â€¢ 0:02:33[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6112, 0.3888]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 59/203 [2m0:01:04 â€¢ 0:02:33[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6133, 0.3867]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 59/203 [2m0:01:04 â€¢ 0:02:33[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6156, 0.3844]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 59/203 [2m0:01:04 â€¢ 0:02:33[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6179, 0.3821]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 59/203 [2m0:01:04 â€¢ 0:02:33[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6202, 0.3798]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 59/203 [2m0:01:04 â€¢ 0:02:33[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6227, 0.3773]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 59/203 [2m0:01:04 â€¢ 0:02:33[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6252, 0.3748]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 59/203 [2m0:01:04 â€¢ 0:02:33[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6277, 0.3723]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 59/203 [2m0:01:04 â€¢ 0:02:33[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5696, 0.4304]], device='cuda:0');237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 59/203 [2m0:01:04 â€¢ 0:02:33[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6429, 0.3571],;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 59/203 [2m0:01:04 â€¢ 0:02:33[0m [2;4m0.94it/s[0m  
        [0.6303, 0.3697],
        [0.5946, 0.4054],
        [0.6535, 0.3465],
        [0.6181, 0.3819],
        [0.6280, 0.3720],
        [0.5821, 0.4179],
        [0.6768, 0.3232],
        [0.6739, 0.3261],
[2K        [0.6652, 0.3348]], device='cuda:0')8;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 59/203 [2m0:01:04 â€¢ 0:02:33[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5733, 0.4267],2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 60/203 [2m0:01:05 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
        [0.5682, 0.4318],
        [0.5720, 0.4280],
        [0.5715, 0.4285],
        [0.5735, 0.4265],
        [0.5707, 0.4293],
        [0.5698, 0.4302],
        [0.5735, 0.4265],
        [0.5725, 0.4275],
[2K        [0.5734, 0.4266]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 60/203 [2m0:01:05 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KClass label: CleanAndJerkâ”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 60/203 [2m0:01:05 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5843, 0.4157]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 60/203 [2m0:01:05 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KStep 0 - TTA Loss: 0.5831â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 60/203 [2m0:01:05 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5855, 0.4145]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 60/203 [2m0:01:05 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5871, 0.4129]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 60/203 [2m0:01:05 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5890, 0.4110]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 60/203 [2m0:01:05 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5911, 0.4089]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 60/203 [2m0:01:05 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5935, 0.4065]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 60/203 [2m0:01:05 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5961, 0.4039]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 60/203 [2m0:01:05 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5988, 0.4012]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 60/203 [2m0:01:05 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6016, 0.3984]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 60/203 [2m0:01:05 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6044, 0.3956]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 60/203 [2m0:01:05 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6072, 0.3928]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 60/203 [2m0:01:05 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KStep 10 - TTA Loss: 0.6051â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 60/203 [2m0:01:05 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6099, 0.3901]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 60/203 [2m0:01:05 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6126, 0.3874]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 60/203 [2m0:01:05 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6152, 0.3848]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 60/203 [2m0:01:05 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6177, 0.3823]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 60/203 [2m0:01:05 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6201, 0.3799]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 60/203 [2m0:01:05 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6223, 0.3777]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 60/203 [2m0:01:05 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6243, 0.3757]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 60/203 [2m0:01:05 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6261, 0.3739]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 60/203 [2m0:01:05 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6278, 0.3722]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 60/203 [2m0:01:05 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6292, 0.3708]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 60/203 [2m0:01:05 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KStep 20 - TTA Loss: 0.5952â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 60/203 [2m0:01:05 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6304, 0.3696]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 60/203 [2m0:01:05 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6314, 0.3686]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 60/203 [2m0:01:05 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6322, 0.3678]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 60/203 [2m0:01:05 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6328, 0.3672]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 60/203 [2m0:01:05 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6333, 0.3667]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 60/203 [2m0:01:05 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6337, 0.3663]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 60/203 [2m0:01:05 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6339, 0.3661]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 60/203 [2m0:01:05 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6340, 0.3660]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 60/203 [2m0:01:05 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6341, 0.3659]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 60/203 [2m0:01:05 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5745, 0.4255]], device='cuda:0');237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 60/203 [2m0:01:05 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6471, 0.3529],;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 60/203 [2m0:01:05 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
        [0.6238, 0.3762],
        [0.6341, 0.3659],
        [0.6658, 0.3342],
        [0.6318, 0.3682],
        [0.6404, 0.3596],
        [0.5921, 0.4079],
        [0.6714, 0.3286],
        [0.6820, 0.3180],
[2K        [0.6775, 0.3225]], device='cuda:0')8;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 60/203 [2m0:01:05 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5737, 0.4263],;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 61/203 [2m0:01:06 â€¢ 0:02:32[0m [2;4m0.94it/s[0m   
        [0.5687, 0.4313],
        [0.5727, 0.4273],
        [0.5718, 0.4282],
        [0.5739, 0.4261],
        [0.5711, 0.4289],
        [0.5702, 0.4298],
        [0.5739, 0.4261],
        [0.5729, 0.4271],
[2K        [0.5739, 0.4261]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 61/203 [2m0:01:06 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KClass label: Billiardsâ”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 61/203 [2m0:01:06 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5934, 0.4066]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 61/203 [2m0:01:06 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KStep 0 - TTA Loss: 0.7646â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 61/203 [2m0:01:06 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5934, 0.4066]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 61/203 [2m0:01:06 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5934, 0.4066]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 61/203 [2m0:01:06 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5934, 0.4066]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 61/203 [2m0:01:06 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5934, 0.4066]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 61/203 [2m0:01:06 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5935, 0.4065]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 61/203 [2m0:01:06 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5935, 0.4065]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 61/203 [2m0:01:06 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5937, 0.4063]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 61/203 [2m0:01:06 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5938, 0.4062]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 61/203 [2m0:01:06 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5940, 0.4060]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 61/203 [2m0:01:06 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5942, 0.4058]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 61/203 [2m0:01:06 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KStep 10 - TTA Loss: 0.7599â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 61/203 [2m0:01:06 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5944, 0.4056]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 61/203 [2m0:01:06 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5946, 0.4054]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 61/203 [2m0:01:06 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5949, 0.4051]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 61/203 [2m0:01:06 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5953, 0.4047]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 61/203 [2m0:01:06 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5956, 0.4044]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 61/203 [2m0:01:06 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5960, 0.4040]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 61/203 [2m0:01:06 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5964, 0.4036]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 61/203 [2m0:01:06 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5968, 0.4032]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 61/203 [2m0:01:06 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5973, 0.4027]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 61/203 [2m0:01:06 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5978, 0.4022]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 61/203 [2m0:01:06 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KStep 20 - TTA Loss: 0.7450â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 61/203 [2m0:01:06 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5983, 0.4017]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 61/203 [2m0:01:06 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5988, 0.4012]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 61/203 [2m0:01:06 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5994, 0.4006]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 61/203 [2m0:01:06 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6001, 0.3999]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 61/203 [2m0:01:06 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6007, 0.3993]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 61/203 [2m0:01:06 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6015, 0.3985]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 61/203 [2m0:01:06 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6022, 0.3978]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 61/203 [2m0:01:06 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6030, 0.3970]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 61/203 [2m0:01:06 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6038, 0.3962]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 61/203 [2m0:01:06 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5691, 0.4309]], device='cuda:0')mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 61/203 [2m0:01:06 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6359, 0.3641],37mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 61/203 [2m0:01:06 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
        [0.6047, 0.3953],
        [0.5959, 0.4041],
        [0.6493, 0.3507],
        [0.6092, 0.3908],
        [0.6189, 0.3811],
        [0.5729, 0.4271],
        [0.6601, 0.3399],
        [0.6682, 0.3318],
[2K        [0.6587, 0.3413]], device='cuda:0')237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 61/203 [2m0:01:06 â€¢ 0:02:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5737, 0.4263],;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 62/203 [2m0:01:07 â€¢ 0:02:31[0m [2;4m0.93it/s[0m  
        [0.5686, 0.4314],
        [0.5728, 0.4272],
        [0.5718, 0.4282],
        [0.5740, 0.4260],
        [0.5711, 0.4289],
        [0.5702, 0.4298],
        [0.5739, 0.4261],
        [0.5729, 0.4271],
[2K        [0.5739, 0.4261]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 62/203 [2m0:01:07 â€¢ 0:02:31[0m [2;4m0.93it/s[0m  
[2KClass label: Billiardsâ”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 62/203 [2m0:01:07 â€¢ 0:02:31[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5905, 0.4095]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 62/203 [2m0:01:07 â€¢ 0:02:31[0m [2;4m0.93it/s[0m  
[2KStep 0 - TTA Loss: 0.4779â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 62/203 [2m0:01:07 â€¢ 0:02:31[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5915, 0.4085]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 62/203 [2m0:01:07 â€¢ 0:02:31[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5926, 0.4074]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 62/203 [2m0:01:07 â€¢ 0:02:31[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5939, 0.4061]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 62/203 [2m0:01:07 â€¢ 0:02:31[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5952, 0.4048]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 62/203 [2m0:01:07 â€¢ 0:02:31[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5966, 0.4034]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 62/203 [2m0:01:07 â€¢ 0:02:31[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5980, 0.4020]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 62/203 [2m0:01:07 â€¢ 0:02:31[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5995, 0.4005]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 62/203 [2m0:01:07 â€¢ 0:02:31[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6011, 0.3989]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 62/203 [2m0:01:07 â€¢ 0:02:31[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6027, 0.3973]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 62/203 [2m0:01:07 â€¢ 0:02:31[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6042, 0.3958]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 62/203 [2m0:01:07 â€¢ 0:02:31[0m [2;4m0.93it/s[0m  
[2KStep 10 - TTA Loss: 0.3993â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 62/203 [2m0:01:07 â€¢ 0:02:31[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6057, 0.3943]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 62/203 [2m0:01:07 â€¢ 0:02:31[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6071, 0.3929]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 62/203 [2m0:01:07 â€¢ 0:02:31[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6085, 0.3915]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 62/203 [2m0:01:07 â€¢ 0:02:31[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6098, 0.3902]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 62/203 [2m0:01:07 â€¢ 0:02:31[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6110, 0.3890]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 62/203 [2m0:01:07 â€¢ 0:02:31[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6121, 0.3879]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 62/203 [2m0:01:07 â€¢ 0:02:31[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6131, 0.3869]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 62/203 [2m0:01:07 â€¢ 0:02:31[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6140, 0.3860]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 62/203 [2m0:01:07 â€¢ 0:02:31[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6148, 0.3852]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 62/203 [2m0:01:07 â€¢ 0:02:31[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6155, 0.3845]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 62/203 [2m0:01:07 â€¢ 0:02:31[0m [2;4m0.93it/s[0m  
[2KStep 20 - TTA Loss: 0.3872â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 62/203 [2m0:01:07 â€¢ 0:02:31[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6161, 0.3839]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 62/203 [2m0:01:07 â€¢ 0:02:31[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6166, 0.3834]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 62/203 [2m0:01:07 â€¢ 0:02:31[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6170, 0.3830]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 62/203 [2m0:01:07 â€¢ 0:02:31[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6173, 0.3827]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 62/203 [2m0:01:07 â€¢ 0:02:31[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6176, 0.3824]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 62/203 [2m0:01:07 â€¢ 0:02:31[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6177, 0.3823]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 62/203 [2m0:01:07 â€¢ 0:02:31[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6179, 0.3821]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 62/203 [2m0:01:07 â€¢ 0:02:31[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6179, 0.3821]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 62/203 [2m0:01:07 â€¢ 0:02:31[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6180, 0.3820]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 62/203 [2m0:01:07 â€¢ 0:02:31[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5696, 0.4304]], device='cuda:0')mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 62/203 [2m0:01:07 â€¢ 0:02:31[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6354, 0.3646],37mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 62/203 [2m0:01:07 â€¢ 0:02:31[0m [2;4m0.93it/s[0m  
        [0.6180, 0.3820],
        [0.5904, 0.4096],
        [0.6475, 0.3525],
        [0.6077, 0.3923],
        [0.6179, 0.3821],
        [0.5696, 0.4304],
        [0.6592, 0.3408],
        [0.6683, 0.3317],
[2K        [0.6583, 0.3417]], device='cuda:0')237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 62/203 [2m0:01:07 â€¢ 0:02:31[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5738, 0.4262],;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 63/203 [2m0:01:08 â€¢ 0:02:32[0m [2;4m0.92it/s[0m  
        [0.5687, 0.4313],
        [0.5728, 0.4272],
        [0.5719, 0.4281],
        [0.5740, 0.4260],
        [0.5712, 0.4288],
        [0.5703, 0.4297],
        [0.5739, 0.4261],
        [0.5729, 0.4271],
[2K        [0.5740, 0.4260]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 63/203 [2m0:01:08 â€¢ 0:02:32[0m [2;4m0.92it/s[0m  
[2KClass label: CleanAndJerkâ”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 63/203 [2m0:01:08 â€¢ 0:02:32[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5823, 0.4177]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 63/203 [2m0:01:08 â€¢ 0:02:32[0m [2;4m0.92it/s[0m  
[2KStep 0 - TTA Loss: 0.4590â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 63/203 [2m0:01:08 â€¢ 0:02:32[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5823, 0.4177]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 63/203 [2m0:01:08 â€¢ 0:02:32[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5823, 0.4177]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 63/203 [2m0:01:08 â€¢ 0:02:32[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5823, 0.4177]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 63/203 [2m0:01:08 â€¢ 0:02:32[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5823, 0.4177]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 63/203 [2m0:01:08 â€¢ 0:02:32[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5824, 0.4176]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 63/203 [2m0:01:08 â€¢ 0:02:32[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5825, 0.4175]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 63/203 [2m0:01:08 â€¢ 0:02:32[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5827, 0.4173]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 63/203 [2m0:01:08 â€¢ 0:02:32[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5830, 0.4170]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 63/203 [2m0:01:08 â€¢ 0:02:32[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5833, 0.4167]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 63/203 [2m0:01:08 â€¢ 0:02:32[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5838, 0.4162]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 63/203 [2m0:01:08 â€¢ 0:02:32[0m [2;4m0.92it/s[0m  
[2KStep 10 - TTA Loss: 0.4764â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 63/203 [2m0:01:08 â€¢ 0:02:32[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5844, 0.4156]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 63/203 [2m0:01:08 â€¢ 0:02:32[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5852, 0.4148]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 63/203 [2m0:01:08 â€¢ 0:02:32[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5861, 0.4139]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 63/203 [2m0:01:08 â€¢ 0:02:32[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5871, 0.4129]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 63/203 [2m0:01:08 â€¢ 0:02:32[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5884, 0.4116]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 63/203 [2m0:01:08 â€¢ 0:02:32[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5898, 0.4102]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 63/203 [2m0:01:08 â€¢ 0:02:32[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5914, 0.4086]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 63/203 [2m0:01:08 â€¢ 0:02:32[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5932, 0.4068]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 63/203 [2m0:01:08 â€¢ 0:02:32[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5952, 0.4048]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 63/203 [2m0:01:08 â€¢ 0:02:32[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5974, 0.4026]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 63/203 [2m0:01:08 â€¢ 0:02:32[0m [2;4m0.92it/s[0m  
[2KStep 20 - TTA Loss: 0.3908â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 63/203 [2m0:01:08 â€¢ 0:02:32[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5997, 0.4003]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 63/203 [2m0:01:08 â€¢ 0:02:32[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6020, 0.3980]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 63/203 [2m0:01:08 â€¢ 0:02:32[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6045, 0.3955]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 63/203 [2m0:01:08 â€¢ 0:02:32[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6072, 0.3928]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 63/203 [2m0:01:08 â€¢ 0:02:32[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6100, 0.3900]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 63/203 [2m0:01:08 â€¢ 0:02:32[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6131, 0.3869]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 63/203 [2m0:01:08 â€¢ 0:02:32[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6162, 0.3838]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 63/203 [2m0:01:08 â€¢ 0:02:32[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6193, 0.3807]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 63/203 [2m0:01:08 â€¢ 0:02:32[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6225, 0.3775]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 63/203 [2m0:01:08 â€¢ 0:02:32[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5748, 0.4252]], device='cuda:0')mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 63/203 [2m0:01:08 â€¢ 0:02:32[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6443, 0.3557],37mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 63/203 [2m0:01:08 â€¢ 0:02:32[0m [2;4m0.92it/s[0m  
        [0.6094, 0.3906],
        [0.6259, 0.3741],
        [0.6577, 0.3423],
        [0.6248, 0.3752],
        [0.6317, 0.3683],
        [0.5843, 0.4157],
        [0.6630, 0.3370],
        [0.6742, 0.3258],
[2K        [0.6710, 0.3290]], device='cuda:0')237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 63/203 [2m0:01:08 â€¢ 0:02:32[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5739, 0.4261],;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 64/203 [2m0:01:09 â€¢ 0:02:33[0m [2;4m0.91it/s[0m  
        [0.5689, 0.4311],
        [0.5733, 0.4267],
        [0.5721, 0.4279],
        [0.5742, 0.4258],
        [0.5713, 0.4287],
        [0.5704, 0.4296],
        [0.5741, 0.4259],
        [0.5731, 0.4269],
[2K        [0.5742, 0.4258]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 64/203 [2m0:01:09 â€¢ 0:02:33[0m [2;4m0.91it/s[0m  
[2KClass label: BaseballPitchâ”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 64/203 [2m0:01:09 â€¢ 0:02:33[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6270, 0.3730]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 64/203 [2m0:01:09 â€¢ 0:02:33[0m [2;4m0.91it/s[0m  
[2KStep 0 - TTA Loss: 0.5681â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 64/203 [2m0:01:09 â€¢ 0:02:33[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6287, 0.3713]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 64/203 [2m0:01:09 â€¢ 0:02:33[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6306, 0.3694]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 64/203 [2m0:01:09 â€¢ 0:02:33[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6328, 0.3672]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 64/203 [2m0:01:09 â€¢ 0:02:33[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6352, 0.3648]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 64/203 [2m0:01:09 â€¢ 0:02:33[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6378, 0.3622]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 64/203 [2m0:01:09 â€¢ 0:02:33[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6405, 0.3595]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 64/203 [2m0:01:09 â€¢ 0:02:33[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6433, 0.3567]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 64/203 [2m0:01:09 â€¢ 0:02:33[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6462, 0.3538]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 64/203 [2m0:01:09 â€¢ 0:02:33[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6490, 0.3510]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 64/203 [2m0:01:09 â€¢ 0:02:33[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6518, 0.3482]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 64/203 [2m0:01:09 â€¢ 0:02:33[0m [2;4m0.91it/s[0m  
[2KStep 10 - TTA Loss: 0.5790â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 64/203 [2m0:01:09 â€¢ 0:02:33[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6546, 0.3454]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 64/203 [2m0:01:09 â€¢ 0:02:33[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6574, 0.3426]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 64/203 [2m0:01:09 â€¢ 0:02:33[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6600, 0.3400]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 64/203 [2m0:01:09 â€¢ 0:02:33[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6625, 0.3375]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 64/203 [2m0:01:09 â€¢ 0:02:33[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6649, 0.3351]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 64/203 [2m0:01:09 â€¢ 0:02:33[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6671, 0.3329]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 64/203 [2m0:01:09 â€¢ 0:02:33[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6691, 0.3309]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 64/203 [2m0:01:09 â€¢ 0:02:33[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6709, 0.3291]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 64/203 [2m0:01:09 â€¢ 0:02:33[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6726, 0.3274]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 64/203 [2m0:01:09 â€¢ 0:02:33[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6740, 0.3260]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 64/203 [2m0:01:09 â€¢ 0:02:33[0m [2;4m0.91it/s[0m  
[2KStep 20 - TTA Loss: 0.5590â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 64/203 [2m0:01:09 â€¢ 0:02:33[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6753, 0.3247]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 64/203 [2m0:01:09 â€¢ 0:02:33[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6763, 0.3237]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 64/203 [2m0:01:09 â€¢ 0:02:33[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6771, 0.3229]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 64/203 [2m0:01:09 â€¢ 0:02:33[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6778, 0.3222]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 64/203 [2m0:01:09 â€¢ 0:02:33[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6783, 0.3217]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 64/203 [2m0:01:09 â€¢ 0:02:33[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6786, 0.3214]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 64/203 [2m0:01:09 â€¢ 0:02:33[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6788, 0.3212]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 64/203 [2m0:01:09 â€¢ 0:02:33[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6790, 0.3210]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 64/203 [2m0:01:09 â€¢ 0:02:33[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6790, 0.3210]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 64/203 [2m0:01:09 â€¢ 0:02:33[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5765, 0.4235]], device='cuda:0')5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 64/203 [2m0:01:09 â€¢ 0:02:33[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6791, 0.3209],8;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 64/203 [2m0:01:09 â€¢ 0:02:33[0m [2;4m0.91it/s[0m  
        [0.6154, 0.3846],
        [0.6232, 0.3768],
        [0.6786, 0.3214],
        [0.6424, 0.3576],
        [0.6471, 0.3529],
        [0.5981, 0.4019],
        [0.6774, 0.3226],
        [0.6896, 0.3104],
[2K        [0.6901, 0.3099]], device='cuda:0')98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 64/203 [2m0:01:09 â€¢ 0:02:33[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5744, 0.4256],;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 65/203 [2m0:01:10 â€¢ 0:02:32[0m [2;4m0.91it/s[0m  
        [0.5692, 0.4308],
        [0.5736, 0.4264],
        [0.5723, 0.4277],
        [0.5745, 0.4255],
        [0.5716, 0.4284],
        [0.5707, 0.4293],
        [0.5744, 0.4256],
        [0.5734, 0.4266],
[2K        [0.5745, 0.4255]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 65/203 [2m0:01:10 â€¢ 0:02:32[0m [2;4m0.91it/s[0m  
[2KClass label: BaseballPitchâ”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 65/203 [2m0:01:10 â€¢ 0:02:32[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6307, 0.3693]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 65/203 [2m0:01:10 â€¢ 0:02:32[0m [2;4m0.91it/s[0m  
[2KStep 0 - TTA Loss: 0.9088â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 65/203 [2m0:01:10 â€¢ 0:02:32[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6307, 0.3693]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 65/203 [2m0:01:10 â€¢ 0:02:32[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6307, 0.3693]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 65/203 [2m0:01:10 â€¢ 0:02:32[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6308, 0.3692]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 65/203 [2m0:01:10 â€¢ 0:02:32[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6309, 0.3691]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 65/203 [2m0:01:10 â€¢ 0:02:32[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6311, 0.3689]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 65/203 [2m0:01:10 â€¢ 0:02:32[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6314, 0.3686]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 65/203 [2m0:01:10 â€¢ 0:02:32[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6317, 0.3683]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 65/203 [2m0:01:10 â€¢ 0:02:32[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6322, 0.3678]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 65/203 [2m0:01:10 â€¢ 0:02:32[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6328, 0.3672]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 65/203 [2m0:01:10 â€¢ 0:02:32[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6335, 0.3665]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 65/203 [2m0:01:10 â€¢ 0:02:32[0m [2;4m0.91it/s[0m  
[2KStep 10 - TTA Loss: 0.8766â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 65/203 [2m0:01:10 â€¢ 0:02:32[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6344, 0.3656]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 65/203 [2m0:01:10 â€¢ 0:02:32[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6354, 0.3646]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 65/203 [2m0:01:10 â€¢ 0:02:32[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6366, 0.3634]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 65/203 [2m0:01:10 â€¢ 0:02:32[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6379, 0.3621]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 65/203 [2m0:01:10 â€¢ 0:02:32[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6393, 0.3607]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 65/203 [2m0:01:10 â€¢ 0:02:32[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6409, 0.3591]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 65/203 [2m0:01:10 â€¢ 0:02:32[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6426, 0.3574]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 65/203 [2m0:01:10 â€¢ 0:02:32[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6444, 0.3556]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 65/203 [2m0:01:10 â€¢ 0:02:32[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6464, 0.3536]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 65/203 [2m0:01:10 â€¢ 0:02:32[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6485, 0.3515]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 65/203 [2m0:01:10 â€¢ 0:02:32[0m [2;4m0.91it/s[0m  
[2KStep 20 - TTA Loss: 0.9201â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 65/203 [2m0:01:10 â€¢ 0:02:32[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6506, 0.3494]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 65/203 [2m0:01:10 â€¢ 0:02:32[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6529, 0.3471]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 65/203 [2m0:01:10 â€¢ 0:02:32[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6553, 0.3447]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 65/203 [2m0:01:10 â€¢ 0:02:32[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6579, 0.3421]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 65/203 [2m0:01:10 â€¢ 0:02:32[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6605, 0.3395]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 65/203 [2m0:01:10 â€¢ 0:02:32[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6632, 0.3368]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 65/203 [2m0:01:10 â€¢ 0:02:32[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6659, 0.3341]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 65/203 [2m0:01:10 â€¢ 0:02:32[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6687, 0.3313]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 65/203 [2m0:01:10 â€¢ 0:02:32[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6716, 0.3284]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 65/203 [2m0:01:10 â€¢ 0:02:32[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5768, 0.4232]], device='cuda:0')5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 65/203 [2m0:01:10 â€¢ 0:02:32[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6745, 0.3255],8;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 65/203 [2m0:01:10 â€¢ 0:02:32[0m [2;4m0.91it/s[0m  
        [0.6091, 0.3909],
        [0.6020, 0.3980],
        [0.6710, 0.3290],
        [0.6319, 0.3681],
        [0.6368, 0.3632],
        [0.5875, 0.4125],
        [0.6755, 0.3245],
        [0.6834, 0.3166],
[2K        [0.6808, 0.3192]], device='cuda:0')98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 65/203 [2m0:01:10 â€¢ 0:02:32[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5749, 0.4251],8;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 66/203 [2m0:01:11 â€¢ 0:02:30[0m [2;4m0.92it/s[0m   
        [0.5694, 0.4306],
        [0.5738, 0.4262],
        [0.5726, 0.4274],
        [0.5748, 0.4252],
        [0.5718, 0.4282],
        [0.5709, 0.4291],
        [0.5747, 0.4253],
        [0.5737, 0.4263],
[2K        [0.5748, 0.4252]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 66/203 [2m0:01:11 â€¢ 0:02:30[0m [2;4m0.92it/s[0m  
[2KClass label: SoccerPenaltyâ”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 66/203 [2m0:01:11 â€¢ 0:02:30[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6546, 0.3454]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 66/203 [2m0:01:11 â€¢ 0:02:30[0m [2;4m0.92it/s[0m  
[2KStep 0 - TTA Loss: 0.7832â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 66/203 [2m0:01:11 â€¢ 0:02:30[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6561, 0.3439]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 66/203 [2m0:01:11 â€¢ 0:02:30[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6575, 0.3425]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 66/203 [2m0:01:11 â€¢ 0:02:30[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6590, 0.3410]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 66/203 [2m0:01:11 â€¢ 0:02:30[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6604, 0.3396]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 66/203 [2m0:01:11 â€¢ 0:02:30[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6618, 0.3382]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 66/203 [2m0:01:11 â€¢ 0:02:30[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6632, 0.3368]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 66/203 [2m0:01:11 â€¢ 0:02:30[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6646, 0.3354]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 66/203 [2m0:01:11 â€¢ 0:02:30[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6659, 0.3341]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 66/203 [2m0:01:11 â€¢ 0:02:30[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6671, 0.3329]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 66/203 [2m0:01:11 â€¢ 0:02:30[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6682, 0.3318]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 66/203 [2m0:01:11 â€¢ 0:02:30[0m [2;4m0.92it/s[0m  
[2KStep 10 - TTA Loss: 0.8057â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 66/203 [2m0:01:11 â€¢ 0:02:30[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6694, 0.3306]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 66/203 [2m0:01:11 â€¢ 0:02:30[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6704, 0.3296]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 66/203 [2m0:01:11 â€¢ 0:02:30[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6714, 0.3286]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 66/203 [2m0:01:11 â€¢ 0:02:30[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6723, 0.3277]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 66/203 [2m0:01:11 â€¢ 0:02:30[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6731, 0.3269]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 66/203 [2m0:01:11 â€¢ 0:02:30[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6739, 0.3261]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 66/203 [2m0:01:11 â€¢ 0:02:30[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6746, 0.3254]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 66/203 [2m0:01:11 â€¢ 0:02:30[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6752, 0.3248]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 66/203 [2m0:01:11 â€¢ 0:02:30[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6758, 0.3242]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 66/203 [2m0:01:11 â€¢ 0:02:30[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6763, 0.3237]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 66/203 [2m0:01:11 â€¢ 0:02:30[0m [2;4m0.92it/s[0m  
[2KStep 20 - TTA Loss: 0.8208â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 66/203 [2m0:01:11 â€¢ 0:02:30[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6767, 0.3233]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 66/203 [2m0:01:11 â€¢ 0:02:30[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6770, 0.3230]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 66/203 [2m0:01:11 â€¢ 0:02:30[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6773, 0.3227]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 66/203 [2m0:01:11 â€¢ 0:02:30[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6775, 0.3225]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 66/203 [2m0:01:11 â€¢ 0:02:30[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6777, 0.3223]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 66/203 [2m0:01:11 â€¢ 0:02:30[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6778, 0.3222]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 66/203 [2m0:01:11 â€¢ 0:02:30[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6779, 0.3221]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 66/203 [2m0:01:11 â€¢ 0:02:30[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6779, 0.3221]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 66/203 [2m0:01:11 â€¢ 0:02:30[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6779, 0.3221]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 66/203 [2m0:01:11 â€¢ 0:02:30[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5761, 0.4239]], device='cuda:0')7mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 66/203 [2m0:01:11 â€¢ 0:02:30[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6585, 0.3415],237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 66/203 [2m0:01:11 â€¢ 0:02:30[0m [2;4m0.92it/s[0m  
        [0.6088, 0.3912],
        [0.5988, 0.4012],
        [0.6643, 0.3357],
        [0.6250, 0.3750],
        [0.6335, 0.3665],
        [0.5874, 0.4126],
        [0.6780, 0.3220],
        [0.6774, 0.3226],
[2K        [0.6736, 0.3264]], device='cuda:0');237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 66/203 [2m0:01:11 â€¢ 0:02:30[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5753, 0.4247],8;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 67/203 [2m0:01:12 â€¢ 0:02:27[0m [2;4m0.93it/s[0m  
        [0.5696, 0.4304],
        [0.5740, 0.4260],
        [0.5729, 0.4271],
        [0.5750, 0.4250],
        [0.5720, 0.4280],
        [0.5711, 0.4289],
        [0.5750, 0.4250],
        [0.5740, 0.4260],
[2K        [0.5750, 0.4250]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 67/203 [2m0:01:12 â€¢ 0:02:27[0m [2;4m0.93it/s[0m  
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 67/203 [2m0:01:12 â€¢ 0:02:27[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6140, 0.3860]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 67/203 [2m0:01:12 â€¢ 0:02:27[0m [2;4m0.93it/s[0m  
[2KStep 0 - TTA Loss: 0.6695â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 67/203 [2m0:01:12 â€¢ 0:02:27[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6140, 0.3860]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 67/203 [2m0:01:12 â€¢ 0:02:27[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6140, 0.3860]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 67/203 [2m0:01:12 â€¢ 0:02:27[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6140, 0.3860]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 67/203 [2m0:01:12 â€¢ 0:02:27[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6140, 0.3860]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 67/203 [2m0:01:12 â€¢ 0:02:27[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6142, 0.3858]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 67/203 [2m0:01:12 â€¢ 0:02:27[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6143, 0.3857]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 67/203 [2m0:01:12 â€¢ 0:02:27[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6146, 0.3854]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 67/203 [2m0:01:12 â€¢ 0:02:27[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6150, 0.3850]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 67/203 [2m0:01:12 â€¢ 0:02:27[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6156, 0.3844]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 67/203 [2m0:01:12 â€¢ 0:02:27[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6163, 0.3837]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 67/203 [2m0:01:12 â€¢ 0:02:27[0m [2;4m0.93it/s[0m  
[2KStep 10 - TTA Loss: 0.6303â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 67/203 [2m0:01:12 â€¢ 0:02:27[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6171, 0.3829]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 67/203 [2m0:01:12 â€¢ 0:02:27[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6182, 0.3818]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 67/203 [2m0:01:12 â€¢ 0:02:27[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6195, 0.3805]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 67/203 [2m0:01:12 â€¢ 0:02:27[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6210, 0.3790]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 67/203 [2m0:01:12 â€¢ 0:02:27[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6228, 0.3772]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 67/203 [2m0:01:12 â€¢ 0:02:27[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6248, 0.3752]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 67/203 [2m0:01:12 â€¢ 0:02:27[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6270, 0.3730]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 67/203 [2m0:01:12 â€¢ 0:02:27[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6295, 0.3705]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 67/203 [2m0:01:12 â€¢ 0:02:27[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6323, 0.3677]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 67/203 [2m0:01:12 â€¢ 0:02:27[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6353, 0.3647]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 67/203 [2m0:01:12 â€¢ 0:02:27[0m [2;4m0.93it/s[0m  
[2KStep 20 - TTA Loss: 0.5997â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 67/203 [2m0:01:12 â€¢ 0:02:27[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6386, 0.3614]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 67/203 [2m0:01:12 â€¢ 0:02:27[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6421, 0.3579]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 67/203 [2m0:01:12 â€¢ 0:02:27[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6459, 0.3541]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 67/203 [2m0:01:12 â€¢ 0:02:27[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6499, 0.3501]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 67/203 [2m0:01:12 â€¢ 0:02:27[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6541, 0.3459]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 67/203 [2m0:01:12 â€¢ 0:02:27[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6585, 0.3415]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 67/203 [2m0:01:12 â€¢ 0:02:27[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6630, 0.3370]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 67/203 [2m0:01:12 â€¢ 0:02:27[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6676, 0.3324]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 67/203 [2m0:01:12 â€¢ 0:02:27[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6723, 0.3277]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 67/203 [2m0:01:12 â€¢ 0:02:27[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5753, 0.4247]], device='cuda:0')7mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 67/203 [2m0:01:12 â€¢ 0:02:27[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6748, 0.3252],237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 67/203 [2m0:01:12 â€¢ 0:02:27[0m [2;4m0.93it/s[0m  
        [0.6290, 0.3710],
        [0.6275, 0.3725],
        [0.6831, 0.3169],
        [0.6543, 0.3457],
        [0.6772, 0.3228],
        [0.6239, 0.3761],
        [0.6959, 0.3041],
        [0.7020, 0.2980],
[2K        [0.7000, 0.3000]], device='cuda:0');237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 67/203 [2m0:01:12 â€¢ 0:02:27[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5756, 0.4244],8;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 68/203 [2m0:01:14 â€¢ 0:02:30[0m [2;4m0.91it/s[0m  
        [0.5700, 0.4300],
        [0.5743, 0.4257],
        [0.5732, 0.4268],
        [0.5753, 0.4247],
        [0.5726, 0.4274],
        [0.5715, 0.4285],
        [0.5753, 0.4247],
        [0.5743, 0.4257],
[2K        [0.5754, 0.4246]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 68/203 [2m0:01:14 â€¢ 0:02:30[0m [2;4m0.91it/s[0m  
[2KClass label: TennisSwingâ”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 68/203 [2m0:01:14 â€¢ 0:02:30[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6596, 0.3404]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 68/203 [2m0:01:14 â€¢ 0:02:30[0m [2;4m0.91it/s[0m  
[2KStep 0 - TTA Loss: 0.6860â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 68/203 [2m0:01:14 â€¢ 0:02:30[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6627, 0.3373]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 68/203 [2m0:01:14 â€¢ 0:02:30[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6658, 0.3342]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 68/203 [2m0:01:14 â€¢ 0:02:30[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6690, 0.3310]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 68/203 [2m0:01:14 â€¢ 0:02:30[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6722, 0.3278]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 68/203 [2m0:01:14 â€¢ 0:02:30[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6752, 0.3248]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 68/203 [2m0:01:14 â€¢ 0:02:30[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6782, 0.3218]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 68/203 [2m0:01:14 â€¢ 0:02:30[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6811, 0.3189]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 68/203 [2m0:01:14 â€¢ 0:02:30[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6837, 0.3163]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 68/203 [2m0:01:14 â€¢ 0:02:30[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6863, 0.3137]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 68/203 [2m0:01:14 â€¢ 0:02:30[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6888, 0.3112]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 68/203 [2m0:01:14 â€¢ 0:02:30[0m [2;4m0.91it/s[0m  
[2KStep 10 - TTA Loss: 0.7797â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 68/203 [2m0:01:14 â€¢ 0:02:30[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6910, 0.3090]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 68/203 [2m0:01:14 â€¢ 0:02:30[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6931, 0.3069]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 68/203 [2m0:01:14 â€¢ 0:02:30[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6950, 0.3050]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 68/203 [2m0:01:14 â€¢ 0:02:30[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6968, 0.3032]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 68/203 [2m0:01:14 â€¢ 0:02:30[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6984, 0.3016]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 68/203 [2m0:01:14 â€¢ 0:02:30[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6998, 0.3002]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 68/203 [2m0:01:14 â€¢ 0:02:30[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.7012, 0.2988]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 68/203 [2m0:01:14 â€¢ 0:02:30[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.7023, 0.2977]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 68/203 [2m0:01:14 â€¢ 0:02:30[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.7034, 0.2966]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 68/203 [2m0:01:14 â€¢ 0:02:30[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.7042, 0.2958]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 68/203 [2m0:01:14 â€¢ 0:02:30[0m [2;4m0.91it/s[0m  
[2KStep 20 - TTA Loss: 0.7253â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 68/203 [2m0:01:14 â€¢ 0:02:30[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.7049, 0.2951]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 68/203 [2m0:01:14 â€¢ 0:02:30[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.7055, 0.2945]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 68/203 [2m0:01:14 â€¢ 0:02:30[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.7060, 0.2940]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 68/203 [2m0:01:14 â€¢ 0:02:30[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.7065, 0.2935]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 68/203 [2m0:01:14 â€¢ 0:02:30[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.7068, 0.2932]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 68/203 [2m0:01:14 â€¢ 0:02:30[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.7070, 0.2930]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 68/203 [2m0:01:14 â€¢ 0:02:30[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.7072, 0.2928]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 68/203 [2m0:01:14 â€¢ 0:02:30[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.7073, 0.2927]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 68/203 [2m0:01:14 â€¢ 0:02:30[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.7073, 0.2927]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 68/203 [2m0:01:14 â€¢ 0:02:30[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5768, 0.4232]], device='cuda:0')7mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 68/203 [2m0:01:14 â€¢ 0:02:30[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6701, 0.3299],237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 68/203 [2m0:01:14 â€¢ 0:02:30[0m [2;4m0.91it/s[0m  
        [0.6225, 0.3775],
        [0.6194, 0.3806],
        [0.6818, 0.3182],
        [0.6476, 0.3524],
        [0.6635, 0.3365],
        [0.6093, 0.3907],
        [0.6868, 0.3132],
        [0.7073, 0.2927],
[2K        [0.6949, 0.3051]], device='cuda:0');237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 68/203 [2m0:01:14 â€¢ 0:02:30[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5761, 0.4239],8;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 69/203 [2m0:01:15 â€¢ 0:02:28[0m [2;4m0.91it/s[0m  
        [0.5703, 0.4297],
        [0.5747, 0.4253],
        [0.5737, 0.4263],
        [0.5757, 0.4243],
        [0.5731, 0.4269],
        [0.5720, 0.4280],
        [0.5757, 0.4243],
        [0.5748, 0.4252],
[2K        [0.5757, 0.4243]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 69/203 [2m0:01:15 â€¢ 0:02:28[0m [2;4m0.91it/s[0m  
[2KClass label: Billiardsâ”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 69/203 [2m0:01:15 â€¢ 0:02:28[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5926, 0.4074]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 69/203 [2m0:01:15 â€¢ 0:02:28[0m [2;4m0.91it/s[0m  
[2KStep 0 - TTA Loss: 0.8575â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 69/203 [2m0:01:15 â€¢ 0:02:28[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5926, 0.4074]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 69/203 [2m0:01:15 â€¢ 0:02:28[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5926, 0.4074]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 69/203 [2m0:01:15 â€¢ 0:02:28[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5926, 0.4074]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 69/203 [2m0:01:15 â€¢ 0:02:28[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5927, 0.4073]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 69/203 [2m0:01:15 â€¢ 0:02:28[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5929, 0.4071]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 69/203 [2m0:01:15 â€¢ 0:02:28[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5931, 0.4069]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 69/203 [2m0:01:15 â€¢ 0:02:28[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5935, 0.4065]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 69/203 [2m0:01:15 â€¢ 0:02:28[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5940, 0.4060]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 69/203 [2m0:01:15 â€¢ 0:02:28[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5946, 0.4054]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 69/203 [2m0:01:15 â€¢ 0:02:28[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5955, 0.4045]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 69/203 [2m0:01:15 â€¢ 0:02:28[0m [2;4m0.91it/s[0m  
[2KStep 10 - TTA Loss: 0.9202â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 69/203 [2m0:01:15 â€¢ 0:02:28[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5965, 0.4035]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 69/203 [2m0:01:15 â€¢ 0:02:28[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5978, 0.4022]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 69/203 [2m0:01:15 â€¢ 0:02:28[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5994, 0.4006]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 69/203 [2m0:01:15 â€¢ 0:02:28[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6011, 0.3989]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 69/203 [2m0:01:15 â€¢ 0:02:28[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6032, 0.3968]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 69/203 [2m0:01:15 â€¢ 0:02:28[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6056, 0.3944]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 69/203 [2m0:01:15 â€¢ 0:02:28[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6082, 0.3918]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 69/203 [2m0:01:15 â€¢ 0:02:28[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6112, 0.3888]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 69/203 [2m0:01:15 â€¢ 0:02:28[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6143, 0.3857]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 69/203 [2m0:01:15 â€¢ 0:02:28[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6177, 0.3823]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 69/203 [2m0:01:15 â€¢ 0:02:28[0m [2;4m0.91it/s[0m  
[2KStep 20 - TTA Loss: 0.8829â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 69/203 [2m0:01:15 â€¢ 0:02:28[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6215, 0.3785]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 69/203 [2m0:01:15 â€¢ 0:02:28[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6255, 0.3745]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 69/203 [2m0:01:15 â€¢ 0:02:28[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6298, 0.3702]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 69/203 [2m0:01:15 â€¢ 0:02:28[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6342, 0.3658]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 69/203 [2m0:01:15 â€¢ 0:02:28[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6389, 0.3611]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 69/203 [2m0:01:15 â€¢ 0:02:28[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6439, 0.3561]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 69/203 [2m0:01:15 â€¢ 0:02:28[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6490, 0.3510]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 69/203 [2m0:01:15 â€¢ 0:02:28[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6541, 0.3459]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 69/203 [2m0:01:15 â€¢ 0:02:28[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6595, 0.3405]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 69/203 [2m0:01:15 â€¢ 0:02:28[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5731, 0.4269]], device='cuda:0');5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 69/203 [2m0:01:15 â€¢ 0:02:28[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6548, 0.3452],98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 69/203 [2m0:01:15 â€¢ 0:02:28[0m [2;4m0.91it/s[0m  
        [0.6650, 0.3350],
        [0.6044, 0.3956],
        [0.6669, 0.3331],
        [0.6245, 0.3755],
        [0.6372, 0.3628],
        [0.5901, 0.4099],
        [0.6784, 0.3216],
        [0.6886, 0.3114],
[2K        [0.6742, 0.3258]], device='cuda:0');98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 69/203 [2m0:01:15 â€¢ 0:02:28[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5761, 0.4239],8;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 70/203 [2m0:01:16 â€¢ 0:02:27[0m [2;4m0.91it/s[0m  
        [0.5704, 0.4296],
        [0.5747, 0.4253],
        [0.5738, 0.4262],
        [0.5758, 0.4242],
        [0.5732, 0.4268],
        [0.5721, 0.4279],
        [0.5758, 0.4242],
        [0.5749, 0.4251],
[2K        [0.5758, 0.4242]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 70/203 [2m0:01:16 â€¢ 0:02:27[0m [2;4m0.91it/s[0m  
[2KClass label: HammerThrowâ”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 70/203 [2m0:01:16 â€¢ 0:02:27[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6052, 0.3948]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 70/203 [2m0:01:16 â€¢ 0:02:27[0m [2;4m0.91it/s[0m  
[2KStep 0 - TTA Loss: 0.5466â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 70/203 [2m0:01:16 â€¢ 0:02:27[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6075, 0.3925]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 70/203 [2m0:01:16 â€¢ 0:02:27[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6099, 0.3901]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 70/203 [2m0:01:16 â€¢ 0:02:27[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6126, 0.3874]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 70/203 [2m0:01:16 â€¢ 0:02:27[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6153, 0.3847]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 70/203 [2m0:01:16 â€¢ 0:02:27[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6182, 0.3818]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 70/203 [2m0:01:16 â€¢ 0:02:27[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6212, 0.3788]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 70/203 [2m0:01:16 â€¢ 0:02:27[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6242, 0.3758]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 70/203 [2m0:01:16 â€¢ 0:02:27[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6272, 0.3728]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 70/203 [2m0:01:16 â€¢ 0:02:27[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6302, 0.3698]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 70/203 [2m0:01:16 â€¢ 0:02:27[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6332, 0.3668]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 70/203 [2m0:01:16 â€¢ 0:02:27[0m [2;4m0.91it/s[0m  
[2KStep 10 - TTA Loss: 0.5293â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 70/203 [2m0:01:16 â€¢ 0:02:27[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6360, 0.3640]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 70/203 [2m0:01:16 â€¢ 0:02:27[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6388, 0.3612]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 70/203 [2m0:01:16 â€¢ 0:02:27[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6414, 0.3586]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 70/203 [2m0:01:16 â€¢ 0:02:27[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6438, 0.3562]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 70/203 [2m0:01:16 â€¢ 0:02:27[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6461, 0.3539]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 70/203 [2m0:01:16 â€¢ 0:02:27[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6482, 0.3518]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 70/203 [2m0:01:16 â€¢ 0:02:27[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6501, 0.3499]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 70/203 [2m0:01:16 â€¢ 0:02:27[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6519, 0.3481]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 70/203 [2m0:01:16 â€¢ 0:02:27[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6534, 0.3466]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 70/203 [2m0:01:16 â€¢ 0:02:27[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6548, 0.3452]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 70/203 [2m0:01:16 â€¢ 0:02:27[0m [2;4m0.91it/s[0m  
[2KStep 20 - TTA Loss: 0.5265â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 70/203 [2m0:01:16 â€¢ 0:02:27[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6559, 0.3441]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 70/203 [2m0:01:16 â€¢ 0:02:27[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6569, 0.3431]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 70/203 [2m0:01:16 â€¢ 0:02:27[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6577, 0.3423]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 70/203 [2m0:01:16 â€¢ 0:02:27[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6583, 0.3417]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 70/203 [2m0:01:16 â€¢ 0:02:27[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6587, 0.3413]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 70/203 [2m0:01:16 â€¢ 0:02:27[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6591, 0.3409]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 70/203 [2m0:01:16 â€¢ 0:02:27[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6593, 0.3407]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 70/203 [2m0:01:16 â€¢ 0:02:27[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6594, 0.3406]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 70/203 [2m0:01:16 â€¢ 0:02:27[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6595, 0.3405]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 70/203 [2m0:01:16 â€¢ 0:02:27[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5784, 0.4216]], device='cuda:0');5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 70/203 [2m0:01:16 â€¢ 0:02:27[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6756, 0.3244],98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 70/203 [2m0:01:16 â€¢ 0:02:27[0m [2;4m0.91it/s[0m  
        [0.6517, 0.3483],
        [0.6239, 0.3761],
        [0.6820, 0.3180],
        [0.6595, 0.3405],
        [0.6589, 0.3411],
        [0.6097, 0.3903],
        [0.6917, 0.3083],
        [0.6985, 0.3015],
[2K        [0.6981, 0.3019]], device='cuda:0');98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 70/203 [2m0:01:16 â€¢ 0:02:27[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5763, 0.4237],8;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 71/203 [2m0:01:17 â€¢ 0:02:26[0m [2;4m0.90it/s[0m  
        [0.5706, 0.4294],
        [0.5750, 0.4250],
        [0.5740, 0.4260],
        [0.5762, 0.4238],
        [0.5735, 0.4265],
        [0.5724, 0.4276],
        [0.5761, 0.4239],
        [0.5752, 0.4248],
[2K        [0.5761, 0.4239]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 71/203 [2m0:01:17 â€¢ 0:02:26[0m [2;4m0.90it/s[0m  
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 71/203 [2m0:01:17 â€¢ 0:02:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6102, 0.3898]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 71/203 [2m0:01:17 â€¢ 0:02:26[0m [2;4m0.90it/s[0m  
[2KStep 0 - TTA Loss: 0.5641â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 71/203 [2m0:01:17 â€¢ 0:02:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6102, 0.3898]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 71/203 [2m0:01:17 â€¢ 0:02:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6102, 0.3898]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 71/203 [2m0:01:17 â€¢ 0:02:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6102, 0.3898]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 71/203 [2m0:01:17 â€¢ 0:02:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6103, 0.3897]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 71/203 [2m0:01:17 â€¢ 0:02:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6105, 0.3895]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 71/203 [2m0:01:17 â€¢ 0:02:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6107, 0.3893]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 71/203 [2m0:01:17 â€¢ 0:02:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6110, 0.3890]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 71/203 [2m0:01:17 â€¢ 0:02:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6115, 0.3885]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 71/203 [2m0:01:17 â€¢ 0:02:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6120, 0.3880]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 71/203 [2m0:01:17 â€¢ 0:02:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6127, 0.3873]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 71/203 [2m0:01:17 â€¢ 0:02:26[0m [2;4m0.90it/s[0m  
[2KStep 10 - TTA Loss: 0.5972â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 71/203 [2m0:01:17 â€¢ 0:02:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6135, 0.3865]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 71/203 [2m0:01:17 â€¢ 0:02:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6145, 0.3855]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 71/203 [2m0:01:17 â€¢ 0:02:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6156, 0.3844]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 71/203 [2m0:01:17 â€¢ 0:02:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6168, 0.3832]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 71/203 [2m0:01:17 â€¢ 0:02:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6183, 0.3817]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 71/203 [2m0:01:17 â€¢ 0:02:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6198, 0.3802]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 71/203 [2m0:01:17 â€¢ 0:02:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6216, 0.3784]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 71/203 [2m0:01:17 â€¢ 0:02:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6234, 0.3766]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 71/203 [2m0:01:17 â€¢ 0:02:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6254, 0.3746]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 71/203 [2m0:01:17 â€¢ 0:02:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6276, 0.3724]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 71/203 [2m0:01:17 â€¢ 0:02:26[0m [2;4m0.90it/s[0m  
[2KStep 20 - TTA Loss: 0.5326â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 71/203 [2m0:01:17 â€¢ 0:02:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6299, 0.3701]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 71/203 [2m0:01:17 â€¢ 0:02:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6322, 0.3678]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 71/203 [2m0:01:17 â€¢ 0:02:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6347, 0.3653]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 71/203 [2m0:01:17 â€¢ 0:02:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6373, 0.3627]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 71/203 [2m0:01:17 â€¢ 0:02:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6400, 0.3600]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 71/203 [2m0:01:17 â€¢ 0:02:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6427, 0.3573]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 71/203 [2m0:01:17 â€¢ 0:02:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6456, 0.3544]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 71/203 [2m0:01:17 â€¢ 0:02:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6486, 0.3514]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 71/203 [2m0:01:17 â€¢ 0:02:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6517, 0.3483]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 71/203 [2m0:01:17 â€¢ 0:02:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5758, 0.4242]], device='cuda:0');5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 71/203 [2m0:01:17 â€¢ 0:02:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6606, 0.3394],98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 71/203 [2m0:01:17 â€¢ 0:02:26[0m [2;4m0.90it/s[0m  
        [0.6158, 0.3842],
        [0.6138, 0.3862],
        [0.6702, 0.3298],
        [0.6414, 0.3586],
        [0.6549, 0.3451],
        [0.6045, 0.3955],
        [0.6816, 0.3184],
        [0.6860, 0.3140],
[2K        [0.6864, 0.3136]], device='cuda:0');98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 71/203 [2m0:01:17 â€¢ 0:02:26[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5765, 0.4235],38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 72/203 [2m0:01:18 â€¢ 0:02:23[0m [2;4m0.92it/s[0m   
        [0.5708, 0.4292],
        [0.5753, 0.4247],
        [0.5741, 0.4259],
        [0.5765, 0.4235],
        [0.5740, 0.4260],
        [0.5727, 0.4273],
        [0.5763, 0.4237],
        [0.5754, 0.4246],
[2K        [0.5764, 0.4236]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 72/203 [2m0:01:18 â€¢ 0:02:23[0m [2;4m0.92it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 72/203 [2m0:01:18 â€¢ 0:02:23[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6546, 0.3454]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 72/203 [2m0:01:18 â€¢ 0:02:23[0m [2;4m0.92it/s[0m  
[2KStep 0 - TTA Loss: 0.5047â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 72/203 [2m0:01:18 â€¢ 0:02:23[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6574, 0.3426]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 72/203 [2m0:01:18 â€¢ 0:02:23[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6605, 0.3395]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 72/203 [2m0:01:18 â€¢ 0:02:23[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6638, 0.3362]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 72/203 [2m0:01:18 â€¢ 0:02:23[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6673, 0.3327]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 72/203 [2m0:01:18 â€¢ 0:02:23[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6710, 0.3290]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 72/203 [2m0:01:18 â€¢ 0:02:23[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6747, 0.3253]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 72/203 [2m0:01:18 â€¢ 0:02:23[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6785, 0.3215]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 72/203 [2m0:01:18 â€¢ 0:02:23[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6823, 0.3177]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 72/203 [2m0:01:18 â€¢ 0:02:23[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6861, 0.3139]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 72/203 [2m0:01:18 â€¢ 0:02:23[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6898, 0.3102]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 72/203 [2m0:01:18 â€¢ 0:02:23[0m [2;4m0.92it/s[0m  
[2KStep 10 - TTA Loss: 0.4714â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 72/203 [2m0:01:18 â€¢ 0:02:23[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6934, 0.3066]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 72/203 [2m0:01:18 â€¢ 0:02:23[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6969, 0.3031]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 72/203 [2m0:01:18 â€¢ 0:02:23[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.7002, 0.2998]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 72/203 [2m0:01:18 â€¢ 0:02:23[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.7033, 0.2967]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 72/203 [2m0:01:18 â€¢ 0:02:23[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.7062, 0.2938]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 72/203 [2m0:01:18 â€¢ 0:02:23[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.7089, 0.2911]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 72/203 [2m0:01:18 â€¢ 0:02:23[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.7114, 0.2886]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 72/203 [2m0:01:18 â€¢ 0:02:23[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.7136, 0.2864]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 72/203 [2m0:01:18 â€¢ 0:02:23[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.7156, 0.2844]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 72/203 [2m0:01:18 â€¢ 0:02:23[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.7174, 0.2826]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 72/203 [2m0:01:18 â€¢ 0:02:23[0m [2;4m0.92it/s[0m  
[2KStep 20 - TTA Loss: 0.4656â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 72/203 [2m0:01:18 â€¢ 0:02:23[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.7189, 0.2811]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 72/203 [2m0:01:18 â€¢ 0:02:23[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.7201, 0.2799]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 72/203 [2m0:01:18 â€¢ 0:02:23[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.7211, 0.2789]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 72/203 [2m0:01:18 â€¢ 0:02:23[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.7219, 0.2781]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 72/203 [2m0:01:18 â€¢ 0:02:23[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.7225, 0.2775]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 72/203 [2m0:01:18 â€¢ 0:02:23[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.7229, 0.2771]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 72/203 [2m0:01:18 â€¢ 0:02:23[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.7232, 0.2768]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 72/203 [2m0:01:18 â€¢ 0:02:23[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.7234, 0.2766]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 72/203 [2m0:01:18 â€¢ 0:02:23[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.7235, 0.2765]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 72/203 [2m0:01:18 â€¢ 0:02:23[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5806, 0.4194]], device='cuda:0')37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 72/203 [2m0:01:18 â€¢ 0:02:23[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6934, 0.3066],;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 72/203 [2m0:01:18 â€¢ 0:02:23[0m [2;4m0.92it/s[0m  
        [0.6313, 0.3687],
        [0.6409, 0.3591],
        [0.6987, 0.3013],
        [0.6744, 0.3256],
        [0.6802, 0.3198],
        [0.6304, 0.3696],
        [0.6987, 0.3013],
        [0.7127, 0.2873],
[2K        [0.7236, 0.2764]], device='cuda:0')5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 72/203 [2m0:01:18 â€¢ 0:02:23[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5769, 0.4231],38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 73/203 [2m0:01:20 â€¢ 0:02:23[0m [2;4m0.91it/s[0m  
        [0.5711, 0.4289],
        [0.5759, 0.4241],
        [0.5745, 0.4255],
        [0.5772, 0.4228],
        [0.5746, 0.4254],
        [0.5732, 0.4268],
        [0.5767, 0.4233],
        [0.5759, 0.4241],
[2K        [0.5774, 0.4226]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 73/203 [2m0:01:20 â€¢ 0:02:23[0m [2;4m0.91it/s[0m  
[2KClass label: GolfSwingâ”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 73/203 [2m0:01:20 â€¢ 0:02:23[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6463, 0.3538]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 73/203 [2m0:01:20 â€¢ 0:02:23[0m [2;4m0.91it/s[0m  
[2KStep 0 - TTA Loss: 0.7747â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 73/203 [2m0:01:20 â€¢ 0:02:23[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6463, 0.3538]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 73/203 [2m0:01:20 â€¢ 0:02:23[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6463, 0.3537]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 73/203 [2m0:01:20 â€¢ 0:02:23[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6463, 0.3537]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 73/203 [2m0:01:20 â€¢ 0:02:23[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6464, 0.3536]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 73/203 [2m0:01:20 â€¢ 0:02:23[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6466, 0.3534]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 73/203 [2m0:01:20 â€¢ 0:02:23[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6468, 0.3532]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 73/203 [2m0:01:20 â€¢ 0:02:23[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6471, 0.3529]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 73/203 [2m0:01:20 â€¢ 0:02:23[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6475, 0.3525]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 73/203 [2m0:01:20 â€¢ 0:02:23[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6480, 0.3520]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 73/203 [2m0:01:20 â€¢ 0:02:23[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6486, 0.3514]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 73/203 [2m0:01:20 â€¢ 0:02:23[0m [2;4m0.91it/s[0m  
[2KStep 10 - TTA Loss: 0.7834â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 73/203 [2m0:01:20 â€¢ 0:02:23[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6492, 0.3508]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 73/203 [2m0:01:20 â€¢ 0:02:23[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6499, 0.3501]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 73/203 [2m0:01:20 â€¢ 0:02:23[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6507, 0.3493]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 73/203 [2m0:01:20 â€¢ 0:02:23[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6515, 0.3485]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 73/203 [2m0:01:20 â€¢ 0:02:23[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6524, 0.3476]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 73/203 [2m0:01:20 â€¢ 0:02:23[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6534, 0.3466]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 73/203 [2m0:01:20 â€¢ 0:02:23[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6544, 0.3456]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 73/203 [2m0:01:20 â€¢ 0:02:23[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6555, 0.3445]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 73/203 [2m0:01:20 â€¢ 0:02:23[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6565, 0.3435]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 73/203 [2m0:01:20 â€¢ 0:02:23[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6576, 0.3424]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 73/203 [2m0:01:20 â€¢ 0:02:23[0m [2;4m0.91it/s[0m  
[2KStep 20 - TTA Loss: 0.7787â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 73/203 [2m0:01:20 â€¢ 0:02:23[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6587, 0.3413]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 73/203 [2m0:01:20 â€¢ 0:02:23[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6599, 0.3401]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 73/203 [2m0:01:20 â€¢ 0:02:23[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6611, 0.3389]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 73/203 [2m0:01:20 â€¢ 0:02:23[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6623, 0.3377]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 73/203 [2m0:01:20 â€¢ 0:02:23[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6636, 0.3364]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 73/203 [2m0:01:20 â€¢ 0:02:23[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6649, 0.3351]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 73/203 [2m0:01:20 â€¢ 0:02:23[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6662, 0.3338]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 73/203 [2m0:01:20 â€¢ 0:02:23[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6675, 0.3325]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 73/203 [2m0:01:20 â€¢ 0:02:23[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6689, 0.3311]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 73/203 [2m0:01:20 â€¢ 0:02:23[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5761, 0.4239]], device='cuda:0')37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 73/203 [2m0:01:20 â€¢ 0:02:23[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6560, 0.3440],;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 73/203 [2m0:01:20 â€¢ 0:02:23[0m [2;4m0.91it/s[0m  
        [0.6099, 0.3901],
        [0.6092, 0.3908],
        [0.6703, 0.3297],
        [0.6302, 0.3698],
        [0.6368, 0.3632],
        [0.5911, 0.4089],
        [0.6680, 0.3320],
        [0.6827, 0.3173],
[2K        [0.6807, 0.3193]], device='cuda:0')5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 73/203 [2m0:01:20 â€¢ 0:02:23[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5771, 0.4229],38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 74/203 [2m0:01:21 â€¢ 0:02:22[0m [2;4m0.91it/s[0m  
        [0.5713, 0.4287],
        [0.5760, 0.4240],
        [0.5748, 0.4252],
        [0.5774, 0.4226],
        [0.5748, 0.4252],
        [0.5734, 0.4266],
        [0.5769, 0.4231],
        [0.5761, 0.4239],
[2K        [0.5777, 0.4223]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 74/203 [2m0:01:21 â€¢ 0:02:22[0m [2;4m0.91it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 74/203 [2m0:01:21 â€¢ 0:02:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6580, 0.3420]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 74/203 [2m0:01:21 â€¢ 0:02:22[0m [2;4m0.91it/s[0m  
[2KStep 0 - TTA Loss: 0.7822â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 74/203 [2m0:01:21 â€¢ 0:02:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6592, 0.3408]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 74/203 [2m0:01:21 â€¢ 0:02:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6605, 0.3395]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 74/203 [2m0:01:21 â€¢ 0:02:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6619, 0.3381]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 74/203 [2m0:01:21 â€¢ 0:02:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6633, 0.3367]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 74/203 [2m0:01:21 â€¢ 0:02:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6648, 0.3352]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 74/203 [2m0:01:21 â€¢ 0:02:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6662, 0.3338]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 74/203 [2m0:01:21 â€¢ 0:02:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6677, 0.3323]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 74/203 [2m0:01:21 â€¢ 0:02:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6691, 0.3309]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 74/203 [2m0:01:21 â€¢ 0:02:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6705, 0.3295]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 74/203 [2m0:01:21 â€¢ 0:02:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6719, 0.3281]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 74/203 [2m0:01:21 â€¢ 0:02:22[0m [2;4m0.91it/s[0m  
[2KStep 10 - TTA Loss: 0.7468â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 74/203 [2m0:01:21 â€¢ 0:02:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6732, 0.3268]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 74/203 [2m0:01:21 â€¢ 0:02:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6745, 0.3255]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 74/203 [2m0:01:21 â€¢ 0:02:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6757, 0.3243]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 74/203 [2m0:01:21 â€¢ 0:02:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6768, 0.3232]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 74/203 [2m0:01:21 â€¢ 0:02:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6779, 0.3221]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 74/203 [2m0:01:21 â€¢ 0:02:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6788, 0.3212]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 74/203 [2m0:01:21 â€¢ 0:02:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6797, 0.3203]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 74/203 [2m0:01:21 â€¢ 0:02:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6804, 0.3196]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 74/203 [2m0:01:21 â€¢ 0:02:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6811, 0.3189]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 74/203 [2m0:01:21 â€¢ 0:02:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6817, 0.3183]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 74/203 [2m0:01:21 â€¢ 0:02:22[0m [2;4m0.91it/s[0m  
[2KStep 20 - TTA Loss: 0.7581â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 74/203 [2m0:01:21 â€¢ 0:02:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6822, 0.3178]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 74/203 [2m0:01:21 â€¢ 0:02:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6826, 0.3174]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 74/203 [2m0:01:21 â€¢ 0:02:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6830, 0.3170]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 74/203 [2m0:01:21 â€¢ 0:02:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6832, 0.3168]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 74/203 [2m0:01:21 â€¢ 0:02:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6834, 0.3166]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 74/203 [2m0:01:21 â€¢ 0:02:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6836, 0.3164]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 74/203 [2m0:01:21 â€¢ 0:02:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6837, 0.3163]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 74/203 [2m0:01:21 â€¢ 0:02:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6837, 0.3163]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 74/203 [2m0:01:21 â€¢ 0:02:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6837, 0.3163]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 74/203 [2m0:01:21 â€¢ 0:02:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5795, 0.4205]], device='cuda:0')8;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 74/203 [2m0:01:21 â€¢ 0:02:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6595, 0.3405],;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 74/203 [2m0:01:21 â€¢ 0:02:22[0m [2;4m0.91it/s[0m  
        [0.6126, 0.3874],
        [0.6109, 0.3891],
        [0.6716, 0.3284],
        [0.6337, 0.3663],
        [0.6380, 0.3620],
        [0.5913, 0.4087],
        [0.6717, 0.3283],
        [0.6850, 0.3150],
[2K        [0.6839, 0.3161]], device='cuda:0')2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 74/203 [2m0:01:21 â€¢ 0:02:22[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5772, 0.4228],38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 75/203 [2m0:01:22 â€¢ 0:02:19[0m [2;4m0.92it/s[0m  
        [0.5714, 0.4286],
        [0.5762, 0.4238],
        [0.5750, 0.4250],
        [0.5776, 0.4224],
        [0.5750, 0.4250],
        [0.5736, 0.4264],
        [0.5770, 0.4230],
        [0.5763, 0.4237],
[2K        [0.5780, 0.4220]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 75/203 [2m0:01:22 â€¢ 0:02:19[0m [2;4m0.92it/s[0m  
[2KClass label: BaseballPitchâ”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 75/203 [2m0:01:22 â€¢ 0:02:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6321, 0.3679]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 75/203 [2m0:01:22 â€¢ 0:02:19[0m [2;4m0.92it/s[0m  
[2KStep 0 - TTA Loss: 0.9793â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 75/203 [2m0:01:22 â€¢ 0:02:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6321, 0.3679]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 75/203 [2m0:01:22 â€¢ 0:02:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6321, 0.3679]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 75/203 [2m0:01:22 â€¢ 0:02:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6321, 0.3679]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 75/203 [2m0:01:22 â€¢ 0:02:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6322, 0.3678]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 75/203 [2m0:01:22 â€¢ 0:02:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6323, 0.3677]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 75/203 [2m0:01:22 â€¢ 0:02:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6325, 0.3675]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 75/203 [2m0:01:22 â€¢ 0:02:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6328, 0.3672]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 75/203 [2m0:01:22 â€¢ 0:02:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6332, 0.3668]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 75/203 [2m0:01:22 â€¢ 0:02:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6337, 0.3663]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 75/203 [2m0:01:22 â€¢ 0:02:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6344, 0.3656]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 75/203 [2m0:01:22 â€¢ 0:02:19[0m [2;4m0.92it/s[0m  
[2KStep 10 - TTA Loss: 1.0080â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 75/203 [2m0:01:22 â€¢ 0:02:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6352, 0.3648]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 75/203 [2m0:01:22 â€¢ 0:02:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6362, 0.3638]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 75/203 [2m0:01:22 â€¢ 0:02:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6373, 0.3627]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 75/203 [2m0:01:22 â€¢ 0:02:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6386, 0.3614]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 75/203 [2m0:01:22 â€¢ 0:02:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6401, 0.3599]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 75/203 [2m0:01:22 â€¢ 0:02:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6418, 0.3582]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 75/203 [2m0:01:22 â€¢ 0:02:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6437, 0.3563]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 75/203 [2m0:01:22 â€¢ 0:02:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6458, 0.3542]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 75/203 [2m0:01:22 â€¢ 0:02:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6482, 0.3518]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 75/203 [2m0:01:22 â€¢ 0:02:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6507, 0.3493]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 75/203 [2m0:01:22 â€¢ 0:02:19[0m [2;4m0.92it/s[0m  
[2KStep 20 - TTA Loss: 0.9649â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 75/203 [2m0:01:22 â€¢ 0:02:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6534, 0.3466]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 75/203 [2m0:01:22 â€¢ 0:02:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6564, 0.3436]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 75/203 [2m0:01:22 â€¢ 0:02:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6597, 0.3403]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 75/203 [2m0:01:22 â€¢ 0:02:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6631, 0.3369]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 75/203 [2m0:01:22 â€¢ 0:02:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6668, 0.3332]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 75/203 [2m0:01:22 â€¢ 0:02:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6707, 0.3293]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 75/203 [2m0:01:22 â€¢ 0:02:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6747, 0.3253]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 75/203 [2m0:01:22 â€¢ 0:02:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6789, 0.3211]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 75/203 [2m0:01:22 â€¢ 0:02:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6833, 0.3167]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 75/203 [2m0:01:22 â€¢ 0:02:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5797, 0.4203]], device='cuda:0')8;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 75/203 [2m0:01:22 â€¢ 0:02:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6880, 0.3120],;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 75/203 [2m0:01:22 â€¢ 0:02:19[0m [2;4m0.92it/s[0m  
        [0.6169, 0.3831],
        [0.6103, 0.3897],
        [0.6836, 0.3164],
        [0.6450, 0.3550],
        [0.6491, 0.3509],
        [0.6005, 0.3995],
        [0.6837, 0.3163],
        [0.6947, 0.3053],
[2K        [0.6943, 0.3057]], device='cuda:0')2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 75/203 [2m0:01:22 â€¢ 0:02:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5774, 0.4226],38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 76/203 [2m0:01:23 â€¢ 0:02:18[0m [2;4m0.92it/s[0m  
        [0.5715, 0.4285],
        [0.5763, 0.4237],
        [0.5752, 0.4248],
        [0.5776, 0.4224],
        [0.5750, 0.4250],
        [0.5736, 0.4264],
        [0.5771, 0.4229],
        [0.5764, 0.4236],
[2K        [0.5781, 0.4219]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 76/203 [2m0:01:23 â€¢ 0:02:18[0m [2;4m0.92it/s[0m  
[2KClass label: Billiardsâ”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 76/203 [2m0:01:23 â€¢ 0:02:18[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5942, 0.4058]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 76/203 [2m0:01:23 â€¢ 0:02:18[0m [2;4m0.92it/s[0m  
[2KStep 0 - TTA Loss: 0.5085â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 76/203 [2m0:01:23 â€¢ 0:02:18[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5962, 0.4038]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 76/203 [2m0:01:23 â€¢ 0:02:18[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5984, 0.4016]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 76/203 [2m0:01:23 â€¢ 0:02:18[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6007, 0.3993]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 76/203 [2m0:01:23 â€¢ 0:02:18[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6032, 0.3968]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 76/203 [2m0:01:23 â€¢ 0:02:18[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6058, 0.3942]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 76/203 [2m0:01:23 â€¢ 0:02:18[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6084, 0.3916]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 76/203 [2m0:01:23 â€¢ 0:02:18[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6110, 0.3890]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 76/203 [2m0:01:23 â€¢ 0:02:18[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6137, 0.3863]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 76/203 [2m0:01:23 â€¢ 0:02:18[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6163, 0.3837]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 76/203 [2m0:01:23 â€¢ 0:02:18[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6189, 0.3811]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 76/203 [2m0:01:23 â€¢ 0:02:18[0m [2;4m0.92it/s[0m  
[2KStep 10 - TTA Loss: 0.5032â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 76/203 [2m0:01:23 â€¢ 0:02:18[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6214, 0.3786]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 76/203 [2m0:01:23 â€¢ 0:02:18[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6238, 0.3762]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 76/203 [2m0:01:23 â€¢ 0:02:18[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6262, 0.3738]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 76/203 [2m0:01:23 â€¢ 0:02:18[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6283, 0.3717]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 76/203 [2m0:01:23 â€¢ 0:02:18[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6304, 0.3696]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 76/203 [2m0:01:23 â€¢ 0:02:18[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6323, 0.3677]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 76/203 [2m0:01:23 â€¢ 0:02:18[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6341, 0.3659]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 76/203 [2m0:01:23 â€¢ 0:02:18[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6356, 0.3644]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 76/203 [2m0:01:23 â€¢ 0:02:18[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6370, 0.3630]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 76/203 [2m0:01:23 â€¢ 0:02:18[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6383, 0.3617]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 76/203 [2m0:01:23 â€¢ 0:02:18[0m [2;4m0.92it/s[0m  
[2KStep 20 - TTA Loss: 0.5497â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 76/203 [2m0:01:23 â€¢ 0:02:18[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6393, 0.3607]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 76/203 [2m0:01:23 â€¢ 0:02:18[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6402, 0.3598]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 76/203 [2m0:01:23 â€¢ 0:02:18[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6409, 0.3591]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 76/203 [2m0:01:23 â€¢ 0:02:18[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6414, 0.3586]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 76/203 [2m0:01:23 â€¢ 0:02:18[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6419, 0.3581]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 76/203 [2m0:01:23 â€¢ 0:02:18[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6422, 0.3578]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 76/203 [2m0:01:23 â€¢ 0:02:18[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6424, 0.3576]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 76/203 [2m0:01:23 â€¢ 0:02:18[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6425, 0.3575]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 76/203 [2m0:01:23 â€¢ 0:02:18[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6425, 0.3575]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 76/203 [2m0:01:23 â€¢ 0:02:18[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5736, 0.4264]], device='cuda:0')8;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 76/203 [2m0:01:23 â€¢ 0:02:18[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6746, 0.3254],;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 76/203 [2m0:01:23 â€¢ 0:02:18[0m [2;4m0.92it/s[0m  
        [0.6426, 0.3574],
        [0.6103, 0.3897],
        [0.6745, 0.3255],
        [0.6352, 0.3648],
        [0.6429, 0.3571],
        [0.5936, 0.4064],
        [0.6797, 0.3203],
        [0.6921, 0.3079],
[2K        [0.6855, 0.3145]], device='cuda:0')2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 76/203 [2m0:01:23 â€¢ 0:02:18[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5776, 0.4224],[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 77/203 [2m0:01:24 â€¢ 0:02:17[0m [2;4m0.92it/s[0m   
        [0.5719, 0.4281],
        [0.5764, 0.4236],
        [0.5755, 0.4245],
        [0.5778, 0.4222],
        [0.5752, 0.4248],
        [0.5738, 0.4262],
        [0.5773, 0.4227],
        [0.5767, 0.4233],
[2K        [0.5783, 0.4217]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 77/203 [2m0:01:24 â€¢ 0:02:17[0m [2;4m0.92it/s[0m  
[2KClass label: SoccerPenaltyâ”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 77/203 [2m0:01:24 â€¢ 0:02:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6559, 0.3441]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 77/203 [2m0:01:24 â€¢ 0:02:17[0m [2;4m0.92it/s[0m  
[2KStep 0 - TTA Loss: 0.7090â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 77/203 [2m0:01:24 â€¢ 0:02:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6559, 0.3441]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 77/203 [2m0:01:24 â€¢ 0:02:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6559, 0.3441]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 77/203 [2m0:01:24 â€¢ 0:02:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6559, 0.3441]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 77/203 [2m0:01:24 â€¢ 0:02:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6560, 0.3440]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 77/203 [2m0:01:24 â€¢ 0:02:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6561, 0.3439]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 77/203 [2m0:01:24 â€¢ 0:02:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6563, 0.3437]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 77/203 [2m0:01:24 â€¢ 0:02:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6566, 0.3434]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 77/203 [2m0:01:24 â€¢ 0:02:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6571, 0.3429]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 77/203 [2m0:01:24 â€¢ 0:02:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6576, 0.3424]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 77/203 [2m0:01:24 â€¢ 0:02:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6584, 0.3416]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 77/203 [2m0:01:24 â€¢ 0:02:17[0m [2;4m0.92it/s[0m  
[2KStep 10 - TTA Loss: 0.5606â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 77/203 [2m0:01:24 â€¢ 0:02:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6593, 0.3407]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 77/203 [2m0:01:24 â€¢ 0:02:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6605, 0.3395]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 77/203 [2m0:01:24 â€¢ 0:02:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6619, 0.3381]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 77/203 [2m0:01:24 â€¢ 0:02:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6635, 0.3365]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 77/203 [2m0:01:24 â€¢ 0:02:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6654, 0.3346]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 77/203 [2m0:01:24 â€¢ 0:02:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6675, 0.3325]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 77/203 [2m0:01:24 â€¢ 0:02:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6698, 0.3302]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 77/203 [2m0:01:24 â€¢ 0:02:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6725, 0.3275]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 77/203 [2m0:01:24 â€¢ 0:02:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6754, 0.3246]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 77/203 [2m0:01:24 â€¢ 0:02:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6785, 0.3215]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 77/203 [2m0:01:24 â€¢ 0:02:17[0m [2;4m0.92it/s[0m  
[2KStep 20 - TTA Loss: 0.6263â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 77/203 [2m0:01:24 â€¢ 0:02:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6820, 0.3180]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 77/203 [2m0:01:24 â€¢ 0:02:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6857, 0.3143]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 77/203 [2m0:01:24 â€¢ 0:02:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6896, 0.3104]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 77/203 [2m0:01:24 â€¢ 0:02:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6938, 0.3062]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 77/203 [2m0:01:24 â€¢ 0:02:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6982, 0.3018]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 77/203 [2m0:01:24 â€¢ 0:02:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.7028, 0.2972]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 77/203 [2m0:01:24 â€¢ 0:02:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.7075, 0.2925]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 77/203 [2m0:01:24 â€¢ 0:02:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.7125, 0.2875]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 77/203 [2m0:01:24 â€¢ 0:02:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.7175, 0.2825]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 77/203 [2m0:01:24 â€¢ 0:02:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5803, 0.4197]], device='cuda:0')237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 77/203 [2m0:01:24 â€¢ 0:02:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6666, 0.3334],5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 77/203 [2m0:01:24 â€¢ 0:02:17[0m [2;4m0.92it/s[0m  
        [0.6288, 0.3712],
        [0.6087, 0.3913],
        [0.6727, 0.3273],
        [0.6415, 0.3585],
        [0.6514, 0.3486],
        [0.6074, 0.3926],
        [0.7227, 0.2773],
        [0.6907, 0.3093],
[2K        [0.6857, 0.3143]], device='cuda:0');5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 77/203 [2m0:01:24 â€¢ 0:02:17[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5776, 0.4224],[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 78/203 [2m0:01:25 â€¢ 0:02:16[0m [2;4m0.93it/s[0m  
        [0.5720, 0.4280],
        [0.5765, 0.4235],
        [0.5756, 0.4244],
        [0.5778, 0.4222],
        [0.5752, 0.4248],
        [0.5738, 0.4262],
        [0.5774, 0.4226],
        [0.5768, 0.4232],
[2K        [0.5783, 0.4217]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 78/203 [2m0:01:25 â€¢ 0:02:16[0m [2;4m0.93it/s[0m  
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 78/203 [2m0:01:25 â€¢ 0:02:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6130, 0.3870]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 78/203 [2m0:01:25 â€¢ 0:02:16[0m [2;4m0.93it/s[0m  
[2KStep 0 - TTA Loss: 0.6109â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 78/203 [2m0:01:25 â€¢ 0:02:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6162, 0.3838]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 78/203 [2m0:01:25 â€¢ 0:02:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6194, 0.3806]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 78/203 [2m0:01:25 â€¢ 0:02:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6227, 0.3773]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 78/203 [2m0:01:25 â€¢ 0:02:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6260, 0.3740]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 78/203 [2m0:01:25 â€¢ 0:02:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6293, 0.3707]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 78/203 [2m0:01:25 â€¢ 0:02:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6326, 0.3674]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 78/203 [2m0:01:25 â€¢ 0:02:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6359, 0.3641]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 78/203 [2m0:01:25 â€¢ 0:02:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6391, 0.3609]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 78/203 [2m0:01:25 â€¢ 0:02:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6422, 0.3578]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 78/203 [2m0:01:25 â€¢ 0:02:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6452, 0.3548]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 78/203 [2m0:01:25 â€¢ 0:02:16[0m [2;4m0.93it/s[0m  
[2KStep 10 - TTA Loss: 0.6160â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 78/203 [2m0:01:25 â€¢ 0:02:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6481, 0.3519]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 78/203 [2m0:01:25 â€¢ 0:02:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6508, 0.3492]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 78/203 [2m0:01:25 â€¢ 0:02:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6534, 0.3466]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 78/203 [2m0:01:25 â€¢ 0:02:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6559, 0.3441]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 78/203 [2m0:01:25 â€¢ 0:02:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6582, 0.3418]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 78/203 [2m0:01:25 â€¢ 0:02:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6603, 0.3397]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 78/203 [2m0:01:25 â€¢ 0:02:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6622, 0.3378]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 78/203 [2m0:01:25 â€¢ 0:02:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6639, 0.3361]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 78/203 [2m0:01:25 â€¢ 0:02:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6654, 0.3346]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 78/203 [2m0:01:25 â€¢ 0:02:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6666, 0.3334]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 78/203 [2m0:01:25 â€¢ 0:02:16[0m [2;4m0.93it/s[0m  
[2KStep 20 - TTA Loss: 0.6185â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 78/203 [2m0:01:25 â€¢ 0:02:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6677, 0.3323]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 78/203 [2m0:01:25 â€¢ 0:02:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6687, 0.3313]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 78/203 [2m0:01:25 â€¢ 0:02:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6694, 0.3306]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 78/203 [2m0:01:25 â€¢ 0:02:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6700, 0.3300]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 78/203 [2m0:01:25 â€¢ 0:02:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6704, 0.3296]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 78/203 [2m0:01:25 â€¢ 0:02:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6707, 0.3293]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 78/203 [2m0:01:25 â€¢ 0:02:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6709, 0.3291]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 78/203 [2m0:01:25 â€¢ 0:02:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6711, 0.3289]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 78/203 [2m0:01:25 â€¢ 0:02:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6711, 0.3289]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 78/203 [2m0:01:25 â€¢ 0:02:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5776, 0.4224]], device='cuda:0')237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 78/203 [2m0:01:25 â€¢ 0:02:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6724, 0.3276],5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 78/203 [2m0:01:25 â€¢ 0:02:16[0m [2;4m0.93it/s[0m  
        [0.6298, 0.3702],
        [0.6216, 0.3784],
        [0.6787, 0.3213],
        [0.6540, 0.3460],
        [0.6712, 0.3288],
        [0.6198, 0.3802],
        [0.7135, 0.2865],
        [0.6990, 0.3010],
[2K        [0.6978, 0.3022]], device='cuda:0');5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 78/203 [2m0:01:25 â€¢ 0:02:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5777, 0.4223],[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 79/203 [2m0:01:26 â€¢ 0:02:17[0m [2;4m0.91it/s[0m  
        [0.5721, 0.4279],
        [0.5765, 0.4235],
        [0.5756, 0.4244],
        [0.5778, 0.4222],
        [0.5753, 0.4247],
        [0.5738, 0.4262],
        [0.5774, 0.4226],
        [0.5768, 0.4232],
[2K        [0.5783, 0.4217]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 79/203 [2m0:01:26 â€¢ 0:02:17[0m [2;4m0.91it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 79/203 [2m0:01:26 â€¢ 0:02:17[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6521, 0.3479]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 79/203 [2m0:01:26 â€¢ 0:02:17[0m [2;4m0.91it/s[0m  
[2KStep 0 - TTA Loss: 0.4960â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 79/203 [2m0:01:26 â€¢ 0:02:17[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6521, 0.3479]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 79/203 [2m0:01:26 â€¢ 0:02:17[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6521, 0.3479]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 79/203 [2m0:01:26 â€¢ 0:02:17[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6521, 0.3479]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 79/203 [2m0:01:26 â€¢ 0:02:17[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6522, 0.3478]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 79/203 [2m0:01:26 â€¢ 0:02:17[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6523, 0.3477]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 79/203 [2m0:01:26 â€¢ 0:02:17[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6524, 0.3476]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 79/203 [2m0:01:26 â€¢ 0:02:17[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6527, 0.3473]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 79/203 [2m0:01:26 â€¢ 0:02:17[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6530, 0.3470]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 79/203 [2m0:01:26 â€¢ 0:02:17[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6533, 0.3467]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 79/203 [2m0:01:26 â€¢ 0:02:17[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6537, 0.3463]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 79/203 [2m0:01:26 â€¢ 0:02:17[0m [2;4m0.91it/s[0m  
[2KStep 10 - TTA Loss: 0.5579â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 79/203 [2m0:01:26 â€¢ 0:02:17[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6542, 0.3458]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 79/203 [2m0:01:26 â€¢ 0:02:17[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6547, 0.3453]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 79/203 [2m0:01:26 â€¢ 0:02:17[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6553, 0.3447]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 79/203 [2m0:01:26 â€¢ 0:02:17[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6559, 0.3441]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 79/203 [2m0:01:26 â€¢ 0:02:17[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6566, 0.3434]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 79/203 [2m0:01:26 â€¢ 0:02:17[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6574, 0.3426]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 79/203 [2m0:01:26 â€¢ 0:02:17[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6582, 0.3418]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 79/203 [2m0:01:26 â€¢ 0:02:17[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6591, 0.3409]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 79/203 [2m0:01:26 â€¢ 0:02:17[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6601, 0.3399]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 79/203 [2m0:01:26 â€¢ 0:02:17[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6611, 0.3389]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 79/203 [2m0:01:26 â€¢ 0:02:17[0m [2;4m0.91it/s[0m  
[2KStep 20 - TTA Loss: 0.4624â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 79/203 [2m0:01:26 â€¢ 0:02:17[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6622, 0.3378]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 79/203 [2m0:01:26 â€¢ 0:02:17[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6634, 0.3366]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 79/203 [2m0:01:26 â€¢ 0:02:17[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6646, 0.3354]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 79/203 [2m0:01:26 â€¢ 0:02:17[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6659, 0.3341]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 79/203 [2m0:01:26 â€¢ 0:02:17[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6673, 0.3327]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 79/203 [2m0:01:26 â€¢ 0:02:17[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6688, 0.3312]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 79/203 [2m0:01:26 â€¢ 0:02:17[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6703, 0.3297]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 79/203 [2m0:01:26 â€¢ 0:02:17[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6719, 0.3281]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 79/203 [2m0:01:26 â€¢ 0:02:17[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6735, 0.3265]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 79/203 [2m0:01:26 â€¢ 0:02:17[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5792, 0.4208]], device='cuda:0')38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 79/203 [2m0:01:26 â€¢ 0:02:17[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6495, 0.3505],2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 79/203 [2m0:01:26 â€¢ 0:02:17[0m [2;4m0.91it/s[0m  
        [0.6082, 0.3918],
        [0.6029, 0.3971],
        [0.6614, 0.3386],
        [0.6261, 0.3739],
        [0.6354, 0.3646],
        [0.5869, 0.4131],
        [0.6663, 0.3337],
        [0.6784, 0.3216],
[2K        [0.6753, 0.3247]], device='cuda:0');2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 79/203 [2m0:01:26 â€¢ 0:02:17[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5777, 0.4223],[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 80/203 [2m0:01:27 â€¢ 0:02:17[0m [2;4m0.90it/s[0m  
        [0.5721, 0.4279],
        [0.5766, 0.4234],
        [0.5757, 0.4243],
        [0.5777, 0.4223],
        [0.5752, 0.4248],
        [0.5738, 0.4262],
        [0.5774, 0.4226],
        [0.5768, 0.4232],
[2K        [0.5781, 0.4219]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 80/203 [2m0:01:27 â€¢ 0:02:17[0m [2;4m0.90it/s[0m  
[2KClass label: BaseballPitchâ”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 80/203 [2m0:01:27 â€¢ 0:02:17[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6366, 0.3634]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 80/203 [2m0:01:27 â€¢ 0:02:17[0m [2;4m0.90it/s[0m  
[2KStep 0 - TTA Loss: 1.0848â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 80/203 [2m0:01:27 â€¢ 0:02:17[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6382, 0.3618]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 80/203 [2m0:01:27 â€¢ 0:02:17[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6397, 0.3603]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 80/203 [2m0:01:27 â€¢ 0:02:17[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6412, 0.3588]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 80/203 [2m0:01:27 â€¢ 0:02:17[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6427, 0.3573]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 80/203 [2m0:01:27 â€¢ 0:02:17[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6442, 0.3558]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 80/203 [2m0:01:27 â€¢ 0:02:17[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6456, 0.3544]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 80/203 [2m0:01:27 â€¢ 0:02:17[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6471, 0.3529]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 80/203 [2m0:01:27 â€¢ 0:02:17[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6485, 0.3515]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 80/203 [2m0:01:27 â€¢ 0:02:17[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6498, 0.3502]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 80/203 [2m0:01:27 â€¢ 0:02:17[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6512, 0.3488]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 80/203 [2m0:01:27 â€¢ 0:02:17[0m [2;4m0.90it/s[0m  
[2KStep 10 - TTA Loss: 1.1056â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 80/203 [2m0:01:27 â€¢ 0:02:17[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6525, 0.3475]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 80/203 [2m0:01:27 â€¢ 0:02:17[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6538, 0.3462]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 80/203 [2m0:01:27 â€¢ 0:02:17[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6549, 0.3451]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 80/203 [2m0:01:27 â€¢ 0:02:17[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6560, 0.3440]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 80/203 [2m0:01:27 â€¢ 0:02:17[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6570, 0.3430]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 80/203 [2m0:01:27 â€¢ 0:02:17[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6579, 0.3421]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 80/203 [2m0:01:27 â€¢ 0:02:17[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6587, 0.3413]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 80/203 [2m0:01:27 â€¢ 0:02:17[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6594, 0.3406]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 80/203 [2m0:01:27 â€¢ 0:02:17[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6600, 0.3400]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 80/203 [2m0:01:27 â€¢ 0:02:17[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6606, 0.3394]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 80/203 [2m0:01:27 â€¢ 0:02:17[0m [2;4m0.90it/s[0m  
[2KStep 20 - TTA Loss: 1.0526â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 80/203 [2m0:01:27 â€¢ 0:02:17[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6610, 0.3390]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 80/203 [2m0:01:27 â€¢ 0:02:17[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6614, 0.3386]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 80/203 [2m0:01:27 â€¢ 0:02:17[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6618, 0.3382]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 80/203 [2m0:01:27 â€¢ 0:02:17[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6620, 0.3380]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 80/203 [2m0:01:27 â€¢ 0:02:17[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6622, 0.3378]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 80/203 [2m0:01:27 â€¢ 0:02:17[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6623, 0.3377]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 80/203 [2m0:01:27 â€¢ 0:02:17[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6624, 0.3376]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 80/203 [2m0:01:27 â€¢ 0:02:17[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6625, 0.3375]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 80/203 [2m0:01:27 â€¢ 0:02:17[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6625, 0.3375]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 80/203 [2m0:01:27 â€¢ 0:02:17[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5788, 0.4212]], device='cuda:0')38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 80/203 [2m0:01:27 â€¢ 0:02:17[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6626, 0.3374],2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 80/203 [2m0:01:27 â€¢ 0:02:17[0m [2;4m0.90it/s[0m  
        [0.6118, 0.3882],
        [0.6057, 0.3943],
        [0.6679, 0.3321],
        [0.6306, 0.3694],
        [0.6392, 0.3608],
        [0.5902, 0.4098],
        [0.6743, 0.3257],
        [0.6838, 0.3162],
[2K        [0.6801, 0.3199]], device='cuda:0');2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 80/203 [2m0:01:27 â€¢ 0:02:17[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5775, 0.4225],[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 81/203 [2m0:01:28 â€¢ 0:02:15[0m [2;4m0.91it/s[0m  
        [0.5720, 0.4280],
        [0.5765, 0.4235],
        [0.5755, 0.4245],
        [0.5775, 0.4225],
        [0.5750, 0.4250],
        [0.5736, 0.4264],
        [0.5772, 0.4228],
        [0.5767, 0.4233],
[2K        [0.5778, 0.4222]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 81/203 [2m0:01:28 â€¢ 0:02:15[0m [2;4m0.91it/s[0m  
[2KClass label: TennisSwingâ”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 81/203 [2m0:01:28 â€¢ 0:02:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6625, 0.3375]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 81/203 [2m0:01:28 â€¢ 0:02:15[0m [2;4m0.91it/s[0m  
[2KStep 0 - TTA Loss: 0.7907â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 81/203 [2m0:01:28 â€¢ 0:02:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6625, 0.3375]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 81/203 [2m0:01:28 â€¢ 0:02:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6625, 0.3375]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 81/203 [2m0:01:28 â€¢ 0:02:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6625, 0.3375]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 81/203 [2m0:01:28 â€¢ 0:02:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6625, 0.3375]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 81/203 [2m0:01:28 â€¢ 0:02:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6625, 0.3375]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 81/203 [2m0:01:28 â€¢ 0:02:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6626, 0.3374]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 81/203 [2m0:01:28 â€¢ 0:02:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6626, 0.3374]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 81/203 [2m0:01:28 â€¢ 0:02:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6627, 0.3373]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 81/203 [2m0:01:28 â€¢ 0:02:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6627, 0.3373]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 81/203 [2m0:01:28 â€¢ 0:02:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6628, 0.3372]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 81/203 [2m0:01:28 â€¢ 0:02:15[0m [2;4m0.91it/s[0m  
[2KStep 10 - TTA Loss: 0.7796â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 81/203 [2m0:01:28 â€¢ 0:02:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6629, 0.3371]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 81/203 [2m0:01:28 â€¢ 0:02:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6629, 0.3371]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 81/203 [2m0:01:28 â€¢ 0:02:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6630, 0.3370]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 81/203 [2m0:01:28 â€¢ 0:02:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6631, 0.3369]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 81/203 [2m0:01:28 â€¢ 0:02:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6631, 0.3369]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 81/203 [2m0:01:28 â€¢ 0:02:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6631, 0.3369]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 81/203 [2m0:01:28 â€¢ 0:02:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6632, 0.3368]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 81/203 [2m0:01:28 â€¢ 0:02:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6632, 0.3368]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 81/203 [2m0:01:28 â€¢ 0:02:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6632, 0.3368]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 81/203 [2m0:01:28 â€¢ 0:02:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6632, 0.3368]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 81/203 [2m0:01:28 â€¢ 0:02:15[0m [2;4m0.91it/s[0m  
[2KStep 20 - TTA Loss: 0.7876â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 81/203 [2m0:01:28 â€¢ 0:02:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6632, 0.3368]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 81/203 [2m0:01:28 â€¢ 0:02:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6632, 0.3368]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 81/203 [2m0:01:28 â€¢ 0:02:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6631, 0.3369]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 81/203 [2m0:01:28 â€¢ 0:02:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6631, 0.3369]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 81/203 [2m0:01:28 â€¢ 0:02:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6630, 0.3370]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 81/203 [2m0:01:28 â€¢ 0:02:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6629, 0.3371]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 81/203 [2m0:01:28 â€¢ 0:02:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6629, 0.3371]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 81/203 [2m0:01:28 â€¢ 0:02:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6628, 0.3372]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 81/203 [2m0:01:28 â€¢ 0:02:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6627, 0.3373]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 81/203 [2m0:01:28 â€¢ 0:02:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5768, 0.4232]], device='cuda:0')38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 81/203 [2m0:01:28 â€¢ 0:02:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6367, 0.3633],2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 81/203 [2m0:01:28 â€¢ 0:02:15[0m [2;4m0.91it/s[0m  
        [0.5975, 0.4025],
        [0.5864, 0.4136],
        [0.6481, 0.3519],
        [0.6071, 0.3929],
        [0.6157, 0.3843],
        [0.5699, 0.4301],
        [0.6549, 0.3451],
        [0.6626, 0.3374],
[2K        [0.6569, 0.3431]], device='cuda:0');2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 81/203 [2m0:01:28 â€¢ 0:02:15[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5775, 0.4225],[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 82/203 [2m0:01:29 â€¢ 0:02:13[0m [2;4m0.91it/s[0m   
        [0.5720, 0.4280],
        [0.5765, 0.4235],
        [0.5755, 0.4245],
        [0.5775, 0.4225],
        [0.5750, 0.4250],
        [0.5736, 0.4264],
        [0.5772, 0.4228],
        [0.5767, 0.4233],
[2K        [0.5778, 0.4222]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 82/203 [2m0:01:29 â€¢ 0:02:13[0m [2;4m0.91it/s[0m  
[2KClass label: SoccerPenaltyâ”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 82/203 [2m0:01:29 â€¢ 0:02:13[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6552, 0.3448]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 82/203 [2m0:01:29 â€¢ 0:02:13[0m [2;4m0.91it/s[0m  
[2KStep 0 - TTA Loss: 0.8427â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 82/203 [2m0:01:29 â€¢ 0:02:13[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6553, 0.3447]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 82/203 [2m0:01:29 â€¢ 0:02:13[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6555, 0.3445]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 82/203 [2m0:01:29 â€¢ 0:02:13[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6557, 0.3443]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 82/203 [2m0:01:29 â€¢ 0:02:13[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6561, 0.3439]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 82/203 [2m0:01:29 â€¢ 0:02:13[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6566, 0.3434]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 82/203 [2m0:01:29 â€¢ 0:02:13[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6570, 0.3430]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 82/203 [2m0:01:29 â€¢ 0:02:13[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6576, 0.3424]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 82/203 [2m0:01:29 â€¢ 0:02:13[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6582, 0.3418]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 82/203 [2m0:01:29 â€¢ 0:02:13[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6588, 0.3412]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 82/203 [2m0:01:29 â€¢ 0:02:13[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6595, 0.3405]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 82/203 [2m0:01:29 â€¢ 0:02:13[0m [2;4m0.91it/s[0m  
[2KStep 10 - TTA Loss: 0.8532â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 82/203 [2m0:01:29 â€¢ 0:02:13[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6601, 0.3399]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 82/203 [2m0:01:29 â€¢ 0:02:13[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6608, 0.3392]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 82/203 [2m0:01:29 â€¢ 0:02:13[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6614, 0.3386]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 82/203 [2m0:01:29 â€¢ 0:02:13[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6620, 0.3380]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 82/203 [2m0:01:29 â€¢ 0:02:13[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6625, 0.3375]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 82/203 [2m0:01:29 â€¢ 0:02:13[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6630, 0.3370]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 82/203 [2m0:01:29 â€¢ 0:02:13[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6635, 0.3365]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 82/203 [2m0:01:29 â€¢ 0:02:13[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6639, 0.3361]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 82/203 [2m0:01:29 â€¢ 0:02:13[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6643, 0.3357]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 82/203 [2m0:01:29 â€¢ 0:02:13[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6646, 0.3354]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 82/203 [2m0:01:29 â€¢ 0:02:13[0m [2;4m0.91it/s[0m  
[2KStep 20 - TTA Loss: 0.8429â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 82/203 [2m0:01:29 â€¢ 0:02:13[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6649, 0.3351]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 82/203 [2m0:01:29 â€¢ 0:02:13[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6651, 0.3349]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 82/203 [2m0:01:29 â€¢ 0:02:13[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6653, 0.3347]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 82/203 [2m0:01:29 â€¢ 0:02:13[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6655, 0.3345]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 82/203 [2m0:01:29 â€¢ 0:02:13[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6656, 0.3344]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 82/203 [2m0:01:29 â€¢ 0:02:13[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6657, 0.3343]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 82/203 [2m0:01:29 â€¢ 0:02:13[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6657, 0.3343]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 82/203 [2m0:01:29 â€¢ 0:02:13[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6658, 0.3342]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 82/203 [2m0:01:29 â€¢ 0:02:13[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6658, 0.3342]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 82/203 [2m0:01:29 â€¢ 0:02:13[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5778, 0.4222]], device='cuda:0');237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 82/203 [2m0:01:29 â€¢ 0:02:13[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6357, 0.3643],;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 82/203 [2m0:01:29 â€¢ 0:02:13[0m [2;4m0.91it/s[0m  
        [0.6009, 0.3991],
        [0.5896, 0.4104],
        [0.6467, 0.3533],
        [0.6078, 0.3922],
        [0.6170, 0.3830],
        [0.5698, 0.4302],
        [0.6658, 0.3342],
        [0.6665, 0.3335],
[2K        [0.6568, 0.3432]], device='cuda:0')8;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 82/203 [2m0:01:29 â€¢ 0:02:13[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5775, 0.4225],[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 83/203 [2m0:01:30 â€¢ 0:02:12[0m [2;4m0.92it/s[0m  
        [0.5720, 0.4280],
        [0.5765, 0.4235],
        [0.5755, 0.4245],
        [0.5775, 0.4225],
        [0.5750, 0.4250],
        [0.5736, 0.4264],
        [0.5772, 0.4228],
        [0.5767, 0.4233],
[2K        [0.5778, 0.4222]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 83/203 [2m0:01:30 â€¢ 0:02:12[0m [2;4m0.92it/s[0m  
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 83/203 [2m0:01:30 â€¢ 0:02:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6177, 0.3823]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 83/203 [2m0:01:30 â€¢ 0:02:12[0m [2;4m0.92it/s[0m  
[2KStep 0 - TTA Loss: 0.6525â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 83/203 [2m0:01:30 â€¢ 0:02:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6177, 0.3823]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 83/203 [2m0:01:30 â€¢ 0:02:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6177, 0.3823]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 83/203 [2m0:01:30 â€¢ 0:02:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6178, 0.3822]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 83/203 [2m0:01:30 â€¢ 0:02:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6178, 0.3822]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 83/203 [2m0:01:30 â€¢ 0:02:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6179, 0.3821]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 83/203 [2m0:01:30 â€¢ 0:02:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6180, 0.3820]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 83/203 [2m0:01:30 â€¢ 0:02:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6182, 0.3818]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 83/203 [2m0:01:30 â€¢ 0:02:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6185, 0.3815]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 83/203 [2m0:01:30 â€¢ 0:02:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6190, 0.3810]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 83/203 [2m0:01:30 â€¢ 0:02:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6195, 0.3805]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 83/203 [2m0:01:30 â€¢ 0:02:12[0m [2;4m0.92it/s[0m  
[2KStep 10 - TTA Loss: 0.6251â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 83/203 [2m0:01:30 â€¢ 0:02:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6202, 0.3798]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 83/203 [2m0:01:30 â€¢ 0:02:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6210, 0.3790]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 83/203 [2m0:01:30 â€¢ 0:02:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6221, 0.3779]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 83/203 [2m0:01:30 â€¢ 0:02:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6233, 0.3767]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 83/203 [2m0:01:30 â€¢ 0:02:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6246, 0.3754]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 83/203 [2m0:01:30 â€¢ 0:02:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6263, 0.3737]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 83/203 [2m0:01:30 â€¢ 0:02:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6281, 0.3719]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 83/203 [2m0:01:30 â€¢ 0:02:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6301, 0.3699]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 83/203 [2m0:01:30 â€¢ 0:02:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6323, 0.3677]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 83/203 [2m0:01:30 â€¢ 0:02:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6347, 0.3653]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 83/203 [2m0:01:30 â€¢ 0:02:12[0m [2;4m0.92it/s[0m  
[2KStep 20 - TTA Loss: 0.7028â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 83/203 [2m0:01:30 â€¢ 0:02:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6372, 0.3628]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 83/203 [2m0:01:30 â€¢ 0:02:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6399, 0.3601]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 83/203 [2m0:01:30 â€¢ 0:02:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6428, 0.3572]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 83/203 [2m0:01:30 â€¢ 0:02:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6459, 0.3541]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 83/203 [2m0:01:30 â€¢ 0:02:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6492, 0.3508]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 83/203 [2m0:01:30 â€¢ 0:02:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6525, 0.3475]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 83/203 [2m0:01:30 â€¢ 0:02:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6560, 0.3440]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 83/203 [2m0:01:30 â€¢ 0:02:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6597, 0.3403]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 83/203 [2m0:01:30 â€¢ 0:02:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6634, 0.3366]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 83/203 [2m0:01:30 â€¢ 0:02:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5772, 0.4228]], device='cuda:0');237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 83/203 [2m0:01:30 â€¢ 0:02:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6683, 0.3317],;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 83/203 [2m0:01:30 â€¢ 0:02:12[0m [2;4m0.92it/s[0m  
        [0.6245, 0.3755],
        [0.6244, 0.3756],
        [0.6787, 0.3213],
        [0.6485, 0.3515],
        [0.6673, 0.3327],
        [0.6164, 0.3836],
        [0.6897, 0.3103],
        [0.6969, 0.3031],
[2K        [0.6938, 0.3062]], device='cuda:0')8;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 83/203 [2m0:01:30 â€¢ 0:02:12[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5774, 0.4226],[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 84/203 [2m0:01:31 â€¢ 0:02:11[0m [2;4m0.92it/s[0m  
        [0.5720, 0.4280],
        [0.5765, 0.4235],
        [0.5755, 0.4245],
        [0.5775, 0.4225],
        [0.5750, 0.4250],
        [0.5735, 0.4265],
        [0.5772, 0.4228],
        [0.5767, 0.4233],
[2K        [0.5778, 0.4222]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 84/203 [2m0:01:31 â€¢ 0:02:11[0m [2;4m0.92it/s[0m  
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 84/203 [2m0:01:31 â€¢ 0:02:11[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6135, 0.3865]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 84/203 [2m0:01:31 â€¢ 0:02:11[0m [2;4m0.92it/s[0m  
[2KStep 0 - TTA Loss: 0.6321â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 84/203 [2m0:01:31 â€¢ 0:02:11[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6174, 0.3826]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 84/203 [2m0:01:31 â€¢ 0:02:11[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6211, 0.3789]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 84/203 [2m0:01:31 â€¢ 0:02:11[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6246, 0.3754]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 84/203 [2m0:01:31 â€¢ 0:02:11[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6279, 0.3721]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 84/203 [2m0:01:31 â€¢ 0:02:11[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6310, 0.3690]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 84/203 [2m0:01:31 â€¢ 0:02:11[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6339, 0.3661]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 84/203 [2m0:01:31 â€¢ 0:02:11[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6367, 0.3633]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 84/203 [2m0:01:31 â€¢ 0:02:11[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6392, 0.3608]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 84/203 [2m0:01:31 â€¢ 0:02:11[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6416, 0.3584]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 84/203 [2m0:01:31 â€¢ 0:02:11[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6437, 0.3563]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 84/203 [2m0:01:31 â€¢ 0:02:11[0m [2;4m0.92it/s[0m  
[2KStep 10 - TTA Loss: 0.6726â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 84/203 [2m0:01:31 â€¢ 0:02:11[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6458, 0.3542]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 84/203 [2m0:01:31 â€¢ 0:02:11[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6476, 0.3524]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 84/203 [2m0:01:31 â€¢ 0:02:11[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6493, 0.3507]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 84/203 [2m0:01:31 â€¢ 0:02:11[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6508, 0.3492]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 84/203 [2m0:01:31 â€¢ 0:02:11[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6522, 0.3478]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 84/203 [2m0:01:31 â€¢ 0:02:11[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6535, 0.3465]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 84/203 [2m0:01:31 â€¢ 0:02:11[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6546, 0.3454]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 84/203 [2m0:01:31 â€¢ 0:02:11[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6556, 0.3444]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 84/203 [2m0:01:31 â€¢ 0:02:11[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6565, 0.3435]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 84/203 [2m0:01:31 â€¢ 0:02:11[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6572, 0.3428]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 84/203 [2m0:01:31 â€¢ 0:02:11[0m [2;4m0.92it/s[0m  
[2KStep 20 - TTA Loss: 0.6887â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 84/203 [2m0:01:31 â€¢ 0:02:11[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6578, 0.3422]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 84/203 [2m0:01:31 â€¢ 0:02:11[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6583, 0.3417]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 84/203 [2m0:01:31 â€¢ 0:02:11[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6587, 0.3413]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 84/203 [2m0:01:31 â€¢ 0:02:11[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6590, 0.3410]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 84/203 [2m0:01:31 â€¢ 0:02:11[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6593, 0.3407]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 84/203 [2m0:01:31 â€¢ 0:02:11[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6594, 0.3406]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 84/203 [2m0:01:31 â€¢ 0:02:11[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6595, 0.3405]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 84/203 [2m0:01:31 â€¢ 0:02:11[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6596, 0.3404]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 84/203 [2m0:01:31 â€¢ 0:02:11[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6596, 0.3404]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 84/203 [2m0:01:31 â€¢ 0:02:11[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5770, 0.4230]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 84/203 [2m0:01:31 â€¢ 0:02:11[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6596, 0.3404],;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 84/203 [2m0:01:31 â€¢ 0:02:11[0m [2;4m0.92it/s[0m  
        [0.6170, 0.3830],
        [0.6177, 0.3823],
        [0.6716, 0.3284],
        [0.6403, 0.3597],
        [0.6597, 0.3403],
        [0.6070, 0.3930],
        [0.6836, 0.3164],
        [0.6915, 0.3085],
[2K        [0.6872, 0.3128]], device='cuda:0')8;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 84/203 [2m0:01:31 â€¢ 0:02:11[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5774, 0.4226],[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 85/203 [2m0:01:32 â€¢ 0:02:09[0m [2;4m0.92it/s[0m  
        [0.5721, 0.4279],
        [0.5765, 0.4235],
        [0.5755, 0.4245],
        [0.5774, 0.4226],
        [0.5751, 0.4249],
        [0.5735, 0.4265],
        [0.5772, 0.4228],
        [0.5767, 0.4233],
[2K        [0.5777, 0.4223]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 85/203 [2m0:01:32 â€¢ 0:02:09[0m [2;4m0.92it/s[0m  
[2KClass label: TennisSwingâ”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 85/203 [2m0:01:32 â€¢ 0:02:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6625, 0.3375]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 85/203 [2m0:01:32 â€¢ 0:02:09[0m [2;4m0.92it/s[0m  
[2KStep 0 - TTA Loss: 0.4494â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 85/203 [2m0:01:32 â€¢ 0:02:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6625, 0.3375]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 85/203 [2m0:01:32 â€¢ 0:02:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6625, 0.3375]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 85/203 [2m0:01:32 â€¢ 0:02:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6626, 0.3374]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 85/203 [2m0:01:32 â€¢ 0:02:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6626, 0.3374]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 85/203 [2m0:01:32 â€¢ 0:02:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6627, 0.3373]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 85/203 [2m0:01:32 â€¢ 0:02:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6629, 0.3371]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 85/203 [2m0:01:32 â€¢ 0:02:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6632, 0.3368]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 85/203 [2m0:01:32 â€¢ 0:02:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6636, 0.3364]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 85/203 [2m0:01:32 â€¢ 0:02:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6641, 0.3359]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 85/203 [2m0:01:32 â€¢ 0:02:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6648, 0.3352]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 85/203 [2m0:01:32 â€¢ 0:02:09[0m [2;4m0.92it/s[0m  
[2KStep 10 - TTA Loss: 0.3845â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 85/203 [2m0:01:32 â€¢ 0:02:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6656, 0.3344]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 85/203 [2m0:01:32 â€¢ 0:02:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6667, 0.3333]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 85/203 [2m0:01:32 â€¢ 0:02:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6678, 0.3322]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 85/203 [2m0:01:32 â€¢ 0:02:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6692, 0.3308]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 85/203 [2m0:01:32 â€¢ 0:02:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6708, 0.3292]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 85/203 [2m0:01:32 â€¢ 0:02:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6726, 0.3274]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 85/203 [2m0:01:32 â€¢ 0:02:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6746, 0.3254]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 85/203 [2m0:01:32 â€¢ 0:02:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6769, 0.3231]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 85/203 [2m0:01:32 â€¢ 0:02:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6795, 0.3205]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 85/203 [2m0:01:32 â€¢ 0:02:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6823, 0.3177]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 85/203 [2m0:01:32 â€¢ 0:02:09[0m [2;4m0.92it/s[0m  
[2KStep 20 - TTA Loss: 0.4584â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 85/203 [2m0:01:32 â€¢ 0:02:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6852, 0.3148]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 85/203 [2m0:01:32 â€¢ 0:02:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6884, 0.3116]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 85/203 [2m0:01:32 â€¢ 0:02:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6917, 0.3083]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 85/203 [2m0:01:32 â€¢ 0:02:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6953, 0.3047]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 85/203 [2m0:01:32 â€¢ 0:02:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6990, 0.3010]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 85/203 [2m0:01:32 â€¢ 0:02:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.7028, 0.2972]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 85/203 [2m0:01:32 â€¢ 0:02:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.7068, 0.2932]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 85/203 [2m0:01:32 â€¢ 0:02:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.7111, 0.2889]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 85/203 [2m0:01:32 â€¢ 0:02:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.7154, 0.2846]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 85/203 [2m0:01:32 â€¢ 0:02:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5799, 0.4201]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 85/203 [2m0:01:32 â€¢ 0:02:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6720, 0.3280],;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 85/203 [2m0:01:32 â€¢ 0:02:09[0m [2;4m0.92it/s[0m  
        [0.6267, 0.3733],
        [0.6141, 0.3859],
        [0.6857, 0.3143],
        [0.6410, 0.3590],
        [0.6529, 0.3471],
        [0.5968, 0.4032],
        [0.6842, 0.3158],
        [0.7198, 0.2802],
[2K        [0.6921, 0.3079]], device='cuda:0')8;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 85/203 [2m0:01:32 â€¢ 0:02:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5779, 0.4221],[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 86/203 [2m0:01:33 â€¢ 0:02:09[0m [2;4m0.91it/s[0m  
        [0.5725, 0.4275],
        [0.5768, 0.4232],
        [0.5762, 0.4238],
        [0.5779, 0.4221],
        [0.5755, 0.4245],
        [0.5739, 0.4261],
        [0.5776, 0.4224],
        [0.5775, 0.4225],
[2K        [0.5781, 0.4219]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 86/203 [2m0:01:33 â€¢ 0:02:09[0m [2;4m0.91it/s[0m  
[2KClass label: HammerThrowâ”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 86/203 [2m0:01:33 â€¢ 0:02:09[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5996, 0.4004]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 86/203 [2m0:01:33 â€¢ 0:02:09[0m [2;4m0.91it/s[0m  
[2KStep 0 - TTA Loss: 0.5728â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 86/203 [2m0:01:33 â€¢ 0:02:09[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6025, 0.3975]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 86/203 [2m0:01:33 â€¢ 0:02:09[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6056, 0.3944]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 86/203 [2m0:01:33 â€¢ 0:02:09[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6088, 0.3912]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 86/203 [2m0:01:33 â€¢ 0:02:09[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6121, 0.3879]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 86/203 [2m0:01:33 â€¢ 0:02:09[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6154, 0.3846]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 86/203 [2m0:01:33 â€¢ 0:02:09[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6187, 0.3813]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 86/203 [2m0:01:33 â€¢ 0:02:09[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6220, 0.3780]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 86/203 [2m0:01:33 â€¢ 0:02:09[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6253, 0.3747]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 86/203 [2m0:01:33 â€¢ 0:02:09[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6285, 0.3715]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 86/203 [2m0:01:33 â€¢ 0:02:09[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6316, 0.3684]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 86/203 [2m0:01:33 â€¢ 0:02:09[0m [2;4m0.91it/s[0m  
[2KStep 10 - TTA Loss: 0.5367â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 86/203 [2m0:01:33 â€¢ 0:02:09[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6346, 0.3654]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 86/203 [2m0:01:33 â€¢ 0:02:09[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6375, 0.3625]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 86/203 [2m0:01:33 â€¢ 0:02:09[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6402, 0.3598]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 86/203 [2m0:01:33 â€¢ 0:02:09[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6428, 0.3572]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 86/203 [2m0:01:33 â€¢ 0:02:09[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6452, 0.3548]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 86/203 [2m0:01:33 â€¢ 0:02:09[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6474, 0.3526]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 86/203 [2m0:01:33 â€¢ 0:02:09[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6494, 0.3506]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 86/203 [2m0:01:33 â€¢ 0:02:09[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6512, 0.3488]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 86/203 [2m0:01:33 â€¢ 0:02:09[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6528, 0.3472]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 86/203 [2m0:01:33 â€¢ 0:02:09[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6542, 0.3458]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 86/203 [2m0:01:33 â€¢ 0:02:09[0m [2;4m0.91it/s[0m  
[2KStep 20 - TTA Loss: 0.5418â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 86/203 [2m0:01:33 â€¢ 0:02:09[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6554, 0.3446]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 86/203 [2m0:01:33 â€¢ 0:02:09[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6564, 0.3436]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 86/203 [2m0:01:33 â€¢ 0:02:09[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6572, 0.3428]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 86/203 [2m0:01:33 â€¢ 0:02:09[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6578, 0.3422]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 86/203 [2m0:01:33 â€¢ 0:02:09[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6583, 0.3417]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 86/203 [2m0:01:33 â€¢ 0:02:09[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6586, 0.3414]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 86/203 [2m0:01:33 â€¢ 0:02:09[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6589, 0.3411]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 86/203 [2m0:01:33 â€¢ 0:02:09[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6590, 0.3410]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 86/203 [2m0:01:33 â€¢ 0:02:09[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6591, 0.3409]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 86/203 [2m0:01:33 â€¢ 0:02:09[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5806, 0.4194]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 86/203 [2m0:01:33 â€¢ 0:02:09[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6773, 0.3227],;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 86/203 [2m0:01:33 â€¢ 0:02:09[0m [2;4m0.91it/s[0m  
        [0.6258, 0.3742],
        [0.6244, 0.3756],
        [0.6890, 0.3110],
        [0.6591, 0.3409],
        [0.6605, 0.3395],
        [0.6062, 0.3938],
        [0.6898, 0.3102],
        [0.7132, 0.2868],
[2K        [0.7012, 0.2988]], device='cuda:0')8;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 86/203 [2m0:01:33 â€¢ 0:02:09[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5786, 0.4214],m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 87/203 [2m0:01:34 â€¢ 0:02:07[0m [2;4m0.91it/s[0m   
        [0.5730, 0.4270],
        [0.5772, 0.4228],
        [0.5768, 0.4232],
        [0.5784, 0.4216],
        [0.5759, 0.4241],
        [0.5743, 0.4257],
        [0.5781, 0.4219],
        [0.5781, 0.4219],
[2K        [0.5786, 0.4214]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 87/203 [2m0:01:34 â€¢ 0:02:07[0m [2;4m0.91it/s[0m  
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 87/203 [2m0:01:34 â€¢ 0:02:07[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6184, 0.3816]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 87/203 [2m0:01:34 â€¢ 0:02:07[0m [2;4m0.91it/s[0m  
[2KStep 0 - TTA Loss: 0.7262â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 87/203 [2m0:01:34 â€¢ 0:02:07[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6184, 0.3816]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 87/203 [2m0:01:34 â€¢ 0:02:07[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6184, 0.3816]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 87/203 [2m0:01:34 â€¢ 0:02:07[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6184, 0.3816]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 87/203 [2m0:01:34 â€¢ 0:02:07[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6185, 0.3815]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 87/203 [2m0:01:34 â€¢ 0:02:07[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6187, 0.3813]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 87/203 [2m0:01:34 â€¢ 0:02:07[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6190, 0.3810]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 87/203 [2m0:01:34 â€¢ 0:02:07[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6193, 0.3807]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 87/203 [2m0:01:34 â€¢ 0:02:07[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6198, 0.3802]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 87/203 [2m0:01:34 â€¢ 0:02:07[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6205, 0.3795]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 87/203 [2m0:01:34 â€¢ 0:02:07[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6213, 0.3787]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 87/203 [2m0:01:34 â€¢ 0:02:07[0m [2;4m0.91it/s[0m  
[2KStep 10 - TTA Loss: 0.7883â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 87/203 [2m0:01:34 â€¢ 0:02:07[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6222, 0.3778]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 87/203 [2m0:01:34 â€¢ 0:02:07[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6234, 0.3766]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 87/203 [2m0:01:34 â€¢ 0:02:07[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6247, 0.3753]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 87/203 [2m0:01:34 â€¢ 0:02:07[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6262, 0.3738]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 87/203 [2m0:01:34 â€¢ 0:02:07[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6278, 0.3722]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 87/203 [2m0:01:34 â€¢ 0:02:07[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6297, 0.3703]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 87/203 [2m0:01:34 â€¢ 0:02:07[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6318, 0.3682]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 87/203 [2m0:01:34 â€¢ 0:02:07[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6340, 0.3660]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 87/203 [2m0:01:34 â€¢ 0:02:07[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6365, 0.3635]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 87/203 [2m0:01:34 â€¢ 0:02:07[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6391, 0.3609]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 87/203 [2m0:01:34 â€¢ 0:02:07[0m [2;4m0.91it/s[0m  
[2KStep 20 - TTA Loss: 0.6164â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 87/203 [2m0:01:34 â€¢ 0:02:07[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6420, 0.3580]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 87/203 [2m0:01:34 â€¢ 0:02:07[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6450, 0.3550]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 87/203 [2m0:01:34 â€¢ 0:02:07[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6482, 0.3518]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 87/203 [2m0:01:34 â€¢ 0:02:07[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6516, 0.3484]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 87/203 [2m0:01:34 â€¢ 0:02:07[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6551, 0.3449]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 87/203 [2m0:01:34 â€¢ 0:02:07[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6588, 0.3412]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 87/203 [2m0:01:34 â€¢ 0:02:07[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6626, 0.3374]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 87/203 [2m0:01:34 â€¢ 0:02:07[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6665, 0.3335]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 87/203 [2m0:01:34 â€¢ 0:02:07[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6704, 0.3296]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 87/203 [2m0:01:34 â€¢ 0:02:07[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5787, 0.4213]], device='cuda:0')5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 87/203 [2m0:01:34 â€¢ 0:02:07[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6735, 0.3265],8;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 87/203 [2m0:01:34 â€¢ 0:02:07[0m [2;4m0.91it/s[0m  
        [0.6288, 0.3712],
        [0.6301, 0.3699],
        [0.6838, 0.3162],
        [0.6568, 0.3432],
        [0.6746, 0.3254],
        [0.6223, 0.3777],
        [0.6949, 0.3051],
        [0.7018, 0.2982],
[2K        [0.7002, 0.2998]], device='cuda:0')38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 87/203 [2m0:01:34 â€¢ 0:02:07[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5787, 0.4213],m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 88/203 [2m0:01:35 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
        [0.5731, 0.4269],
        [0.5773, 0.4227],
        [0.5769, 0.4231],
        [0.5786, 0.4214],
        [0.5762, 0.4238],
        [0.5745, 0.4255],
        [0.5783, 0.4217],
        [0.5783, 0.4217],
[2K        [0.5787, 0.4213]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 88/203 [2m0:01:35 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KClass label: CleanAndJerkâ”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 88/203 [2m0:01:35 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5903, 0.4097]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 88/203 [2m0:01:35 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KStep 0 - TTA Loss: 0.6162â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 88/203 [2m0:01:35 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5932, 0.4068]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 88/203 [2m0:01:35 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5963, 0.4037]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 88/203 [2m0:01:35 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5994, 0.4006]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 88/203 [2m0:01:35 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6027, 0.3973]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 88/203 [2m0:01:35 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6060, 0.3940]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 88/203 [2m0:01:35 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6093, 0.3907]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 88/203 [2m0:01:35 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6125, 0.3875]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 88/203 [2m0:01:35 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6157, 0.3843]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 88/203 [2m0:01:35 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6188, 0.3812]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 88/203 [2m0:01:35 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6219, 0.3781]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 88/203 [2m0:01:35 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KStep 10 - TTA Loss: 0.6409â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 88/203 [2m0:01:35 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6249, 0.3751]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 88/203 [2m0:01:35 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6277, 0.3723]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 88/203 [2m0:01:35 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6305, 0.3695]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 88/203 [2m0:01:35 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6330, 0.3670]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 88/203 [2m0:01:35 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6354, 0.3646]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 88/203 [2m0:01:35 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6375, 0.3625]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 88/203 [2m0:01:35 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6395, 0.3605]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 88/203 [2m0:01:35 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6413, 0.3587]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 88/203 [2m0:01:35 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6429, 0.3571]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 88/203 [2m0:01:35 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6443, 0.3557]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 88/203 [2m0:01:35 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KStep 20 - TTA Loss: 0.6897â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 88/203 [2m0:01:35 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6454, 0.3546]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 88/203 [2m0:01:35 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6464, 0.3536]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 88/203 [2m0:01:35 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6472, 0.3528]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 88/203 [2m0:01:35 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6478, 0.3522]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 88/203 [2m0:01:35 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6483, 0.3517]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 88/203 [2m0:01:35 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6486, 0.3514]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 88/203 [2m0:01:35 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6489, 0.3511]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 88/203 [2m0:01:35 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6490, 0.3510]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 88/203 [2m0:01:35 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6490, 0.3510]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 88/203 [2m0:01:35 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5800, 0.4200]], device='cuda:0')5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 88/203 [2m0:01:35 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6677, 0.3323],8;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 88/203 [2m0:01:35 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
        [0.6263, 0.3737],
        [0.6491, 0.3509],
        [0.6794, 0.3206],
        [0.6531, 0.3469],
        [0.6654, 0.3346],
        [0.6144, 0.3856],
        [0.6868, 0.3132],
        [0.6974, 0.3026],
[2K        [0.6964, 0.3036]], device='cuda:0')38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 88/203 [2m0:01:35 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5790, 0.4210],m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 89/203 [2m0:01:37 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
        [0.5734, 0.4266],
        [0.5779, 0.4221],
        [0.5772, 0.4228],
        [0.5788, 0.4212],
        [0.5765, 0.4235],
        [0.5748, 0.4252],
        [0.5786, 0.4214],
        [0.5785, 0.4215],
[2K        [0.5790, 0.4210]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 89/203 [2m0:01:37 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 89/203 [2m0:01:37 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6191, 0.3809]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 89/203 [2m0:01:37 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KStep 0 - TTA Loss: 0.6419â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 89/203 [2m0:01:37 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6191, 0.3809]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 89/203 [2m0:01:37 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6191, 0.3809]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 89/203 [2m0:01:37 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6191, 0.3809]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 89/203 [2m0:01:37 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6192, 0.3808]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 89/203 [2m0:01:37 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6193, 0.3807]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 89/203 [2m0:01:37 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6195, 0.3805]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 89/203 [2m0:01:37 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6198, 0.3802]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 89/203 [2m0:01:37 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6201, 0.3799]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 89/203 [2m0:01:37 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6206, 0.3794]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 89/203 [2m0:01:37 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6212, 0.3788]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 89/203 [2m0:01:37 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KStep 10 - TTA Loss: 0.6940â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 89/203 [2m0:01:37 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6219, 0.3781]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 89/203 [2m0:01:37 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6227, 0.3773]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 89/203 [2m0:01:37 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6237, 0.3763]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 89/203 [2m0:01:37 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6249, 0.3751]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 89/203 [2m0:01:37 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6262, 0.3738]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 89/203 [2m0:01:37 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6276, 0.3724]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 89/203 [2m0:01:37 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6293, 0.3707]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 89/203 [2m0:01:37 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6311, 0.3689]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 89/203 [2m0:01:37 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6330, 0.3670]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 89/203 [2m0:01:37 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6350, 0.3650]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 89/203 [2m0:01:37 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KStep 20 - TTA Loss: 0.6517â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 89/203 [2m0:01:37 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6372, 0.3628]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 89/203 [2m0:01:37 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6396, 0.3604]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 89/203 [2m0:01:37 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6420, 0.3580]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 89/203 [2m0:01:37 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6445, 0.3555]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 89/203 [2m0:01:37 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6472, 0.3528]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 89/203 [2m0:01:37 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6499, 0.3501]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 89/203 [2m0:01:37 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6527, 0.3473]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 89/203 [2m0:01:37 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6556, 0.3444]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 89/203 [2m0:01:37 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6586, 0.3414]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 89/203 [2m0:01:37 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5786, 0.4214]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 89/203 [2m0:01:37 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6637, 0.3363],8;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 89/203 [2m0:01:37 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
        [0.6191, 0.3809],
        [0.6246, 0.3754],
        [0.6748, 0.3252],
        [0.6449, 0.3551],
        [0.6618, 0.3382],
        [0.6099, 0.3901],
        [0.6843, 0.3157],
        [0.6919, 0.3081],
[2K        [0.6901, 0.3099]], device='cuda:0')38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 89/203 [2m0:01:37 â€¢ 0:02:06[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5791, 0.4209],m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 90/203 [2m0:01:38 â€¢ 0:02:06[0m [2;4m0.90it/s[0m  
        [0.5735, 0.4265],
        [0.5780, 0.4220],
        [0.5773, 0.4227],
        [0.5789, 0.4211],
        [0.5767, 0.4233],
        [0.5750, 0.4250],
        [0.5787, 0.4213],
        [0.5786, 0.4214],
[2K        [0.5791, 0.4209]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 90/203 [2m0:01:38 â€¢ 0:02:06[0m [2;4m0.90it/s[0m  
[2KClass label: HammerThrowâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 90/203 [2m0:01:38 â€¢ 0:02:06[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6055, 0.3945]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 90/203 [2m0:01:38 â€¢ 0:02:06[0m [2;4m0.90it/s[0m  
[2KStep 0 - TTA Loss: 0.7067â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 90/203 [2m0:01:38 â€¢ 0:02:06[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6079, 0.3921]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 90/203 [2m0:01:38 â€¢ 0:02:06[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6102, 0.3898]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 90/203 [2m0:01:38 â€¢ 0:02:06[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6123, 0.3877]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 90/203 [2m0:01:38 â€¢ 0:02:06[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6142, 0.3858]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 90/203 [2m0:01:38 â€¢ 0:02:06[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6160, 0.3840]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 90/203 [2m0:01:38 â€¢ 0:02:06[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6177, 0.3823]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 90/203 [2m0:01:38 â€¢ 0:02:06[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6193, 0.3807]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 90/203 [2m0:01:38 â€¢ 0:02:06[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6206, 0.3794]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 90/203 [2m0:01:38 â€¢ 0:02:06[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6220, 0.3780]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 90/203 [2m0:01:38 â€¢ 0:02:06[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6232, 0.3768]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 90/203 [2m0:01:38 â€¢ 0:02:06[0m [2;4m0.90it/s[0m  
[2KStep 10 - TTA Loss: 0.6149â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 90/203 [2m0:01:38 â€¢ 0:02:06[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6243, 0.3757]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 90/203 [2m0:01:38 â€¢ 0:02:06[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6253, 0.3747]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 90/203 [2m0:01:38 â€¢ 0:02:06[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6263, 0.3737]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 90/203 [2m0:01:38 â€¢ 0:02:06[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6271, 0.3729]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 90/203 [2m0:01:38 â€¢ 0:02:06[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6279, 0.3721]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 90/203 [2m0:01:38 â€¢ 0:02:06[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6285, 0.3715]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 90/203 [2m0:01:38 â€¢ 0:02:06[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6291, 0.3709]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 90/203 [2m0:01:38 â€¢ 0:02:06[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6297, 0.3703]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 90/203 [2m0:01:38 â€¢ 0:02:06[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6301, 0.3699]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 90/203 [2m0:01:38 â€¢ 0:02:06[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6305, 0.3695]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 90/203 [2m0:01:38 â€¢ 0:02:06[0m [2;4m0.90it/s[0m  
[2KStep 20 - TTA Loss: 0.6837â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 90/203 [2m0:01:38 â€¢ 0:02:06[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6309, 0.3691]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 90/203 [2m0:01:38 â€¢ 0:02:06[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6311, 0.3689]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 90/203 [2m0:01:38 â€¢ 0:02:06[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6313, 0.3687]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 90/203 [2m0:01:38 â€¢ 0:02:06[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6315, 0.3685]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 90/203 [2m0:01:38 â€¢ 0:02:06[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6316, 0.3684]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 90/203 [2m0:01:38 â€¢ 0:02:06[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6317, 0.3683]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 90/203 [2m0:01:38 â€¢ 0:02:06[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6318, 0.3682]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 90/203 [2m0:01:38 â€¢ 0:02:06[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6318, 0.3682]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 90/203 [2m0:01:38 â€¢ 0:02:06[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6318, 0.3682]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 90/203 [2m0:01:38 â€¢ 0:02:06[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5800, 0.4200]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 90/203 [2m0:01:38 â€¢ 0:02:06[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6527, 0.3473],8;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 90/203 [2m0:01:38 â€¢ 0:02:06[0m [2;4m0.90it/s[0m  
        [0.6105, 0.3895],
        [0.6083, 0.3917],
        [0.6628, 0.3372],
        [0.6319, 0.3681],
        [0.6453, 0.3547],
        [0.5957, 0.4043],
        [0.6775, 0.3225],
        [0.6797, 0.3203],
[2K        [0.6768, 0.3232]], device='cuda:0')38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 90/203 [2m0:01:38 â€¢ 0:02:06[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5792, 0.4208],m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 91/203 [2m0:01:39 â€¢ 0:02:04[0m [2;4m0.91it/s[0m  
        [0.5736, 0.4264],
        [0.5780, 0.4220],
        [0.5773, 0.4227],
        [0.5789, 0.4211],
        [0.5767, 0.4233],
        [0.5750, 0.4250],
        [0.5787, 0.4213],
        [0.5786, 0.4214],
[2K        [0.5790, 0.4210]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 91/203 [2m0:01:39 â€¢ 0:02:04[0m [2;4m0.91it/s[0m  
[2KClass label: SoccerPenaltyâ”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 91/203 [2m0:01:39 â€¢ 0:02:04[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6574, 0.3426]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 91/203 [2m0:01:39 â€¢ 0:02:04[0m [2;4m0.91it/s[0m  
[2KStep 0 - TTA Loss: 1.2173â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 91/203 [2m0:01:39 â€¢ 0:02:04[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6574, 0.3426]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 91/203 [2m0:01:39 â€¢ 0:02:04[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6574, 0.3426]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 91/203 [2m0:01:39 â€¢ 0:02:04[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6574, 0.3426]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 91/203 [2m0:01:39 â€¢ 0:02:04[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6574, 0.3426]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 91/203 [2m0:01:39 â€¢ 0:02:04[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6574, 0.3426]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 91/203 [2m0:01:39 â€¢ 0:02:04[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6574, 0.3426]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 91/203 [2m0:01:39 â€¢ 0:02:04[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6574, 0.3426]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 91/203 [2m0:01:39 â€¢ 0:02:04[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6574, 0.3426]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 91/203 [2m0:01:39 â€¢ 0:02:04[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6574, 0.3426]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 91/203 [2m0:01:39 â€¢ 0:02:04[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6575, 0.3425]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 91/203 [2m0:01:39 â€¢ 0:02:04[0m [2;4m0.91it/s[0m  
[2KStep 10 - TTA Loss: 1.1388â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 91/203 [2m0:01:39 â€¢ 0:02:04[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6575, 0.3425]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 91/203 [2m0:01:39 â€¢ 0:02:04[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6575, 0.3425]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 91/203 [2m0:01:39 â€¢ 0:02:04[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6576, 0.3424]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 91/203 [2m0:01:39 â€¢ 0:02:04[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6576, 0.3424]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 91/203 [2m0:01:39 â€¢ 0:02:04[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6577, 0.3423]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 91/203 [2m0:01:39 â€¢ 0:02:04[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6578, 0.3422]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 91/203 [2m0:01:39 â€¢ 0:02:04[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6578, 0.3422]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 91/203 [2m0:01:39 â€¢ 0:02:04[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6579, 0.3421]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 91/203 [2m0:01:39 â€¢ 0:02:04[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6579, 0.3421]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 91/203 [2m0:01:39 â€¢ 0:02:04[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6578, 0.3422]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 91/203 [2m0:01:39 â€¢ 0:02:04[0m [2;4m0.91it/s[0m  
[2KStep 20 - TTA Loss: 1.1929â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 91/203 [2m0:01:39 â€¢ 0:02:04[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6577, 0.3423]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 91/203 [2m0:01:39 â€¢ 0:02:04[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6576, 0.3424]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 91/203 [2m0:01:39 â€¢ 0:02:04[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6575, 0.3425]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 91/203 [2m0:01:39 â€¢ 0:02:04[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6574, 0.3426]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 91/203 [2m0:01:39 â€¢ 0:02:04[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6572, 0.3428]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 91/203 [2m0:01:39 â€¢ 0:02:04[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6570, 0.3430]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 91/203 [2m0:01:39 â€¢ 0:02:04[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6569, 0.3431]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 91/203 [2m0:01:39 â€¢ 0:02:04[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6568, 0.3432]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 91/203 [2m0:01:39 â€¢ 0:02:04[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6567, 0.3433]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 91/203 [2m0:01:39 â€¢ 0:02:04[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5787, 0.4213]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 91/203 [2m0:01:39 â€¢ 0:02:04[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6346, 0.3654],8;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 91/203 [2m0:01:39 â€¢ 0:02:04[0m [2;4m0.91it/s[0m  
        [0.5985, 0.4015],
        [0.5894, 0.4106],
        [0.6481, 0.3519],
        [0.6055, 0.3945],
        [0.6164, 0.3836],
        [0.5700, 0.4300],
        [0.6565, 0.3435],
        [0.6645, 0.3355],
[2K        [0.6561, 0.3439]], device='cuda:0')38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 91/203 [2m0:01:39 â€¢ 0:02:04[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5791, 0.4209],0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 92/203 [2m0:01:40 â€¢ 0:02:02[0m [2;4m0.91it/s[0m   
        [0.5735, 0.4265],
        [0.5780, 0.4220],
        [0.5773, 0.4227],
        [0.5788, 0.4212],
        [0.5766, 0.4234],
        [0.5750, 0.4250],
        [0.5785, 0.4215],
        [0.5786, 0.4214],
[2K        [0.5790, 0.4210]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 92/203 [2m0:01:40 â€¢ 0:02:02[0m [2;4m0.91it/s[0m  
[2KClass label: BaseballPitchâ”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 92/203 [2m0:01:40 â€¢ 0:02:02[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6310, 0.3690]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 92/203 [2m0:01:40 â€¢ 0:02:02[0m [2;4m0.91it/s[0m  
[2KStep 0 - TTA Loss: 1.0462â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 92/203 [2m0:01:40 â€¢ 0:02:02[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6310, 0.3690]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 92/203 [2m0:01:40 â€¢ 0:02:02[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6311, 0.3689]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 92/203 [2m0:01:40 â€¢ 0:02:02[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6312, 0.3688]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 92/203 [2m0:01:40 â€¢ 0:02:02[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6314, 0.3686]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 92/203 [2m0:01:40 â€¢ 0:02:02[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6316, 0.3684]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 92/203 [2m0:01:40 â€¢ 0:02:02[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6317, 0.3683]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 92/203 [2m0:01:40 â€¢ 0:02:02[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6319, 0.3681]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 92/203 [2m0:01:40 â€¢ 0:02:02[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6320, 0.3680]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 92/203 [2m0:01:40 â€¢ 0:02:02[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6321, 0.3679]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 92/203 [2m0:01:40 â€¢ 0:02:02[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6323, 0.3677]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 92/203 [2m0:01:40 â€¢ 0:02:02[0m [2;4m0.91it/s[0m  
[2KStep 10 - TTA Loss: 1.0703â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 92/203 [2m0:01:40 â€¢ 0:02:02[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6324, 0.3676]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 92/203 [2m0:01:40 â€¢ 0:02:02[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6326, 0.3674]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 92/203 [2m0:01:40 â€¢ 0:02:02[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6327, 0.3673]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 92/203 [2m0:01:40 â€¢ 0:02:02[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6329, 0.3671]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 92/203 [2m0:01:40 â€¢ 0:02:02[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6331, 0.3669]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 92/203 [2m0:01:40 â€¢ 0:02:02[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6332, 0.3668]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 92/203 [2m0:01:40 â€¢ 0:02:02[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6333, 0.3667]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 92/203 [2m0:01:40 â€¢ 0:02:02[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6334, 0.3666]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 92/203 [2m0:01:40 â€¢ 0:02:02[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6335, 0.3665]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 92/203 [2m0:01:40 â€¢ 0:02:02[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6336, 0.3664]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 92/203 [2m0:01:40 â€¢ 0:02:02[0m [2;4m0.91it/s[0m  
[2KStep 20 - TTA Loss: 0.9949â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 92/203 [2m0:01:40 â€¢ 0:02:02[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6337, 0.3663]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 92/203 [2m0:01:40 â€¢ 0:02:02[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6338, 0.3662]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 92/203 [2m0:01:40 â€¢ 0:02:02[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6339, 0.3661]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 92/203 [2m0:01:40 â€¢ 0:02:02[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6340, 0.3660]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 92/203 [2m0:01:40 â€¢ 0:02:02[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6340, 0.3660]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 92/203 [2m0:01:40 â€¢ 0:02:02[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6340, 0.3660]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 92/203 [2m0:01:40 â€¢ 0:02:02[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6341, 0.3659]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 92/203 [2m0:01:40 â€¢ 0:02:02[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6341, 0.3659]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 92/203 [2m0:01:40 â€¢ 0:02:02[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6341, 0.3659]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 92/203 [2m0:01:40 â€¢ 0:02:02[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5792, 0.4208]], device='cuda:0');5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 92/203 [2m0:01:40 â€¢ 0:02:02[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6341, 0.3659],38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 92/203 [2m0:01:40 â€¢ 0:02:02[0m [2;4m0.91it/s[0m  
        [0.5930, 0.4070],
        [0.5891, 0.4109],
        [0.6476, 0.3524],
        [0.6050, 0.3950],
        [0.6135, 0.3865],
        [0.5660, 0.4340],
        [0.6570, 0.3430],
        [0.6625, 0.3375],
[2K        [0.6558, 0.3442]], device='cuda:0')[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 92/203 [2m0:01:40 â€¢ 0:02:02[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5790, 0.4210],0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 93/203 [2m0:01:41 â€¢ 0:02:01[0m [2;4m0.91it/s[0m  
        [0.5735, 0.4265],
        [0.5779, 0.4221],
        [0.5772, 0.4228],
        [0.5788, 0.4212],
        [0.5766, 0.4234],
        [0.5749, 0.4251],
        [0.5785, 0.4215],
        [0.5785, 0.4215],
[2K        [0.5790, 0.4210]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 93/203 [2m0:01:41 â€¢ 0:02:01[0m [2;4m0.91it/s[0m  
[2KClass label: SoccerPenaltyâ”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 93/203 [2m0:01:41 â€¢ 0:02:01[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6573, 0.3427]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 93/203 [2m0:01:41 â€¢ 0:02:01[0m [2;4m0.91it/s[0m  
[2KStep 0 - TTA Loss: 0.6648â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 93/203 [2m0:01:41 â€¢ 0:02:01[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6573, 0.3427]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 93/203 [2m0:01:41 â€¢ 0:02:01[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6573, 0.3427]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 93/203 [2m0:01:41 â€¢ 0:02:01[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6573, 0.3427]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 93/203 [2m0:01:41 â€¢ 0:02:01[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6574, 0.3426]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 93/203 [2m0:01:41 â€¢ 0:02:01[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6574, 0.3426]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 93/203 [2m0:01:41 â€¢ 0:02:01[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6575, 0.3425]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 93/203 [2m0:01:41 â€¢ 0:02:01[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6577, 0.3423]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 93/203 [2m0:01:41 â€¢ 0:02:01[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6580, 0.3420]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 93/203 [2m0:01:41 â€¢ 0:02:01[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6584, 0.3416]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 93/203 [2m0:01:41 â€¢ 0:02:01[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6590, 0.3410]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 93/203 [2m0:01:41 â€¢ 0:02:01[0m [2;4m0.91it/s[0m  
[2KStep 10 - TTA Loss: 0.7640â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 93/203 [2m0:01:41 â€¢ 0:02:01[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6596, 0.3404]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 93/203 [2m0:01:41 â€¢ 0:02:01[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6604, 0.3396]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 93/203 [2m0:01:41 â€¢ 0:02:01[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6614, 0.3386]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 93/203 [2m0:01:41 â€¢ 0:02:01[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6625, 0.3375]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 93/203 [2m0:01:41 â€¢ 0:02:01[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6637, 0.3363]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 93/203 [2m0:01:41 â€¢ 0:02:01[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6652, 0.3348]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 93/203 [2m0:01:41 â€¢ 0:02:01[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6668, 0.3332]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 93/203 [2m0:01:41 â€¢ 0:02:01[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6686, 0.3314]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 93/203 [2m0:01:41 â€¢ 0:02:01[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6707, 0.3293]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 93/203 [2m0:01:41 â€¢ 0:02:01[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6729, 0.3271]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 93/203 [2m0:01:41 â€¢ 0:02:01[0m [2;4m0.91it/s[0m  
[2KStep 20 - TTA Loss: 0.6851â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 93/203 [2m0:01:41 â€¢ 0:02:01[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6753, 0.3247]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 93/203 [2m0:01:41 â€¢ 0:02:01[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6779, 0.3221]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 93/203 [2m0:01:41 â€¢ 0:02:01[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6808, 0.3192]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 93/203 [2m0:01:41 â€¢ 0:02:01[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6837, 0.3163]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 93/203 [2m0:01:41 â€¢ 0:02:01[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6866, 0.3134]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 93/203 [2m0:01:41 â€¢ 0:02:01[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6897, 0.3103]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 93/203 [2m0:01:41 â€¢ 0:02:01[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6929, 0.3071]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 93/203 [2m0:01:41 â€¢ 0:02:01[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6963, 0.3037]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 93/203 [2m0:01:41 â€¢ 0:02:01[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6997, 0.3003]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 93/203 [2m0:01:41 â€¢ 0:02:01[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5807, 0.4193]], device='cuda:0');5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 93/203 [2m0:01:41 â€¢ 0:02:01[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6565, 0.3435],38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 93/203 [2m0:01:41 â€¢ 0:02:01[0m [2;4m0.91it/s[0m  
        [0.6161, 0.3839],
        [0.6022, 0.3978],
        [0.6671, 0.3329],
        [0.6310, 0.3690],
        [0.6395, 0.3605],
        [0.5925, 0.4075],
        [0.7034, 0.2966],
        [0.6825, 0.3175],
[2K        [0.6767, 0.3233]], device='cuda:0')[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 93/203 [2m0:01:41 â€¢ 0:02:01[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5792, 0.4208],0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 94/203 [2m0:01:42 â€¢ 0:02:00[0m [2;4m0.91it/s[0m  
        [0.5736, 0.4264],
        [0.5780, 0.4220],
        [0.5774, 0.4226],
        [0.5789, 0.4211],
        [0.5767, 0.4233],
        [0.5750, 0.4250],
        [0.5787, 0.4213],
        [0.5787, 0.4213],
[2K        [0.5791, 0.4209]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 94/203 [2m0:01:42 â€¢ 0:02:00[0m [2;4m0.91it/s[0m  
[2KClass label: SoccerPenaltyâ”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 94/203 [2m0:01:42 â€¢ 0:02:00[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6561, 0.3439]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 94/203 [2m0:01:42 â€¢ 0:02:00[0m [2;4m0.91it/s[0m  
[2KStep 0 - TTA Loss: 0.6613â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 94/203 [2m0:01:42 â€¢ 0:02:00[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6600, 0.3400]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 94/203 [2m0:01:42 â€¢ 0:02:00[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6639, 0.3361]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 94/203 [2m0:01:42 â€¢ 0:02:00[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6677, 0.3323]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 94/203 [2m0:01:42 â€¢ 0:02:00[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6715, 0.3285]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 94/203 [2m0:01:42 â€¢ 0:02:00[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6752, 0.3248]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 94/203 [2m0:01:42 â€¢ 0:02:00[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6789, 0.3211]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 94/203 [2m0:01:42 â€¢ 0:02:00[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6825, 0.3175]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 94/203 [2m0:01:42 â€¢ 0:02:00[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6860, 0.3140]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 94/203 [2m0:01:42 â€¢ 0:02:00[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6894, 0.3106]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 94/203 [2m0:01:42 â€¢ 0:02:00[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6925, 0.3075]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 94/203 [2m0:01:42 â€¢ 0:02:00[0m [2;4m0.91it/s[0m  
[2KStep 10 - TTA Loss: 0.6907â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 94/203 [2m0:01:42 â€¢ 0:02:00[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6954, 0.3046]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 94/203 [2m0:01:42 â€¢ 0:02:00[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6981, 0.3019]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 94/203 [2m0:01:42 â€¢ 0:02:00[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.7006, 0.2994]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 94/203 [2m0:01:42 â€¢ 0:02:00[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.7029, 0.2971]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 94/203 [2m0:01:42 â€¢ 0:02:00[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.7049, 0.2951]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 94/203 [2m0:01:42 â€¢ 0:02:00[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.7069, 0.2931]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 94/203 [2m0:01:42 â€¢ 0:02:00[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.7086, 0.2914]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 94/203 [2m0:01:42 â€¢ 0:02:00[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.7102, 0.2898]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 94/203 [2m0:01:42 â€¢ 0:02:00[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.7116, 0.2884]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 94/203 [2m0:01:42 â€¢ 0:02:00[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.7127, 0.2873]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 94/203 [2m0:01:42 â€¢ 0:02:00[0m [2;4m0.91it/s[0m  
[2KStep 20 - TTA Loss: 0.6845â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 94/203 [2m0:01:42 â€¢ 0:02:00[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.7138, 0.2862]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 94/203 [2m0:01:42 â€¢ 0:02:00[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.7146, 0.2854]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 94/203 [2m0:01:42 â€¢ 0:02:00[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.7153, 0.2847]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 94/203 [2m0:01:42 â€¢ 0:02:00[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.7158, 0.2842]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 94/203 [2m0:01:42 â€¢ 0:02:00[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.7162, 0.2838]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 94/203 [2m0:01:42 â€¢ 0:02:00[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.7165, 0.2835]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 94/203 [2m0:01:42 â€¢ 0:02:00[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.7167, 0.2833]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 94/203 [2m0:01:42 â€¢ 0:02:00[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.7168, 0.2832]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 94/203 [2m0:01:42 â€¢ 0:02:00[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.7169, 0.2831]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 94/203 [2m0:01:42 â€¢ 0:02:00[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5815, 0.4185]], device='cuda:0')m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 94/203 [2m0:01:42 â€¢ 0:02:00[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6621, 0.3379],38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 94/203 [2m0:01:42 â€¢ 0:02:00[0m [2;4m0.91it/s[0m  
        [0.6232, 0.3768],
        [0.6060, 0.3940],
        [0.6687, 0.3313],
        [0.6372, 0.3628],
        [0.6470, 0.3530],
        [0.5989, 0.4011],
        [0.7169, 0.2831],
        [0.6866, 0.3134],
[2K        [0.6816, 0.3184]], device='cuda:0')[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 94/203 [2m0:01:42 â€¢ 0:02:00[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5793, 0.4207],0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 95/203 [2m0:01:43 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
        [0.5737, 0.4263],
        [0.5782, 0.4218],
        [0.5776, 0.4224],
        [0.5790, 0.4210],
        [0.5768, 0.4232],
        [0.5751, 0.4249],
        [0.5789, 0.4211],
        [0.5789, 0.4211],
[2K        [0.5792, 0.4208]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 95/203 [2m0:01:43 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 95/203 [2m0:01:43 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6544, 0.3456]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 95/203 [2m0:01:43 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KStep 0 - TTA Loss: 0.7191â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 95/203 [2m0:01:43 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6544, 0.3456]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 95/203 [2m0:01:43 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6544, 0.3456]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 95/203 [2m0:01:43 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6545, 0.3455]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 95/203 [2m0:01:43 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6545, 0.3455]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 95/203 [2m0:01:43 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6546, 0.3454]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 95/203 [2m0:01:43 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6548, 0.3452]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 95/203 [2m0:01:43 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6551, 0.3449]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 95/203 [2m0:01:43 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6554, 0.3446]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 95/203 [2m0:01:43 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6559, 0.3441]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 95/203 [2m0:01:43 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6565, 0.3435]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 95/203 [2m0:01:43 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KStep 10 - TTA Loss: 0.6094â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 95/203 [2m0:01:43 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6573, 0.3427]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 95/203 [2m0:01:43 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6582, 0.3418]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 95/203 [2m0:01:43 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6593, 0.3407]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 95/203 [2m0:01:43 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6606, 0.3394]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 95/203 [2m0:01:43 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6621, 0.3379]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 95/203 [2m0:01:43 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6638, 0.3362]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 95/203 [2m0:01:43 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6657, 0.3343]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 95/203 [2m0:01:43 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6678, 0.3322]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 95/203 [2m0:01:43 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6700, 0.3300]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 95/203 [2m0:01:43 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6724, 0.3276]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 95/203 [2m0:01:43 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KStep 20 - TTA Loss: 0.6949â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 95/203 [2m0:01:43 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6750, 0.3250]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 95/203 [2m0:01:43 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6775, 0.3225]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 95/203 [2m0:01:43 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6802, 0.3198]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 95/203 [2m0:01:43 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6831, 0.3169]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 95/203 [2m0:01:43 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6863, 0.3137]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 95/203 [2m0:01:43 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6895, 0.3105]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 95/203 [2m0:01:43 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6930, 0.3070]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 95/203 [2m0:01:43 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6965, 0.3035]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 95/203 [2m0:01:43 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.7002, 0.2998]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 95/203 [2m0:01:43 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.5822, 0.4178]], device='cuda:0')m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 95/203 [2m0:01:43 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6764, 0.3236],38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 95/203 [2m0:01:43 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
        [0.6239, 0.3761],
        [0.6225, 0.3775],
        [0.6856, 0.3144],
        [0.6540, 0.3460],
        [0.6590, 0.3410],
        [0.6115, 0.3885],
        [0.6921, 0.3079],
        [0.6992, 0.3008],
[2K        [0.7040, 0.2960]], device='cuda:0')[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 95/203 [2m0:01:43 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.5795, 0.4205],0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 96/203 [2m0:01:44 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
        [0.5739, 0.4261],
        [0.5786, 0.4214],
        [0.5778, 0.4222],
        [0.5794, 0.4206],
        [0.5772, 0.4228],
        [0.5754, 0.4246],
        [0.5792, 0.4208],
        [0.5791, 0.4209],
[2K        [0.5799, 0.4201]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 96/203 [2m0:01:44 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 96/203 [2m0:01:44 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.5698, 0.4302]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 96/203 [2m0:01:44 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KStep 0 - TTA Loss: 0.6454â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 96/203 [2m0:01:44 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.5730, 0.4270]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 96/203 [2m0:01:44 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.5760, 0.4240]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 96/203 [2m0:01:44 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.5791, 0.4209]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 96/203 [2m0:01:44 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.5820, 0.4180]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 96/203 [2m0:01:44 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.5848, 0.4152]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 96/203 [2m0:01:44 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.5875, 0.4125]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 96/203 [2m0:01:44 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.5901, 0.4099]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 96/203 [2m0:01:44 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.5926, 0.4074]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 96/203 [2m0:01:44 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.5950, 0.4050]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 96/203 [2m0:01:44 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.5973, 0.4027]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 96/203 [2m0:01:44 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KStep 10 - TTA Loss: 0.6036â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 96/203 [2m0:01:44 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.5994, 0.4006]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 96/203 [2m0:01:44 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6014, 0.3986]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 96/203 [2m0:01:44 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6033, 0.3967]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 96/203 [2m0:01:44 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6050, 0.3950]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 96/203 [2m0:01:44 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6065, 0.3935]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 96/203 [2m0:01:44 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6080, 0.3920]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 96/203 [2m0:01:44 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6092, 0.3908]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 96/203 [2m0:01:44 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6103, 0.3897]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 96/203 [2m0:01:44 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6113, 0.3887]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 96/203 [2m0:01:44 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6122, 0.3878]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 96/203 [2m0:01:44 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KStep 20 - TTA Loss: 0.5882â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 96/203 [2m0:01:44 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6129, 0.3871]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 96/203 [2m0:01:44 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6135, 0.3865]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 96/203 [2m0:01:44 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6140, 0.3860]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 96/203 [2m0:01:44 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6144, 0.3856]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 96/203 [2m0:01:44 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6147, 0.3853]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 96/203 [2m0:01:44 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6149, 0.3851]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 96/203 [2m0:01:44 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6150, 0.3850]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 96/203 [2m0:01:44 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6151, 0.3849]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 96/203 [2m0:01:44 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6151, 0.3849]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 96/203 [2m0:01:44 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.5772, 0.4228]], device='cuda:0')m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 96/203 [2m0:01:44 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6700, 0.3300],38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 96/203 [2m0:01:44 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
        [0.6208, 0.3792],
        [0.6213, 0.3787],
        [0.6795, 0.3205],
        [0.6492, 0.3508],
        [0.6582, 0.3418],
        [0.6152, 0.3848],
        [0.6872, 0.3128],
        [0.6938, 0.3062],
[2K        [0.6983, 0.3017]], device='cuda:0')[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 96/203 [2m0:01:44 â€¢ 0:01:54[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.5797, 0.4203],[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 97/203 [2m0:01:45 â€¢ 0:01:52[0m [2;4m0.95it/s[0m   
        [0.5740, 0.4260],
        [0.5788, 0.4212],
        [0.5779, 0.4221],
        [0.5796, 0.4204],
        [0.5773, 0.4227],
        [0.5755, 0.4245],
        [0.5793, 0.4207],
        [0.5792, 0.4208],
[2K        [0.5803, 0.4197]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 97/203 [2m0:01:45 â€¢ 0:01:52[0m [2;4m0.95it/s[0m  
[2KClass label: TennisSwingâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 97/203 [2m0:01:45 â€¢ 0:01:52[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6662, 0.3338]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 97/203 [2m0:01:45 â€¢ 0:01:52[0m [2;4m0.95it/s[0m  
[2KStep 0 - TTA Loss: 0.4842â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 97/203 [2m0:01:45 â€¢ 0:01:52[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6662, 0.3338]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 97/203 [2m0:01:45 â€¢ 0:01:52[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6663, 0.3337]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 97/203 [2m0:01:45 â€¢ 0:01:52[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6663, 0.3337]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 97/203 [2m0:01:45 â€¢ 0:01:52[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6663, 0.3337]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 97/203 [2m0:01:45 â€¢ 0:01:52[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6664, 0.3336]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 97/203 [2m0:01:45 â€¢ 0:01:52[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6665, 0.3335]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 97/203 [2m0:01:45 â€¢ 0:01:52[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6667, 0.3333]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 97/203 [2m0:01:45 â€¢ 0:01:52[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6670, 0.3330]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 97/203 [2m0:01:45 â€¢ 0:01:52[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6674, 0.3326]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 97/203 [2m0:01:45 â€¢ 0:01:52[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6679, 0.3321]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 97/203 [2m0:01:45 â€¢ 0:01:52[0m [2;4m0.95it/s[0m  
[2KStep 10 - TTA Loss: 0.4639â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 97/203 [2m0:01:45 â€¢ 0:01:52[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6686, 0.3314]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 97/203 [2m0:01:45 â€¢ 0:01:52[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6693, 0.3307]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 97/203 [2m0:01:45 â€¢ 0:01:52[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6703, 0.3297]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 97/203 [2m0:01:45 â€¢ 0:01:52[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6713, 0.3287]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 97/203 [2m0:01:45 â€¢ 0:01:52[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6725, 0.3275]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 97/203 [2m0:01:45 â€¢ 0:01:52[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6739, 0.3261]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 97/203 [2m0:01:45 â€¢ 0:01:52[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6755, 0.3245]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 97/203 [2m0:01:45 â€¢ 0:01:52[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6771, 0.3229]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 97/203 [2m0:01:45 â€¢ 0:01:52[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6790, 0.3210]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 97/203 [2m0:01:45 â€¢ 0:01:52[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6810, 0.3190]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 97/203 [2m0:01:45 â€¢ 0:01:52[0m [2;4m0.95it/s[0m  
[2KStep 20 - TTA Loss: 0.5106â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 97/203 [2m0:01:45 â€¢ 0:01:52[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6832, 0.3168]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 97/203 [2m0:01:45 â€¢ 0:01:52[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6856, 0.3144]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 97/203 [2m0:01:45 â€¢ 0:01:52[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6882, 0.3118]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 97/203 [2m0:01:45 â€¢ 0:01:52[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6910, 0.3090]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 97/203 [2m0:01:45 â€¢ 0:01:52[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6939, 0.3061]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 97/203 [2m0:01:45 â€¢ 0:01:52[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6969, 0.3031]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 97/203 [2m0:01:45 â€¢ 0:01:52[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.7000, 0.3000]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 97/203 [2m0:01:45 â€¢ 0:01:52[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.7033, 0.2967]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 97/203 [2m0:01:45 â€¢ 0:01:52[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.7066, 0.2934]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 97/203 [2m0:01:45 â€¢ 0:01:52[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.5814, 0.4186]], device='cuda:0')8;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 97/203 [2m0:01:45 â€¢ 0:01:52[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6645, 0.3355],[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 97/203 [2m0:01:45 â€¢ 0:01:52[0m [2;4m0.95it/s[0m  
        [0.6254, 0.3746],
        [0.6171, 0.3829],
        [0.6796, 0.3204],
        [0.6367, 0.3633],
        [0.6481, 0.3519],
        [0.5947, 0.4053],
        [0.6803, 0.3197],
        [0.7100, 0.2900],
[2K        [0.6876, 0.3124]], device='cuda:0')[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 97/203 [2m0:01:45 â€¢ 0:01:52[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.5799, 0.4201],[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 98/203 [2m0:01:46 â€¢ 0:01:51[0m [2;4m0.95it/s[0m  
        [0.5741, 0.4259],
        [0.5789, 0.4211],
        [0.5781, 0.4219],
        [0.5798, 0.4202],
        [0.5774, 0.4226],
        [0.5756, 0.4244],
        [0.5794, 0.4206],
        [0.5795, 0.4205],
[2K        [0.5805, 0.4195]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 98/203 [2m0:01:46 â€¢ 0:01:51[0m [2;4m0.95it/s[0m  
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 98/203 [2m0:01:46 â€¢ 0:01:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6174, 0.3826]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 98/203 [2m0:01:46 â€¢ 0:01:51[0m [2;4m0.95it/s[0m  
[2KStep 0 - TTA Loss: 1.1174â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 98/203 [2m0:01:46 â€¢ 0:01:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6193, 0.3807]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 98/203 [2m0:01:46 â€¢ 0:01:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6210, 0.3790]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 98/203 [2m0:01:46 â€¢ 0:01:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6226, 0.3774]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 98/203 [2m0:01:46 â€¢ 0:01:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6242, 0.3758]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 98/203 [2m0:01:46 â€¢ 0:01:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6255, 0.3745]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 98/203 [2m0:01:46 â€¢ 0:01:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6267, 0.3733]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 98/203 [2m0:01:46 â€¢ 0:01:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6278, 0.3722]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 98/203 [2m0:01:46 â€¢ 0:01:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6287, 0.3713]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 98/203 [2m0:01:46 â€¢ 0:01:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6295, 0.3705]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 98/203 [2m0:01:46 â€¢ 0:01:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6302, 0.3698]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 98/203 [2m0:01:46 â€¢ 0:01:51[0m [2;4m0.95it/s[0m  
[2KStep 10 - TTA Loss: 1.0756â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 98/203 [2m0:01:46 â€¢ 0:01:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6306, 0.3694]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 98/203 [2m0:01:46 â€¢ 0:01:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6310, 0.3690]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 98/203 [2m0:01:46 â€¢ 0:01:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6314, 0.3686]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 98/203 [2m0:01:46 â€¢ 0:01:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6317, 0.3683]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 98/203 [2m0:01:46 â€¢ 0:01:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6319, 0.3681]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 98/203 [2m0:01:46 â€¢ 0:01:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6321, 0.3679]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 98/203 [2m0:01:46 â€¢ 0:01:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6323, 0.3677]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 98/203 [2m0:01:46 â€¢ 0:01:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6324, 0.3676]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 98/203 [2m0:01:46 â€¢ 0:01:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6325, 0.3675]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 98/203 [2m0:01:46 â€¢ 0:01:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6326, 0.3674]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 98/203 [2m0:01:46 â€¢ 0:01:51[0m [2;4m0.95it/s[0m  
[2KStep 20 - TTA Loss: 0.9861â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 98/203 [2m0:01:46 â€¢ 0:01:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6327, 0.3673]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 98/203 [2m0:01:46 â€¢ 0:01:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6327, 0.3673]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 98/203 [2m0:01:46 â€¢ 0:01:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6328, 0.3672]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 98/203 [2m0:01:46 â€¢ 0:01:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6328, 0.3672]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 98/203 [2m0:01:46 â€¢ 0:01:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6328, 0.3672]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 98/203 [2m0:01:46 â€¢ 0:01:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6328, 0.3672]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 98/203 [2m0:01:46 â€¢ 0:01:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6328, 0.3672]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 98/203 [2m0:01:46 â€¢ 0:01:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6328, 0.3672]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 98/203 [2m0:01:46 â€¢ 0:01:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6328, 0.3672]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 98/203 [2m0:01:46 â€¢ 0:01:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.5781, 0.4219]], device='cuda:0')8;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 98/203 [2m0:01:46 â€¢ 0:01:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6505, 0.3495],[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 98/203 [2m0:01:46 â€¢ 0:01:51[0m [2;4m0.95it/s[0m  
        [0.6094, 0.3906],
        [0.6026, 0.3974],
        [0.6650, 0.3350],
        [0.6220, 0.3780],
        [0.6329, 0.3671],
        [0.5811, 0.4189],
        [0.6684, 0.3316],
        [0.6917, 0.3083],
[2K        [0.6729, 0.3271]], device='cuda:0')[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 98/203 [2m0:01:46 â€¢ 0:01:51[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.5799, 0.4201],[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 99/203 [2m0:01:47 â€¢ 0:01:49[0m [2;4m0.96it/s[0m  
        [0.5742, 0.4258],
        [0.5789, 0.4211],
        [0.5781, 0.4219],
        [0.5797, 0.4203],
        [0.5773, 0.4227],
        [0.5756, 0.4244],
        [0.5794, 0.4206],
        [0.5795, 0.4205],
[2K        [0.5805, 0.4195]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 99/203 [2m0:01:47 â€¢ 0:01:49[0m [2;4m0.96it/s[0m  
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 99/203 [2m0:01:47 â€¢ 0:01:49[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6225, 0.3775]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 99/203 [2m0:01:47 â€¢ 0:01:49[0m [2;4m0.96it/s[0m  
[2KStep 0 - TTA Loss: 0.6463â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 99/203 [2m0:01:47 â€¢ 0:01:49[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6225, 0.3775]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 99/203 [2m0:01:47 â€¢ 0:01:49[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6225, 0.3775]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 99/203 [2m0:01:47 â€¢ 0:01:49[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6225, 0.3775]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 99/203 [2m0:01:47 â€¢ 0:01:49[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6226, 0.3774]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 99/203 [2m0:01:47 â€¢ 0:01:49[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6227, 0.3773]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 99/203 [2m0:01:47 â€¢ 0:01:49[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6229, 0.3771]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 99/203 [2m0:01:47 â€¢ 0:01:49[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6232, 0.3768]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 99/203 [2m0:01:47 â€¢ 0:01:49[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6237, 0.3763]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 99/203 [2m0:01:47 â€¢ 0:01:49[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6244, 0.3756]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 99/203 [2m0:01:47 â€¢ 0:01:49[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6252, 0.3748]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 99/203 [2m0:01:47 â€¢ 0:01:49[0m [2;4m0.96it/s[0m  
[2KStep 10 - TTA Loss: 0.7063â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 99/203 [2m0:01:47 â€¢ 0:01:49[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6264, 0.3736]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 99/203 [2m0:01:47 â€¢ 0:01:49[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6277, 0.3723]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 99/203 [2m0:01:47 â€¢ 0:01:49[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6293, 0.3707]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 99/203 [2m0:01:47 â€¢ 0:01:49[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6313, 0.3687]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 99/203 [2m0:01:47 â€¢ 0:01:49[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6336, 0.3664]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 99/203 [2m0:01:47 â€¢ 0:01:49[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6362, 0.3638]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 99/203 [2m0:01:47 â€¢ 0:01:49[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6391, 0.3609]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 99/203 [2m0:01:47 â€¢ 0:01:49[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6424, 0.3576]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 99/203 [2m0:01:47 â€¢ 0:01:49[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6461, 0.3539]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 99/203 [2m0:01:47 â€¢ 0:01:49[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6500, 0.3500]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 99/203 [2m0:01:47 â€¢ 0:01:49[0m [2;4m0.96it/s[0m  
[2KStep 20 - TTA Loss: 0.7321â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 99/203 [2m0:01:47 â€¢ 0:01:49[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6543, 0.3457]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 99/203 [2m0:01:47 â€¢ 0:01:49[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6589, 0.3411]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 99/203 [2m0:01:47 â€¢ 0:01:49[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6638, 0.3362]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 99/203 [2m0:01:47 â€¢ 0:01:49[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6689, 0.3311]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 99/203 [2m0:01:47 â€¢ 0:01:49[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6744, 0.3256]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 99/203 [2m0:01:47 â€¢ 0:01:49[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6802, 0.3198]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 99/203 [2m0:01:47 â€¢ 0:01:49[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6863, 0.3137]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 99/203 [2m0:01:47 â€¢ 0:01:49[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6925, 0.3075]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 99/203 [2m0:01:47 â€¢ 0:01:49[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6990, 0.3010]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 99/203 [2m0:01:47 â€¢ 0:01:49[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.5816, 0.4184]], device='cuda:0')0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 99/203 [2m0:01:47 â€¢ 0:01:49[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6973, 0.3027],[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 99/203 [2m0:01:47 â€¢ 0:01:49[0m [2;4m0.96it/s[0m  
        [0.6416, 0.3584],
        [0.6496, 0.3504],
        [0.7011, 0.2989],
        [0.6794, 0.3206],
        [0.7057, 0.2943],
        [0.6472, 0.3528],
        [0.7129, 0.2871],
        [0.7206, 0.2794],
[2K        [0.7236, 0.2764]], device='cuda:0')[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 99/203 [2m0:01:47 â€¢ 0:01:49[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.5801, 0.4199],[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 100/203 [2m0:01:48 â€¢ 0:01:48[0m [2;4m0.96it/s[0m  
        [0.5744, 0.4256],
        [0.5791, 0.4209],
        [0.5784, 0.4216],
        [0.5800, 0.4200],
        [0.5779, 0.4221],
        [0.5759, 0.4241],
        [0.5797, 0.4203],
        [0.5799, 0.4201],
[2K        [0.5807, 0.4193]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 100/203 [2m0:01:48 â€¢ 0:01:48[0m [2;4m0.96it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 100/203 [2m0:01:48 â€¢ 0:01:48[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.5750, 0.4250]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 100/203 [2m0:01:48 â€¢ 0:01:48[0m [2;4m0.96it/s[0m  
[2KStep 0 - TTA Loss: 0.7283â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 100/203 [2m0:01:48 â€¢ 0:01:48[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.5811, 0.4189]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 100/203 [2m0:01:48 â€¢ 0:01:48[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.5869, 0.4131]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 100/203 [2m0:01:48 â€¢ 0:01:48[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.5923, 0.4077]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 100/203 [2m0:01:48 â€¢ 0:01:48[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.5973, 0.4027]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 100/203 [2m0:01:48 â€¢ 0:01:48[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6020, 0.3980]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 100/203 [2m0:01:48 â€¢ 0:01:48[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6064, 0.3936]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 100/203 [2m0:01:48 â€¢ 0:01:48[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6105, 0.3895]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 100/203 [2m0:01:48 â€¢ 0:01:48[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6144, 0.3856]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 100/203 [2m0:01:48 â€¢ 0:01:48[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6179, 0.3821]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 100/203 [2m0:01:48 â€¢ 0:01:48[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6212, 0.3788]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 100/203 [2m0:01:48 â€¢ 0:01:48[0m [2;4m0.96it/s[0m  
[2KStep 10 - TTA Loss: 0.6675â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 100/203 [2m0:01:48 â€¢ 0:01:48[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6242, 0.3758]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 100/203 [2m0:01:48 â€¢ 0:01:48[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6269, 0.3731]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 100/203 [2m0:01:48 â€¢ 0:01:48[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6295, 0.3705]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 100/203 [2m0:01:48 â€¢ 0:01:48[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6318, 0.3682]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 100/203 [2m0:01:48 â€¢ 0:01:48[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6338, 0.3662]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 100/203 [2m0:01:48 â€¢ 0:01:48[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6357, 0.3643]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 100/203 [2m0:01:48 â€¢ 0:01:48[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6373, 0.3627]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 100/203 [2m0:01:48 â€¢ 0:01:48[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6388, 0.3612]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 100/203 [2m0:01:48 â€¢ 0:01:48[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6400, 0.3600]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 100/203 [2m0:01:48 â€¢ 0:01:48[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6411, 0.3589]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 100/203 [2m0:01:48 â€¢ 0:01:48[0m [2;4m0.96it/s[0m  
[2KStep 20 - TTA Loss: 0.7091â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 100/203 [2m0:01:48 â€¢ 0:01:48[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6420, 0.3580]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 100/203 [2m0:01:48 â€¢ 0:01:48[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6428, 0.3572]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 100/203 [2m0:01:48 â€¢ 0:01:48[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6434, 0.3566]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 100/203 [2m0:01:48 â€¢ 0:01:48[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6438, 0.3562]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 100/203 [2m0:01:48 â€¢ 0:01:48[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6442, 0.3558]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 100/203 [2m0:01:48 â€¢ 0:01:48[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6444, 0.3556]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 100/203 [2m0:01:48 â€¢ 0:01:48[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6446, 0.3554]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 100/203 [2m0:01:48 â€¢ 0:01:48[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6446, 0.3554]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 100/203 [2m0:01:48 â€¢ 0:01:48[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6447, 0.3553]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 100/203 [2m0:01:48 â€¢ 0:01:48[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.5789, 0.4211]], device='cuda:0')0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 100/203 [2m0:01:48 â€¢ 0:01:48[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6836, 0.3164],[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 100/203 [2m0:01:48 â€¢ 0:01:48[0m [2;4m0.96it/s[0m  
        [0.6348, 0.3652],
        [0.6385, 0.3615],
        [0.6912, 0.3088],
        [0.6694, 0.3306],
        [0.6905, 0.3095],
        [0.6448, 0.3552],
        [0.7055, 0.2945],
        [0.7078, 0.2922],
[2K        [0.7134, 0.2866]], device='cuda:0')[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 100/203 [2m0:01:48 â€¢ 0:01:48[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.5801, 0.4199],[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 101/203 [2m0:01:49 â€¢ 0:01:46[0m [2;4m0.97it/s[0m  
        [0.5746, 0.4254],
        [0.5792, 0.4208],
        [0.5785, 0.4215],
        [0.5802, 0.4198],
        [0.5781, 0.4219],
        [0.5761, 0.4239],
        [0.5799, 0.4201],
        [0.5800, 0.4200],
[2K        [0.5809, 0.4191]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 101/203 [2m0:01:49 â€¢ 0:01:46[0m [2;4m0.97it/s[0m  
[2KClass label: Billiardsâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 101/203 [2m0:01:49 â€¢ 0:01:46[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6003, 0.3997]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 101/203 [2m0:01:49 â€¢ 0:01:46[0m [2;4m0.97it/s[0m  
[2KStep 0 - TTA Loss: 0.5673â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 101/203 [2m0:01:49 â€¢ 0:01:46[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6003, 0.3997]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 101/203 [2m0:01:49 â€¢ 0:01:46[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6003, 0.3997]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 101/203 [2m0:01:49 â€¢ 0:01:46[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6003, 0.3997]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 101/203 [2m0:01:49 â€¢ 0:01:46[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6003, 0.3997]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 101/203 [2m0:01:49 â€¢ 0:01:46[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6004, 0.3996]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 101/203 [2m0:01:49 â€¢ 0:01:46[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6004, 0.3996]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 101/203 [2m0:01:49 â€¢ 0:01:46[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6005, 0.3995]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 101/203 [2m0:01:49 â€¢ 0:01:46[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6006, 0.3994]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 101/203 [2m0:01:49 â€¢ 0:01:46[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6006, 0.3994]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 101/203 [2m0:01:49 â€¢ 0:01:46[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6007, 0.3993]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 101/203 [2m0:01:49 â€¢ 0:01:46[0m [2;4m0.97it/s[0m  
[2KStep 10 - TTA Loss: 0.5717â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 101/203 [2m0:01:49 â€¢ 0:01:46[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6008, 0.3992]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 101/203 [2m0:01:49 â€¢ 0:01:46[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6009, 0.3991]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 101/203 [2m0:01:49 â€¢ 0:01:46[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6010, 0.3990]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 101/203 [2m0:01:49 â€¢ 0:01:46[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6012, 0.3988]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 101/203 [2m0:01:49 â€¢ 0:01:46[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6013, 0.3987]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 101/203 [2m0:01:49 â€¢ 0:01:46[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6014, 0.3986]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 101/203 [2m0:01:49 â€¢ 0:01:46[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6016, 0.3984]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 101/203 [2m0:01:49 â€¢ 0:01:46[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6017, 0.3983]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 101/203 [2m0:01:49 â€¢ 0:01:46[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6019, 0.3981]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 101/203 [2m0:01:49 â€¢ 0:01:46[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6022, 0.3978]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 101/203 [2m0:01:49 â€¢ 0:01:46[0m [2;4m0.97it/s[0m  
[2KStep 20 - TTA Loss: 0.5384â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 101/203 [2m0:01:49 â€¢ 0:01:46[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6024, 0.3976]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 101/203 [2m0:01:49 â€¢ 0:01:46[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6027, 0.3973]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 101/203 [2m0:01:49 â€¢ 0:01:46[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6031, 0.3969]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 101/203 [2m0:01:49 â€¢ 0:01:46[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6035, 0.3965]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 101/203 [2m0:01:49 â€¢ 0:01:46[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6039, 0.3961]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 101/203 [2m0:01:49 â€¢ 0:01:46[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6045, 0.3955]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 101/203 [2m0:01:49 â€¢ 0:01:46[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6051, 0.3949]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 101/203 [2m0:01:49 â€¢ 0:01:46[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6058, 0.3942]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 101/203 [2m0:01:49 â€¢ 0:01:46[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6066, 0.3934]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 101/203 [2m0:01:49 â€¢ 0:01:46[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.5746, 0.4254]], device='cuda:0')0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 101/203 [2m0:01:49 â€¢ 0:01:46[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6406, 0.3594],[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 101/203 [2m0:01:49 â€¢ 0:01:46[0m [2;4m0.97it/s[0m  
        [0.6074, 0.3926],
        [0.5971, 0.4029],
        [0.6540, 0.3460],
        [0.6121, 0.3879],
        [0.6244, 0.3756],
        [0.5780, 0.4220],
        [0.6635, 0.3365],
        [0.6699, 0.3301],
[2K        [0.6624, 0.3376]], device='cuda:0')[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 101/203 [2m0:01:49 â€¢ 0:01:46[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.5799, 0.4201],[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 102/203 [2m0:01:50 â€¢ 0:01:45[0m [2;4m0.97it/s[0m   
        [0.5743, 0.4257],
        [0.5790, 0.4210],
        [0.5782, 0.4218],
        [0.5800, 0.4200],
        [0.5780, 0.4220],
        [0.5759, 0.4241],
        [0.5797, 0.4203],
        [0.5797, 0.4203],
[2K        [0.5807, 0.4193]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 102/203 [2m0:01:50 â€¢ 0:01:45[0m [2;4m0.97it/s[0m  
[2KClass label: CleanAndJerkâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 102/203 [2m0:01:50 â€¢ 0:01:45[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.5920, 0.4080]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 102/203 [2m0:01:50 â€¢ 0:01:45[0m [2;4m0.97it/s[0m  
[2KStep 0 - TTA Loss: 0.8154â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 102/203 [2m0:01:50 â€¢ 0:01:45[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.5924, 0.4076]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 102/203 [2m0:01:50 â€¢ 0:01:45[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.5928, 0.4072]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 102/203 [2m0:01:50 â€¢ 0:01:45[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.5934, 0.4066]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 102/203 [2m0:01:50 â€¢ 0:01:45[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.5940, 0.4060]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 102/203 [2m0:01:50 â€¢ 0:01:45[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.5948, 0.4052]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 102/203 [2m0:01:50 â€¢ 0:01:45[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.5956, 0.4044]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 102/203 [2m0:01:50 â€¢ 0:01:45[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.5965, 0.4035]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 102/203 [2m0:01:50 â€¢ 0:01:45[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.5974, 0.4026]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 102/203 [2m0:01:50 â€¢ 0:01:45[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.5984, 0.4016]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 102/203 [2m0:01:50 â€¢ 0:01:45[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.5993, 0.4007]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 102/203 [2m0:01:50 â€¢ 0:01:45[0m [2;4m0.97it/s[0m  
[2KStep 10 - TTA Loss: 0.7408â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 102/203 [2m0:01:50 â€¢ 0:01:45[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6003, 0.3997]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 102/203 [2m0:01:50 â€¢ 0:01:45[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6012, 0.3988]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 102/203 [2m0:01:50 â€¢ 0:01:45[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6021, 0.3979]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 102/203 [2m0:01:50 â€¢ 0:01:45[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6028, 0.3972]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 102/203 [2m0:01:50 â€¢ 0:01:45[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6036, 0.3964]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 102/203 [2m0:01:50 â€¢ 0:01:45[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6044, 0.3956]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 102/203 [2m0:01:50 â€¢ 0:01:45[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6050, 0.3950]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 102/203 [2m0:01:50 â€¢ 0:01:45[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6056, 0.3944]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 102/203 [2m0:01:50 â€¢ 0:01:45[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6061, 0.3939]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 102/203 [2m0:01:50 â€¢ 0:01:45[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6065, 0.3935]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 102/203 [2m0:01:50 â€¢ 0:01:45[0m [2;4m0.97it/s[0m  
[2KStep 20 - TTA Loss: 0.8018â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 102/203 [2m0:01:50 â€¢ 0:01:45[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6069, 0.3931]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 102/203 [2m0:01:50 â€¢ 0:01:45[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6072, 0.3928]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 102/203 [2m0:01:50 â€¢ 0:01:45[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6074, 0.3926]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 102/203 [2m0:01:50 â€¢ 0:01:45[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6076, 0.3924]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 102/203 [2m0:01:50 â€¢ 0:01:45[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6077, 0.3923]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 102/203 [2m0:01:50 â€¢ 0:01:45[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6078, 0.3922]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 102/203 [2m0:01:50 â€¢ 0:01:45[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6079, 0.3921]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 102/203 [2m0:01:50 â€¢ 0:01:45[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6079, 0.3921]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 102/203 [2m0:01:50 â€¢ 0:01:45[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6080, 0.3920]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 102/203 [2m0:01:50 â€¢ 0:01:45[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.5796, 0.4204]], device='cuda:0')38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 102/203 [2m0:01:50 â€¢ 0:01:45[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6436, 0.3564],[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 102/203 [2m0:01:50 â€¢ 0:01:45[0m [2;4m0.97it/s[0m  
        [0.6105, 0.3895],
        [0.6080, 0.3920],
        [0.6567, 0.3433],
        [0.6184, 0.3816],
        [0.6288, 0.3712],
        [0.5816, 0.4184],
        [0.6640, 0.3360],
        [0.6742, 0.3258],
[2K        [0.6675, 0.3325]], device='cuda:0')m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 102/203 [2m0:01:50 â€¢ 0:01:45[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.5798, 0.4202],[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 103/203 [2m0:01:51 â€¢ 0:01:44[0m [2;4m0.96it/s[0m  
        [0.5741, 0.4259],
        [0.5790, 0.4210],
        [0.5781, 0.4219],
        [0.5799, 0.4201],
        [0.5779, 0.4221],
        [0.5759, 0.4241],
        [0.5796, 0.4204],
        [0.5796, 0.4204],
[2K        [0.5807, 0.4193]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 103/203 [2m0:01:51 â€¢ 0:01:44[0m [2;4m0.96it/s[0m  
[2KClass label: TennisSwingâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 103/203 [2m0:01:51 â€¢ 0:01:44[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6705, 0.3295]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 103/203 [2m0:01:51 â€¢ 0:01:44[0m [2;4m0.96it/s[0m  
[2KStep 0 - TTA Loss: 0.4049â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 103/203 [2m0:01:51 â€¢ 0:01:44[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6705, 0.3295]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 103/203 [2m0:01:51 â€¢ 0:01:44[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6705, 0.3295]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 103/203 [2m0:01:51 â€¢ 0:01:44[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6706, 0.3294]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 103/203 [2m0:01:51 â€¢ 0:01:44[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6706, 0.3294]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 103/203 [2m0:01:51 â€¢ 0:01:44[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6707, 0.3293]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 103/203 [2m0:01:51 â€¢ 0:01:44[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6708, 0.3292]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 103/203 [2m0:01:51 â€¢ 0:01:44[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6710, 0.3290]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 103/203 [2m0:01:51 â€¢ 0:01:44[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6713, 0.3287]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 103/203 [2m0:01:51 â€¢ 0:01:44[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6717, 0.3283]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 103/203 [2m0:01:51 â€¢ 0:01:44[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6722, 0.3278]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 103/203 [2m0:01:51 â€¢ 0:01:44[0m [2;4m0.96it/s[0m  
[2KStep 10 - TTA Loss: 0.4502â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 103/203 [2m0:01:51 â€¢ 0:01:44[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6728, 0.3272]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 103/203 [2m0:01:51 â€¢ 0:01:44[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6735, 0.3265]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 103/203 [2m0:01:51 â€¢ 0:01:44[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6744, 0.3256]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 103/203 [2m0:01:51 â€¢ 0:01:44[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6755, 0.3245]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 103/203 [2m0:01:51 â€¢ 0:01:44[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6767, 0.3233]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 103/203 [2m0:01:51 â€¢ 0:01:44[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6781, 0.3219]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 103/203 [2m0:01:51 â€¢ 0:01:44[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6797, 0.3203]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 103/203 [2m0:01:51 â€¢ 0:01:44[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6814, 0.3186]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 103/203 [2m0:01:51 â€¢ 0:01:44[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6834, 0.3166]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 103/203 [2m0:01:51 â€¢ 0:01:44[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6854, 0.3146]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 103/203 [2m0:01:51 â€¢ 0:01:44[0m [2;4m0.96it/s[0m  
[2KStep 20 - TTA Loss: 0.4432â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 103/203 [2m0:01:51 â€¢ 0:01:44[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6877, 0.3123]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 103/203 [2m0:01:51 â€¢ 0:01:44[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6900, 0.3100]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 103/203 [2m0:01:51 â€¢ 0:01:44[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6925, 0.3075]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 103/203 [2m0:01:51 â€¢ 0:01:44[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6952, 0.3048]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 103/203 [2m0:01:51 â€¢ 0:01:44[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6981, 0.3019]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 103/203 [2m0:01:51 â€¢ 0:01:44[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.7011, 0.2989]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 103/203 [2m0:01:51 â€¢ 0:01:44[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.7043, 0.2957]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 103/203 [2m0:01:51 â€¢ 0:01:44[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.7075, 0.2925]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 103/203 [2m0:01:51 â€¢ 0:01:44[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.7108, 0.2892]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 103/203 [2m0:01:51 â€¢ 0:01:44[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.5816, 0.4184]], device='cuda:0')38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 103/203 [2m0:01:51 â€¢ 0:01:44[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6688, 0.3312],[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 103/203 [2m0:01:51 â€¢ 0:01:44[0m [2;4m0.96it/s[0m  
        [0.6254, 0.3746],
        [0.6178, 0.3822],
        [0.6822, 0.3178],
        [0.6390, 0.3610],
        [0.6496, 0.3504],
        [0.5929, 0.4071],
        [0.6817, 0.3183],
        [0.7141, 0.2859],
[2K        [0.6898, 0.3102]], device='cuda:0')m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 103/203 [2m0:01:51 â€¢ 0:01:44[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.5798, 0.4202],[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 104/203 [2m0:01:52 â€¢ 0:01:43[0m [2;4m0.97it/s[0m  
        [0.5740, 0.4260],
        [0.5790, 0.4210],
        [0.5781, 0.4219],
        [0.5799, 0.4201],
        [0.5779, 0.4221],
        [0.5758, 0.4242],
        [0.5795, 0.4205],
        [0.5795, 0.4205],
[2K        [0.5806, 0.4194]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 104/203 [2m0:01:52 â€¢ 0:01:43[0m [2;4m0.97it/s[0m  
[2KClass label: HammerThrowâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 104/203 [2m0:01:52 â€¢ 0:01:43[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6084, 0.3916]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 104/203 [2m0:01:52 â€¢ 0:01:43[0m [2;4m0.97it/s[0m  
[2KStep 0 - TTA Loss: 0.5339â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 104/203 [2m0:01:52 â€¢ 0:01:43[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6106, 0.3894]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 104/203 [2m0:01:52 â€¢ 0:01:43[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6130, 0.3870]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 104/203 [2m0:01:52 â€¢ 0:01:43[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6153, 0.3847]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 104/203 [2m0:01:52 â€¢ 0:01:43[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6177, 0.3823]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 104/203 [2m0:01:52 â€¢ 0:01:43[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6201, 0.3799]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 104/203 [2m0:01:52 â€¢ 0:01:43[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6226, 0.3774]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 104/203 [2m0:01:52 â€¢ 0:01:43[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6250, 0.3750]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 104/203 [2m0:01:52 â€¢ 0:01:43[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6273, 0.3727]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 104/203 [2m0:01:52 â€¢ 0:01:43[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6297, 0.3703]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 104/203 [2m0:01:52 â€¢ 0:01:43[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6319, 0.3681]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 104/203 [2m0:01:52 â€¢ 0:01:43[0m [2;4m0.97it/s[0m  
[2KStep 10 - TTA Loss: 0.6337â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 104/203 [2m0:01:52 â€¢ 0:01:43[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6341, 0.3659]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 104/203 [2m0:01:52 â€¢ 0:01:43[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6361, 0.3639]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 104/203 [2m0:01:52 â€¢ 0:01:43[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6380, 0.3620]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 104/203 [2m0:01:52 â€¢ 0:01:43[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6398, 0.3602]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 104/203 [2m0:01:52 â€¢ 0:01:43[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6415, 0.3585]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 104/203 [2m0:01:52 â€¢ 0:01:43[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6430, 0.3570]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 104/203 [2m0:01:52 â€¢ 0:01:43[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6444, 0.3556]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 104/203 [2m0:01:52 â€¢ 0:01:43[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6456, 0.3544]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 104/203 [2m0:01:52 â€¢ 0:01:43[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6467, 0.3533]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 104/203 [2m0:01:52 â€¢ 0:01:43[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6476, 0.3524]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 104/203 [2m0:01:52 â€¢ 0:01:43[0m [2;4m0.97it/s[0m  
[2KStep 20 - TTA Loss: 0.5423â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 104/203 [2m0:01:52 â€¢ 0:01:43[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6485, 0.3515]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 104/203 [2m0:01:52 â€¢ 0:01:43[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6491, 0.3509]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 104/203 [2m0:01:52 â€¢ 0:01:43[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6497, 0.3503]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 104/203 [2m0:01:52 â€¢ 0:01:43[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6501, 0.3499]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 104/203 [2m0:01:52 â€¢ 0:01:43[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6504, 0.3496]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 104/203 [2m0:01:52 â€¢ 0:01:43[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6506, 0.3494]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 104/203 [2m0:01:52 â€¢ 0:01:43[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6507, 0.3493]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 104/203 [2m0:01:52 â€¢ 0:01:43[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6508, 0.3492]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 104/203 [2m0:01:52 â€¢ 0:01:43[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6509, 0.3491]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 104/203 [2m0:01:52 â€¢ 0:01:43[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.5817, 0.4183]], device='cuda:0')38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 104/203 [2m0:01:52 â€¢ 0:01:43[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6721, 0.3279],[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 104/203 [2m0:01:52 â€¢ 0:01:43[0m [2;4m0.97it/s[0m  
        [0.6256, 0.3744],
        [0.6224, 0.3776],
        [0.6834, 0.3166],
        [0.6509, 0.3491],
        [0.6540, 0.3460],
        [0.6030, 0.3970],
        [0.6862, 0.3138],
        [0.7064, 0.2936],
[2K        [0.6951, 0.3049]], device='cuda:0')m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 104/203 [2m0:01:52 â€¢ 0:01:43[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.5798, 0.4202],[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 105/203 [2m0:01:53 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
        [0.5740, 0.4260],
        [0.5790, 0.4210],
        [0.5780, 0.4220],
        [0.5799, 0.4201],
        [0.5779, 0.4221],
        [0.5758, 0.4242],
        [0.5796, 0.4204],
        [0.5795, 0.4205],
[2K        [0.5806, 0.4194]], device='cuda:0', grad_fn=<SoftmaxBackward0>);237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 105/203 [2m0:01:53 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 105/203 [2m0:01:53 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6179, 0.3821]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 105/203 [2m0:01:53 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KStep 0 - TTA Loss: 0.8132â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 105/203 [2m0:01:53 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6179, 0.3821]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 105/203 [2m0:01:53 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6180, 0.3820]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 105/203 [2m0:01:53 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6180, 0.3820]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 105/203 [2m0:01:53 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6181, 0.3819]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 105/203 [2m0:01:53 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6182, 0.3818]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 105/203 [2m0:01:53 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6184, 0.3816]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 105/203 [2m0:01:53 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6187, 0.3813]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 105/203 [2m0:01:53 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6191, 0.3809]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 105/203 [2m0:01:53 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6196, 0.3804]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 105/203 [2m0:01:53 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6203, 0.3797]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 105/203 [2m0:01:53 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KStep 10 - TTA Loss: 0.7922â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 105/203 [2m0:01:53 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6211, 0.3789]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 105/203 [2m0:01:53 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6220, 0.3780]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 105/203 [2m0:01:53 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6232, 0.3768]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 105/203 [2m0:01:53 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6245, 0.3755]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 105/203 [2m0:01:53 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6260, 0.3740]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 105/203 [2m0:01:53 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6277, 0.3723]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 105/203 [2m0:01:53 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6296, 0.3704]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 105/203 [2m0:01:53 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6317, 0.3683]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 105/203 [2m0:01:53 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6341, 0.3659]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 105/203 [2m0:01:53 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6366, 0.3634]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 105/203 [2m0:01:53 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KStep 20 - TTA Loss: 0.7832â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 105/203 [2m0:01:53 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6393, 0.3607]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 105/203 [2m0:01:53 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6423, 0.3577]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 105/203 [2m0:01:53 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6453, 0.3547]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 105/203 [2m0:01:53 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6484, 0.3516]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 105/203 [2m0:01:53 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6518, 0.3482]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 105/203 [2m0:01:53 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6554, 0.3446]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 105/203 [2m0:01:53 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6590, 0.3410]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 105/203 [2m0:01:53 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6628, 0.3372]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 105/203 [2m0:01:53 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6667, 0.3333]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 105/203 [2m0:01:53 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.5802, 0.4198]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 105/203 [2m0:01:53 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6721, 0.3279],[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 105/203 [2m0:01:53 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
        [0.6231, 0.3769],
        [0.6254, 0.3746],
        [0.6804, 0.3196],
        [0.6535, 0.3465],
        [0.6708, 0.3292],
        [0.6168, 0.3832],
        [0.6913, 0.3087],
        [0.6976, 0.3024],
[2K        [0.6990, 0.3010]], device='cuda:0')m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 105/203 [2m0:01:53 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.5799, 0.4201],[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 106/203 [2m0:01:54 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
        [0.5742, 0.4258],
        [0.5791, 0.4209],
        [0.5781, 0.4219],
        [0.5799, 0.4201],
        [0.5780, 0.4220],
        [0.5759, 0.4241],
        [0.5797, 0.4203],
        [0.5796, 0.4204],
[2K        [0.5806, 0.4194]], device='cuda:0', grad_fn=<SoftmaxBackward0>);237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 106/203 [2m0:01:54 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KClass label: TennisSwingâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 106/203 [2m0:01:54 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6638, 0.3362]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 106/203 [2m0:01:54 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KStep 0 - TTA Loss: 0.7010â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 106/203 [2m0:01:54 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6660, 0.3340]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 106/203 [2m0:01:54 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6679, 0.3321]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 106/203 [2m0:01:54 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6695, 0.3305]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 106/203 [2m0:01:54 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6708, 0.3292]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 106/203 [2m0:01:54 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6718, 0.3282]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 106/203 [2m0:01:54 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6727, 0.3273]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 106/203 [2m0:01:54 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6733, 0.3267]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 106/203 [2m0:01:54 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6738, 0.3262]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 106/203 [2m0:01:54 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6741, 0.3259]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 106/203 [2m0:01:54 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6743, 0.3257]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 106/203 [2m0:01:54 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KStep 10 - TTA Loss: 0.6724â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 106/203 [2m0:01:54 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6744, 0.3256]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 106/203 [2m0:01:54 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6745, 0.3255]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 106/203 [2m0:01:54 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6746, 0.3254]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 106/203 [2m0:01:54 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6745, 0.3255]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 106/203 [2m0:01:54 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6745, 0.3255]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 106/203 [2m0:01:54 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6745, 0.3255]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 106/203 [2m0:01:54 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6744, 0.3256]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 106/203 [2m0:01:54 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6744, 0.3256]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 106/203 [2m0:01:54 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6743, 0.3257]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 106/203 [2m0:01:54 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6743, 0.3257]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 106/203 [2m0:01:54 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KStep 20 - TTA Loss: 0.5915â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 106/203 [2m0:01:54 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6742, 0.3258]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 106/203 [2m0:01:54 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6742, 0.3258]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 106/203 [2m0:01:54 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6742, 0.3258]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 106/203 [2m0:01:54 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6742, 0.3258]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 106/203 [2m0:01:54 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6742, 0.3258]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 106/203 [2m0:01:54 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6741, 0.3259]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 106/203 [2m0:01:54 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6741, 0.3259]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 106/203 [2m0:01:54 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6741, 0.3259]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 106/203 [2m0:01:54 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6741, 0.3259]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 106/203 [2m0:01:54 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.5800, 0.4200]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 106/203 [2m0:01:54 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6498, 0.3502],[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 106/203 [2m0:01:54 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
        [0.6097, 0.3903],
        [0.6042, 0.3958],
        [0.6592, 0.3408],
        [0.6282, 0.3718],
        [0.6437, 0.3563],
        [0.5953, 0.4047],
        [0.6742, 0.3258],
        [0.6742, 0.3258],
[2K        [0.6759, 0.3241]], device='cuda:0')m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 106/203 [2m0:01:54 â€¢ 0:01:42[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.5799, 0.4201],â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 107/203 [2m0:01:55 â€¢ 0:01:40[0m [2;4m0.97it/s[0m   
        [0.5741, 0.4259],
        [0.5791, 0.4209],
        [0.5781, 0.4219],
        [0.5798, 0.4202],
        [0.5779, 0.4221],
        [0.5759, 0.4241],
        [0.5796, 0.4204],
        [0.5796, 0.4204],
[2K        [0.5806, 0.4194]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 107/203 [2m0:01:55 â€¢ 0:01:40[0m [2;4m0.97it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 107/203 [2m0:01:55 â€¢ 0:01:40[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6539, 0.3461]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 107/203 [2m0:01:55 â€¢ 0:01:40[0m [2;4m0.97it/s[0m  
[2KStep 0 - TTA Loss: 0.5036â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 107/203 [2m0:01:55 â€¢ 0:01:40[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6539, 0.3461]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 107/203 [2m0:01:55 â€¢ 0:01:40[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6539, 0.3461]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 107/203 [2m0:01:55 â€¢ 0:01:40[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6539, 0.3461]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 107/203 [2m0:01:55 â€¢ 0:01:40[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6540, 0.3460]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 107/203 [2m0:01:55 â€¢ 0:01:40[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6541, 0.3459]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 107/203 [2m0:01:55 â€¢ 0:01:40[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6543, 0.3457]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 107/203 [2m0:01:55 â€¢ 0:01:40[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6546, 0.3454]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 107/203 [2m0:01:55 â€¢ 0:01:40[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6550, 0.3450]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 107/203 [2m0:01:55 â€¢ 0:01:40[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6557, 0.3443]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 107/203 [2m0:01:55 â€¢ 0:01:40[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6565, 0.3435]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 107/203 [2m0:01:55 â€¢ 0:01:40[0m [2;4m0.97it/s[0m  
[2KStep 10 - TTA Loss: 0.4983â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 107/203 [2m0:01:55 â€¢ 0:01:40[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6575, 0.3425]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 107/203 [2m0:01:55 â€¢ 0:01:40[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6589, 0.3411]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 107/203 [2m0:01:55 â€¢ 0:01:40[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6604, 0.3396]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 107/203 [2m0:01:55 â€¢ 0:01:40[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6623, 0.3377]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 107/203 [2m0:01:55 â€¢ 0:01:40[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6645, 0.3355]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 107/203 [2m0:01:55 â€¢ 0:01:40[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6670, 0.3330]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 107/203 [2m0:01:55 â€¢ 0:01:40[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6699, 0.3301]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 107/203 [2m0:01:55 â€¢ 0:01:40[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6731, 0.3269]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 107/203 [2m0:01:55 â€¢ 0:01:40[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6766, 0.3234]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 107/203 [2m0:01:55 â€¢ 0:01:40[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6805, 0.3195]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 107/203 [2m0:01:55 â€¢ 0:01:40[0m [2;4m0.97it/s[0m  
[2KStep 20 - TTA Loss: 0.5053â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 107/203 [2m0:01:55 â€¢ 0:01:40[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6846, 0.3154]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 107/203 [2m0:01:55 â€¢ 0:01:40[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6891, 0.3109]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 107/203 [2m0:01:55 â€¢ 0:01:40[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6938, 0.3062]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 107/203 [2m0:01:55 â€¢ 0:01:40[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6989, 0.3011]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 107/203 [2m0:01:55 â€¢ 0:01:40[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.7043, 0.2957]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 107/203 [2m0:01:55 â€¢ 0:01:40[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.7098, 0.2902]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 107/203 [2m0:01:55 â€¢ 0:01:40[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.7156, 0.2844]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 107/203 [2m0:01:55 â€¢ 0:01:40[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.7215, 0.2785]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 107/203 [2m0:01:55 â€¢ 0:01:40[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.7276, 0.2724]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 107/203 [2m0:01:55 â€¢ 0:01:40[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.5848, 0.4152]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 107/203 [2m0:01:55 â€¢ 0:01:40[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.7018, 0.2982],m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 107/203 [2m0:01:55 â€¢ 0:01:40[0m [2;4m0.97it/s[0m  
        [0.6397, 0.3603],
        [0.6471, 0.3529],
        [0.7079, 0.2921],
        [0.6812, 0.3188],
        [0.6831, 0.3169],
        [0.6334, 0.3666],
        [0.7020, 0.2980],
        [0.7207, 0.2793],
[2K        [0.7339, 0.2661]], device='cuda:0')0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 107/203 [2m0:01:55 â€¢ 0:01:40[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.5801, 0.4199],â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 108/203 [2m0:01:56 â€¢ 0:01:39[0m [2;4m0.97it/s[0m  
        [0.5743, 0.4257],
        [0.5793, 0.4207],
        [0.5782, 0.4218],
        [0.5801, 0.4199],
        [0.5781, 0.4219],
        [0.5760, 0.4240],
        [0.5798, 0.4202],
        [0.5797, 0.4203],
[2K        [0.5812, 0.4188]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 108/203 [2m0:01:56 â€¢ 0:01:39[0m [2;4m0.97it/s[0m  
[2KClass label: CleanAndJerkâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 108/203 [2m0:01:56 â€¢ 0:01:39[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.5910, 0.4090]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 108/203 [2m0:01:56 â€¢ 0:01:39[0m [2;4m0.97it/s[0m  
[2KStep 0 - TTA Loss: 0.5765â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 108/203 [2m0:01:56 â€¢ 0:01:39[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.5957, 0.4043]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 108/203 [2m0:01:56 â€¢ 0:01:39[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6003, 0.3997]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 108/203 [2m0:01:56 â€¢ 0:01:39[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6046, 0.3954]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 108/203 [2m0:01:56 â€¢ 0:01:39[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6087, 0.3913]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 108/203 [2m0:01:56 â€¢ 0:01:39[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6126, 0.3874]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 108/203 [2m0:01:56 â€¢ 0:01:39[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6163, 0.3837]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 108/203 [2m0:01:56 â€¢ 0:01:39[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6198, 0.3802]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 108/203 [2m0:01:56 â€¢ 0:01:39[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6231, 0.3769]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 108/203 [2m0:01:56 â€¢ 0:01:39[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6263, 0.3737]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 108/203 [2m0:01:56 â€¢ 0:01:39[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6293, 0.3707]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 108/203 [2m0:01:56 â€¢ 0:01:39[0m [2;4m0.97it/s[0m  
[2KStep 10 - TTA Loss: 0.5905â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 108/203 [2m0:01:56 â€¢ 0:01:39[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6320, 0.3680]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 108/203 [2m0:01:56 â€¢ 0:01:39[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6346, 0.3654]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 108/203 [2m0:01:56 â€¢ 0:01:39[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6369, 0.3631]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 108/203 [2m0:01:56 â€¢ 0:01:39[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6390, 0.3610]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 108/203 [2m0:01:56 â€¢ 0:01:39[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6409, 0.3591]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 108/203 [2m0:01:56 â€¢ 0:01:39[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6426, 0.3574]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 108/203 [2m0:01:56 â€¢ 0:01:39[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6442, 0.3558]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 108/203 [2m0:01:56 â€¢ 0:01:39[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6455, 0.3545]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 108/203 [2m0:01:56 â€¢ 0:01:39[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6467, 0.3533]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 108/203 [2m0:01:56 â€¢ 0:01:39[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6477, 0.3523]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 108/203 [2m0:01:56 â€¢ 0:01:39[0m [2;4m0.97it/s[0m  
[2KStep 20 - TTA Loss: 0.5934â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 108/203 [2m0:01:56 â€¢ 0:01:39[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6485, 0.3515]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 108/203 [2m0:01:56 â€¢ 0:01:39[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6492, 0.3508]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 108/203 [2m0:01:56 â€¢ 0:01:39[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6498, 0.3502]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 108/203 [2m0:01:56 â€¢ 0:01:39[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6502, 0.3498]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 108/203 [2m0:01:56 â€¢ 0:01:39[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6505, 0.3495]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 108/203 [2m0:01:56 â€¢ 0:01:39[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6508, 0.3492]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 108/203 [2m0:01:56 â€¢ 0:01:39[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6509, 0.3491]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 108/203 [2m0:01:56 â€¢ 0:01:39[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6510, 0.3490]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 108/203 [2m0:01:56 â€¢ 0:01:39[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6510, 0.3490]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 108/203 [2m0:01:56 â€¢ 0:01:39[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.5819, 0.4181]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 108/203 [2m0:01:56 â€¢ 0:01:39[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.6873, 0.3127],m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 108/203 [2m0:01:56 â€¢ 0:01:39[0m [2;4m0.97it/s[0m  
        [0.6337, 0.3663],
        [0.6511, 0.3489],
        [0.6967, 0.3033],
        [0.6697, 0.3303],
        [0.6729, 0.3271],
        [0.6249, 0.3751],
        [0.6951, 0.3049],
        [0.7093, 0.2907],
[2K        [0.7190, 0.2810]], device='cuda:0')0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 108/203 [2m0:01:56 â€¢ 0:01:39[0m [2;4m0.97it/s[0m  
[2KAttention weights: tensor([[0.5803, 0.4197],â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 109/203 [2m0:01:57 â€¢ 0:01:39[0m [2;4m0.96it/s[0m  
        [0.5745, 0.4255],
        [0.5797, 0.4203],
        [0.5784, 0.4216],
        [0.5803, 0.4197],
        [0.5783, 0.4217],
        [0.5762, 0.4238],
        [0.5800, 0.4200],
        [0.5799, 0.4201],
[2K        [0.5816, 0.4184]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 109/203 [2m0:01:57 â€¢ 0:01:39[0m [2;4m0.96it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 109/203 [2m0:01:57 â€¢ 0:01:39[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6581, 0.3419]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 109/203 [2m0:01:57 â€¢ 0:01:39[0m [2;4m0.96it/s[0m  
[2KStep 0 - TTA Loss: 0.8134â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 109/203 [2m0:01:57 â€¢ 0:01:39[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6581, 0.3419]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 109/203 [2m0:01:57 â€¢ 0:01:39[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6581, 0.3419]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 109/203 [2m0:01:57 â€¢ 0:01:39[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6581, 0.3419]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 109/203 [2m0:01:57 â€¢ 0:01:39[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6582, 0.3418]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 109/203 [2m0:01:57 â€¢ 0:01:39[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6583, 0.3417]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 109/203 [2m0:01:57 â€¢ 0:01:39[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6585, 0.3415]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 109/203 [2m0:01:57 â€¢ 0:01:39[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6587, 0.3413]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 109/203 [2m0:01:57 â€¢ 0:01:39[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6591, 0.3409]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 109/203 [2m0:01:57 â€¢ 0:01:39[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6596, 0.3404]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 109/203 [2m0:01:57 â€¢ 0:01:39[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6601, 0.3399]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 109/203 [2m0:01:57 â€¢ 0:01:39[0m [2;4m0.96it/s[0m  
[2KStep 10 - TTA Loss: 0.7921â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 109/203 [2m0:01:57 â€¢ 0:01:39[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6609, 0.3391]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 109/203 [2m0:01:57 â€¢ 0:01:39[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6618, 0.3382]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 109/203 [2m0:01:57 â€¢ 0:01:39[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6628, 0.3372]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 109/203 [2m0:01:57 â€¢ 0:01:39[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6641, 0.3359]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 109/203 [2m0:01:57 â€¢ 0:01:39[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6655, 0.3345]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 109/203 [2m0:01:57 â€¢ 0:01:39[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6671, 0.3329]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 109/203 [2m0:01:57 â€¢ 0:01:39[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6689, 0.3311]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 109/203 [2m0:01:57 â€¢ 0:01:39[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6709, 0.3291]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 109/203 [2m0:01:57 â€¢ 0:01:39[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6731, 0.3269]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 109/203 [2m0:01:57 â€¢ 0:01:39[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6756, 0.3244]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 109/203 [2m0:01:57 â€¢ 0:01:39[0m [2;4m0.96it/s[0m  
[2KStep 20 - TTA Loss: 0.7788â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 109/203 [2m0:01:57 â€¢ 0:01:39[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6782, 0.3218]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 109/203 [2m0:01:57 â€¢ 0:01:39[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6810, 0.3190]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 109/203 [2m0:01:57 â€¢ 0:01:39[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6839, 0.3161]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 109/203 [2m0:01:57 â€¢ 0:01:39[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6870, 0.3130]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 109/203 [2m0:01:57 â€¢ 0:01:39[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6902, 0.3098]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 109/203 [2m0:01:57 â€¢ 0:01:39[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6934, 0.3066]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 109/203 [2m0:01:57 â€¢ 0:01:39[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6968, 0.3032]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 109/203 [2m0:01:57 â€¢ 0:01:39[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.7003, 0.2997]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 109/203 [2m0:01:57 â€¢ 0:01:39[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.7038, 0.2962]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 109/203 [2m0:01:57 â€¢ 0:01:39[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.5845, 0.4155]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 109/203 [2m0:01:57 â€¢ 0:01:39[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6783, 0.3217],m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 109/203 [2m0:01:57 â€¢ 0:01:39[0m [2;4m0.96it/s[0m  
        [0.6250, 0.3750],
        [0.6296, 0.3704],
        [0.6880, 0.3120],
        [0.6558, 0.3442],
        [0.6605, 0.3395],
        [0.6127, 0.3873],
        [0.6854, 0.3146],
        [0.7013, 0.2987],
[2K        [0.7075, 0.2925]], device='cuda:0')0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 109/203 [2m0:01:57 â€¢ 0:01:39[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.5805, 0.4195],â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 110/203 [2m0:01:59 â€¢ 0:01:38[0m [2;4m0.96it/s[0m  
        [0.5746, 0.4254],
        [0.5802, 0.4198],
        [0.5786, 0.4214],
        [0.5807, 0.4193],
        [0.5785, 0.4215],
        [0.5765, 0.4235],
        [0.5801, 0.4199],
        [0.5800, 0.4200],
[2K        [0.5821, 0.4179]], device='cuda:0', grad_fn=<SoftmaxBackward0>)5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 110/203 [2m0:01:59 â€¢ 0:01:38[0m [2;4m0.96it/s[0m  
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 110/203 [2m0:01:59 â€¢ 0:01:38[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6167, 0.3833]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 110/203 [2m0:01:59 â€¢ 0:01:38[0m [2;4m0.96it/s[0m  
[2KStep 0 - TTA Loss: 0.6473â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 110/203 [2m0:01:59 â€¢ 0:01:38[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6200, 0.3800]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 110/203 [2m0:01:59 â€¢ 0:01:38[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6231, 0.3769]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 110/203 [2m0:01:59 â€¢ 0:01:38[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6261, 0.3739]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 110/203 [2m0:01:59 â€¢ 0:01:38[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6291, 0.3709]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 110/203 [2m0:01:59 â€¢ 0:01:38[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6320, 0.3680]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 110/203 [2m0:01:59 â€¢ 0:01:38[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6347, 0.3653]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 110/203 [2m0:01:59 â€¢ 0:01:38[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6374, 0.3626]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 110/203 [2m0:01:59 â€¢ 0:01:38[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6400, 0.3600]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 110/203 [2m0:01:59 â€¢ 0:01:38[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6424, 0.3576]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 110/203 [2m0:01:59 â€¢ 0:01:38[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6447, 0.3553]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 110/203 [2m0:01:59 â€¢ 0:01:38[0m [2;4m0.96it/s[0m  
[2KStep 10 - TTA Loss: 0.6565â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 110/203 [2m0:01:59 â€¢ 0:01:38[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6469, 0.3531]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 110/203 [2m0:01:59 â€¢ 0:01:38[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6489, 0.3511]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 110/203 [2m0:01:59 â€¢ 0:01:38[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6507, 0.3493]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 110/203 [2m0:01:59 â€¢ 0:01:38[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6525, 0.3475]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 110/203 [2m0:01:59 â€¢ 0:01:38[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6542, 0.3458]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 110/203 [2m0:01:59 â€¢ 0:01:38[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6556, 0.3444]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 110/203 [2m0:01:59 â€¢ 0:01:38[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6570, 0.3430]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 110/203 [2m0:01:59 â€¢ 0:01:38[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6582, 0.3418]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 110/203 [2m0:01:59 â€¢ 0:01:38[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6592, 0.3408]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 110/203 [2m0:01:59 â€¢ 0:01:38[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6601, 0.3399]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 110/203 [2m0:01:59 â€¢ 0:01:38[0m [2;4m0.96it/s[0m  
[2KStep 20 - TTA Loss: 0.6925â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 110/203 [2m0:01:59 â€¢ 0:01:38[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6609, 0.3391]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 110/203 [2m0:01:59 â€¢ 0:01:38[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6615, 0.3385]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 110/203 [2m0:01:59 â€¢ 0:01:38[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6620, 0.3380]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 110/203 [2m0:01:59 â€¢ 0:01:38[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6624, 0.3376]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 110/203 [2m0:01:59 â€¢ 0:01:38[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6627, 0.3373]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 110/203 [2m0:01:59 â€¢ 0:01:38[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6629, 0.3371]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 110/203 [2m0:01:59 â€¢ 0:01:38[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6631, 0.3369]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 110/203 [2m0:01:59 â€¢ 0:01:38[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6632, 0.3368]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 110/203 [2m0:01:59 â€¢ 0:01:38[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6632, 0.3368]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 110/203 [2m0:01:59 â€¢ 0:01:38[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.5807, 0.4193]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 110/203 [2m0:01:59 â€¢ 0:01:38[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6688, 0.3312],m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 110/203 [2m0:01:59 â€¢ 0:01:38[0m [2;4m0.96it/s[0m  
        [0.6242, 0.3758],
        [0.6242, 0.3758],
        [0.6784, 0.3216],
        [0.6517, 0.3483],
        [0.6633, 0.3367],
        [0.6123, 0.3877],
        [0.6884, 0.3116],
        [0.6969, 0.3031],
[2K        [0.7000, 0.3000]], device='cuda:0')0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 110/203 [2m0:01:59 â€¢ 0:01:38[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.5806, 0.4194],â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 111/203 [2m0:02:00 â€¢ 0:01:37[0m [2;4m0.95it/s[0m  
        [0.5747, 0.4253],
        [0.5805, 0.4195],
        [0.5787, 0.4213],
        [0.5809, 0.4191],
        [0.5788, 0.4212],
        [0.5767, 0.4233],
        [0.5803, 0.4197],
        [0.5802, 0.4198],
[2K        [0.5824, 0.4176]], device='cuda:0', grad_fn=<SoftmaxBackward0>)5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 111/203 [2m0:02:00 â€¢ 0:01:37[0m [2;4m0.95it/s[0m  
[2KClass label: HammerThrowâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 111/203 [2m0:02:00 â€¢ 0:01:37[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6127, 0.3873]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 111/203 [2m0:02:00 â€¢ 0:01:37[0m [2;4m0.95it/s[0m  
[2KStep 0 - TTA Loss: 0.5540â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 111/203 [2m0:02:00 â€¢ 0:01:37[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6127, 0.3873]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 111/203 [2m0:02:00 â€¢ 0:01:37[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6127, 0.3873]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 111/203 [2m0:02:00 â€¢ 0:01:37[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6127, 0.3873]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 111/203 [2m0:02:00 â€¢ 0:01:37[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6128, 0.3872]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 111/203 [2m0:02:00 â€¢ 0:01:37[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6129, 0.3871]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 111/203 [2m0:02:00 â€¢ 0:01:37[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6131, 0.3869]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 111/203 [2m0:02:00 â€¢ 0:01:37[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6134, 0.3866]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 111/203 [2m0:02:00 â€¢ 0:01:37[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6137, 0.3863]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 111/203 [2m0:02:00 â€¢ 0:01:37[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6142, 0.3858]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 111/203 [2m0:02:00 â€¢ 0:01:37[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6148, 0.3852]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 111/203 [2m0:02:00 â€¢ 0:01:37[0m [2;4m0.95it/s[0m  
[2KStep 10 - TTA Loss: 0.5355â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 111/203 [2m0:02:00 â€¢ 0:01:37[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6155, 0.3845]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 111/203 [2m0:02:00 â€¢ 0:01:37[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6163, 0.3837]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 111/203 [2m0:02:00 â€¢ 0:01:37[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6173, 0.3827]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 111/203 [2m0:02:00 â€¢ 0:01:37[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6184, 0.3816]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 111/203 [2m0:02:00 â€¢ 0:01:37[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6196, 0.3804]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 111/203 [2m0:02:00 â€¢ 0:01:37[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6211, 0.3789]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 111/203 [2m0:02:00 â€¢ 0:01:37[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6227, 0.3773]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 111/203 [2m0:02:00 â€¢ 0:01:37[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6246, 0.3754]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 111/203 [2m0:02:00 â€¢ 0:01:37[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6266, 0.3734]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 111/203 [2m0:02:00 â€¢ 0:01:37[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6288, 0.3712]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 111/203 [2m0:02:00 â€¢ 0:01:37[0m [2;4m0.95it/s[0m  
[2KStep 20 - TTA Loss: 0.4911â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 111/203 [2m0:02:00 â€¢ 0:01:37[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6312, 0.3688]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 111/203 [2m0:02:00 â€¢ 0:01:37[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6337, 0.3663]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 111/203 [2m0:02:00 â€¢ 0:01:37[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6365, 0.3635]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 111/203 [2m0:02:00 â€¢ 0:01:37[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6394, 0.3606]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 111/203 [2m0:02:00 â€¢ 0:01:37[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6425, 0.3575]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 111/203 [2m0:02:00 â€¢ 0:01:37[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6457, 0.3543]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 111/203 [2m0:02:00 â€¢ 0:01:37[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6491, 0.3509]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 111/203 [2m0:02:00 â€¢ 0:01:37[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6527, 0.3473]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 111/203 [2m0:02:00 â€¢ 0:01:37[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6564, 0.3436]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 111/203 [2m0:02:00 â€¢ 0:01:37[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.5831, 0.4169]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 111/203 [2m0:02:00 â€¢ 0:01:37[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6743, 0.3257],m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 111/203 [2m0:02:00 â€¢ 0:01:37[0m [2;4m0.95it/s[0m  
        [0.6205, 0.3795],
        [0.6284, 0.3716],
        [0.6830, 0.3170],
        [0.6603, 0.3397],
        [0.6588, 0.3412],
        [0.6101, 0.3899],
        [0.6901, 0.3099],
        [0.6962, 0.3038],
[2K        [0.7000, 0.3000]], device='cuda:0')0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 111/203 [2m0:02:00 â€¢ 0:01:37[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.5805, 0.4195],â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 112/203 [2m0:02:01 â€¢ 0:01:37[0m [2;4m0.94it/s[0m   
        [0.5748, 0.4252],
        [0.5805, 0.4195],
        [0.5787, 0.4213],
        [0.5810, 0.4190],
        [0.5789, 0.4211],
        [0.5766, 0.4234],
        [0.5803, 0.4197],
        [0.5801, 0.4199],
[2K        [0.5824, 0.4176]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 112/203 [2m0:02:01 â€¢ 0:01:37[0m [2;4m0.94it/s[0m  
[2KClass label: GolfSwingâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 112/203 [2m0:02:01 â€¢ 0:01:37[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6491, 0.3509]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 112/203 [2m0:02:01 â€¢ 0:01:37[0m [2;4m0.94it/s[0m  
[2KStep 0 - TTA Loss: 0.8023â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 112/203 [2m0:02:01 â€¢ 0:01:37[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6517, 0.3483]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 112/203 [2m0:02:01 â€¢ 0:01:37[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6541, 0.3459]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 112/203 [2m0:02:01 â€¢ 0:01:37[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6566, 0.3434]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 112/203 [2m0:02:01 â€¢ 0:01:37[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6589, 0.3411]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 112/203 [2m0:02:01 â€¢ 0:01:37[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6611, 0.3389]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 112/203 [2m0:02:01 â€¢ 0:01:37[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6632, 0.3368]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 112/203 [2m0:02:01 â€¢ 0:01:37[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6651, 0.3349]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 112/203 [2m0:02:01 â€¢ 0:01:37[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6668, 0.3332]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 112/203 [2m0:02:01 â€¢ 0:01:37[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6685, 0.3315]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 112/203 [2m0:02:01 â€¢ 0:01:37[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6701, 0.3299]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 112/203 [2m0:02:01 â€¢ 0:01:37[0m [2;4m0.94it/s[0m  
[2KStep 10 - TTA Loss: 0.7832â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 112/203 [2m0:02:01 â€¢ 0:01:37[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6717, 0.3283]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 112/203 [2m0:02:01 â€¢ 0:01:37[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6733, 0.3267]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 112/203 [2m0:02:01 â€¢ 0:01:37[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6747, 0.3253]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 112/203 [2m0:02:01 â€¢ 0:01:37[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6760, 0.3240]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 112/203 [2m0:02:01 â€¢ 0:01:37[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6771, 0.3229]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 112/203 [2m0:02:01 â€¢ 0:01:37[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6782, 0.3218]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 112/203 [2m0:02:01 â€¢ 0:01:37[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6791, 0.3209]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 112/203 [2m0:02:01 â€¢ 0:01:37[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6799, 0.3201]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 112/203 [2m0:02:01 â€¢ 0:01:37[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6806, 0.3194]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 112/203 [2m0:02:01 â€¢ 0:01:37[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6812, 0.3188]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 112/203 [2m0:02:01 â€¢ 0:01:37[0m [2;4m0.94it/s[0m  
[2KStep 20 - TTA Loss: 0.7785â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 112/203 [2m0:02:01 â€¢ 0:01:37[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6818, 0.3182]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 112/203 [2m0:02:01 â€¢ 0:01:37[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6822, 0.3178]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 112/203 [2m0:02:01 â€¢ 0:01:37[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6826, 0.3174]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 112/203 [2m0:02:01 â€¢ 0:01:37[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6829, 0.3171]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 112/203 [2m0:02:01 â€¢ 0:01:37[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6831, 0.3169]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 112/203 [2m0:02:01 â€¢ 0:01:37[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6832, 0.3168]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 112/203 [2m0:02:01 â€¢ 0:01:37[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6833, 0.3167]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 112/203 [2m0:02:01 â€¢ 0:01:37[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6834, 0.3166]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 112/203 [2m0:02:01 â€¢ 0:01:37[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6834, 0.3166]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 112/203 [2m0:02:01 â€¢ 0:01:37[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5805, 0.4195]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 112/203 [2m0:02:01 â€¢ 0:01:37[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6694, 0.3306],0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 112/203 [2m0:02:01 â€¢ 0:01:37[0m [2;4m0.94it/s[0m  
        [0.6192, 0.3808],
        [0.6206, 0.3794],
        [0.6835, 0.3165],
        [0.6487, 0.3513],
        [0.6492, 0.3508],
        [0.6018, 0.3982],
        [0.6827, 0.3173],
        [0.6914, 0.3086],
[2K        [0.6925, 0.3075]], device='cuda:0')[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 112/203 [2m0:02:01 â€¢ 0:01:37[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5806, 0.4194],â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 113/203 [2m0:02:02 â€¢ 0:01:36[0m [2;4m0.95it/s[0m  
        [0.5749, 0.4251],
        [0.5806, 0.4194],
        [0.5789, 0.4211],
        [0.5810, 0.4190],
        [0.5789, 0.4211],
        [0.5767, 0.4233],
        [0.5804, 0.4196],
        [0.5802, 0.4198],
[2K        [0.5824, 0.4176]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 113/203 [2m0:02:02 â€¢ 0:01:36[0m [2;4m0.95it/s[0m  
[2KClass label: TennisSwingâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 113/203 [2m0:02:02 â€¢ 0:01:36[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6638, 0.3362]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 113/203 [2m0:02:02 â€¢ 0:01:36[0m [2;4m0.95it/s[0m  
[2KStep 0 - TTA Loss: 0.3796â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 113/203 [2m0:02:02 â€¢ 0:01:36[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6638, 0.3362]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 113/203 [2m0:02:02 â€¢ 0:01:36[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6638, 0.3362]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 113/203 [2m0:02:02 â€¢ 0:01:36[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6638, 0.3362]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 113/203 [2m0:02:02 â€¢ 0:01:36[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6639, 0.3361]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 113/203 [2m0:02:02 â€¢ 0:01:36[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6640, 0.3360]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 113/203 [2m0:02:02 â€¢ 0:01:36[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6642, 0.3358]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 113/203 [2m0:02:02 â€¢ 0:01:36[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6644, 0.3356]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 113/203 [2m0:02:02 â€¢ 0:01:36[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6648, 0.3352]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 113/203 [2m0:02:02 â€¢ 0:01:36[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6652, 0.3348]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 113/203 [2m0:02:02 â€¢ 0:01:36[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6658, 0.3342]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 113/203 [2m0:02:02 â€¢ 0:01:36[0m [2;4m0.95it/s[0m  
[2KStep 10 - TTA Loss: 0.3821â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 113/203 [2m0:02:02 â€¢ 0:01:36[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6666, 0.3334]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 113/203 [2m0:02:02 â€¢ 0:01:36[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6674, 0.3326]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 113/203 [2m0:02:02 â€¢ 0:01:36[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6684, 0.3316]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 113/203 [2m0:02:02 â€¢ 0:01:36[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6695, 0.3305]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 113/203 [2m0:02:02 â€¢ 0:01:36[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6708, 0.3292]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 113/203 [2m0:02:02 â€¢ 0:01:36[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6724, 0.3276]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 113/203 [2m0:02:02 â€¢ 0:01:36[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6741, 0.3259]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 113/203 [2m0:02:02 â€¢ 0:01:36[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6759, 0.3241]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 113/203 [2m0:02:02 â€¢ 0:01:36[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6780, 0.3220]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 113/203 [2m0:02:02 â€¢ 0:01:36[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6802, 0.3198]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 113/203 [2m0:02:02 â€¢ 0:01:36[0m [2;4m0.95it/s[0m  
[2KStep 20 - TTA Loss: 0.4367â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 113/203 [2m0:02:02 â€¢ 0:01:36[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6826, 0.3174]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 113/203 [2m0:02:02 â€¢ 0:01:36[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6853, 0.3147]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 113/203 [2m0:02:02 â€¢ 0:01:36[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6880, 0.3120]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 113/203 [2m0:02:02 â€¢ 0:01:36[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6909, 0.3091]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 113/203 [2m0:02:02 â€¢ 0:01:36[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6938, 0.3062]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 113/203 [2m0:02:02 â€¢ 0:01:36[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6970, 0.3030]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 113/203 [2m0:02:02 â€¢ 0:01:36[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.7002, 0.2998]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 113/203 [2m0:02:02 â€¢ 0:01:36[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.7035, 0.2965]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 113/203 [2m0:02:02 â€¢ 0:01:36[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.7070, 0.2930]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 113/203 [2m0:02:02 â€¢ 0:01:36[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.5825, 0.4175]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 113/203 [2m0:02:02 â€¢ 0:01:36[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6640, 0.3360],0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 113/203 [2m0:02:02 â€¢ 0:01:36[0m [2;4m0.95it/s[0m  
        [0.6221, 0.3779],
        [0.6140, 0.3860],
        [0.6799, 0.3201],
        [0.6341, 0.3659],
        [0.6448, 0.3552],
        [0.5885, 0.4115],
        [0.6753, 0.3247],
        [0.7105, 0.2895],
[2K        [0.6855, 0.3145]], device='cuda:0')[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 113/203 [2m0:02:02 â€¢ 0:01:36[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.5808, 0.4192],â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 114/203 [2m0:02:03 â€¢ 0:01:35[0m [2;4m0.94it/s[0m  
        [0.5751, 0.4249],
        [0.5807, 0.4193],
        [0.5792, 0.4208],
        [0.5812, 0.4188],
        [0.5791, 0.4209],
        [0.5768, 0.4232],
        [0.5806, 0.4194],
        [0.5806, 0.4194],
[2K        [0.5825, 0.4175]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 114/203 [2m0:02:03 â€¢ 0:01:35[0m [2;4m0.94it/s[0m  
[2KClass label: HammerThrowâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 114/203 [2m0:02:03 â€¢ 0:01:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6107, 0.3893]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 114/203 [2m0:02:03 â€¢ 0:01:35[0m [2;4m0.94it/s[0m  
[2KStep 0 - TTA Loss: 0.5653â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 114/203 [2m0:02:03 â€¢ 0:01:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6131, 0.3869]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 114/203 [2m0:02:03 â€¢ 0:01:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6157, 0.3843]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 114/203 [2m0:02:03 â€¢ 0:01:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6185, 0.3815]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 114/203 [2m0:02:03 â€¢ 0:01:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6214, 0.3786]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 114/203 [2m0:02:03 â€¢ 0:01:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6244, 0.3756]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 114/203 [2m0:02:03 â€¢ 0:01:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6274, 0.3726]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 114/203 [2m0:02:03 â€¢ 0:01:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6305, 0.3695]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 114/203 [2m0:02:03 â€¢ 0:01:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6336, 0.3664]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 114/203 [2m0:02:03 â€¢ 0:01:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6368, 0.3632]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 114/203 [2m0:02:03 â€¢ 0:01:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6398, 0.3602]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 114/203 [2m0:02:03 â€¢ 0:01:35[0m [2;4m0.94it/s[0m  
[2KStep 10 - TTA Loss: 0.5588â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 114/203 [2m0:02:03 â€¢ 0:01:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6428, 0.3572]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 114/203 [2m0:02:03 â€¢ 0:01:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6457, 0.3543]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 114/203 [2m0:02:03 â€¢ 0:01:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6484, 0.3516]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 114/203 [2m0:02:03 â€¢ 0:01:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6510, 0.3490]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 114/203 [2m0:02:03 â€¢ 0:01:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6534, 0.3466]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 114/203 [2m0:02:03 â€¢ 0:01:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6557, 0.3443]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 114/203 [2m0:02:03 â€¢ 0:01:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6577, 0.3423]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 114/203 [2m0:02:03 â€¢ 0:01:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6596, 0.3404]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 114/203 [2m0:02:03 â€¢ 0:01:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6612, 0.3388]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 114/203 [2m0:02:03 â€¢ 0:01:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6626, 0.3374]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 114/203 [2m0:02:03 â€¢ 0:01:35[0m [2;4m0.94it/s[0m  
[2KStep 20 - TTA Loss: 0.6262â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 114/203 [2m0:02:03 â€¢ 0:01:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6637, 0.3363]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 114/203 [2m0:02:03 â€¢ 0:01:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6647, 0.3353]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 114/203 [2m0:02:03 â€¢ 0:01:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6655, 0.3345]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 114/203 [2m0:02:03 â€¢ 0:01:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6661, 0.3339]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 114/203 [2m0:02:03 â€¢ 0:01:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6666, 0.3334]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 114/203 [2m0:02:03 â€¢ 0:01:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6669, 0.3331]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 114/203 [2m0:02:03 â€¢ 0:01:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6672, 0.3328]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 114/203 [2m0:02:03 â€¢ 0:01:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6673, 0.3327]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 114/203 [2m0:02:03 â€¢ 0:01:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6673, 0.3327]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 114/203 [2m0:02:03 â€¢ 0:01:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5836, 0.4164]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 114/203 [2m0:02:03 â€¢ 0:01:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6848, 0.3152],0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 114/203 [2m0:02:03 â€¢ 0:01:35[0m [2;4m0.94it/s[0m  
        [0.6333, 0.3667],
        [0.6336, 0.3664],
        [0.6946, 0.3054],
        [0.6674, 0.3326],
        [0.6662, 0.3338],
        [0.6137, 0.3863],
        [0.6942, 0.3058],
        [0.7162, 0.2838],
[2K        [0.7095, 0.2905]], device='cuda:0')[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 114/203 [2m0:02:03 â€¢ 0:01:35[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5810, 0.4190],â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 115/203 [2m0:02:05 â€¢ 0:01:36[0m [2;4m0.92it/s[0m  
        [0.5752, 0.4248],
        [0.5807, 0.4193],
        [0.5794, 0.4206],
        [0.5813, 0.4187],
        [0.5791, 0.4209],
        [0.5769, 0.4231],
        [0.5807, 0.4193],
        [0.5808, 0.4192],
[2K        [0.5826, 0.4174]], device='cuda:0', grad_fn=<SoftmaxBackward0>);5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 115/203 [2m0:02:05 â€¢ 0:01:36[0m [2;4m0.92it/s[0m  
[2KClass label: HammerThrowâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 115/203 [2m0:02:05 â€¢ 0:01:36[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6068, 0.3932]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 115/203 [2m0:02:05 â€¢ 0:01:36[0m [2;4m0.92it/s[0m  
[2KStep 0 - TTA Loss: 0.8329â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 115/203 [2m0:02:05 â€¢ 0:01:36[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6068, 0.3932]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 115/203 [2m0:02:05 â€¢ 0:01:36[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6068, 0.3932]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 115/203 [2m0:02:05 â€¢ 0:01:36[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6068, 0.3932]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 115/203 [2m0:02:05 â€¢ 0:01:36[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6069, 0.3931]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 115/203 [2m0:02:05 â€¢ 0:01:36[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6071, 0.3929]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 115/203 [2m0:02:05 â€¢ 0:01:36[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6073, 0.3927]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 115/203 [2m0:02:05 â€¢ 0:01:36[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6076, 0.3924]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 115/203 [2m0:02:05 â€¢ 0:01:36[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6080, 0.3920]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 115/203 [2m0:02:05 â€¢ 0:01:36[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6085, 0.3915]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 115/203 [2m0:02:05 â€¢ 0:01:36[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6090, 0.3910]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 115/203 [2m0:02:05 â€¢ 0:01:36[0m [2;4m0.92it/s[0m  
[2KStep 10 - TTA Loss: 0.7568â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 115/203 [2m0:02:05 â€¢ 0:01:36[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6097, 0.3903]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 115/203 [2m0:02:05 â€¢ 0:01:36[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6104, 0.3896]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 115/203 [2m0:02:05 â€¢ 0:01:36[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6112, 0.3888]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 115/203 [2m0:02:05 â€¢ 0:01:36[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6120, 0.3880]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 115/203 [2m0:02:05 â€¢ 0:01:36[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6129, 0.3871]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 115/203 [2m0:02:05 â€¢ 0:01:36[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6139, 0.3861]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 115/203 [2m0:02:05 â€¢ 0:01:36[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6150, 0.3850]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 115/203 [2m0:02:05 â€¢ 0:01:36[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6161, 0.3839]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 115/203 [2m0:02:05 â€¢ 0:01:36[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6172, 0.3828]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 115/203 [2m0:02:05 â€¢ 0:01:36[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6184, 0.3816]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 115/203 [2m0:02:05 â€¢ 0:01:36[0m [2;4m0.92it/s[0m  
[2KStep 20 - TTA Loss: 0.7316â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 115/203 [2m0:02:05 â€¢ 0:01:36[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6197, 0.3803]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 115/203 [2m0:02:05 â€¢ 0:01:36[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6210, 0.3790]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 115/203 [2m0:02:05 â€¢ 0:01:36[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6223, 0.3777]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 115/203 [2m0:02:05 â€¢ 0:01:36[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6236, 0.3764]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 115/203 [2m0:02:05 â€¢ 0:01:36[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6250, 0.3750]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 115/203 [2m0:02:05 â€¢ 0:01:36[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6264, 0.3736]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 115/203 [2m0:02:05 â€¢ 0:01:36[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6278, 0.3722]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 115/203 [2m0:02:05 â€¢ 0:01:36[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6292, 0.3708]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 115/203 [2m0:02:05 â€¢ 0:01:36[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6306, 0.3694]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 115/203 [2m0:02:05 â€¢ 0:01:36[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5824, 0.4176]], device='cuda:0')â•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 115/203 [2m0:02:05 â€¢ 0:01:36[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6497, 0.3503],0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 115/203 [2m0:02:05 â€¢ 0:01:36[0m [2;4m0.92it/s[0m  
        [0.6094, 0.3906],
        [0.6079, 0.3921],
        [0.6602, 0.3398],
        [0.6321, 0.3679],
        [0.6358, 0.3642],
        [0.5889, 0.4111],
        [0.6695, 0.3305],
        [0.6751, 0.3249],
[2K        [0.6759, 0.3241]], device='cuda:0')[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 115/203 [2m0:02:05 â€¢ 0:01:36[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5810, 0.4190],â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 116/203 [2m0:02:06 â€¢ 0:01:34[0m [2;4m0.93it/s[0m  
        [0.5752, 0.4248],
        [0.5808, 0.4192],
        [0.5794, 0.4206],
        [0.5814, 0.4186],
        [0.5792, 0.4208],
        [0.5770, 0.4230],
        [0.5807, 0.4193],
        [0.5809, 0.4191],
[2K        [0.5826, 0.4174]], device='cuda:0', grad_fn=<SoftmaxBackward0>);5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 116/203 [2m0:02:06 â€¢ 0:01:34[0m [2;4m0.93it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 116/203 [2m0:02:06 â€¢ 0:01:34[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5747, 0.4253]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 116/203 [2m0:02:06 â€¢ 0:01:34[0m [2;4m0.93it/s[0m  
[2KStep 0 - TTA Loss: 0.7168â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 116/203 [2m0:02:06 â€¢ 0:01:34[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5760, 0.4240]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 116/203 [2m0:02:06 â€¢ 0:01:34[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5774, 0.4226]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 116/203 [2m0:02:06 â€¢ 0:01:34[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5790, 0.4210]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 116/203 [2m0:02:06 â€¢ 0:01:34[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5807, 0.4193]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 116/203 [2m0:02:06 â€¢ 0:01:34[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5824, 0.4176]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 116/203 [2m0:02:06 â€¢ 0:01:34[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5842, 0.4158]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 116/203 [2m0:02:06 â€¢ 0:01:34[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5860, 0.4140]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 116/203 [2m0:02:06 â€¢ 0:01:34[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5879, 0.4121]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 116/203 [2m0:02:06 â€¢ 0:01:34[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5897, 0.4103]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 116/203 [2m0:02:06 â€¢ 0:01:34[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5915, 0.4085]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 116/203 [2m0:02:06 â€¢ 0:01:34[0m [2;4m0.93it/s[0m  
[2KStep 10 - TTA Loss: 0.7027â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 116/203 [2m0:02:06 â€¢ 0:01:34[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5933, 0.4067]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 116/203 [2m0:02:06 â€¢ 0:01:34[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5950, 0.4050]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 116/203 [2m0:02:06 â€¢ 0:01:34[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5967, 0.4033]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 116/203 [2m0:02:06 â€¢ 0:01:34[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5982, 0.4018]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 116/203 [2m0:02:06 â€¢ 0:01:34[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5997, 0.4003]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 116/203 [2m0:02:06 â€¢ 0:01:34[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6011, 0.3989]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 116/203 [2m0:02:06 â€¢ 0:01:34[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6024, 0.3976]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 116/203 [2m0:02:06 â€¢ 0:01:34[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6036, 0.3964]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 116/203 [2m0:02:06 â€¢ 0:01:34[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6046, 0.3954]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 116/203 [2m0:02:06 â€¢ 0:01:34[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6055, 0.3945]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 116/203 [2m0:02:06 â€¢ 0:01:34[0m [2;4m0.93it/s[0m  
[2KStep 20 - TTA Loss: 0.6722â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 116/203 [2m0:02:06 â€¢ 0:01:34[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6062, 0.3938]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 116/203 [2m0:02:06 â€¢ 0:01:34[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6069, 0.3931]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 116/203 [2m0:02:06 â€¢ 0:01:34[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6074, 0.3926]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 116/203 [2m0:02:06 â€¢ 0:01:34[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6078, 0.3922]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 116/203 [2m0:02:06 â€¢ 0:01:34[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6081, 0.3919]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 116/203 [2m0:02:06 â€¢ 0:01:34[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6083, 0.3917]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 116/203 [2m0:02:06 â€¢ 0:01:34[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6085, 0.3915]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 116/203 [2m0:02:06 â€¢ 0:01:34[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6086, 0.3914]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 116/203 [2m0:02:06 â€¢ 0:01:34[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6086, 0.3914]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 116/203 [2m0:02:06 â€¢ 0:01:34[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5783, 0.4217]], device='cuda:0')â•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 116/203 [2m0:02:06 â€¢ 0:01:34[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6602, 0.3398],0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 116/203 [2m0:02:06 â€¢ 0:01:34[0m [2;4m0.93it/s[0m  
        [0.6166, 0.3834],
        [0.6158, 0.3842],
        [0.6709, 0.3291],
        [0.6395, 0.3605],
        [0.6490, 0.3510],
        [0.6086, 0.3914],
        [0.6781, 0.3219],
        [0.6838, 0.3162],
[2K        [0.6848, 0.3152]], device='cuda:0')[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 116/203 [2m0:02:06 â€¢ 0:01:34[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5810, 0.4190],â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 117/203 [2m0:02:07 â€¢ 0:01:32[0m [2;4m0.94it/s[0m   
        [0.5753, 0.4247],
        [0.5808, 0.4192],
        [0.5795, 0.4205],
        [0.5813, 0.4187],
        [0.5791, 0.4209],
        [0.5769, 0.4231],
        [0.5807, 0.4193],
        [0.5809, 0.4191],
[2K        [0.5826, 0.4174]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 117/203 [2m0:02:07 â€¢ 0:01:32[0m [2;4m0.94it/s[0m  
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 117/203 [2m0:02:07 â€¢ 0:01:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6201, 0.3799]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 117/203 [2m0:02:07 â€¢ 0:01:32[0m [2;4m0.94it/s[0m  
[2KStep 0 - TTA Loss: 0.9340â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 117/203 [2m0:02:07 â€¢ 0:01:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6201, 0.3799]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 117/203 [2m0:02:07 â€¢ 0:01:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6201, 0.3799]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 117/203 [2m0:02:07 â€¢ 0:01:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6201, 0.3799]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 117/203 [2m0:02:07 â€¢ 0:01:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6202, 0.3798]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 117/203 [2m0:02:07 â€¢ 0:01:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6203, 0.3797]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 117/203 [2m0:02:07 â€¢ 0:01:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6205, 0.3795]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 117/203 [2m0:02:07 â€¢ 0:01:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6208, 0.3792]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 117/203 [2m0:02:07 â€¢ 0:01:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6211, 0.3789]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 117/203 [2m0:02:07 â€¢ 0:01:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6216, 0.3784]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 117/203 [2m0:02:07 â€¢ 0:01:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6222, 0.3778]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 117/203 [2m0:02:07 â€¢ 0:01:32[0m [2;4m0.94it/s[0m  
[2KStep 10 - TTA Loss: 0.8997â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 117/203 [2m0:02:07 â€¢ 0:01:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6229, 0.3771]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 117/203 [2m0:02:07 â€¢ 0:01:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6238, 0.3762]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 117/203 [2m0:02:07 â€¢ 0:01:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6248, 0.3752]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 117/203 [2m0:02:07 â€¢ 0:01:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6259, 0.3741]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 117/203 [2m0:02:07 â€¢ 0:01:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6272, 0.3728]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 117/203 [2m0:02:07 â€¢ 0:01:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6286, 0.3714]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 117/203 [2m0:02:07 â€¢ 0:01:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6303, 0.3697]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 117/203 [2m0:02:07 â€¢ 0:01:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6320, 0.3680]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 117/203 [2m0:02:07 â€¢ 0:01:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6340, 0.3660]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 117/203 [2m0:02:07 â€¢ 0:01:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6361, 0.3639]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 117/203 [2m0:02:07 â€¢ 0:01:32[0m [2;4m0.94it/s[0m  
[2KStep 20 - TTA Loss: 0.8503â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 117/203 [2m0:02:07 â€¢ 0:01:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6384, 0.3616]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 117/203 [2m0:02:07 â€¢ 0:01:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6408, 0.3592]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 117/203 [2m0:02:07 â€¢ 0:01:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6434, 0.3566]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 117/203 [2m0:02:07 â€¢ 0:01:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6460, 0.3540]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 117/203 [2m0:02:07 â€¢ 0:01:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6487, 0.3513]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 117/203 [2m0:02:07 â€¢ 0:01:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6513, 0.3487]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 117/203 [2m0:02:07 â€¢ 0:01:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6542, 0.3458]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 117/203 [2m0:02:07 â€¢ 0:01:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6572, 0.3428]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 117/203 [2m0:02:07 â€¢ 0:01:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6603, 0.3397]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 117/203 [2m0:02:07 â€¢ 0:01:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5809, 0.4191]], device='cuda:0')m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 117/203 [2m0:02:07 â€¢ 0:01:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6668, 0.3332],[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 117/203 [2m0:02:07 â€¢ 0:01:32[0m [2;4m0.94it/s[0m  
        [0.6183, 0.3817],
        [0.6219, 0.3781],
        [0.6756, 0.3244],
        [0.6442, 0.3558],
        [0.6635, 0.3365],
        [0.6138, 0.3862],
        [0.6864, 0.3136],
        [0.6935, 0.3065],
[2K        [0.6924, 0.3076]], device='cuda:0')[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 117/203 [2m0:02:07 â€¢ 0:01:32[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5809, 0.4191],â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 118/203 [2m0:02:08 â€¢ 0:01:31[0m [2;4m0.94it/s[0m  
        [0.5753, 0.4247],
        [0.5807, 0.4193],
        [0.5795, 0.4205],
        [0.5812, 0.4188],
        [0.5790, 0.4210],
        [0.5769, 0.4231],
        [0.5807, 0.4193],
        [0.5809, 0.4191],
[2K        [0.5825, 0.4175]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 118/203 [2m0:02:08 â€¢ 0:01:31[0m [2;4m0.94it/s[0m  
[2KClass label: Billiardsâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 118/203 [2m0:02:08 â€¢ 0:01:31[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6017, 0.3983]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 118/203 [2m0:02:08 â€¢ 0:01:31[0m [2;4m0.94it/s[0m  
[2KStep 0 - TTA Loss: 0.4819â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 118/203 [2m0:02:08 â€¢ 0:01:31[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6038, 0.3962]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 118/203 [2m0:02:08 â€¢ 0:01:31[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6064, 0.3936]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 118/203 [2m0:02:08 â€¢ 0:01:31[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6094, 0.3906]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 118/203 [2m0:02:08 â€¢ 0:01:31[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6127, 0.3873]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 118/203 [2m0:02:08 â€¢ 0:01:31[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6164, 0.3836]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 118/203 [2m0:02:08 â€¢ 0:01:31[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6202, 0.3798]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 118/203 [2m0:02:08 â€¢ 0:01:31[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6242, 0.3758]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 118/203 [2m0:02:08 â€¢ 0:01:31[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6282, 0.3718]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 118/203 [2m0:02:08 â€¢ 0:01:31[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6323, 0.3677]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 118/203 [2m0:02:08 â€¢ 0:01:31[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6363, 0.3637]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 118/203 [2m0:02:08 â€¢ 0:01:31[0m [2;4m0.94it/s[0m  
[2KStep 10 - TTA Loss: 0.4675â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 118/203 [2m0:02:08 â€¢ 0:01:31[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6402, 0.3598]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 118/203 [2m0:02:08 â€¢ 0:01:31[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6440, 0.3560]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 118/203 [2m0:02:08 â€¢ 0:01:31[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6476, 0.3524]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 118/203 [2m0:02:08 â€¢ 0:01:31[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6510, 0.3490]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 118/203 [2m0:02:08 â€¢ 0:01:31[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6542, 0.3458]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 118/203 [2m0:02:08 â€¢ 0:01:31[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6571, 0.3429]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 118/203 [2m0:02:08 â€¢ 0:01:31[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6598, 0.3402]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 118/203 [2m0:02:08 â€¢ 0:01:31[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6622, 0.3378]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 118/203 [2m0:02:08 â€¢ 0:01:31[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6643, 0.3357]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 118/203 [2m0:02:08 â€¢ 0:01:31[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6661, 0.3339]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 118/203 [2m0:02:08 â€¢ 0:01:31[0m [2;4m0.94it/s[0m  
[2KStep 20 - TTA Loss: 0.4342â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 118/203 [2m0:02:08 â€¢ 0:01:31[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6676, 0.3324]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 118/203 [2m0:02:08 â€¢ 0:01:31[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6689, 0.3311]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 118/203 [2m0:02:08 â€¢ 0:01:31[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6700, 0.3300]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 118/203 [2m0:02:08 â€¢ 0:01:31[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6708, 0.3292]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 118/203 [2m0:02:08 â€¢ 0:01:31[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6714, 0.3286]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 118/203 [2m0:02:08 â€¢ 0:01:31[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6718, 0.3282]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 118/203 [2m0:02:08 â€¢ 0:01:31[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6721, 0.3279]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 118/203 [2m0:02:08 â€¢ 0:01:31[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6723, 0.3277]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 118/203 [2m0:02:08 â€¢ 0:01:31[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6724, 0.3276]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 118/203 [2m0:02:08 â€¢ 0:01:31[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5783, 0.4217]], device='cuda:0')m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 118/203 [2m0:02:08 â€¢ 0:01:31[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6659, 0.3341],[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 118/203 [2m0:02:08 â€¢ 0:01:31[0m [2;4m0.94it/s[0m  
        [0.6724, 0.3276],
        [0.6227, 0.3773],
        [0.6779, 0.3221],
        [0.6428, 0.3572],
        [0.6580, 0.3420],
        [0.6085, 0.3915],
        [0.6905, 0.3095],
        [0.6997, 0.3003],
[2K        [0.6917, 0.3083]], device='cuda:0')[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 118/203 [2m0:02:08 â€¢ 0:01:31[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5809, 0.4191],â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 119/203 [2m0:02:09 â€¢ 0:01:29[0m [2;4m0.94it/s[0m  
        [0.5755, 0.4245],
        [0.5808, 0.4192],
        [0.5795, 0.4205],
        [0.5812, 0.4188],
        [0.5790, 0.4210],
        [0.5769, 0.4231],
        [0.5807, 0.4193],
        [0.5809, 0.4191],
[2K        [0.5825, 0.4175]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 119/203 [2m0:02:09 â€¢ 0:01:29[0m [2;4m0.94it/s[0m  
[2KClass label: HammerThrowâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 119/203 [2m0:02:09 â€¢ 0:01:29[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6068, 0.3932]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 119/203 [2m0:02:09 â€¢ 0:01:29[0m [2;4m0.94it/s[0m  
[2KStep 0 - TTA Loss: 0.6065â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 119/203 [2m0:02:09 â€¢ 0:01:29[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6068, 0.3932]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 119/203 [2m0:02:09 â€¢ 0:01:29[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6068, 0.3932]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 119/203 [2m0:02:09 â€¢ 0:01:29[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6068, 0.3932]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 119/203 [2m0:02:09 â€¢ 0:01:29[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6069, 0.3931]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 119/203 [2m0:02:09 â€¢ 0:01:29[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6070, 0.3930]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 119/203 [2m0:02:09 â€¢ 0:01:29[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6073, 0.3927]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 119/203 [2m0:02:09 â€¢ 0:01:29[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6076, 0.3924]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 119/203 [2m0:02:09 â€¢ 0:01:29[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6081, 0.3919]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 119/203 [2m0:02:09 â€¢ 0:01:29[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6088, 0.3912]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 119/203 [2m0:02:09 â€¢ 0:01:29[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6096, 0.3904]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 119/203 [2m0:02:09 â€¢ 0:01:29[0m [2;4m0.94it/s[0m  
[2KStep 10 - TTA Loss: 0.5666â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 119/203 [2m0:02:09 â€¢ 0:01:29[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6107, 0.3893]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 119/203 [2m0:02:09 â€¢ 0:01:29[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6120, 0.3880]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 119/203 [2m0:02:09 â€¢ 0:01:29[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6135, 0.3865]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 119/203 [2m0:02:09 â€¢ 0:01:29[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6153, 0.3847]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 119/203 [2m0:02:09 â€¢ 0:01:29[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6173, 0.3827]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 119/203 [2m0:02:09 â€¢ 0:01:29[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6196, 0.3804]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 119/203 [2m0:02:09 â€¢ 0:01:29[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6222, 0.3778]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 119/203 [2m0:02:09 â€¢ 0:01:29[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6251, 0.3749]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 119/203 [2m0:02:09 â€¢ 0:01:29[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6282, 0.3718]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 119/203 [2m0:02:09 â€¢ 0:01:29[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6317, 0.3683]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 119/203 [2m0:02:09 â€¢ 0:01:29[0m [2;4m0.94it/s[0m  
[2KStep 20 - TTA Loss: 0.6473â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 119/203 [2m0:02:09 â€¢ 0:01:29[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6353, 0.3647]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 119/203 [2m0:02:09 â€¢ 0:01:29[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6393, 0.3607]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 119/203 [2m0:02:09 â€¢ 0:01:29[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6436, 0.3564]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 119/203 [2m0:02:09 â€¢ 0:01:29[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6481, 0.3519]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 119/203 [2m0:02:09 â€¢ 0:01:29[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6529, 0.3471]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 119/203 [2m0:02:09 â€¢ 0:01:29[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6578, 0.3422]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 119/203 [2m0:02:09 â€¢ 0:01:29[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6630, 0.3370]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 119/203 [2m0:02:09 â€¢ 0:01:29[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6683, 0.3317]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 119/203 [2m0:02:09 â€¢ 0:01:29[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6738, 0.3262]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 119/203 [2m0:02:09 â€¢ 0:01:29[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5845, 0.4155]], device='cuda:0')m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 119/203 [2m0:02:09 â€¢ 0:01:29[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6873, 0.3127],[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 119/203 [2m0:02:09 â€¢ 0:01:29[0m [2;4m0.94it/s[0m  
        [0.6395, 0.3605],
        [0.6377, 0.3623],
        [0.6941, 0.3059],
        [0.6794, 0.3206],
        [0.6735, 0.3265],
        [0.6253, 0.3747],
        [0.6993, 0.3007],
        [0.7065, 0.2935],
[2K        [0.7142, 0.2858]], device='cuda:0')[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 119/203 [2m0:02:09 â€¢ 0:01:29[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5812, 0.4188],â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 120/203 [2m0:02:10 â€¢ 0:01:30[0m [2;4m0.93it/s[0m  
        [0.5757, 0.4243],
        [0.5811, 0.4189],
        [0.5798, 0.4202],
        [0.5816, 0.4184],
        [0.5793, 0.4207],
        [0.5772, 0.4228],
        [0.5810, 0.4190],
        [0.5812, 0.4188],
[2K        [0.5827, 0.4173]], device='cuda:0', grad_fn=<SoftmaxBackward0>)8;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 120/203 [2m0:02:10 â€¢ 0:01:30[0m [2;4m0.93it/s[0m  
[2KClass label: SoccerPenaltyâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 120/203 [2m0:02:10 â€¢ 0:01:30[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6575, 0.3425]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 120/203 [2m0:02:10 â€¢ 0:01:30[0m [2;4m0.93it/s[0m  
[2KStep 0 - TTA Loss: 1.0734â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 120/203 [2m0:02:10 â€¢ 0:01:30[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6613, 0.3387]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 120/203 [2m0:02:10 â€¢ 0:01:30[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6654, 0.3346]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 120/203 [2m0:02:10 â€¢ 0:01:30[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6693, 0.3307]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 120/203 [2m0:02:10 â€¢ 0:01:30[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6731, 0.3269]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 120/203 [2m0:02:10 â€¢ 0:01:30[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6772, 0.3228]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 120/203 [2m0:02:10 â€¢ 0:01:30[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6809, 0.3191]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 120/203 [2m0:02:10 â€¢ 0:01:30[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6847, 0.3153]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 120/203 [2m0:02:10 â€¢ 0:01:30[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6883, 0.3117]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 120/203 [2m0:02:10 â€¢ 0:01:30[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6919, 0.3081]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 120/203 [2m0:02:10 â€¢ 0:01:30[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6954, 0.3046]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 120/203 [2m0:02:10 â€¢ 0:01:30[0m [2;4m0.93it/s[0m  
[2KStep 10 - TTA Loss: 1.0440â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 120/203 [2m0:02:10 â€¢ 0:01:30[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6985, 0.3015]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 120/203 [2m0:02:10 â€¢ 0:01:30[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7013, 0.2987]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 120/203 [2m0:02:10 â€¢ 0:01:30[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7037, 0.2963]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 120/203 [2m0:02:10 â€¢ 0:01:30[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7061, 0.2939]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 120/203 [2m0:02:10 â€¢ 0:01:30[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7084, 0.2916]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 120/203 [2m0:02:10 â€¢ 0:01:30[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7104, 0.2896]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 120/203 [2m0:02:10 â€¢ 0:01:30[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7124, 0.2876]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 120/203 [2m0:02:10 â€¢ 0:01:30[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7142, 0.2858]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 120/203 [2m0:02:10 â€¢ 0:01:30[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7157, 0.2843]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 120/203 [2m0:02:10 â€¢ 0:01:30[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7171, 0.2829]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 120/203 [2m0:02:10 â€¢ 0:01:30[0m [2;4m0.93it/s[0m  
[2KStep 20 - TTA Loss: 1.0196â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 120/203 [2m0:02:10 â€¢ 0:01:30[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7183, 0.2817]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 120/203 [2m0:02:10 â€¢ 0:01:30[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7193, 0.2807]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 120/203 [2m0:02:10 â€¢ 0:01:30[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7202, 0.2798]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 120/203 [2m0:02:10 â€¢ 0:01:30[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7208, 0.2792]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 120/203 [2m0:02:10 â€¢ 0:01:30[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7213, 0.2787]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 120/203 [2m0:02:10 â€¢ 0:01:30[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7216, 0.2784]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 120/203 [2m0:02:10 â€¢ 0:01:30[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7219, 0.2781]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 120/203 [2m0:02:10 â€¢ 0:01:30[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7220, 0.2780]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 120/203 [2m0:02:10 â€¢ 0:01:30[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7221, 0.2779]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 120/203 [2m0:02:10 â€¢ 0:01:30[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5840, 0.4160]], device='cuda:0')mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 120/203 [2m0:02:10 â€¢ 0:01:30[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6825, 0.3175],[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 120/203 [2m0:02:10 â€¢ 0:01:30[0m [2;4m0.93it/s[0m  
        [0.6352, 0.3648],
        [0.6296, 0.3704],
        [0.6880, 0.3120],
        [0.6685, 0.3315],
        [0.6694, 0.3306],
        [0.6187, 0.3813],
        [0.7221, 0.2779],
        [0.7037, 0.2963],
[2K        [0.7064, 0.2936]], device='cuda:0')[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 120/203 [2m0:02:10 â€¢ 0:01:30[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5813, 0.4187],â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 121/203 [2m0:02:11 â€¢ 0:01:28[0m [2;4m0.93it/s[0m  
        [0.5759, 0.4241],
        [0.5812, 0.4188],
        [0.5799, 0.4201],
        [0.5818, 0.4182],
        [0.5795, 0.4205],
        [0.5774, 0.4226],
        [0.5813, 0.4187],
        [0.5814, 0.4186],
[2K        [0.5829, 0.4171]], device='cuda:0', grad_fn=<SoftmaxBackward0>)8;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 121/203 [2m0:02:11 â€¢ 0:01:28[0m [2;4m0.93it/s[0m  
[2KClass label: SoccerPenaltyâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 121/203 [2m0:02:11 â€¢ 0:01:28[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6562, 0.3438]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 121/203 [2m0:02:11 â€¢ 0:01:28[0m [2;4m0.93it/s[0m  
[2KStep 0 - TTA Loss: 0.9928â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 121/203 [2m0:02:11 â€¢ 0:01:28[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6562, 0.3438]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 121/203 [2m0:02:11 â€¢ 0:01:28[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6562, 0.3438]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 121/203 [2m0:02:11 â€¢ 0:01:28[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6562, 0.3438]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 121/203 [2m0:02:11 â€¢ 0:01:28[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6564, 0.3436]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 121/203 [2m0:02:11 â€¢ 0:01:28[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6566, 0.3434]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 121/203 [2m0:02:11 â€¢ 0:01:28[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6569, 0.3431]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 121/203 [2m0:02:11 â€¢ 0:01:28[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6573, 0.3427]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 121/203 [2m0:02:11 â€¢ 0:01:28[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6579, 0.3421]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 121/203 [2m0:02:11 â€¢ 0:01:28[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6587, 0.3413]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 121/203 [2m0:02:11 â€¢ 0:01:28[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6596, 0.3404]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 121/203 [2m0:02:11 â€¢ 0:01:28[0m [2;4m0.93it/s[0m  
[2KStep 10 - TTA Loss: 1.1047â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 121/203 [2m0:02:11 â€¢ 0:01:28[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6606, 0.3394]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 121/203 [2m0:02:11 â€¢ 0:01:28[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6617, 0.3383]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 121/203 [2m0:02:11 â€¢ 0:01:28[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6629, 0.3371]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 121/203 [2m0:02:11 â€¢ 0:01:28[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6644, 0.3356]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 121/203 [2m0:02:11 â€¢ 0:01:28[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6661, 0.3339]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 121/203 [2m0:02:11 â€¢ 0:01:28[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6680, 0.3320]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 121/203 [2m0:02:11 â€¢ 0:01:28[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6700, 0.3300]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 121/203 [2m0:02:11 â€¢ 0:01:28[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6721, 0.3279]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 121/203 [2m0:02:11 â€¢ 0:01:28[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6743, 0.3257]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 121/203 [2m0:02:11 â€¢ 0:01:28[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6768, 0.3232]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 121/203 [2m0:02:11 â€¢ 0:01:28[0m [2;4m0.93it/s[0m  
[2KStep 20 - TTA Loss: 0.9934â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 121/203 [2m0:02:11 â€¢ 0:01:28[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6794, 0.3206]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 121/203 [2m0:02:11 â€¢ 0:01:28[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6821, 0.3179]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 121/203 [2m0:02:11 â€¢ 0:01:28[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6849, 0.3151]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 121/203 [2m0:02:11 â€¢ 0:01:28[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6878, 0.3122]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 121/203 [2m0:02:11 â€¢ 0:01:28[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6906, 0.3094]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 121/203 [2m0:02:11 â€¢ 0:01:28[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6931, 0.3069]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 121/203 [2m0:02:11 â€¢ 0:01:28[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6957, 0.3043]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 121/203 [2m0:02:11 â€¢ 0:01:28[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6983, 0.3017]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 121/203 [2m0:02:11 â€¢ 0:01:28[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7008, 0.2992]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 121/203 [2m0:02:11 â€¢ 0:01:28[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5835, 0.4165]], device='cuda:0')mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 121/203 [2m0:02:11 â€¢ 0:01:28[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6613, 0.3387],[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 121/203 [2m0:02:11 â€¢ 0:01:28[0m [2;4m0.93it/s[0m  
        [0.6234, 0.3766],
        [0.6091, 0.3909],
        [0.6696, 0.3304],
        [0.6332, 0.3668],
        [0.6438, 0.3562],
        [0.5954, 0.4046],
        [0.7034, 0.2966],
        [0.6868, 0.3132],
[2K        [0.6802, 0.3198]], device='cuda:0')[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 121/203 [2m0:02:11 â€¢ 0:01:28[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5814, 0.4186],â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 122/203 [2m0:02:12 â€¢ 0:01:27[0m [2;4m0.94it/s[0m   
        [0.5761, 0.4239],
        [0.5813, 0.4187],
        [0.5800, 0.4200],
        [0.5819, 0.4181],
        [0.5795, 0.4205],
        [0.5774, 0.4226],
        [0.5816, 0.4184],
        [0.5815, 0.4185],
[2K        [0.5829, 0.4171]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 122/203 [2m0:02:12 â€¢ 0:01:27[0m [2;4m0.94it/s[0m  
[2KClass label: SoccerPenaltyâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 122/203 [2m0:02:12 â€¢ 0:01:27[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6574, 0.3426]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 122/203 [2m0:02:12 â€¢ 0:01:27[0m [2;4m0.94it/s[0m  
[2KStep 0 - TTA Loss: 0.3469â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 122/203 [2m0:02:12 â€¢ 0:01:27[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6606, 0.3394]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 122/203 [2m0:02:12 â€¢ 0:01:27[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6640, 0.3360]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 122/203 [2m0:02:12 â€¢ 0:01:27[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6677, 0.3323]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 122/203 [2m0:02:12 â€¢ 0:01:27[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6715, 0.3285]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 122/203 [2m0:02:12 â€¢ 0:01:27[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6754, 0.3246]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 122/203 [2m0:02:12 â€¢ 0:01:27[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6794, 0.3206]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 122/203 [2m0:02:12 â€¢ 0:01:27[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6834, 0.3166]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 122/203 [2m0:02:12 â€¢ 0:01:27[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6875, 0.3125]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 122/203 [2m0:02:12 â€¢ 0:01:27[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6915, 0.3085]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 122/203 [2m0:02:12 â€¢ 0:01:27[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6954, 0.3046]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 122/203 [2m0:02:12 â€¢ 0:01:27[0m [2;4m0.94it/s[0m  
[2KStep 10 - TTA Loss: 0.3139â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 122/203 [2m0:02:12 â€¢ 0:01:27[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6992, 0.3008]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 122/203 [2m0:02:12 â€¢ 0:01:27[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.7028, 0.2972]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 122/203 [2m0:02:12 â€¢ 0:01:27[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.7063, 0.2937]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 122/203 [2m0:02:12 â€¢ 0:01:27[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.7096, 0.2904]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 122/203 [2m0:02:12 â€¢ 0:01:27[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.7127, 0.2873]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 122/203 [2m0:02:12 â€¢ 0:01:27[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.7155, 0.2845]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 122/203 [2m0:02:12 â€¢ 0:01:27[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.7181, 0.2819]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 122/203 [2m0:02:12 â€¢ 0:01:27[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.7205, 0.2795]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 122/203 [2m0:02:12 â€¢ 0:01:27[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.7225, 0.2775]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 122/203 [2m0:02:12 â€¢ 0:01:27[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.7243, 0.2757]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 122/203 [2m0:02:12 â€¢ 0:01:27[0m [2;4m0.94it/s[0m  
[2KStep 20 - TTA Loss: 0.3092â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 122/203 [2m0:02:12 â€¢ 0:01:27[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.7259, 0.2741]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 122/203 [2m0:02:12 â€¢ 0:01:27[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.7272, 0.2728]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 122/203 [2m0:02:12 â€¢ 0:01:27[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.7282, 0.2718]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 122/203 [2m0:02:12 â€¢ 0:01:27[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.7290, 0.2710]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 122/203 [2m0:02:12 â€¢ 0:01:27[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.7296, 0.2704]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 122/203 [2m0:02:12 â€¢ 0:01:27[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.7301, 0.2699]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 122/203 [2m0:02:12 â€¢ 0:01:27[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.7303, 0.2697]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 122/203 [2m0:02:12 â€¢ 0:01:27[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.7305, 0.2695]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 122/203 [2m0:02:12 â€¢ 0:01:27[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.7306, 0.2694]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 122/203 [2m0:02:12 â€¢ 0:01:27[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5846, 0.4154]], device='cuda:0')0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 122/203 [2m0:02:12 â€¢ 0:01:27[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6741, 0.3259],[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 122/203 [2m0:02:12 â€¢ 0:01:27[0m [2;4m0.94it/s[0m  
        [0.6311, 0.3689],
        [0.6147, 0.3853],
        [0.6812, 0.3188],
        [0.6482, 0.3518],
        [0.6584, 0.3416],
        [0.6103, 0.3897],
        [0.7306, 0.2694],
        [0.6982, 0.3018],
[2K        [0.6922, 0.3078]], device='cuda:0')â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 122/203 [2m0:02:12 â€¢ 0:01:27[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5814, 0.4186],â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 123/203 [2m0:02:13 â€¢ 0:01:26[0m [2;4m0.93it/s[0m  
        [0.5760, 0.4240],
        [0.5813, 0.4187],
        [0.5800, 0.4200],
        [0.5818, 0.4182],
        [0.5794, 0.4206],
        [0.5774, 0.4226],
        [0.5816, 0.4184],
        [0.5814, 0.4186],
[2K        [0.5828, 0.4172]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 123/203 [2m0:02:13 â€¢ 0:01:26[0m [2;4m0.93it/s[0m  
[2KClass label: TennisSwingâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 123/203 [2m0:02:13 â€¢ 0:01:26[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6651, 0.3349]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 123/203 [2m0:02:13 â€¢ 0:01:26[0m [2;4m0.93it/s[0m  
[2KStep 0 - TTA Loss: 0.4936â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 123/203 [2m0:02:13 â€¢ 0:01:26[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6651, 0.3349]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 123/203 [2m0:02:13 â€¢ 0:01:26[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6651, 0.3349]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 123/203 [2m0:02:13 â€¢ 0:01:26[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6652, 0.3348]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 123/203 [2m0:02:13 â€¢ 0:01:26[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6653, 0.3347]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 123/203 [2m0:02:13 â€¢ 0:01:26[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6655, 0.3345]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 123/203 [2m0:02:13 â€¢ 0:01:26[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6658, 0.3342]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 123/203 [2m0:02:13 â€¢ 0:01:26[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6662, 0.3338]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 123/203 [2m0:02:13 â€¢ 0:01:26[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6668, 0.3332]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 123/203 [2m0:02:13 â€¢ 0:01:26[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6676, 0.3324]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 123/203 [2m0:02:13 â€¢ 0:01:26[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6687, 0.3313]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 123/203 [2m0:02:13 â€¢ 0:01:26[0m [2;4m0.93it/s[0m  
[2KStep 10 - TTA Loss: 0.5208â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 123/203 [2m0:02:13 â€¢ 0:01:26[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6700, 0.3300]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 123/203 [2m0:02:13 â€¢ 0:01:26[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6716, 0.3284]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 123/203 [2m0:02:13 â€¢ 0:01:26[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6734, 0.3266]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 123/203 [2m0:02:13 â€¢ 0:01:26[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6756, 0.3244]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 123/203 [2m0:02:13 â€¢ 0:01:26[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6780, 0.3220]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 123/203 [2m0:02:13 â€¢ 0:01:26[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6807, 0.3193]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 123/203 [2m0:02:13 â€¢ 0:01:26[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6836, 0.3164]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 123/203 [2m0:02:13 â€¢ 0:01:26[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6867, 0.3133]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 123/203 [2m0:02:13 â€¢ 0:01:26[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6901, 0.3099]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 123/203 [2m0:02:13 â€¢ 0:01:26[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6938, 0.3062]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 123/203 [2m0:02:13 â€¢ 0:01:26[0m [2;4m0.93it/s[0m  
[2KStep 20 - TTA Loss: 0.4503â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 123/203 [2m0:02:13 â€¢ 0:01:26[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6976, 0.3024]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 123/203 [2m0:02:13 â€¢ 0:01:26[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7017, 0.2983]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 123/203 [2m0:02:13 â€¢ 0:01:26[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7061, 0.2939]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 123/203 [2m0:02:13 â€¢ 0:01:26[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7106, 0.2894]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 123/203 [2m0:02:13 â€¢ 0:01:26[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7153, 0.2847]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 123/203 [2m0:02:13 â€¢ 0:01:26[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7201, 0.2799]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 123/203 [2m0:02:13 â€¢ 0:01:26[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7251, 0.2749]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 123/203 [2m0:02:13 â€¢ 0:01:26[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7302, 0.2698]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 123/203 [2m0:02:13 â€¢ 0:01:26[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7354, 0.2646]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 123/203 [2m0:02:13 â€¢ 0:01:26[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5848, 0.4152]], device='cuda:0')0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 123/203 [2m0:02:13 â€¢ 0:01:26[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6868, 0.3132],[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 123/203 [2m0:02:13 â€¢ 0:01:26[0m [2;4m0.93it/s[0m  
        [0.6394, 0.3606],
        [0.6278, 0.3722],
        [0.7024, 0.2976],
        [0.6551, 0.3449],
        [0.6665, 0.3335],
        [0.6081, 0.3919],
        [0.7017, 0.2983],
        [0.7407, 0.2593],
[2K        [0.7080, 0.2920]], device='cuda:0')â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 123/203 [2m0:02:13 â€¢ 0:01:26[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5814, 0.4186],â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 124/203 [2m0:02:14 â€¢ 0:01:25[0m [2;4m0.93it/s[0m  
        [0.5761, 0.4239],
        [0.5813, 0.4187],
        [0.5801, 0.4199],
        [0.5818, 0.4182],
        [0.5794, 0.4206],
        [0.5774, 0.4226],
        [0.5816, 0.4184],
        [0.5816, 0.4184],
[2K        [0.5829, 0.4171]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 124/203 [2m0:02:14 â€¢ 0:01:25[0m [2;4m0.93it/s[0m  
[2KClass label: SoccerPenaltyâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 124/203 [2m0:02:14 â€¢ 0:01:25[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6604, 0.3396]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 124/203 [2m0:02:14 â€¢ 0:01:25[0m [2;4m0.93it/s[0m  
[2KStep 0 - TTA Loss: 0.8072â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 124/203 [2m0:02:14 â€¢ 0:01:25[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6634, 0.3366]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 124/203 [2m0:02:14 â€¢ 0:01:25[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6666, 0.3334]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 124/203 [2m0:02:14 â€¢ 0:01:25[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6700, 0.3300]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 124/203 [2m0:02:14 â€¢ 0:01:25[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6734, 0.3266]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 124/203 [2m0:02:14 â€¢ 0:01:25[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6769, 0.3231]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 124/203 [2m0:02:14 â€¢ 0:01:25[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6804, 0.3196]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 124/203 [2m0:02:14 â€¢ 0:01:25[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6840, 0.3160]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 124/203 [2m0:02:14 â€¢ 0:01:25[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6875, 0.3125]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 124/203 [2m0:02:14 â€¢ 0:01:25[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6910, 0.3090]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 124/203 [2m0:02:14 â€¢ 0:01:25[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6944, 0.3056]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 124/203 [2m0:02:14 â€¢ 0:01:25[0m [2;4m0.93it/s[0m  
[2KStep 10 - TTA Loss: 0.7163â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 124/203 [2m0:02:14 â€¢ 0:01:25[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6976, 0.3024]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 124/203 [2m0:02:14 â€¢ 0:01:25[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7007, 0.2993]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 124/203 [2m0:02:14 â€¢ 0:01:25[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7037, 0.2963]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 124/203 [2m0:02:14 â€¢ 0:01:25[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7065, 0.2935]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 124/203 [2m0:02:14 â€¢ 0:01:25[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7092, 0.2908]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 124/203 [2m0:02:14 â€¢ 0:01:25[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7116, 0.2884]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 124/203 [2m0:02:14 â€¢ 0:01:25[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7138, 0.2862]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 124/203 [2m0:02:14 â€¢ 0:01:25[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7157, 0.2843]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 124/203 [2m0:02:14 â€¢ 0:01:25[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7175, 0.2825]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 124/203 [2m0:02:14 â€¢ 0:01:25[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7190, 0.2810]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 124/203 [2m0:02:14 â€¢ 0:01:25[0m [2;4m0.93it/s[0m  
[2KStep 20 - TTA Loss: 0.7749â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 124/203 [2m0:02:14 â€¢ 0:01:25[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7203, 0.2797]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 124/203 [2m0:02:14 â€¢ 0:01:25[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7214, 0.2786]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 124/203 [2m0:02:14 â€¢ 0:01:25[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7223, 0.2777]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 124/203 [2m0:02:14 â€¢ 0:01:25[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7231, 0.2769]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 124/203 [2m0:02:14 â€¢ 0:01:25[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7236, 0.2764]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 124/203 [2m0:02:14 â€¢ 0:01:25[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7240, 0.2760]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 124/203 [2m0:02:14 â€¢ 0:01:25[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7242, 0.2758]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 124/203 [2m0:02:14 â€¢ 0:01:25[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7244, 0.2756]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 124/203 [2m0:02:14 â€¢ 0:01:25[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7244, 0.2756]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 124/203 [2m0:02:14 â€¢ 0:01:25[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5844, 0.4156]], device='cuda:0')0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 124/203 [2m0:02:14 â€¢ 0:01:25[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6844, 0.3156],[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 124/203 [2m0:02:14 â€¢ 0:01:25[0m [2;4m0.93it/s[0m  
        [0.6409, 0.3591],
        [0.6242, 0.3758],
        [0.6937, 0.3063],
        [0.6563, 0.3437],
        [0.6659, 0.3341],
        [0.6135, 0.3865],
        [0.7245, 0.2755],
        [0.7233, 0.2767],
[2K        [0.7035, 0.2965]], device='cuda:0')â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 124/203 [2m0:02:14 â€¢ 0:01:25[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5813, 0.4187],â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 125/203 [2m0:02:15 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
        [0.5761, 0.4239],
        [0.5813, 0.4187],
        [0.5801, 0.4199],
        [0.5818, 0.4182],
        [0.5794, 0.4206],
        [0.5774, 0.4226],
        [0.5816, 0.4184],
        [0.5816, 0.4184],
[2K        [0.5829, 0.4171]], device='cuda:0', grad_fn=<SoftmaxBackward0>)38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 125/203 [2m0:02:15 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KClass label: Billiardsâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 125/203 [2m0:02:15 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5943, 0.4057]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 125/203 [2m0:02:15 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KStep 0 - TTA Loss: 0.5435â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 125/203 [2m0:02:15 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5943, 0.4057]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 125/203 [2m0:02:15 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5943, 0.4057]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 125/203 [2m0:02:15 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5943, 0.4057]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 125/203 [2m0:02:15 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5944, 0.4056]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 125/203 [2m0:02:15 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5946, 0.4054]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 125/203 [2m0:02:15 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5948, 0.4052]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 125/203 [2m0:02:15 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5952, 0.4048]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 125/203 [2m0:02:15 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5958, 0.4042]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 125/203 [2m0:02:15 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5965, 0.4035]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 125/203 [2m0:02:15 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5974, 0.4026]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 125/203 [2m0:02:15 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KStep 10 - TTA Loss: 0.5406â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 125/203 [2m0:02:15 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5986, 0.4014]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 125/203 [2m0:02:15 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6000, 0.4000]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 125/203 [2m0:02:15 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6017, 0.3983]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 125/203 [2m0:02:15 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6036, 0.3964]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 125/203 [2m0:02:15 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6058, 0.3942]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 125/203 [2m0:02:15 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6084, 0.3916]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 125/203 [2m0:02:15 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6112, 0.3888]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 125/203 [2m0:02:15 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6142, 0.3858]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 125/203 [2m0:02:15 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6176, 0.3824]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 125/203 [2m0:02:15 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6213, 0.3787]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 125/203 [2m0:02:15 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KStep 20 - TTA Loss: 0.5187â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 125/203 [2m0:02:15 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6253, 0.3747]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 125/203 [2m0:02:15 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6295, 0.3705]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 125/203 [2m0:02:15 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6341, 0.3659]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 125/203 [2m0:02:15 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6388, 0.3612]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 125/203 [2m0:02:15 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6438, 0.3562]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 125/203 [2m0:02:15 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6489, 0.3511]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 125/203 [2m0:02:15 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6543, 0.3457]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 125/203 [2m0:02:15 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6599, 0.3401]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 125/203 [2m0:02:15 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6655, 0.3345]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 125/203 [2m0:02:15 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5791, 0.4209]], device='cuda:0')4mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 125/203 [2m0:02:15 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6605, 0.3395],[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 125/203 [2m0:02:15 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
        [0.6714, 0.3286],
        [0.6140, 0.3860],
        [0.6729, 0.3271],
        [0.6328, 0.3672],
        [0.6429, 0.3571],
        [0.5949, 0.4051],
        [0.6900, 0.3100],
        [0.6929, 0.3071],
[2K        [0.6810, 0.3190]], device='cuda:0')â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 125/203 [2m0:02:15 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5815, 0.4185],â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 126/203 [2m0:02:16 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
        [0.5764, 0.4236],
        [0.5814, 0.4186],
        [0.5803, 0.4197],
        [0.5820, 0.4180],
        [0.5795, 0.4205],
        [0.5775, 0.4225],
        [0.5816, 0.4184],
        [0.5818, 0.4182],
[2K        [0.5830, 0.4170]], device='cuda:0', grad_fn=<SoftmaxBackward0>)38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 126/203 [2m0:02:16 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KClass label: BaseballPitchâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 126/203 [2m0:02:16 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6367, 0.3633]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 126/203 [2m0:02:16 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KStep 0 - TTA Loss: 0.6939â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 126/203 [2m0:02:16 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6391, 0.3609]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 126/203 [2m0:02:16 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6418, 0.3582]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 126/203 [2m0:02:16 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6448, 0.3552]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 126/203 [2m0:02:16 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6481, 0.3519]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 126/203 [2m0:02:16 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6516, 0.3484]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 126/203 [2m0:02:16 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6553, 0.3447]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 126/203 [2m0:02:16 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6591, 0.3409]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 126/203 [2m0:02:16 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6630, 0.3370]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 126/203 [2m0:02:16 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6670, 0.3330]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 126/203 [2m0:02:16 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6708, 0.3292]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 126/203 [2m0:02:16 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KStep 10 - TTA Loss: 0.7186â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 126/203 [2m0:02:16 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6746, 0.3254]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 126/203 [2m0:02:16 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6782, 0.3218]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 126/203 [2m0:02:16 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6817, 0.3183]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 126/203 [2m0:02:16 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6849, 0.3151]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 126/203 [2m0:02:16 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6880, 0.3120]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 126/203 [2m0:02:16 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6908, 0.3092]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 126/203 [2m0:02:16 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6934, 0.3066]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 126/203 [2m0:02:16 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6958, 0.3042]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 126/203 [2m0:02:16 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6980, 0.3020]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 126/203 [2m0:02:16 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6998, 0.3002]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 126/203 [2m0:02:16 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KStep 20 - TTA Loss: 0.7110â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 126/203 [2m0:02:16 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7014, 0.2986]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 126/203 [2m0:02:16 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7027, 0.2973]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 126/203 [2m0:02:16 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7038, 0.2962]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 126/203 [2m0:02:16 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7047, 0.2953]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 126/203 [2m0:02:16 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7053, 0.2947]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 126/203 [2m0:02:16 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7057, 0.2943]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 126/203 [2m0:02:16 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7060, 0.2940]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 126/203 [2m0:02:16 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7062, 0.2938]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 126/203 [2m0:02:16 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7063, 0.2937]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 126/203 [2m0:02:16 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5846, 0.4154]], device='cuda:0')4mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 126/203 [2m0:02:16 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7064, 0.2936],[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 126/203 [2m0:02:16 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
        [0.6628, 0.3372],
        [0.6293, 0.3707],
        [0.6998, 0.3002],
        [0.6596, 0.3404],
        [0.6655, 0.3345],
        [0.6161, 0.3839],
        [0.6984, 0.3016],
        [0.7127, 0.2873],
[2K        [0.7084, 0.2916]], device='cuda:0')â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 126/203 [2m0:02:16 â€¢ 0:01:24[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5817, 0.4183],â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 127/203 [2m0:02:17 â€¢ 0:01:23[0m [2;4m0.93it/s[0m   
        [0.5767, 0.4233],
        [0.5815, 0.4185],
        [0.5805, 0.4195],
        [0.5821, 0.4179],
        [0.5796, 0.4204],
        [0.5777, 0.4223],
        [0.5818, 0.4182],
        [0.5820, 0.4180],
[2K        [0.5832, 0.4168]], device='cuda:0', grad_fn=<SoftmaxBackward0>);237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 127/203 [2m0:02:17 â€¢ 0:01:23[0m [2;4m0.93it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 127/203 [2m0:02:17 â€¢ 0:01:23[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6609, 0.3391]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 127/203 [2m0:02:17 â€¢ 0:01:23[0m [2;4m0.93it/s[0m  
[2KStep 0 - TTA Loss: 0.8037â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 127/203 [2m0:02:17 â€¢ 0:01:23[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6609, 0.3391]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 127/203 [2m0:02:17 â€¢ 0:01:23[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6609, 0.3391]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 127/203 [2m0:02:17 â€¢ 0:01:23[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6609, 0.3391]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 127/203 [2m0:02:17 â€¢ 0:01:23[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6610, 0.3390]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 127/203 [2m0:02:17 â€¢ 0:01:23[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6612, 0.3388]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 127/203 [2m0:02:17 â€¢ 0:01:23[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6615, 0.3385]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 127/203 [2m0:02:17 â€¢ 0:01:23[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6619, 0.3381]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 127/203 [2m0:02:17 â€¢ 0:01:23[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6624, 0.3376]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 127/203 [2m0:02:17 â€¢ 0:01:23[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6630, 0.3370]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 127/203 [2m0:02:17 â€¢ 0:01:23[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6638, 0.3362]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 127/203 [2m0:02:17 â€¢ 0:01:23[0m [2;4m0.93it/s[0m  
[2KStep 10 - TTA Loss: 0.8950â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 127/203 [2m0:02:17 â€¢ 0:01:23[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6647, 0.3353]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 127/203 [2m0:02:17 â€¢ 0:01:23[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6658, 0.3342]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 127/203 [2m0:02:17 â€¢ 0:01:23[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6670, 0.3330]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 127/203 [2m0:02:17 â€¢ 0:01:23[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6685, 0.3315]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 127/203 [2m0:02:17 â€¢ 0:01:23[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6700, 0.3300]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 127/203 [2m0:02:17 â€¢ 0:01:23[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6718, 0.3282]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 127/203 [2m0:02:17 â€¢ 0:01:23[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6737, 0.3263]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 127/203 [2m0:02:17 â€¢ 0:01:23[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6759, 0.3241]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 127/203 [2m0:02:17 â€¢ 0:01:23[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6783, 0.3217]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 127/203 [2m0:02:17 â€¢ 0:01:23[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6809, 0.3191]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 127/203 [2m0:02:17 â€¢ 0:01:23[0m [2;4m0.93it/s[0m  
[2KStep 20 - TTA Loss: 0.7032â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 127/203 [2m0:02:17 â€¢ 0:01:23[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6837, 0.3163]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 127/203 [2m0:02:17 â€¢ 0:01:23[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6867, 0.3133]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 127/203 [2m0:02:17 â€¢ 0:01:23[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6898, 0.3102]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 127/203 [2m0:02:17 â€¢ 0:01:23[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6930, 0.3070]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 127/203 [2m0:02:17 â€¢ 0:01:23[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6964, 0.3036]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 127/203 [2m0:02:17 â€¢ 0:01:23[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6999, 0.3001]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 127/203 [2m0:02:17 â€¢ 0:01:23[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7035, 0.2965]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 127/203 [2m0:02:17 â€¢ 0:01:23[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7074, 0.2926]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 127/203 [2m0:02:17 â€¢ 0:01:23[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7112, 0.2888]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 127/203 [2m0:02:17 â€¢ 0:01:23[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5860, 0.4140]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 127/203 [2m0:02:17 â€¢ 0:01:23[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6923, 0.3077],â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 127/203 [2m0:02:17 â€¢ 0:01:23[0m [2;4m0.93it/s[0m  
        [0.6295, 0.3705],
        [0.6305, 0.3695],
        [0.6973, 0.3027],
        [0.6649, 0.3351],
        [0.6667, 0.3333],
        [0.6186, 0.3814],
        [0.6923, 0.3077],
        [0.7070, 0.2930],
[2K        [0.7152, 0.2848]], device='cuda:0')â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 127/203 [2m0:02:17 â€¢ 0:01:23[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5818, 0.4182],â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 128/203 [2m0:02:18 â€¢ 0:01:21[0m [2;4m0.93it/s[0m  
        [0.5767, 0.4233],
        [0.5817, 0.4183],
        [0.5806, 0.4194],
        [0.5823, 0.4177],
        [0.5798, 0.4202],
        [0.5778, 0.4222],
        [0.5819, 0.4181],
        [0.5820, 0.4180],
[2K        [0.5834, 0.4166]], device='cuda:0', grad_fn=<SoftmaxBackward0>);237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 128/203 [2m0:02:18 â€¢ 0:01:21[0m [2;4m0.93it/s[0m  
[2KClass label: Billiardsâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 128/203 [2m0:02:18 â€¢ 0:01:21[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6032, 0.3968]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 128/203 [2m0:02:18 â€¢ 0:01:21[0m [2;4m0.93it/s[0m  
[2KStep 0 - TTA Loss: 0.4085â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 128/203 [2m0:02:18 â€¢ 0:01:21[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6057, 0.3943]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 128/203 [2m0:02:18 â€¢ 0:01:21[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6086, 0.3914]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 128/203 [2m0:02:18 â€¢ 0:01:21[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6118, 0.3882]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 128/203 [2m0:02:18 â€¢ 0:01:21[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6153, 0.3847]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 128/203 [2m0:02:18 â€¢ 0:01:21[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6191, 0.3809]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 128/203 [2m0:02:18 â€¢ 0:01:21[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6230, 0.3770]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 128/203 [2m0:02:18 â€¢ 0:01:21[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6271, 0.3729]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 128/203 [2m0:02:18 â€¢ 0:01:21[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6312, 0.3688]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 128/203 [2m0:02:18 â€¢ 0:01:21[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6353, 0.3647]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 128/203 [2m0:02:18 â€¢ 0:01:21[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6394, 0.3606]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 128/203 [2m0:02:18 â€¢ 0:01:21[0m [2;4m0.93it/s[0m  
[2KStep 10 - TTA Loss: 0.3952â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 128/203 [2m0:02:18 â€¢ 0:01:21[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6434, 0.3566]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 128/203 [2m0:02:18 â€¢ 0:01:21[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6473, 0.3527]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 128/203 [2m0:02:18 â€¢ 0:01:21[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6510, 0.3490]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 128/203 [2m0:02:18 â€¢ 0:01:21[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6545, 0.3455]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 128/203 [2m0:02:18 â€¢ 0:01:21[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6578, 0.3422]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 128/203 [2m0:02:18 â€¢ 0:01:21[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6608, 0.3392]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 128/203 [2m0:02:18 â€¢ 0:01:21[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6636, 0.3364]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 128/203 [2m0:02:18 â€¢ 0:01:21[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6661, 0.3339]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 128/203 [2m0:02:18 â€¢ 0:01:21[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6683, 0.3317]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 128/203 [2m0:02:18 â€¢ 0:01:21[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6702, 0.3298]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 128/203 [2m0:02:18 â€¢ 0:01:21[0m [2;4m0.93it/s[0m  
[2KStep 20 - TTA Loss: 0.3801â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 128/203 [2m0:02:18 â€¢ 0:01:21[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6718, 0.3282]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 128/203 [2m0:02:18 â€¢ 0:01:21[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6732, 0.3268]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 128/203 [2m0:02:18 â€¢ 0:01:21[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6743, 0.3257]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 128/203 [2m0:02:18 â€¢ 0:01:21[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6752, 0.3248]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 128/203 [2m0:02:18 â€¢ 0:01:21[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6759, 0.3241]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 128/203 [2m0:02:18 â€¢ 0:01:21[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6763, 0.3237]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 128/203 [2m0:02:18 â€¢ 0:01:21[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6767, 0.3233]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 128/203 [2m0:02:18 â€¢ 0:01:21[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6768, 0.3232]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 128/203 [2m0:02:18 â€¢ 0:01:21[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6769, 0.3231]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 128/203 [2m0:02:18 â€¢ 0:01:21[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5799, 0.4201]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 128/203 [2m0:02:18 â€¢ 0:01:21[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6822, 0.3178],â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 128/203 [2m0:02:18 â€¢ 0:01:21[0m [2;4m0.93it/s[0m  
        [0.6770, 0.3230],
        [0.6329, 0.3671],
        [0.6914, 0.3086],
        [0.6563, 0.3437],
        [0.6640, 0.3360],
        [0.6153, 0.3847],
        [0.6983, 0.3017],
        [0.7086, 0.2914],
[2K        [0.7057, 0.2943]], device='cuda:0')â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 128/203 [2m0:02:18 â€¢ 0:01:21[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5819, 0.4181],â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 129/203 [2m0:02:19 â€¢ 0:01:20[0m [2;4m0.93it/s[0m  
        [0.5770, 0.4230],
        [0.5819, 0.4181],
        [0.5807, 0.4193],
        [0.5825, 0.4175],
        [0.5799, 0.4201],
        [0.5780, 0.4220],
        [0.5820, 0.4180],
        [0.5821, 0.4179],
[2K        [0.5836, 0.4164]], device='cuda:0', grad_fn=<SoftmaxBackward0>);237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 129/203 [2m0:02:19 â€¢ 0:01:20[0m [2;4m0.93it/s[0m  
[2KClass label: Billiardsâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 129/203 [2m0:02:19 â€¢ 0:01:20[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6044, 0.3956]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 129/203 [2m0:02:19 â€¢ 0:01:20[0m [2;4m0.93it/s[0m  
[2KStep 0 - TTA Loss: 0.5191â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 129/203 [2m0:02:19 â€¢ 0:01:20[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6044, 0.3956]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 129/203 [2m0:02:19 â€¢ 0:01:20[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6045, 0.3955]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 129/203 [2m0:02:19 â€¢ 0:01:20[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6045, 0.3955]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 129/203 [2m0:02:19 â€¢ 0:01:20[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6047, 0.3953]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 129/203 [2m0:02:19 â€¢ 0:01:20[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6050, 0.3950]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 129/203 [2m0:02:19 â€¢ 0:01:20[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6054, 0.3946]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 129/203 [2m0:02:19 â€¢ 0:01:20[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6060, 0.3940]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 129/203 [2m0:02:19 â€¢ 0:01:20[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6068, 0.3932]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 129/203 [2m0:02:19 â€¢ 0:01:20[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6079, 0.3921]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 129/203 [2m0:02:19 â€¢ 0:01:20[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6091, 0.3909]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 129/203 [2m0:02:19 â€¢ 0:01:20[0m [2;4m0.93it/s[0m  
[2KStep 10 - TTA Loss: 0.5185â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 129/203 [2m0:02:19 â€¢ 0:01:20[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6106, 0.3894]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 129/203 [2m0:02:19 â€¢ 0:01:20[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6124, 0.3876]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 129/203 [2m0:02:19 â€¢ 0:01:20[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6144, 0.3856]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 129/203 [2m0:02:19 â€¢ 0:01:20[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6167, 0.3833]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 129/203 [2m0:02:19 â€¢ 0:01:20[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6193, 0.3807]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 129/203 [2m0:02:19 â€¢ 0:01:20[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6221, 0.3779]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 129/203 [2m0:02:19 â€¢ 0:01:20[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6253, 0.3747]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 129/203 [2m0:02:19 â€¢ 0:01:20[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6287, 0.3713]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 129/203 [2m0:02:19 â€¢ 0:01:20[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6325, 0.3675]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 129/203 [2m0:02:19 â€¢ 0:01:20[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6365, 0.3635]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 129/203 [2m0:02:19 â€¢ 0:01:20[0m [2;4m0.93it/s[0m  
[2KStep 20 - TTA Loss: 0.5067â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 129/203 [2m0:02:19 â€¢ 0:01:20[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6408, 0.3592]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 129/203 [2m0:02:19 â€¢ 0:01:20[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6453, 0.3547]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 129/203 [2m0:02:19 â€¢ 0:01:20[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6502, 0.3498]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 129/203 [2m0:02:19 â€¢ 0:01:20[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6553, 0.3447]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 129/203 [2m0:02:19 â€¢ 0:01:20[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6606, 0.3394]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 129/203 [2m0:02:19 â€¢ 0:01:20[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6662, 0.3338]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 129/203 [2m0:02:19 â€¢ 0:01:20[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6719, 0.3281]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 129/203 [2m0:02:19 â€¢ 0:01:20[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6779, 0.3221]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 129/203 [2m0:02:19 â€¢ 0:01:20[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6840, 0.3160]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 129/203 [2m0:02:19 â€¢ 0:01:20[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5812, 0.4188]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 129/203 [2m0:02:19 â€¢ 0:01:20[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6684, 0.3316],â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 129/203 [2m0:02:19 â€¢ 0:01:20[0m [2;4m0.93it/s[0m  
        [0.6903, 0.3097],
        [0.6260, 0.3740],
        [0.6831, 0.3169],
        [0.6427, 0.3573],
        [0.6542, 0.3458],
        [0.6076, 0.3924],
        [0.6918, 0.3082],
        [0.7032, 0.2968],
[2K        [0.6923, 0.3077]], device='cuda:0')â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 129/203 [2m0:02:19 â€¢ 0:01:20[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5822, 0.4178],â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 130/203 [2m0:02:20 â€¢ 0:01:20[0m [2;4m0.92it/s[0m  
        [0.5777, 0.4223],
        [0.5821, 0.4179],
        [0.5811, 0.4189],
        [0.5827, 0.4173],
        [0.5802, 0.4198],
        [0.5782, 0.4218],
        [0.5823, 0.4177],
        [0.5825, 0.4175],
[2K        [0.5838, 0.4162]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 130/203 [2m0:02:20 â€¢ 0:01:20[0m [2;4m0.92it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 130/203 [2m0:02:20 â€¢ 0:01:20[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5778, 0.4222]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 130/203 [2m0:02:20 â€¢ 0:01:20[0m [2;4m0.92it/s[0m  
[2KStep 0 - TTA Loss: 0.8998â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 130/203 [2m0:02:20 â€¢ 0:01:20[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5800, 0.4200]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 130/203 [2m0:02:20 â€¢ 0:01:20[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5822, 0.4178]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 130/203 [2m0:02:20 â€¢ 0:01:20[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5843, 0.4157]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 130/203 [2m0:02:20 â€¢ 0:01:20[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5865, 0.4135]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 130/203 [2m0:02:20 â€¢ 0:01:20[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5887, 0.4113]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 130/203 [2m0:02:20 â€¢ 0:01:20[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5909, 0.4091]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 130/203 [2m0:02:20 â€¢ 0:01:20[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5931, 0.4069]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 130/203 [2m0:02:20 â€¢ 0:01:20[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5953, 0.4047]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 130/203 [2m0:02:20 â€¢ 0:01:20[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5974, 0.4026]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 130/203 [2m0:02:20 â€¢ 0:01:20[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5995, 0.4005]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 130/203 [2m0:02:20 â€¢ 0:01:20[0m [2;4m0.92it/s[0m  
[2KStep 10 - TTA Loss: 0.8338â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 130/203 [2m0:02:20 â€¢ 0:01:20[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6014, 0.3986]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 130/203 [2m0:02:20 â€¢ 0:01:20[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6032, 0.3968]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 130/203 [2m0:02:20 â€¢ 0:01:20[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6049, 0.3951]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 130/203 [2m0:02:20 â€¢ 0:01:20[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6066, 0.3934]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 130/203 [2m0:02:20 â€¢ 0:01:20[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6082, 0.3918]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 130/203 [2m0:02:20 â€¢ 0:01:20[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6097, 0.3903]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 130/203 [2m0:02:20 â€¢ 0:01:20[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6110, 0.3890]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 130/203 [2m0:02:20 â€¢ 0:01:20[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6122, 0.3878]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 130/203 [2m0:02:20 â€¢ 0:01:20[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6132, 0.3868]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 130/203 [2m0:02:20 â€¢ 0:01:20[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6141, 0.3859]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 130/203 [2m0:02:20 â€¢ 0:01:20[0m [2;4m0.92it/s[0m  
[2KStep 20 - TTA Loss: 0.9352â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 130/203 [2m0:02:20 â€¢ 0:01:20[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6149, 0.3851]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 130/203 [2m0:02:20 â€¢ 0:01:20[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6155, 0.3845]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 130/203 [2m0:02:20 â€¢ 0:01:20[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6159, 0.3841]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 130/203 [2m0:02:20 â€¢ 0:01:20[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6163, 0.3837]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 130/203 [2m0:02:20 â€¢ 0:01:20[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6166, 0.3834]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 130/203 [2m0:02:20 â€¢ 0:01:20[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6168, 0.3832]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 130/203 [2m0:02:20 â€¢ 0:01:20[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6170, 0.3830]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 130/203 [2m0:02:20 â€¢ 0:01:20[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6170, 0.3830]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 130/203 [2m0:02:20 â€¢ 0:01:20[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6171, 0.3829]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 130/203 [2m0:02:20 â€¢ 0:01:20[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5798, 0.4202]], device='cuda:0')24mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 130/203 [2m0:02:20 â€¢ 0:01:20[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6667, 0.3333],â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 130/203 [2m0:02:20 â€¢ 0:01:20[0m [2;4m0.92it/s[0m  
        [0.6592, 0.3408],
        [0.6261, 0.3739],
        [0.6794, 0.3206],
        [0.6467, 0.3533],
        [0.6587, 0.3413],
        [0.6171, 0.3829],
        [0.6909, 0.3091],
        [0.6968, 0.3032],
[2K        [0.6925, 0.3075]], device='cuda:0')â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 130/203 [2m0:02:20 â€¢ 0:01:20[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5823, 0.4177],â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 131/203 [2m0:02:21 â€¢ 0:01:19[0m [2;4m0.92it/s[0m  
        [0.5780, 0.4220],
        [0.5821, 0.4179],
        [0.5811, 0.4189],
        [0.5827, 0.4173],
        [0.5799, 0.4201],
        [0.5781, 0.4219],
        [0.5823, 0.4177],
        [0.5825, 0.4175],
[2K        [0.5837, 0.4163]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 131/203 [2m0:02:21 â€¢ 0:01:19[0m [2;4m0.92it/s[0m  
[2KClass label: CleanAndJerkâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 131/203 [2m0:02:21 â€¢ 0:01:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5905, 0.4095]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 131/203 [2m0:02:21 â€¢ 0:01:19[0m [2;4m0.92it/s[0m  
[2KStep 0 - TTA Loss: 0.4682â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 131/203 [2m0:02:21 â€¢ 0:01:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5905, 0.4095]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 131/203 [2m0:02:21 â€¢ 0:01:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5905, 0.4095]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 131/203 [2m0:02:21 â€¢ 0:01:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5905, 0.4095]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 131/203 [2m0:02:21 â€¢ 0:01:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5906, 0.4094]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 131/203 [2m0:02:21 â€¢ 0:01:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5907, 0.4093]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 131/203 [2m0:02:21 â€¢ 0:01:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5910, 0.4090]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 131/203 [2m0:02:21 â€¢ 0:01:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5913, 0.4087]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 131/203 [2m0:02:21 â€¢ 0:01:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5917, 0.4083]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 131/203 [2m0:02:21 â€¢ 0:01:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5924, 0.4076]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 131/203 [2m0:02:21 â€¢ 0:01:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5931, 0.4069]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 131/203 [2m0:02:21 â€¢ 0:01:19[0m [2;4m0.92it/s[0m  
[2KStep 10 - TTA Loss: 0.5354â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 131/203 [2m0:02:21 â€¢ 0:01:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5941, 0.4059]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 131/203 [2m0:02:21 â€¢ 0:01:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5953, 0.4047]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 131/203 [2m0:02:21 â€¢ 0:01:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5968, 0.4032]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 131/203 [2m0:02:21 â€¢ 0:01:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5985, 0.4015]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 131/203 [2m0:02:21 â€¢ 0:01:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6004, 0.3996]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 131/203 [2m0:02:21 â€¢ 0:01:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6026, 0.3974]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 131/203 [2m0:02:21 â€¢ 0:01:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6051, 0.3949]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 131/203 [2m0:02:21 â€¢ 0:01:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6078, 0.3922]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 131/203 [2m0:02:21 â€¢ 0:01:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6107, 0.3893]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 131/203 [2m0:02:21 â€¢ 0:01:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6139, 0.3861]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 131/203 [2m0:02:21 â€¢ 0:01:19[0m [2;4m0.92it/s[0m  
[2KStep 20 - TTA Loss: 0.5478â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 131/203 [2m0:02:21 â€¢ 0:01:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6173, 0.3827]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 131/203 [2m0:02:21 â€¢ 0:01:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6211, 0.3789]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 131/203 [2m0:02:21 â€¢ 0:01:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6251, 0.3749]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 131/203 [2m0:02:21 â€¢ 0:01:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6293, 0.3707]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 131/203 [2m0:02:21 â€¢ 0:01:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6338, 0.3662]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 131/203 [2m0:02:21 â€¢ 0:01:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6384, 0.3616]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 131/203 [2m0:02:21 â€¢ 0:01:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6431, 0.3569]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 131/203 [2m0:02:21 â€¢ 0:01:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6480, 0.3520]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 131/203 [2m0:02:21 â€¢ 0:01:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6530, 0.3470]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 131/203 [2m0:02:21 â€¢ 0:01:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5846, 0.4154]], device='cuda:0')24mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 131/203 [2m0:02:21 â€¢ 0:01:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6627, 0.3373],â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 131/203 [2m0:02:21 â€¢ 0:01:19[0m [2;4m0.92it/s[0m  
        [0.6232, 0.3768],
        [0.6582, 0.3418],
        [0.6777, 0.3223],
        [0.6462, 0.3538],
        [0.6545, 0.3455],
        [0.6091, 0.3909],
        [0.6784, 0.3216],
        [0.6919, 0.3081],
[2K        [0.6921, 0.3079]], device='cuda:0')â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 131/203 [2m0:02:21 â€¢ 0:01:19[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5824, 0.4176],â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 132/203 [2m0:02:22 â€¢ 0:01:17[0m [2;4m0.93it/s[0m   
        [0.5781, 0.4219],
        [0.5823, 0.4177],
        [0.5812, 0.4188],
        [0.5828, 0.4172],
        [0.5800, 0.4200],
        [0.5782, 0.4218],
        [0.5824, 0.4176],
        [0.5826, 0.4174],
[2K        [0.5838, 0.4162]], device='cuda:0', grad_fn=<SoftmaxBackward0>)5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 132/203 [2m0:02:22 â€¢ 0:01:17[0m [2;4m0.93it/s[0m  
[2KClass label: Billiardsâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 132/203 [2m0:02:22 â€¢ 0:01:17[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6042, 0.3958]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 132/203 [2m0:02:22 â€¢ 0:01:17[0m [2;4m0.93it/s[0m  
[2KStep 0 - TTA Loss: 0.5344â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 132/203 [2m0:02:22 â€¢ 0:01:17[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6064, 0.3936]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 132/203 [2m0:02:22 â€¢ 0:01:17[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6092, 0.3908]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 132/203 [2m0:02:22 â€¢ 0:01:17[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6124, 0.3876]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 132/203 [2m0:02:22 â€¢ 0:01:17[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6159, 0.3841]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 132/203 [2m0:02:22 â€¢ 0:01:17[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6196, 0.3804]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 132/203 [2m0:02:22 â€¢ 0:01:17[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6237, 0.3763]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 132/203 [2m0:02:22 â€¢ 0:01:17[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6278, 0.3722]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 132/203 [2m0:02:22 â€¢ 0:01:17[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6321, 0.3679]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 132/203 [2m0:02:22 â€¢ 0:01:17[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6364, 0.3636]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 132/203 [2m0:02:22 â€¢ 0:01:17[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6406, 0.3594]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 132/203 [2m0:02:22 â€¢ 0:01:17[0m [2;4m0.93it/s[0m  
[2KStep 10 - TTA Loss: 0.5229â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 132/203 [2m0:02:22 â€¢ 0:01:17[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6448, 0.3552]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 132/203 [2m0:02:22 â€¢ 0:01:17[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6488, 0.3512]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 132/203 [2m0:02:22 â€¢ 0:01:17[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6527, 0.3473]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 132/203 [2m0:02:22 â€¢ 0:01:17[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6564, 0.3436]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 132/203 [2m0:02:22 â€¢ 0:01:17[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6599, 0.3401]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 132/203 [2m0:02:22 â€¢ 0:01:17[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6632, 0.3368]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 132/203 [2m0:02:22 â€¢ 0:01:17[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6661, 0.3339]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 132/203 [2m0:02:22 â€¢ 0:01:17[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6688, 0.3312]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 132/203 [2m0:02:22 â€¢ 0:01:17[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6712, 0.3288]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 132/203 [2m0:02:22 â€¢ 0:01:17[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6733, 0.3267]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 132/203 [2m0:02:22 â€¢ 0:01:17[0m [2;4m0.93it/s[0m  
[2KStep 20 - TTA Loss: 0.4958â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 132/203 [2m0:02:22 â€¢ 0:01:17[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6751, 0.3249]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 132/203 [2m0:02:22 â€¢ 0:01:17[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6766, 0.3234]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 132/203 [2m0:02:22 â€¢ 0:01:17[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6779, 0.3221]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 132/203 [2m0:02:22 â€¢ 0:01:17[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6788, 0.3212]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 132/203 [2m0:02:22 â€¢ 0:01:17[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6796, 0.3204]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 132/203 [2m0:02:22 â€¢ 0:01:17[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6801, 0.3199]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 132/203 [2m0:02:22 â€¢ 0:01:17[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6804, 0.3196]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 132/203 [2m0:02:22 â€¢ 0:01:17[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6806, 0.3194]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 132/203 [2m0:02:22 â€¢ 0:01:17[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6807, 0.3193]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 132/203 [2m0:02:22 â€¢ 0:01:17[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5815, 0.4185]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 132/203 [2m0:02:22 â€¢ 0:01:17[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6699, 0.3301],â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 132/203 [2m0:02:22 â€¢ 0:01:17[0m [2;4m0.93it/s[0m  
        [0.6808, 0.3192],
        [0.6485, 0.3515],
        [0.6854, 0.3146],
        [0.6493, 0.3507],
        [0.6591, 0.3409],
        [0.6117, 0.3883],
        [0.6928, 0.3072],
        [0.7047, 0.2953],
[2K        [0.6968, 0.3032]], device='cuda:0')â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 132/203 [2m0:02:22 â€¢ 0:01:17[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5825, 0.4175],â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 133/203 [2m0:02:23 â€¢ 0:01:16[0m [2;4m0.93it/s[0m  
        [0.5785, 0.4215],
        [0.5824, 0.4176],
        [0.5814, 0.4186],
        [0.5829, 0.4171],
        [0.5801, 0.4199],
        [0.5783, 0.4217],
        [0.5826, 0.4174],
        [0.5828, 0.4172],
[2K        [0.5839, 0.4161]], device='cuda:0', grad_fn=<SoftmaxBackward0>)5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 133/203 [2m0:02:23 â€¢ 0:01:16[0m [2;4m0.93it/s[0m  
[2KClass label: TennisSwingâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 133/203 [2m0:02:23 â€¢ 0:01:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6703, 0.3297]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 133/203 [2m0:02:23 â€¢ 0:01:16[0m [2;4m0.93it/s[0m  
[2KStep 0 - TTA Loss: 0.6979â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 133/203 [2m0:02:23 â€¢ 0:01:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6703, 0.3297]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 133/203 [2m0:02:23 â€¢ 0:01:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6703, 0.3297]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 133/203 [2m0:02:23 â€¢ 0:01:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6704, 0.3296]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 133/203 [2m0:02:23 â€¢ 0:01:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6704, 0.3296]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 133/203 [2m0:02:23 â€¢ 0:01:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6705, 0.3295]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 133/203 [2m0:02:23 â€¢ 0:01:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6706, 0.3294]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 133/203 [2m0:02:23 â€¢ 0:01:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6708, 0.3292]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 133/203 [2m0:02:23 â€¢ 0:01:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6711, 0.3289]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 133/203 [2m0:02:23 â€¢ 0:01:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6713, 0.3287]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 133/203 [2m0:02:23 â€¢ 0:01:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6717, 0.3283]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 133/203 [2m0:02:23 â€¢ 0:01:16[0m [2;4m0.93it/s[0m  
[2KStep 10 - TTA Loss: 0.6910â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 133/203 [2m0:02:23 â€¢ 0:01:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6720, 0.3280]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 133/203 [2m0:02:23 â€¢ 0:01:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6725, 0.3275]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 133/203 [2m0:02:23 â€¢ 0:01:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6729, 0.3271]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 133/203 [2m0:02:23 â€¢ 0:01:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6735, 0.3265]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 133/203 [2m0:02:23 â€¢ 0:01:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6741, 0.3259]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 133/203 [2m0:02:23 â€¢ 0:01:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6748, 0.3252]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 133/203 [2m0:02:23 â€¢ 0:01:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6755, 0.3245]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 133/203 [2m0:02:23 â€¢ 0:01:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6763, 0.3237]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 133/203 [2m0:02:23 â€¢ 0:01:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6771, 0.3229]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 133/203 [2m0:02:23 â€¢ 0:01:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6780, 0.3220]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 133/203 [2m0:02:23 â€¢ 0:01:16[0m [2;4m0.93it/s[0m  
[2KStep 20 - TTA Loss: 0.6377â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 133/203 [2m0:02:23 â€¢ 0:01:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6790, 0.3210]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 133/203 [2m0:02:23 â€¢ 0:01:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6800, 0.3200]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 133/203 [2m0:02:23 â€¢ 0:01:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6811, 0.3189]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 133/203 [2m0:02:23 â€¢ 0:01:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6823, 0.3177]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 133/203 [2m0:02:23 â€¢ 0:01:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6835, 0.3165]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 133/203 [2m0:02:23 â€¢ 0:01:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6849, 0.3151]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 133/203 [2m0:02:23 â€¢ 0:01:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6864, 0.3136]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 133/203 [2m0:02:23 â€¢ 0:01:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6879, 0.3121]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 133/203 [2m0:02:23 â€¢ 0:01:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6895, 0.3105]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 133/203 [2m0:02:23 â€¢ 0:01:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5831, 0.4169]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 133/203 [2m0:02:23 â€¢ 0:01:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6542, 0.3458],â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 133/203 [2m0:02:23 â€¢ 0:01:16[0m [2;4m0.93it/s[0m  
        [0.6290, 0.3710],
        [0.6098, 0.3902],
        [0.6676, 0.3324],
        [0.6280, 0.3720],
        [0.6363, 0.3637],
        [0.5877, 0.4123],
        [0.6758, 0.3242],
        [0.6912, 0.3088],
[2K        [0.6775, 0.3225]], device='cuda:0')â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 133/203 [2m0:02:23 â€¢ 0:01:16[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5821, 0.4179],â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 134/203 [2m0:02:24 â€¢ 0:01:15[0m [2;4m0.93it/s[0m  
        [0.5783, 0.4217],
        [0.5822, 0.4178],
        [0.5809, 0.4191],
        [0.5826, 0.4174],
        [0.5799, 0.4201],
        [0.5781, 0.4219],
        [0.5822, 0.4178],
        [0.5821, 0.4179],
[2K        [0.5837, 0.4163]], device='cuda:0', grad_fn=<SoftmaxBackward0>)5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 134/203 [2m0:02:24 â€¢ 0:01:15[0m [2;4m0.93it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 134/203 [2m0:02:24 â€¢ 0:01:15[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6606, 0.3394]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 134/203 [2m0:02:24 â€¢ 0:01:15[0m [2;4m0.93it/s[0m  
[2KStep 0 - TTA Loss: 0.5842â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 134/203 [2m0:02:24 â€¢ 0:01:15[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6620, 0.3380]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 134/203 [2m0:02:24 â€¢ 0:01:15[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6637, 0.3363]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 134/203 [2m0:02:24 â€¢ 0:01:15[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6656, 0.3344]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 134/203 [2m0:02:24 â€¢ 0:01:15[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6677, 0.3323]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 134/203 [2m0:02:24 â€¢ 0:01:15[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6699, 0.3301]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 134/203 [2m0:02:24 â€¢ 0:01:15[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6722, 0.3278]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 134/203 [2m0:02:24 â€¢ 0:01:15[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6746, 0.3254]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 134/203 [2m0:02:24 â€¢ 0:01:15[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6770, 0.3230]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 134/203 [2m0:02:24 â€¢ 0:01:15[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6794, 0.3206]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 134/203 [2m0:02:24 â€¢ 0:01:15[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6818, 0.3182]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 134/203 [2m0:02:24 â€¢ 0:01:15[0m [2;4m0.93it/s[0m  
[2KStep 10 - TTA Loss: 0.6002â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 134/203 [2m0:02:24 â€¢ 0:01:15[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6841, 0.3159]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 134/203 [2m0:02:24 â€¢ 0:01:15[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6863, 0.3137]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 134/203 [2m0:02:24 â€¢ 0:01:15[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6884, 0.3116]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 134/203 [2m0:02:24 â€¢ 0:01:15[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6903, 0.3097]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 134/203 [2m0:02:24 â€¢ 0:01:15[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6921, 0.3079]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 134/203 [2m0:02:24 â€¢ 0:01:15[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6938, 0.3062]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 134/203 [2m0:02:24 â€¢ 0:01:15[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6953, 0.3047]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 134/203 [2m0:02:24 â€¢ 0:01:15[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6967, 0.3033]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 134/203 [2m0:02:24 â€¢ 0:01:15[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6980, 0.3020]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 134/203 [2m0:02:24 â€¢ 0:01:15[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6990, 0.3010]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 134/203 [2m0:02:24 â€¢ 0:01:15[0m [2;4m0.93it/s[0m  
[2KStep 20 - TTA Loss: 0.5350â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 134/203 [2m0:02:24 â€¢ 0:01:15[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7000, 0.3000]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 134/203 [2m0:02:24 â€¢ 0:01:15[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7007, 0.2993]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 134/203 [2m0:02:24 â€¢ 0:01:15[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7014, 0.2986]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 134/203 [2m0:02:24 â€¢ 0:01:15[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7019, 0.2981]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 134/203 [2m0:02:24 â€¢ 0:01:15[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7023, 0.2977]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 134/203 [2m0:02:24 â€¢ 0:01:15[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7025, 0.2975]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 134/203 [2m0:02:24 â€¢ 0:01:15[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7027, 0.2973]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 134/203 [2m0:02:24 â€¢ 0:01:15[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7028, 0.2972]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 134/203 [2m0:02:24 â€¢ 0:01:15[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7028, 0.2972]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 134/203 [2m0:02:24 â€¢ 0:01:15[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5857, 0.4143]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 134/203 [2m0:02:24 â€¢ 0:01:15[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6749, 0.3251],â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 134/203 [2m0:02:24 â€¢ 0:01:15[0m [2;4m0.93it/s[0m  
        [0.6233, 0.3767],
        [0.6260, 0.3740],
        [0.6848, 0.3152],
        [0.6518, 0.3482],
        [0.6574, 0.3426],
        [0.6089, 0.3911],
        [0.6852, 0.3148],
        [0.7025, 0.2975],
[2K        [0.7030, 0.2970]], device='cuda:0')â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 134/203 [2m0:02:24 â€¢ 0:01:15[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5818, 0.4182],â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 135/203 [2m0:02:25 â€¢ 0:01:14[0m [2;4m0.93it/s[0m  
        [0.5781, 0.4219],
        [0.5821, 0.4179],
        [0.5806, 0.4194],
        [0.5825, 0.4175],
        [0.5797, 0.4203],
        [0.5779, 0.4221],
        [0.5820, 0.4180],
        [0.5817, 0.4183],
[2K        [0.5836, 0.4164]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 135/203 [2m0:02:25 â€¢ 0:01:14[0m [2;4m0.93it/s[0m  
[2KClass label: CleanAndJerkâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 135/203 [2m0:02:25 â€¢ 0:01:14[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5989, 0.4011]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 135/203 [2m0:02:25 â€¢ 0:01:14[0m [2;4m0.93it/s[0m  
[2KStep 0 - TTA Loss: 0.4703â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 135/203 [2m0:02:25 â€¢ 0:01:14[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5989, 0.4011]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 135/203 [2m0:02:25 â€¢ 0:01:14[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5989, 0.4011]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 135/203 [2m0:02:25 â€¢ 0:01:14[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5990, 0.4010]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 135/203 [2m0:02:25 â€¢ 0:01:14[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5990, 0.4010]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 135/203 [2m0:02:25 â€¢ 0:01:14[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5992, 0.4008]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 135/203 [2m0:02:25 â€¢ 0:01:14[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5994, 0.4006]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 135/203 [2m0:02:25 â€¢ 0:01:14[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5998, 0.4002]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 135/203 [2m0:02:25 â€¢ 0:01:14[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6002, 0.3998]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 135/203 [2m0:02:25 â€¢ 0:01:14[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6009, 0.3991]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 135/203 [2m0:02:25 â€¢ 0:01:14[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6016, 0.3984]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 135/203 [2m0:02:25 â€¢ 0:01:14[0m [2;4m0.93it/s[0m  
[2KStep 10 - TTA Loss: 0.4788â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 135/203 [2m0:02:25 â€¢ 0:01:14[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6026, 0.3974]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 135/203 [2m0:02:25 â€¢ 0:01:14[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6037, 0.3963]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 135/203 [2m0:02:25 â€¢ 0:01:14[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6050, 0.3950]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 135/203 [2m0:02:25 â€¢ 0:01:14[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6065, 0.3935]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 135/203 [2m0:02:25 â€¢ 0:01:14[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6083, 0.3917]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 135/203 [2m0:02:25 â€¢ 0:01:14[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6102, 0.3898]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 135/203 [2m0:02:25 â€¢ 0:01:14[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6124, 0.3876]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 135/203 [2m0:02:25 â€¢ 0:01:14[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6148, 0.3852]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 135/203 [2m0:02:25 â€¢ 0:01:14[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6174, 0.3826]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 135/203 [2m0:02:25 â€¢ 0:01:14[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6202, 0.3798]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 135/203 [2m0:02:25 â€¢ 0:01:14[0m [2;4m0.93it/s[0m  
[2KStep 20 - TTA Loss: 0.4543â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 135/203 [2m0:02:25 â€¢ 0:01:14[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6233, 0.3767]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 135/203 [2m0:02:25 â€¢ 0:01:14[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6265, 0.3735]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 135/203 [2m0:02:25 â€¢ 0:01:14[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6299, 0.3701]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 135/203 [2m0:02:25 â€¢ 0:01:14[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6336, 0.3664]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 135/203 [2m0:02:25 â€¢ 0:01:14[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6374, 0.3626]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 135/203 [2m0:02:25 â€¢ 0:01:14[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6414, 0.3586]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 135/203 [2m0:02:25 â€¢ 0:01:14[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6455, 0.3545]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 135/203 [2m0:02:25 â€¢ 0:01:14[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6497, 0.3503]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 135/203 [2m0:02:25 â€¢ 0:01:14[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6541, 0.3459]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 135/203 [2m0:02:25 â€¢ 0:01:14[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5840, 0.4160]], device='cuda:0')224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 135/203 [2m0:02:25 â€¢ 0:01:14[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6656, 0.3344],â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 135/203 [2m0:02:25 â€¢ 0:01:14[0m [2;4m0.93it/s[0m  
        [0.6281, 0.3719],
        [0.6585, 0.3415],
        [0.6817, 0.3183],
        [0.6532, 0.3468],
        [0.6579, 0.3421],
        [0.6073, 0.3927],
        [0.6835, 0.3165],
        [0.6971, 0.3029],
[2K        [0.6987, 0.3013]], device='cuda:0')â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 135/203 [2m0:02:25 â€¢ 0:01:14[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5817, 0.4183],â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 136/203 [2m0:02:27 â€¢ 0:01:13[0m [2;4m0.93it/s[0m  
        [0.5779, 0.4221],
        [0.5817, 0.4183],
        [0.5804, 0.4196],
        [0.5823, 0.4177],
        [0.5795, 0.4205],
        [0.5778, 0.4222],
        [0.5819, 0.4181],
        [0.5815, 0.4185],
[2K        [0.5835, 0.4165]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 136/203 [2m0:02:27 â€¢ 0:01:13[0m [2;4m0.93it/s[0m  
[2KClass label: GolfSwingâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 136/203 [2m0:02:27 â€¢ 0:01:13[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6495, 0.3505]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 136/203 [2m0:02:27 â€¢ 0:01:13[0m [2;4m0.93it/s[0m  
[2KStep 0 - TTA Loss: 0.5203â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 136/203 [2m0:02:27 â€¢ 0:01:13[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6519, 0.3481]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 136/203 [2m0:02:27 â€¢ 0:01:13[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6546, 0.3454]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 136/203 [2m0:02:27 â€¢ 0:01:13[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6575, 0.3425]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 136/203 [2m0:02:27 â€¢ 0:01:13[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6607, 0.3393]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 136/203 [2m0:02:27 â€¢ 0:01:13[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6641, 0.3359]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 136/203 [2m0:02:27 â€¢ 0:01:13[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6676, 0.3324]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 136/203 [2m0:02:27 â€¢ 0:01:13[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6712, 0.3288]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 136/203 [2m0:02:27 â€¢ 0:01:13[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6748, 0.3252]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 136/203 [2m0:02:27 â€¢ 0:01:13[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6785, 0.3215]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 136/203 [2m0:02:27 â€¢ 0:01:13[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6820, 0.3180]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 136/203 [2m0:02:27 â€¢ 0:01:13[0m [2;4m0.93it/s[0m  
[2KStep 10 - TTA Loss: 0.5243â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 136/203 [2m0:02:27 â€¢ 0:01:13[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6855, 0.3145]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 136/203 [2m0:02:27 â€¢ 0:01:13[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6889, 0.3111]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 136/203 [2m0:02:27 â€¢ 0:01:13[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6921, 0.3079]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 136/203 [2m0:02:27 â€¢ 0:01:13[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6952, 0.3048]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 136/203 [2m0:02:27 â€¢ 0:01:13[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6981, 0.3019]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 136/203 [2m0:02:27 â€¢ 0:01:13[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7007, 0.2993]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 136/203 [2m0:02:27 â€¢ 0:01:13[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7032, 0.2968]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 136/203 [2m0:02:27 â€¢ 0:01:13[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7054, 0.2946]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 136/203 [2m0:02:27 â€¢ 0:01:13[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7073, 0.2927]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 136/203 [2m0:02:27 â€¢ 0:01:13[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7090, 0.2910]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 136/203 [2m0:02:27 â€¢ 0:01:13[0m [2;4m0.93it/s[0m  
[2KStep 20 - TTA Loss: 0.5052â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 136/203 [2m0:02:27 â€¢ 0:01:13[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7105, 0.2895]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 136/203 [2m0:02:27 â€¢ 0:01:13[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7117, 0.2883]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 136/203 [2m0:02:27 â€¢ 0:01:13[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7127, 0.2873]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 136/203 [2m0:02:27 â€¢ 0:01:13[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7134, 0.2866]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 136/203 [2m0:02:27 â€¢ 0:01:13[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7140, 0.2860]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 136/203 [2m0:02:27 â€¢ 0:01:13[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7144, 0.2856]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 136/203 [2m0:02:27 â€¢ 0:01:13[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7147, 0.2853]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 136/203 [2m0:02:27 â€¢ 0:01:13[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7149, 0.2851]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 136/203 [2m0:02:27 â€¢ 0:01:13[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7149, 0.2851]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 136/203 [2m0:02:27 â€¢ 0:01:13[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5833, 0.4167]], device='cuda:0')224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 136/203 [2m0:02:27 â€¢ 0:01:13[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6913, 0.3087],â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 136/203 [2m0:02:27 â€¢ 0:01:13[0m [2;4m0.93it/s[0m  
        [0.6363, 0.3637],
        [0.6522, 0.3478],
        [0.7151, 0.2849],
        [0.6673, 0.3327],
        [0.6700, 0.3300],
        [0.6230, 0.3770],
        [0.6938, 0.3062],
        [0.7157, 0.2843],
[2K        [0.7140, 0.2860]], device='cuda:0')â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 136/203 [2m0:02:27 â€¢ 0:01:13[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5818, 0.4182],â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 137/203 [2m0:02:28 â€¢ 0:01:12[0m [2;4m0.93it/s[0m  
        [0.5778, 0.4222],
        [0.5814, 0.4186],
        [0.5804, 0.4196],
        [0.5823, 0.4177],
        [0.5795, 0.4205],
        [0.5777, 0.4223],
        [0.5818, 0.4182],
        [0.5815, 0.4185],
[2K        [0.5834, 0.4166]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 137/203 [2m0:02:28 â€¢ 0:01:12[0m [2;4m0.93it/s[0m  
[2KClass label: SoccerPenaltyâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 137/203 [2m0:02:28 â€¢ 0:01:12[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6557, 0.3443]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 137/203 [2m0:02:28 â€¢ 0:01:12[0m [2;4m0.93it/s[0m  
[2KStep 0 - TTA Loss: 0.8147â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 137/203 [2m0:02:28 â€¢ 0:01:12[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6557, 0.3443]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 137/203 [2m0:02:28 â€¢ 0:01:12[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6557, 0.3443]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 137/203 [2m0:02:28 â€¢ 0:01:12[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6557, 0.3443]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 137/203 [2m0:02:28 â€¢ 0:01:12[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6558, 0.3442]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 137/203 [2m0:02:28 â€¢ 0:01:12[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6559, 0.3441]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 137/203 [2m0:02:28 â€¢ 0:01:12[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6560, 0.3440]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 137/203 [2m0:02:28 â€¢ 0:01:12[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6563, 0.3437]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 137/203 [2m0:02:28 â€¢ 0:01:12[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6566, 0.3434]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 137/203 [2m0:02:28 â€¢ 0:01:12[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6569, 0.3431]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 137/203 [2m0:02:28 â€¢ 0:01:12[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6573, 0.3427]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 137/203 [2m0:02:28 â€¢ 0:01:12[0m [2;4m0.93it/s[0m  
[2KStep 10 - TTA Loss: 0.8412â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 137/203 [2m0:02:28 â€¢ 0:01:12[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6578, 0.3422]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 137/203 [2m0:02:28 â€¢ 0:01:12[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6584, 0.3416]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 137/203 [2m0:02:28 â€¢ 0:01:12[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6591, 0.3409]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 137/203 [2m0:02:28 â€¢ 0:01:12[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6598, 0.3402]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 137/203 [2m0:02:28 â€¢ 0:01:12[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6606, 0.3394]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 137/203 [2m0:02:28 â€¢ 0:01:12[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6615, 0.3385]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 137/203 [2m0:02:28 â€¢ 0:01:12[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6624, 0.3376]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 137/203 [2m0:02:28 â€¢ 0:01:12[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6634, 0.3366]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 137/203 [2m0:02:28 â€¢ 0:01:12[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6645, 0.3355]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 137/203 [2m0:02:28 â€¢ 0:01:12[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6656, 0.3344]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 137/203 [2m0:02:28 â€¢ 0:01:12[0m [2;4m0.93it/s[0m  
[2KStep 20 - TTA Loss: 0.8548â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 137/203 [2m0:02:28 â€¢ 0:01:12[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6668, 0.3332]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 137/203 [2m0:02:28 â€¢ 0:01:12[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6681, 0.3319]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 137/203 [2m0:02:28 â€¢ 0:01:12[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6694, 0.3306]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 137/203 [2m0:02:28 â€¢ 0:01:12[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6708, 0.3292]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 137/203 [2m0:02:28 â€¢ 0:01:12[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6722, 0.3278]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 137/203 [2m0:02:28 â€¢ 0:01:12[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6737, 0.3263]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 137/203 [2m0:02:28 â€¢ 0:01:12[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6753, 0.3247]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 137/203 [2m0:02:28 â€¢ 0:01:12[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6769, 0.3231]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 137/203 [2m0:02:28 â€¢ 0:01:12[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6785, 0.3215]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 137/203 [2m0:02:28 â€¢ 0:01:12[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5823, 0.4177]], device='cuda:0')224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 137/203 [2m0:02:28 â€¢ 0:01:12[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6531, 0.3469],â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 137/203 [2m0:02:28 â€¢ 0:01:12[0m [2;4m0.93it/s[0m  
        [0.6129, 0.3871],
        [0.6009, 0.3991],
        [0.6673, 0.3327],
        [0.6265, 0.3735],
        [0.6333, 0.3667],
        [0.5880, 0.4120],
        [0.6802, 0.3198],
        [0.6809, 0.3191],
[2K        [0.6760, 0.3240]], device='cuda:0')â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 137/203 [2m0:02:28 â€¢ 0:01:12[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5815, 0.4185],â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 138/203 [2m0:02:29 â€¢ 0:01:11[0m [2;4m0.93it/s[0m   
        [0.5776, 0.4224],
        [0.5811, 0.4189],
        [0.5802, 0.4198],
        [0.5820, 0.4180],
        [0.5792, 0.4208],
        [0.5775, 0.4225],
        [0.5814, 0.4186],
        [0.5812, 0.4188],
[2K        [0.5831, 0.4169]], device='cuda:0', grad_fn=<SoftmaxBackward0>);5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 138/203 [2m0:02:29 â€¢ 0:01:11[0m [2;4m0.93it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 138/203 [2m0:02:29 â€¢ 0:01:11[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5704, 0.4296]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 138/203 [2m0:02:29 â€¢ 0:01:11[0m [2;4m0.93it/s[0m  
[2KStep 0 - TTA Loss: 0.7285â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 138/203 [2m0:02:29 â€¢ 0:01:11[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5716, 0.4284]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 138/203 [2m0:02:29 â€¢ 0:01:11[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5730, 0.4270]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 138/203 [2m0:02:29 â€¢ 0:01:11[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5746, 0.4254]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 138/203 [2m0:02:29 â€¢ 0:01:11[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5763, 0.4237]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 138/203 [2m0:02:29 â€¢ 0:01:11[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5782, 0.4218]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 138/203 [2m0:02:29 â€¢ 0:01:11[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5802, 0.4198]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 138/203 [2m0:02:29 â€¢ 0:01:11[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5823, 0.4177]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 138/203 [2m0:02:29 â€¢ 0:01:11[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5844, 0.4156]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 138/203 [2m0:02:29 â€¢ 0:01:11[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5865, 0.4135]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 138/203 [2m0:02:29 â€¢ 0:01:11[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5886, 0.4114]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 138/203 [2m0:02:29 â€¢ 0:01:11[0m [2;4m0.93it/s[0m  
[2KStep 10 - TTA Loss: 0.8948â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 138/203 [2m0:02:29 â€¢ 0:01:11[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5906, 0.4094]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 138/203 [2m0:02:29 â€¢ 0:01:11[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5925, 0.4075]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 138/203 [2m0:02:29 â€¢ 0:01:11[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5943, 0.4057]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 138/203 [2m0:02:29 â€¢ 0:01:11[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5961, 0.4039]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 138/203 [2m0:02:29 â€¢ 0:01:11[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5977, 0.4023]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 138/203 [2m0:02:29 â€¢ 0:01:11[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5992, 0.4008]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 138/203 [2m0:02:29 â€¢ 0:01:11[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6006, 0.3994]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 138/203 [2m0:02:29 â€¢ 0:01:11[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6019, 0.3981]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 138/203 [2m0:02:29 â€¢ 0:01:11[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6030, 0.3970]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 138/203 [2m0:02:29 â€¢ 0:01:11[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6040, 0.3960]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 138/203 [2m0:02:29 â€¢ 0:01:11[0m [2;4m0.93it/s[0m  
[2KStep 20 - TTA Loss: 0.6898â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 138/203 [2m0:02:29 â€¢ 0:01:11[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6049, 0.3951]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 138/203 [2m0:02:29 â€¢ 0:01:11[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6056, 0.3944]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 138/203 [2m0:02:29 â€¢ 0:01:11[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6061, 0.3939]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 138/203 [2m0:02:29 â€¢ 0:01:11[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6066, 0.3934]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 138/203 [2m0:02:29 â€¢ 0:01:11[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6069, 0.3931]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 138/203 [2m0:02:29 â€¢ 0:01:11[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6072, 0.3928]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 138/203 [2m0:02:29 â€¢ 0:01:11[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6073, 0.3927]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 138/203 [2m0:02:29 â€¢ 0:01:11[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6074, 0.3926]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 138/203 [2m0:02:29 â€¢ 0:01:11[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6074, 0.3926]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 138/203 [2m0:02:29 â€¢ 0:01:11[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5787, 0.4213]], device='cuda:0')â•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 138/203 [2m0:02:29 â€¢ 0:01:11[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6580, 0.3420],â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 138/203 [2m0:02:29 â€¢ 0:01:11[0m [2;4m0.93it/s[0m  
        [0.6185, 0.3815],
        [0.6125, 0.3875],
        [0.6695, 0.3305],
        [0.6356, 0.3644],
        [0.6470, 0.3530],
        [0.6075, 0.3925],
        [0.6867, 0.3133],
        [0.6819, 0.3181],
[2K        [0.6837, 0.3163]], device='cuda:0')â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 138/203 [2m0:02:29 â€¢ 0:01:11[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5813, 0.4187],â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 139/203 [2m0:02:30 â€¢ 0:01:10[0m [2;4m0.93it/s[0m  
        [0.5774, 0.4226],
        [0.5810, 0.4190],
        [0.5800, 0.4200],
        [0.5819, 0.4181],
        [0.5790, 0.4210],
        [0.5774, 0.4226],
        [0.5810, 0.4190],
        [0.5810, 0.4190],
[2K        [0.5830, 0.4170]], device='cuda:0', grad_fn=<SoftmaxBackward0>);5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 139/203 [2m0:02:30 â€¢ 0:01:10[0m [2;4m0.93it/s[0m  
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 139/203 [2m0:02:30 â€¢ 0:01:10[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6205, 0.3795]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 139/203 [2m0:02:30 â€¢ 0:01:10[0m [2;4m0.93it/s[0m  
[2KStep 0 - TTA Loss: 0.6494â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 139/203 [2m0:02:30 â€¢ 0:01:10[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6205, 0.3795]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 139/203 [2m0:02:30 â€¢ 0:01:10[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6205, 0.3795]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 139/203 [2m0:02:30 â€¢ 0:01:10[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6205, 0.3795]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 139/203 [2m0:02:30 â€¢ 0:01:10[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6206, 0.3794]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 139/203 [2m0:02:30 â€¢ 0:01:10[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6208, 0.3792]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 139/203 [2m0:02:30 â€¢ 0:01:10[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6210, 0.3790]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 139/203 [2m0:02:30 â€¢ 0:01:10[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6213, 0.3787]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 139/203 [2m0:02:30 â€¢ 0:01:10[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6217, 0.3783]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 139/203 [2m0:02:30 â€¢ 0:01:10[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6222, 0.3778]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 139/203 [2m0:02:30 â€¢ 0:01:10[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6228, 0.3772]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 139/203 [2m0:02:30 â€¢ 0:01:10[0m [2;4m0.93it/s[0m  
[2KStep 10 - TTA Loss: 0.7736â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 139/203 [2m0:02:30 â€¢ 0:01:10[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6236, 0.3764]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 139/203 [2m0:02:30 â€¢ 0:01:10[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6244, 0.3756]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 139/203 [2m0:02:30 â€¢ 0:01:10[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6255, 0.3745]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 139/203 [2m0:02:30 â€¢ 0:01:10[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6267, 0.3733]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 139/203 [2m0:02:30 â€¢ 0:01:10[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6280, 0.3720]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 139/203 [2m0:02:30 â€¢ 0:01:10[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6295, 0.3705]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 139/203 [2m0:02:30 â€¢ 0:01:10[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6312, 0.3688]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 139/203 [2m0:02:30 â€¢ 0:01:10[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6330, 0.3670]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 139/203 [2m0:02:30 â€¢ 0:01:10[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6350, 0.3650]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 139/203 [2m0:02:30 â€¢ 0:01:10[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6372, 0.3628]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 139/203 [2m0:02:30 â€¢ 0:01:10[0m [2;4m0.93it/s[0m  
[2KStep 20 - TTA Loss: 0.8048â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 139/203 [2m0:02:30 â€¢ 0:01:10[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6395, 0.3605]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 139/203 [2m0:02:30 â€¢ 0:01:10[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6419, 0.3581]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 139/203 [2m0:02:30 â€¢ 0:01:10[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6444, 0.3556]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 139/203 [2m0:02:30 â€¢ 0:01:10[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6471, 0.3529]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 139/203 [2m0:02:30 â€¢ 0:01:10[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6499, 0.3501]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 139/203 [2m0:02:30 â€¢ 0:01:10[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6528, 0.3472]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 139/203 [2m0:02:30 â€¢ 0:01:10[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6559, 0.3441]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 139/203 [2m0:02:30 â€¢ 0:01:10[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6592, 0.3408]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 139/203 [2m0:02:30 â€¢ 0:01:10[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6624, 0.3376]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 139/203 [2m0:02:30 â€¢ 0:01:10[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5810, 0.4190]], device='cuda:0')â•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 139/203 [2m0:02:30 â€¢ 0:01:10[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6659, 0.3341],â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 139/203 [2m0:02:30 â€¢ 0:01:10[0m [2;4m0.93it/s[0m  
        [0.6233, 0.3767],
        [0.6212, 0.3788],
        [0.6757, 0.3243],
        [0.6476, 0.3524],
        [0.6658, 0.3342],
        [0.6172, 0.3828],
        [0.6864, 0.3136],
        [0.6939, 0.3061],
[2K        [0.6935, 0.3065]], device='cuda:0')â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 139/203 [2m0:02:30 â€¢ 0:01:10[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5813, 0.4187],â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 140/203 [2m0:02:31 â€¢ 0:01:09[0m [2;4m0.92it/s[0m  
        [0.5774, 0.4226],
        [0.5810, 0.4190],
        [0.5801, 0.4199],
        [0.5819, 0.4181],
        [0.5790, 0.4210],
        [0.5774, 0.4226],
        [0.5810, 0.4190],
        [0.5811, 0.4189],
[2K        [0.5830, 0.4170]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 140/203 [2m0:02:31 â€¢ 0:01:09[0m [2;4m0.92it/s[0m  
[2KClass label: HammerThrowâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 140/203 [2m0:02:31 â€¢ 0:01:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6042, 0.3958]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 140/203 [2m0:02:31 â€¢ 0:01:09[0m [2;4m0.92it/s[0m  
[2KStep 0 - TTA Loss: 0.6020â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 140/203 [2m0:02:31 â€¢ 0:01:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6069, 0.3931]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 140/203 [2m0:02:31 â€¢ 0:01:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6095, 0.3905]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 140/203 [2m0:02:31 â€¢ 0:01:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6121, 0.3879]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 140/203 [2m0:02:31 â€¢ 0:01:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6145, 0.3855]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 140/203 [2m0:02:31 â€¢ 0:01:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6169, 0.3831]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 140/203 [2m0:02:31 â€¢ 0:01:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6192, 0.3808]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 140/203 [2m0:02:31 â€¢ 0:01:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6214, 0.3786]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 140/203 [2m0:02:31 â€¢ 0:01:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6235, 0.3765]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 140/203 [2m0:02:31 â€¢ 0:01:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6255, 0.3745]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 140/203 [2m0:02:31 â€¢ 0:01:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6275, 0.3725]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 140/203 [2m0:02:31 â€¢ 0:01:09[0m [2;4m0.92it/s[0m  
[2KStep 10 - TTA Loss: 0.6624â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 140/203 [2m0:02:31 â€¢ 0:01:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6293, 0.3707]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 140/203 [2m0:02:31 â€¢ 0:01:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6311, 0.3689]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 140/203 [2m0:02:31 â€¢ 0:01:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6327, 0.3673]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 140/203 [2m0:02:31 â€¢ 0:01:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6341, 0.3659]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 140/203 [2m0:02:31 â€¢ 0:01:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6354, 0.3646]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 140/203 [2m0:02:31 â€¢ 0:01:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6366, 0.3634]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 140/203 [2m0:02:31 â€¢ 0:01:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6377, 0.3623]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 140/203 [2m0:02:31 â€¢ 0:01:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6387, 0.3613]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 140/203 [2m0:02:31 â€¢ 0:01:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6395, 0.3605]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 140/203 [2m0:02:31 â€¢ 0:01:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6403, 0.3597]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 140/203 [2m0:02:31 â€¢ 0:01:09[0m [2;4m0.92it/s[0m  
[2KStep 20 - TTA Loss: 0.6383â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 140/203 [2m0:02:31 â€¢ 0:01:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6409, 0.3591]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 140/203 [2m0:02:31 â€¢ 0:01:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6414, 0.3586]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 140/203 [2m0:02:31 â€¢ 0:01:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6418, 0.3582]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 140/203 [2m0:02:31 â€¢ 0:01:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6421, 0.3579]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 140/203 [2m0:02:31 â€¢ 0:01:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6424, 0.3576]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 140/203 [2m0:02:31 â€¢ 0:01:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6426, 0.3574]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 140/203 [2m0:02:31 â€¢ 0:01:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6427, 0.3573]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 140/203 [2m0:02:31 â€¢ 0:01:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6428, 0.3572]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 140/203 [2m0:02:31 â€¢ 0:01:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6428, 0.3572]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 140/203 [2m0:02:31 â€¢ 0:01:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5831, 0.4169]], device='cuda:0');224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 140/203 [2m0:02:31 â€¢ 0:01:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6624, 0.3376],â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 140/203 [2m0:02:31 â€¢ 0:01:09[0m [2;4m0.92it/s[0m  
        [0.6189, 0.3811],
        [0.6173, 0.3827],
        [0.6744, 0.3256],
        [0.6428, 0.3572],
        [0.6525, 0.3475],
        [0.6038, 0.3962],
        [0.6814, 0.3186],
        [0.6878, 0.3122],
[2K        [0.6884, 0.3116]], device='cuda:0')â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 140/203 [2m0:02:31 â€¢ 0:01:09[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5812, 0.4188],â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 141/203 [2m0:02:32 â€¢ 0:01:07[0m [2;4m0.93it/s[0m  
        [0.5774, 0.4226],
        [0.5808, 0.4192],
        [0.5800, 0.4200],
        [0.5817, 0.4183],
        [0.5789, 0.4211],
        [0.5773, 0.4227],
        [0.5809, 0.4191],
        [0.5810, 0.4190],
[2K        [0.5828, 0.4172]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 141/203 [2m0:02:32 â€¢ 0:01:07[0m [2;4m0.93it/s[0m  
[2KClass label: TennisSwingâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 141/203 [2m0:02:32 â€¢ 0:01:07[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6687, 0.3313]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 141/203 [2m0:02:32 â€¢ 0:01:07[0m [2;4m0.93it/s[0m  
[2KStep 0 - TTA Loss: 1.0559â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 141/203 [2m0:02:32 â€¢ 0:01:07[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6687, 0.3313]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 141/203 [2m0:02:32 â€¢ 0:01:07[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6687, 0.3313]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 141/203 [2m0:02:32 â€¢ 0:01:07[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6687, 0.3313]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 141/203 [2m0:02:32 â€¢ 0:01:07[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6688, 0.3312]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 141/203 [2m0:02:32 â€¢ 0:01:07[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6688, 0.3312]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 141/203 [2m0:02:32 â€¢ 0:01:07[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6689, 0.3311]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 141/203 [2m0:02:32 â€¢ 0:01:07[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6691, 0.3309]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 141/203 [2m0:02:32 â€¢ 0:01:07[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6693, 0.3307]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 141/203 [2m0:02:32 â€¢ 0:01:07[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6695, 0.3305]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 141/203 [2m0:02:32 â€¢ 0:01:07[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6698, 0.3302]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 141/203 [2m0:02:32 â€¢ 0:01:07[0m [2;4m0.93it/s[0m  
[2KStep 10 - TTA Loss: 1.2316â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 141/203 [2m0:02:32 â€¢ 0:01:07[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6702, 0.3298]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 141/203 [2m0:02:32 â€¢ 0:01:07[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6705, 0.3295]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 141/203 [2m0:02:32 â€¢ 0:01:07[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6710, 0.3290]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 141/203 [2m0:02:32 â€¢ 0:01:07[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6715, 0.3285]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 141/203 [2m0:02:32 â€¢ 0:01:07[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6721, 0.3279]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 141/203 [2m0:02:32 â€¢ 0:01:07[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6726, 0.3274]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 141/203 [2m0:02:32 â€¢ 0:01:07[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6733, 0.3267]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 141/203 [2m0:02:32 â€¢ 0:01:07[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6742, 0.3258]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 141/203 [2m0:02:32 â€¢ 0:01:07[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6751, 0.3249]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 141/203 [2m0:02:32 â€¢ 0:01:07[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6763, 0.3237]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 141/203 [2m0:02:32 â€¢ 0:01:07[0m [2;4m0.93it/s[0m  
[2KStep 20 - TTA Loss: 0.6966â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 141/203 [2m0:02:32 â€¢ 0:01:07[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6775, 0.3225]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 141/203 [2m0:02:32 â€¢ 0:01:07[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6789, 0.3211]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 141/203 [2m0:02:32 â€¢ 0:01:07[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6804, 0.3196]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 141/203 [2m0:02:32 â€¢ 0:01:07[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6820, 0.3180]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 141/203 [2m0:02:32 â€¢ 0:01:07[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6836, 0.3164]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 141/203 [2m0:02:32 â€¢ 0:01:07[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6853, 0.3147]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 141/203 [2m0:02:32 â€¢ 0:01:07[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6870, 0.3130]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 141/203 [2m0:02:32 â€¢ 0:01:07[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6887, 0.3113]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 141/203 [2m0:02:32 â€¢ 0:01:07[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6903, 0.3097]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 141/203 [2m0:02:32 â€¢ 0:01:07[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5820, 0.4180]], device='cuda:0');224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 141/203 [2m0:02:32 â€¢ 0:01:07[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6561, 0.3439],â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 141/203 [2m0:02:32 â€¢ 0:01:07[0m [2;4m0.93it/s[0m  
        [0.6179, 0.3821],
        [0.6088, 0.3912],
        [0.6694, 0.3306],
        [0.6281, 0.3719],
        [0.6375, 0.3625],
        [0.5854, 0.4146],
        [0.6717, 0.3283],
        [0.6920, 0.3080],
[2K        [0.6784, 0.3216]], device='cuda:0')â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 141/203 [2m0:02:32 â€¢ 0:01:07[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5812, 0.4188],â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 142/203 [2m0:02:33 â€¢ 0:01:06[0m [2;4m0.94it/s[0m  
        [0.5773, 0.4227],
        [0.5807, 0.4193],
        [0.5799, 0.4201],
        [0.5816, 0.4184],
        [0.5789, 0.4211],
        [0.5772, 0.4228],
        [0.5809, 0.4191],
        [0.5809, 0.4191],
[2K        [0.5828, 0.4172]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 142/203 [2m0:02:33 â€¢ 0:01:06[0m [2;4m0.94it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 142/203 [2m0:02:33 â€¢ 0:01:06[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5709, 0.4291]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 142/203 [2m0:02:33 â€¢ 0:01:06[0m [2;4m0.94it/s[0m  
[2KStep 0 - TTA Loss: 0.6983â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 142/203 [2m0:02:33 â€¢ 0:01:06[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5720, 0.4280]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 142/203 [2m0:02:33 â€¢ 0:01:06[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5733, 0.4267]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 142/203 [2m0:02:33 â€¢ 0:01:06[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5747, 0.4253]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 142/203 [2m0:02:33 â€¢ 0:01:06[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5763, 0.4237]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 142/203 [2m0:02:33 â€¢ 0:01:06[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5780, 0.4220]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 142/203 [2m0:02:33 â€¢ 0:01:06[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5797, 0.4203]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 142/203 [2m0:02:33 â€¢ 0:01:06[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5815, 0.4185]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 142/203 [2m0:02:33 â€¢ 0:01:06[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5833, 0.4167]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 142/203 [2m0:02:33 â€¢ 0:01:06[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5851, 0.4149]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 142/203 [2m0:02:33 â€¢ 0:01:06[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5870, 0.4130]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 142/203 [2m0:02:33 â€¢ 0:01:06[0m [2;4m0.94it/s[0m  
[2KStep 10 - TTA Loss: 0.7108â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 142/203 [2m0:02:33 â€¢ 0:01:06[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5887, 0.4113]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 142/203 [2m0:02:33 â€¢ 0:01:06[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5905, 0.4095]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 142/203 [2m0:02:33 â€¢ 0:01:06[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5921, 0.4079]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 142/203 [2m0:02:33 â€¢ 0:01:06[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5937, 0.4063]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 142/203 [2m0:02:33 â€¢ 0:01:06[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5952, 0.4048]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 142/203 [2m0:02:33 â€¢ 0:01:06[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5966, 0.4034]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 142/203 [2m0:02:33 â€¢ 0:01:06[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5979, 0.4021]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 142/203 [2m0:02:33 â€¢ 0:01:06[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5990, 0.4010]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 142/203 [2m0:02:33 â€¢ 0:01:06[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6000, 0.4000]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 142/203 [2m0:02:33 â€¢ 0:01:06[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6009, 0.3991]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 142/203 [2m0:02:33 â€¢ 0:01:06[0m [2;4m0.94it/s[0m  
[2KStep 20 - TTA Loss: 0.6418â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 142/203 [2m0:02:33 â€¢ 0:01:06[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6017, 0.3983]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 142/203 [2m0:02:33 â€¢ 0:01:06[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6023, 0.3977]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 142/203 [2m0:02:33 â€¢ 0:01:06[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6028, 0.3972]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 142/203 [2m0:02:33 â€¢ 0:01:06[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6032, 0.3968]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 142/203 [2m0:02:33 â€¢ 0:01:06[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6035, 0.3965]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 142/203 [2m0:02:33 â€¢ 0:01:06[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6037, 0.3963]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 142/203 [2m0:02:33 â€¢ 0:01:06[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6039, 0.3961]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 142/203 [2m0:02:33 â€¢ 0:01:06[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6040, 0.3960]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 142/203 [2m0:02:33 â€¢ 0:01:06[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6040, 0.3960]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 142/203 [2m0:02:33 â€¢ 0:01:06[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5783, 0.4217]], device='cuda:0');224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 142/203 [2m0:02:33 â€¢ 0:01:06[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6571, 0.3429],â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 142/203 [2m0:02:33 â€¢ 0:01:06[0m [2;4m0.94it/s[0m  
        [0.6192, 0.3808],
        [0.6127, 0.3873],
        [0.6700, 0.3300],
        [0.6339, 0.3661],
        [0.6462, 0.3538],
        [0.6040, 0.3960],
        [0.6775, 0.3225],
        [0.6876, 0.3124],
[2K        [0.6822, 0.3178]], device='cuda:0')â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 142/203 [2m0:02:33 â€¢ 0:01:06[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5812, 0.4188],â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 143/203 [2m0:02:34 â€¢ 0:01:03[0m [2;4m0.96it/s[0m   
        [0.5773, 0.4227],
        [0.5807, 0.4193],
        [0.5799, 0.4201],
        [0.5815, 0.4185],
        [0.5787, 0.4213],
        [0.5771, 0.4229],
        [0.5809, 0.4191],
        [0.5808, 0.4192],
[2K        [0.5827, 0.4173]], device='cuda:0', grad_fn=<SoftmaxBackward0>)8;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 143/203 [2m0:02:34 â€¢ 0:01:03[0m [2;4m0.96it/s[0m  
[2KClass label: HammerThrowâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 143/203 [2m0:02:34 â€¢ 0:01:03[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6074, 0.3926]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 143/203 [2m0:02:34 â€¢ 0:01:03[0m [2;4m0.96it/s[0m  
[2KStep 0 - TTA Loss: 0.6047â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 143/203 [2m0:02:34 â€¢ 0:01:03[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6074, 0.3926]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 143/203 [2m0:02:34 â€¢ 0:01:03[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6074, 0.3926]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 143/203 [2m0:02:34 â€¢ 0:01:03[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6074, 0.3926]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 143/203 [2m0:02:34 â€¢ 0:01:03[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6075, 0.3925]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 143/203 [2m0:02:34 â€¢ 0:01:03[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6076, 0.3924]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 143/203 [2m0:02:34 â€¢ 0:01:03[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6079, 0.3921]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 143/203 [2m0:02:34 â€¢ 0:01:03[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6082, 0.3918]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 143/203 [2m0:02:34 â€¢ 0:01:03[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6087, 0.3913]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 143/203 [2m0:02:34 â€¢ 0:01:03[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6093, 0.3907]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 143/203 [2m0:02:34 â€¢ 0:01:03[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6101, 0.3899]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 143/203 [2m0:02:34 â€¢ 0:01:03[0m [2;4m0.96it/s[0m  
[2KStep 10 - TTA Loss: 0.5980â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 143/203 [2m0:02:34 â€¢ 0:01:03[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6111, 0.3889]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 143/203 [2m0:02:34 â€¢ 0:01:03[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6123, 0.3877]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 143/203 [2m0:02:34 â€¢ 0:01:03[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6137, 0.3863]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 143/203 [2m0:02:34 â€¢ 0:01:03[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6154, 0.3846]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 143/203 [2m0:02:34 â€¢ 0:01:03[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6173, 0.3827]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 143/203 [2m0:02:34 â€¢ 0:01:03[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6195, 0.3805]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 143/203 [2m0:02:34 â€¢ 0:01:03[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6219, 0.3781]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 143/203 [2m0:02:34 â€¢ 0:01:03[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6246, 0.3754]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 143/203 [2m0:02:34 â€¢ 0:01:03[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6276, 0.3724]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 143/203 [2m0:02:34 â€¢ 0:01:03[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6307, 0.3693]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 143/203 [2m0:02:34 â€¢ 0:01:03[0m [2;4m0.96it/s[0m  
[2KStep 20 - TTA Loss: 0.5606â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 143/203 [2m0:02:34 â€¢ 0:01:03[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6341, 0.3659]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 143/203 [2m0:02:34 â€¢ 0:01:03[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6378, 0.3622]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 143/203 [2m0:02:34 â€¢ 0:01:03[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6417, 0.3583]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 143/203 [2m0:02:34 â€¢ 0:01:03[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6457, 0.3543]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 143/203 [2m0:02:34 â€¢ 0:01:03[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6501, 0.3499]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 143/203 [2m0:02:34 â€¢ 0:01:03[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6546, 0.3454]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 143/203 [2m0:02:34 â€¢ 0:01:03[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6593, 0.3407]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 143/203 [2m0:02:34 â€¢ 0:01:03[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6641, 0.3359]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 143/203 [2m0:02:34 â€¢ 0:01:03[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6691, 0.3309]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 143/203 [2m0:02:34 â€¢ 0:01:03[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.5844, 0.4156]], device='cuda:0')mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 143/203 [2m0:02:34 â€¢ 0:01:03[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.6841, 0.3159],â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 143/203 [2m0:02:34 â€¢ 0:01:03[0m [2;4m0.96it/s[0m  
        [0.6267, 0.3733],
        [0.6342, 0.3658],
        [0.6927, 0.3073],
        [0.6742, 0.3258],
        [0.6710, 0.3290],
        [0.6236, 0.3764],
        [0.6963, 0.3037],
        [0.7027, 0.2973],
[2K        [0.7116, 0.2884]], device='cuda:0')â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 143/203 [2m0:02:34 â€¢ 0:01:03[0m [2;4m0.96it/s[0m  
[2KAttention weights: tensor([[0.5814, 0.4186],â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 144/203 [2m0:02:35 â€¢ 0:01:03[0m [2;4m0.95it/s[0m  
        [0.5774, 0.4226],
        [0.5807, 0.4193],
        [0.5801, 0.4199],
        [0.5818, 0.4182],
        [0.5788, 0.4212],
        [0.5772, 0.4228],
        [0.5810, 0.4190],
        [0.5810, 0.4190],
[2K        [0.5829, 0.4171]], device='cuda:0', grad_fn=<SoftmaxBackward0>)8;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 144/203 [2m0:02:35 â€¢ 0:01:03[0m [2;4m0.95it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 144/203 [2m0:02:35 â€¢ 0:01:03[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.5705, 0.4295]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 144/203 [2m0:02:35 â€¢ 0:01:03[0m [2;4m0.95it/s[0m  
[2KStep 0 - TTA Loss: 0.5120â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 144/203 [2m0:02:35 â€¢ 0:01:03[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.5744, 0.4256]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 144/203 [2m0:02:35 â€¢ 0:01:03[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.5783, 0.4217]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 144/203 [2m0:02:35 â€¢ 0:01:03[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.5820, 0.4180]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 144/203 [2m0:02:35 â€¢ 0:01:03[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.5857, 0.4143]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 144/203 [2m0:02:35 â€¢ 0:01:03[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.5893, 0.4107]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 144/203 [2m0:02:35 â€¢ 0:01:03[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.5927, 0.4073]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 144/203 [2m0:02:35 â€¢ 0:01:03[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.5960, 0.4040]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 144/203 [2m0:02:35 â€¢ 0:01:03[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.5993, 0.4007]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 144/203 [2m0:02:35 â€¢ 0:01:03[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6023, 0.3977]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 144/203 [2m0:02:35 â€¢ 0:01:03[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6053, 0.3947]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 144/203 [2m0:02:35 â€¢ 0:01:03[0m [2;4m0.95it/s[0m  
[2KStep 10 - TTA Loss: 0.4879â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 144/203 [2m0:02:35 â€¢ 0:01:03[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6080, 0.3920]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 144/203 [2m0:02:35 â€¢ 0:01:03[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6106, 0.3894]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 144/203 [2m0:02:35 â€¢ 0:01:03[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6130, 0.3870]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 144/203 [2m0:02:35 â€¢ 0:01:03[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6153, 0.3847]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 144/203 [2m0:02:35 â€¢ 0:01:03[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6174, 0.3826]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 144/203 [2m0:02:35 â€¢ 0:01:03[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6192, 0.3808]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 144/203 [2m0:02:35 â€¢ 0:01:03[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6209, 0.3791]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 144/203 [2m0:02:35 â€¢ 0:01:03[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6224, 0.3776]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 144/203 [2m0:02:35 â€¢ 0:01:03[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6238, 0.3762]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 144/203 [2m0:02:35 â€¢ 0:01:03[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6249, 0.3751]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 144/203 [2m0:02:35 â€¢ 0:01:03[0m [2;4m0.95it/s[0m  
[2KStep 20 - TTA Loss: 0.5099â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 144/203 [2m0:02:35 â€¢ 0:01:03[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6259, 0.3741]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 144/203 [2m0:02:35 â€¢ 0:01:03[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6267, 0.3733]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 144/203 [2m0:02:35 â€¢ 0:01:03[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6273, 0.3727]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 144/203 [2m0:02:35 â€¢ 0:01:03[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6278, 0.3722]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 144/203 [2m0:02:35 â€¢ 0:01:03[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6282, 0.3718]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 144/203 [2m0:02:35 â€¢ 0:01:03[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6285, 0.3715]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 144/203 [2m0:02:35 â€¢ 0:01:03[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6287, 0.3713]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 144/203 [2m0:02:35 â€¢ 0:01:03[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6288, 0.3712]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 144/203 [2m0:02:35 â€¢ 0:01:03[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6288, 0.3712]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 144/203 [2m0:02:35 â€¢ 0:01:03[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.5793, 0.4207]], device='cuda:0')mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 144/203 [2m0:02:35 â€¢ 0:01:03[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.6789, 0.3211],â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 144/203 [2m0:02:35 â€¢ 0:01:03[0m [2;4m0.95it/s[0m  
        [0.6267, 0.3733],
        [0.6318, 0.3682],
        [0.6887, 0.3113],
        [0.6653, 0.3347],
        [0.6703, 0.3297],
        [0.6289, 0.3711],
        [0.6958, 0.3042],
        [0.6984, 0.3016],
[2K        [0.7066, 0.2934]], device='cuda:0')â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 144/203 [2m0:02:35 â€¢ 0:01:03[0m [2;4m0.95it/s[0m  
[2KAttention weights: tensor([[0.5814, 0.4186],â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 145/203 [2m0:02:37 â€¢ 0:01:03[0m [2;4m0.92it/s[0m  
        [0.5774, 0.4226],
        [0.5807, 0.4193],
        [0.5801, 0.4199],
        [0.5818, 0.4182],
        [0.5788, 0.4212],
        [0.5772, 0.4228],
        [0.5810, 0.4190],
        [0.5810, 0.4190],
[2K        [0.5829, 0.4171]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 145/203 [2m0:02:37 â€¢ 0:01:03[0m [2;4m0.92it/s[0m  
[2KClass label: TennisSwingâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 145/203 [2m0:02:37 â€¢ 0:01:03[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6683, 0.3317]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 145/203 [2m0:02:37 â€¢ 0:01:03[0m [2;4m0.92it/s[0m  
[2KStep 0 - TTA Loss: 0.7039â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 145/203 [2m0:02:37 â€¢ 0:01:03[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6683, 0.3317]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 145/203 [2m0:02:37 â€¢ 0:01:03[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6684, 0.3316]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 145/203 [2m0:02:37 â€¢ 0:01:03[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6684, 0.3316]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 145/203 [2m0:02:37 â€¢ 0:01:03[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6684, 0.3316]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 145/203 [2m0:02:37 â€¢ 0:01:03[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6685, 0.3315]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 145/203 [2m0:02:37 â€¢ 0:01:03[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6687, 0.3313]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 145/203 [2m0:02:37 â€¢ 0:01:03[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6689, 0.3311]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 145/203 [2m0:02:37 â€¢ 0:01:03[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6691, 0.3309]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 145/203 [2m0:02:37 â€¢ 0:01:03[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6695, 0.3305]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 145/203 [2m0:02:37 â€¢ 0:01:03[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6698, 0.3302]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 145/203 [2m0:02:37 â€¢ 0:01:03[0m [2;4m0.92it/s[0m  
[2KStep 10 - TTA Loss: 0.6806â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 145/203 [2m0:02:37 â€¢ 0:01:03[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6704, 0.3296]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 145/203 [2m0:02:37 â€¢ 0:01:03[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6711, 0.3289]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 145/203 [2m0:02:37 â€¢ 0:01:03[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6718, 0.3282]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 145/203 [2m0:02:37 â€¢ 0:01:03[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6727, 0.3273]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 145/203 [2m0:02:37 â€¢ 0:01:03[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6738, 0.3262]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 145/203 [2m0:02:37 â€¢ 0:01:03[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6750, 0.3250]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 145/203 [2m0:02:37 â€¢ 0:01:03[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6764, 0.3236]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 145/203 [2m0:02:37 â€¢ 0:01:03[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6780, 0.3220]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 145/203 [2m0:02:37 â€¢ 0:01:03[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6797, 0.3203]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 145/203 [2m0:02:37 â€¢ 0:01:03[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6817, 0.3183]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 145/203 [2m0:02:37 â€¢ 0:01:03[0m [2;4m0.92it/s[0m  
[2KStep 20 - TTA Loss: 0.6662â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 145/203 [2m0:02:37 â€¢ 0:01:03[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6838, 0.3162]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 145/203 [2m0:02:37 â€¢ 0:01:03[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6861, 0.3139]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 145/203 [2m0:02:37 â€¢ 0:01:03[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6887, 0.3113]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 145/203 [2m0:02:37 â€¢ 0:01:03[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6913, 0.3087]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 145/203 [2m0:02:37 â€¢ 0:01:03[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6942, 0.3058]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 145/203 [2m0:02:37 â€¢ 0:01:03[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6972, 0.3028]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 145/203 [2m0:02:37 â€¢ 0:01:03[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.7003, 0.2997]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 145/203 [2m0:02:37 â€¢ 0:01:03[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.7035, 0.2965]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 145/203 [2m0:02:37 â€¢ 0:01:03[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.7068, 0.2932]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 145/203 [2m0:02:37 â€¢ 0:01:03[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5827, 0.4173]], device='cuda:0')6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 145/203 [2m0:02:37 â€¢ 0:01:03[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6668, 0.3332],â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 145/203 [2m0:02:37 â€¢ 0:01:03[0m [2;4m0.92it/s[0m  
        [0.6238, 0.3762],
        [0.6156, 0.3844],
        [0.6822, 0.3178],
        [0.6398, 0.3602],
        [0.6482, 0.3518],
        [0.5985, 0.4015],
        [0.6796, 0.3204],
        [0.7102, 0.2898],
[2K        [0.6902, 0.3098]], device='cuda:0')â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 145/203 [2m0:02:37 â€¢ 0:01:03[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5813, 0.4187],â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 146/203 [2m0:02:38 â€¢ 0:01:02[0m [2;4m0.93it/s[0m  
        [0.5773, 0.4227],
        [0.5806, 0.4194],
        [0.5799, 0.4201],
        [0.5817, 0.4183],
        [0.5786, 0.4214],
        [0.5771, 0.4229],
        [0.5809, 0.4191],
        [0.5808, 0.4192],
[2K        [0.5827, 0.4173]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 146/203 [2m0:02:38 â€¢ 0:01:02[0m [2;4m0.93it/s[0m  
[2KClass label: HammerThrowâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 146/203 [2m0:02:38 â€¢ 0:01:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6071, 0.3929]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 146/203 [2m0:02:38 â€¢ 0:01:02[0m [2;4m0.93it/s[0m  
[2KStep 0 - TTA Loss: 0.6901â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 146/203 [2m0:02:38 â€¢ 0:01:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6095, 0.3905]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 146/203 [2m0:02:38 â€¢ 0:01:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6120, 0.3880]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 146/203 [2m0:02:38 â€¢ 0:01:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6145, 0.3855]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 146/203 [2m0:02:38 â€¢ 0:01:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6172, 0.3828]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 146/203 [2m0:02:38 â€¢ 0:01:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6200, 0.3800]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 146/203 [2m0:02:38 â€¢ 0:01:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6228, 0.3772]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 146/203 [2m0:02:38 â€¢ 0:01:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6256, 0.3744]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 146/203 [2m0:02:38 â€¢ 0:01:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6284, 0.3716]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 146/203 [2m0:02:38 â€¢ 0:01:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6312, 0.3688]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 146/203 [2m0:02:38 â€¢ 0:01:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6340, 0.3660]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 146/203 [2m0:02:38 â€¢ 0:01:02[0m [2;4m0.93it/s[0m  
[2KStep 10 - TTA Loss: 0.5813â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 146/203 [2m0:02:38 â€¢ 0:01:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6367, 0.3633]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 146/203 [2m0:02:38 â€¢ 0:01:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6393, 0.3607]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 146/203 [2m0:02:38 â€¢ 0:01:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6418, 0.3582]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 146/203 [2m0:02:38 â€¢ 0:01:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6441, 0.3559]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 146/203 [2m0:02:38 â€¢ 0:01:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6463, 0.3537]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 146/203 [2m0:02:38 â€¢ 0:01:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6483, 0.3517]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 146/203 [2m0:02:38 â€¢ 0:01:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6501, 0.3499]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 146/203 [2m0:02:38 â€¢ 0:01:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6517, 0.3483]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 146/203 [2m0:02:38 â€¢ 0:01:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6532, 0.3468]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 146/203 [2m0:02:38 â€¢ 0:01:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6545, 0.3455]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 146/203 [2m0:02:38 â€¢ 0:01:02[0m [2;4m0.93it/s[0m  
[2KStep 20 - TTA Loss: 0.5667â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 146/203 [2m0:02:38 â€¢ 0:01:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6555, 0.3445]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 146/203 [2m0:02:38 â€¢ 0:01:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6564, 0.3436]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 146/203 [2m0:02:38 â€¢ 0:01:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6572, 0.3428]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 146/203 [2m0:02:38 â€¢ 0:01:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6577, 0.3423]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 146/203 [2m0:02:38 â€¢ 0:01:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6582, 0.3418]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 146/203 [2m0:02:38 â€¢ 0:01:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6585, 0.3415]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 146/203 [2m0:02:38 â€¢ 0:01:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6587, 0.3413]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 146/203 [2m0:02:38 â€¢ 0:01:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6588, 0.3412]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 146/203 [2m0:02:38 â€¢ 0:01:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6588, 0.3412]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 146/203 [2m0:02:38 â€¢ 0:01:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5836, 0.4164]], device='cuda:0')6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 146/203 [2m0:02:38 â€¢ 0:01:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6771, 0.3229],â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 146/203 [2m0:02:38 â€¢ 0:01:02[0m [2;4m0.93it/s[0m  
        [0.6309, 0.3691],
        [0.6274, 0.3726],
        [0.6875, 0.3125],
        [0.6589, 0.3411],
        [0.6602, 0.3398],
        [0.6084, 0.3916],
        [0.6909, 0.3091],
        [0.7087, 0.2913],
[2K        [0.7023, 0.2977]], device='cuda:0')â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 146/203 [2m0:02:38 â€¢ 0:01:02[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5812, 0.4188],â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 147/203 [2m0:02:39 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
        [0.5772, 0.4228],
        [0.5804, 0.4196],
        [0.5797, 0.4203],
        [0.5816, 0.4184],
        [0.5785, 0.4215],
        [0.5769, 0.4231],
        [0.5808, 0.4192],
        [0.5807, 0.4193],
[2K        [0.5826, 0.4174]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 147/203 [2m0:02:39 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KClass label: GolfSwingâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 147/203 [2m0:02:39 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6450, 0.3550]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 147/203 [2m0:02:39 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KStep 0 - TTA Loss: 0.3760â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 147/203 [2m0:02:39 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6450, 0.3550]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 147/203 [2m0:02:39 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6450, 0.3550]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 147/203 [2m0:02:39 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6451, 0.3549]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 147/203 [2m0:02:39 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6452, 0.3548]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 147/203 [2m0:02:39 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6453, 0.3547]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 147/203 [2m0:02:39 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6455, 0.3545]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 147/203 [2m0:02:39 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6459, 0.3541]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 147/203 [2m0:02:39 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6463, 0.3537]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 147/203 [2m0:02:39 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6469, 0.3531]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 147/203 [2m0:02:39 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6476, 0.3524]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 147/203 [2m0:02:39 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KStep 10 - TTA Loss: 0.3773â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 147/203 [2m0:02:39 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6485, 0.3515]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 147/203 [2m0:02:39 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6495, 0.3505]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 147/203 [2m0:02:39 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6508, 0.3492]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 147/203 [2m0:02:39 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6522, 0.3478]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 147/203 [2m0:02:39 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6538, 0.3462]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 147/203 [2m0:02:39 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6557, 0.3443]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 147/203 [2m0:02:39 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6577, 0.3423]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 147/203 [2m0:02:39 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6600, 0.3400]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 147/203 [2m0:02:39 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6624, 0.3376]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 147/203 [2m0:02:39 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6650, 0.3350]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 147/203 [2m0:02:39 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KStep 20 - TTA Loss: 0.3648â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 147/203 [2m0:02:39 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6677, 0.3323]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 147/203 [2m0:02:39 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6707, 0.3293]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 147/203 [2m0:02:39 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6738, 0.3262]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 147/203 [2m0:02:39 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6771, 0.3229]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 147/203 [2m0:02:39 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6804, 0.3196]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 147/203 [2m0:02:39 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6840, 0.3160]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 147/203 [2m0:02:39 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6876, 0.3124]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 147/203 [2m0:02:39 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6913, 0.3087]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 147/203 [2m0:02:39 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6951, 0.3049]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 147/203 [2m0:02:39 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5821, 0.4179]], device='cuda:0')6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 147/203 [2m0:02:39 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6808, 0.3192],â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 147/203 [2m0:02:39 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
        [0.6266, 0.3734],
        [0.6226, 0.3774],
        [0.6992, 0.3008],
        [0.6504, 0.3496],
        [0.6514, 0.3486],
        [0.6040, 0.3960],
        [0.6845, 0.3155],
        [0.7036, 0.2964],
[2K        [0.6990, 0.3010]], device='cuda:0')â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 147/203 [2m0:02:39 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5813, 0.4187],â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 148/203 [2m0:02:40 â€¢ 0:01:00[0m [2;4m0.93it/s[0m   
        [0.5773, 0.4227],
        [0.5805, 0.4195],
        [0.5799, 0.4201],
        [0.5817, 0.4183],
        [0.5785, 0.4215],
        [0.5770, 0.4230],
        [0.5808, 0.4192],
        [0.5808, 0.4192],
[2K        [0.5827, 0.4173]], device='cuda:0', grad_fn=<SoftmaxBackward0>)38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 148/203 [2m0:02:40 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KClass label: SoccerPenaltyâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 148/203 [2m0:02:40 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6597, 0.3403]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 148/203 [2m0:02:40 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KStep 0 - TTA Loss: 0.8061â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 148/203 [2m0:02:40 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6620, 0.3380]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 148/203 [2m0:02:40 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6644, 0.3356]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 148/203 [2m0:02:40 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6671, 0.3329]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 148/203 [2m0:02:40 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6698, 0.3302]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 148/203 [2m0:02:40 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6727, 0.3273]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 148/203 [2m0:02:40 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6755, 0.3245]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 148/203 [2m0:02:40 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6784, 0.3216]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 148/203 [2m0:02:40 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6813, 0.3187]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 148/203 [2m0:02:40 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6842, 0.3158]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 148/203 [2m0:02:40 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6871, 0.3129]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 148/203 [2m0:02:40 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KStep 10 - TTA Loss: 0.7768â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 148/203 [2m0:02:40 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6899, 0.3101]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 148/203 [2m0:02:40 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6925, 0.3075]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 148/203 [2m0:02:40 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6951, 0.3049]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 148/203 [2m0:02:40 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6974, 0.3026]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 148/203 [2m0:02:40 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6996, 0.3004]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 148/203 [2m0:02:40 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7016, 0.2984]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 148/203 [2m0:02:40 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7035, 0.2965]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 148/203 [2m0:02:40 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7051, 0.2949]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 148/203 [2m0:02:40 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7065, 0.2935]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 148/203 [2m0:02:40 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7078, 0.2922]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 148/203 [2m0:02:40 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KStep 20 - TTA Loss: 0.8117â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 148/203 [2m0:02:40 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7089, 0.2911]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 148/203 [2m0:02:40 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7098, 0.2902]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 148/203 [2m0:02:40 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7105, 0.2895]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 148/203 [2m0:02:40 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7111, 0.2889]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 148/203 [2m0:02:40 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7115, 0.2885]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 148/203 [2m0:02:40 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7118, 0.2882]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 148/203 [2m0:02:40 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7120, 0.2880]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 148/203 [2m0:02:40 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7121, 0.2879]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 148/203 [2m0:02:40 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.7122, 0.2878]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 148/203 [2m0:02:40 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5833, 0.4167]], device='cuda:0')7mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 148/203 [2m0:02:40 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.6772, 0.3228],â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 148/203 [2m0:02:40 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
        [0.6298, 0.3702],
        [0.6201, 0.3799],
        [0.6912, 0.3088],
        [0.6498, 0.3502],
        [0.6576, 0.3424],
        [0.6087, 0.3913],
        [0.7123, 0.2877],
        [0.7004, 0.2996],
[2K        [0.6958, 0.3042]], device='cuda:0')â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 148/203 [2m0:02:40 â€¢ 0:01:00[0m [2;4m0.93it/s[0m  
[2KAttention weights: tensor([[0.5812, 0.4188],â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 149/203 [2m0:02:41 â€¢ 0:00:59[0m [2;4m0.92it/s[0m  
        [0.5773, 0.4227],
        [0.5805, 0.4195],
        [0.5800, 0.4200],
        [0.5817, 0.4183],
        [0.5785, 0.4215],
        [0.5769, 0.4231],
        [0.5809, 0.4191],
        [0.5809, 0.4191],
[2K        [0.5827, 0.4173]], device='cuda:0', grad_fn=<SoftmaxBackward0>)38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 149/203 [2m0:02:41 â€¢ 0:00:59[0m [2;4m0.92it/s[0m  
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 149/203 [2m0:02:41 â€¢ 0:00:59[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6195, 0.3805]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 149/203 [2m0:02:41 â€¢ 0:00:59[0m [2;4m0.92it/s[0m  
[2KStep 0 - TTA Loss: 0.6958â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 149/203 [2m0:02:41 â€¢ 0:00:59[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6195, 0.3805]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 149/203 [2m0:02:41 â€¢ 0:00:59[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6195, 0.3805]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 149/203 [2m0:02:41 â€¢ 0:00:59[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6195, 0.3805]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 149/203 [2m0:02:41 â€¢ 0:00:59[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6196, 0.3804]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 149/203 [2m0:02:41 â€¢ 0:00:59[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6197, 0.3803]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 149/203 [2m0:02:41 â€¢ 0:00:59[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6198, 0.3802]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 149/203 [2m0:02:41 â€¢ 0:00:59[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6200, 0.3800]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 149/203 [2m0:02:41 â€¢ 0:00:59[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6202, 0.3798]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 149/203 [2m0:02:41 â€¢ 0:00:59[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6206, 0.3794]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 149/203 [2m0:02:41 â€¢ 0:00:59[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6209, 0.3791]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 149/203 [2m0:02:41 â€¢ 0:00:59[0m [2;4m0.92it/s[0m  
[2KStep 10 - TTA Loss: 0.7261â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 149/203 [2m0:02:41 â€¢ 0:00:59[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6213, 0.3787]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 149/203 [2m0:02:41 â€¢ 0:00:59[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6218, 0.3782]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 149/203 [2m0:02:41 â€¢ 0:00:59[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6224, 0.3776]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 149/203 [2m0:02:41 â€¢ 0:00:59[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6231, 0.3769]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 149/203 [2m0:02:41 â€¢ 0:00:59[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6239, 0.3761]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 149/203 [2m0:02:41 â€¢ 0:00:59[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6248, 0.3752]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 149/203 [2m0:02:41 â€¢ 0:00:59[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6259, 0.3741]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 149/203 [2m0:02:41 â€¢ 0:00:59[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6269, 0.3731]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 149/203 [2m0:02:41 â€¢ 0:00:59[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6281, 0.3719]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 149/203 [2m0:02:41 â€¢ 0:00:59[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6293, 0.3707]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 149/203 [2m0:02:41 â€¢ 0:00:59[0m [2;4m0.92it/s[0m  
[2KStep 20 - TTA Loss: 0.8062â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 149/203 [2m0:02:41 â€¢ 0:00:59[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6306, 0.3694]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 149/203 [2m0:02:41 â€¢ 0:00:59[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6320, 0.3680]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 149/203 [2m0:02:41 â€¢ 0:00:59[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6334, 0.3666]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 149/203 [2m0:02:41 â€¢ 0:00:59[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6348, 0.3652]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 149/203 [2m0:02:41 â€¢ 0:00:59[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6364, 0.3636]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 149/203 [2m0:02:41 â€¢ 0:00:59[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6380, 0.3620]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 149/203 [2m0:02:41 â€¢ 0:00:59[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6396, 0.3604]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 149/203 [2m0:02:41 â€¢ 0:00:59[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6413, 0.3587]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 149/203 [2m0:02:41 â€¢ 0:00:59[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6431, 0.3569]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 149/203 [2m0:02:41 â€¢ 0:00:59[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5795, 0.4205]], device='cuda:0')7mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 149/203 [2m0:02:41 â€¢ 0:00:59[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6573, 0.3427],â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 149/203 [2m0:02:41 â€¢ 0:00:59[0m [2;4m0.92it/s[0m  
        [0.6119, 0.3881],
        [0.6101, 0.3899],
        [0.6666, 0.3334],
        [0.6331, 0.3669],
        [0.6450, 0.3550],
        [0.5973, 0.4027],
        [0.6797, 0.3203],
        [0.6830, 0.3170],
[2K        [0.6800, 0.3200]], device='cuda:0')â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 149/203 [2m0:02:41 â€¢ 0:00:59[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5812, 0.4188],â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 150/203 [2m0:02:42 â€¢ 0:00:58[0m [2;4m0.92it/s[0m  
        [0.5772, 0.4228],
        [0.5804, 0.4196],
        [0.5800, 0.4200],
        [0.5816, 0.4184],
        [0.5783, 0.4217],
        [0.5768, 0.4232],
        [0.5809, 0.4191],
        [0.5809, 0.4191],
[2K        [0.5827, 0.4173]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 150/203 [2m0:02:42 â€¢ 0:00:58[0m [2;4m0.92it/s[0m  
[2KClass label: BaseballPitchâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 150/203 [2m0:02:42 â€¢ 0:00:58[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6307, 0.3693]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 150/203 [2m0:02:42 â€¢ 0:00:58[0m [2;4m0.92it/s[0m  
[2KStep 0 - TTA Loss: 0.6596â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 150/203 [2m0:02:42 â€¢ 0:00:58[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6325, 0.3675]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 150/203 [2m0:02:42 â€¢ 0:00:58[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6347, 0.3653]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 150/203 [2m0:02:42 â€¢ 0:00:58[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6372, 0.3628]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 150/203 [2m0:02:42 â€¢ 0:00:58[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6402, 0.3598]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 150/203 [2m0:02:42 â€¢ 0:00:58[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6434, 0.3566]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 150/203 [2m0:02:42 â€¢ 0:00:58[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6466, 0.3534]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 150/203 [2m0:02:42 â€¢ 0:00:58[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6500, 0.3500]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 150/203 [2m0:02:42 â€¢ 0:00:58[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6534, 0.3466]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 150/203 [2m0:02:42 â€¢ 0:00:58[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6568, 0.3432]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 150/203 [2m0:02:42 â€¢ 0:00:58[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6602, 0.3398]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 150/203 [2m0:02:42 â€¢ 0:00:58[0m [2;4m0.92it/s[0m  
[2KStep 10 - TTA Loss: 0.6732â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 150/203 [2m0:02:42 â€¢ 0:00:58[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6635, 0.3365]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 150/203 [2m0:02:42 â€¢ 0:00:58[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6667, 0.3333]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 150/203 [2m0:02:42 â€¢ 0:00:58[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6698, 0.3302]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 150/203 [2m0:02:42 â€¢ 0:00:58[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6726, 0.3274]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 150/203 [2m0:02:42 â€¢ 0:00:58[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6752, 0.3248]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 150/203 [2m0:02:42 â€¢ 0:00:58[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6776, 0.3224]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 150/203 [2m0:02:42 â€¢ 0:00:58[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6798, 0.3202]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 150/203 [2m0:02:42 â€¢ 0:00:58[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6817, 0.3183]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 150/203 [2m0:02:42 â€¢ 0:00:58[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6834, 0.3166]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 150/203 [2m0:02:42 â€¢ 0:00:58[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6849, 0.3151]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 150/203 [2m0:02:42 â€¢ 0:00:58[0m [2;4m0.92it/s[0m  
[2KStep 20 - TTA Loss: 0.6615â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 150/203 [2m0:02:42 â€¢ 0:00:58[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6861, 0.3139]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 150/203 [2m0:02:42 â€¢ 0:00:58[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6872, 0.3128]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 150/203 [2m0:02:42 â€¢ 0:00:58[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6881, 0.3119]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 150/203 [2m0:02:42 â€¢ 0:00:58[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6888, 0.3112]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 150/203 [2m0:02:42 â€¢ 0:00:58[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6894, 0.3106]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 150/203 [2m0:02:42 â€¢ 0:00:58[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6897, 0.3103]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 150/203 [2m0:02:42 â€¢ 0:00:58[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6900, 0.3100]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 150/203 [2m0:02:42 â€¢ 0:00:58[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6901, 0.3099]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 150/203 [2m0:02:42 â€¢ 0:00:58[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6902, 0.3098]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 150/203 [2m0:02:42 â€¢ 0:00:58[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5839, 0.4161]], device='cuda:0');6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 150/203 [2m0:02:42 â€¢ 0:00:58[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6903, 0.3097],â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 150/203 [2m0:02:42 â€¢ 0:00:58[0m [2;4m0.92it/s[0m  
        [0.6209, 0.3791],
        [0.6165, 0.3835],
        [0.6875, 0.3125],
        [0.6486, 0.3514],
        [0.6563, 0.3437],
        [0.6069, 0.3931],
        [0.6900, 0.3100],
        [0.6975, 0.3025],
[2K        [0.6965, 0.3035]], device='cuda:0')â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 150/203 [2m0:02:42 â€¢ 0:00:58[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5817, 0.4183],â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 151/203 [2m0:02:43 â€¢ 0:00:57[0m [2;4m0.92it/s[0m  
        [0.5774, 0.4226],
        [0.5806, 0.4194],
        [0.5803, 0.4197],
        [0.5818, 0.4182],
        [0.5784, 0.4216],
        [0.5769, 0.4231],
        [0.5811, 0.4189],
        [0.5812, 0.4188],
[2K        [0.5828, 0.4172]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 151/203 [2m0:02:43 â€¢ 0:00:57[0m [2;4m0.92it/s[0m  
[2KClass label: GolfSwingâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 151/203 [2m0:02:43 â€¢ 0:00:57[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6518, 0.3482]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 151/203 [2m0:02:43 â€¢ 0:00:57[0m [2;4m0.92it/s[0m  
[2KStep 0 - TTA Loss: 0.7651â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 151/203 [2m0:02:43 â€¢ 0:00:57[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6518, 0.3482]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 151/203 [2m0:02:43 â€¢ 0:00:57[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6518, 0.3482]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 151/203 [2m0:02:43 â€¢ 0:00:57[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6518, 0.3482]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 151/203 [2m0:02:43 â€¢ 0:00:57[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6519, 0.3481]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 151/203 [2m0:02:43 â€¢ 0:00:57[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6521, 0.3479]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 151/203 [2m0:02:43 â€¢ 0:00:57[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6524, 0.3476]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 151/203 [2m0:02:43 â€¢ 0:00:57[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6528, 0.3472]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 151/203 [2m0:02:43 â€¢ 0:00:57[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6533, 0.3467]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 151/203 [2m0:02:43 â€¢ 0:00:57[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6540, 0.3460]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 151/203 [2m0:02:43 â€¢ 0:00:57[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6548, 0.3452]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 151/203 [2m0:02:43 â€¢ 0:00:57[0m [2;4m0.92it/s[0m  
[2KStep 10 - TTA Loss: 0.7771â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 151/203 [2m0:02:43 â€¢ 0:00:57[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6558, 0.3442]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 151/203 [2m0:02:43 â€¢ 0:00:57[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6570, 0.3430]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 151/203 [2m0:02:43 â€¢ 0:00:57[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6584, 0.3416]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 151/203 [2m0:02:43 â€¢ 0:00:57[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6600, 0.3400]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 151/203 [2m0:02:43 â€¢ 0:00:57[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6617, 0.3383]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 151/203 [2m0:02:43 â€¢ 0:00:57[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6637, 0.3363]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 151/203 [2m0:02:43 â€¢ 0:00:57[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6658, 0.3342]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 151/203 [2m0:02:43 â€¢ 0:00:57[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6681, 0.3319]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 151/203 [2m0:02:43 â€¢ 0:00:57[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6706, 0.3294]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 151/203 [2m0:02:43 â€¢ 0:00:57[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6732, 0.3268]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 151/203 [2m0:02:43 â€¢ 0:00:57[0m [2;4m0.92it/s[0m  
[2KStep 20 - TTA Loss: 0.8189â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 151/203 [2m0:02:43 â€¢ 0:00:57[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6760, 0.3240]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 151/203 [2m0:02:43 â€¢ 0:00:57[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6789, 0.3211]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 151/203 [2m0:02:43 â€¢ 0:00:57[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6819, 0.3181]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 151/203 [2m0:02:43 â€¢ 0:00:57[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6850, 0.3150]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 151/203 [2m0:02:43 â€¢ 0:00:57[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6881, 0.3119]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 151/203 [2m0:02:43 â€¢ 0:00:57[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6914, 0.3086]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 151/203 [2m0:02:43 â€¢ 0:00:57[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6948, 0.3052]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 151/203 [2m0:02:43 â€¢ 0:00:57[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6982, 0.3018]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 151/203 [2m0:02:43 â€¢ 0:00:57[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.7016, 0.2984]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 151/203 [2m0:02:43 â€¢ 0:00:57[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5830, 0.4170]], device='cuda:0');6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 151/203 [2m0:02:43 â€¢ 0:00:57[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6880, 0.3120],â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 151/203 [2m0:02:43 â€¢ 0:00:57[0m [2;4m0.92it/s[0m  
        [0.6285, 0.3715],
        [0.6256, 0.3744],
        [0.7051, 0.2949],
        [0.6512, 0.3488],
        [0.6551, 0.3449],
        [0.6065, 0.3935],
        [0.6884, 0.3116],
        [0.7061, 0.2939],
[2K        [0.7013, 0.2987]], device='cuda:0')â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 151/203 [2m0:02:43 â€¢ 0:00:57[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5819, 0.4181],â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 152/203 [2m0:02:44 â€¢ 0:00:56[0m [2;4m0.92it/s[0m  
        [0.5776, 0.4224],
        [0.5807, 0.4193],
        [0.5805, 0.4195],
        [0.5820, 0.4180],
        [0.5786, 0.4214],
        [0.5771, 0.4229],
        [0.5814, 0.4186],
        [0.5814, 0.4186],
[2K        [0.5830, 0.4170]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 152/203 [2m0:02:44 â€¢ 0:00:56[0m [2;4m0.92it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 152/203 [2m0:02:44 â€¢ 0:00:56[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5765, 0.4235]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 152/203 [2m0:02:44 â€¢ 0:00:56[0m [2;4m0.92it/s[0m  
[2KStep 0 - TTA Loss: 0.5470â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 152/203 [2m0:02:44 â€¢ 0:00:56[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5788, 0.4212]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 152/203 [2m0:02:44 â€¢ 0:00:56[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5810, 0.4190]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 152/203 [2m0:02:44 â€¢ 0:00:56[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5833, 0.4167]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 152/203 [2m0:02:44 â€¢ 0:00:56[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5854, 0.4146]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 152/203 [2m0:02:44 â€¢ 0:00:56[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5876, 0.4124]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 152/203 [2m0:02:44 â€¢ 0:00:56[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5897, 0.4103]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 152/203 [2m0:02:44 â€¢ 0:00:56[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5918, 0.4082]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 152/203 [2m0:02:44 â€¢ 0:00:56[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5938, 0.4062]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 152/203 [2m0:02:44 â€¢ 0:00:56[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5958, 0.4042]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 152/203 [2m0:02:44 â€¢ 0:00:56[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5976, 0.4024]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 152/203 [2m0:02:44 â€¢ 0:00:56[0m [2;4m0.92it/s[0m  
[2KStep 10 - TTA Loss: 0.6422â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 152/203 [2m0:02:44 â€¢ 0:00:56[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5993, 0.4007]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 152/203 [2m0:02:44 â€¢ 0:00:56[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6010, 0.3990]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 152/203 [2m0:02:44 â€¢ 0:00:56[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6025, 0.3975]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 152/203 [2m0:02:44 â€¢ 0:00:56[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6039, 0.3961]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 152/203 [2m0:02:44 â€¢ 0:00:56[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6053, 0.3947]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 152/203 [2m0:02:44 â€¢ 0:00:56[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6065, 0.3935]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 152/203 [2m0:02:44 â€¢ 0:00:56[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6076, 0.3924]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 152/203 [2m0:02:44 â€¢ 0:00:56[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6086, 0.3914]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 152/203 [2m0:02:44 â€¢ 0:00:56[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6095, 0.3905]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 152/203 [2m0:02:44 â€¢ 0:00:56[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6102, 0.3898]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 152/203 [2m0:02:44 â€¢ 0:00:56[0m [2;4m0.92it/s[0m  
[2KStep 20 - TTA Loss: 0.5396â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 152/203 [2m0:02:44 â€¢ 0:00:56[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6109, 0.3891]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 152/203 [2m0:02:44 â€¢ 0:00:56[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6114, 0.3886]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 152/203 [2m0:02:44 â€¢ 0:00:56[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6118, 0.3882]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 152/203 [2m0:02:44 â€¢ 0:00:56[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6122, 0.3878]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 152/203 [2m0:02:44 â€¢ 0:00:56[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6124, 0.3876]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 152/203 [2m0:02:44 â€¢ 0:00:56[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6126, 0.3874]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 152/203 [2m0:02:44 â€¢ 0:00:56[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6127, 0.3873]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 152/203 [2m0:02:44 â€¢ 0:00:56[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6128, 0.3872]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 152/203 [2m0:02:44 â€¢ 0:00:56[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6128, 0.3872]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 152/203 [2m0:02:44 â€¢ 0:00:56[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5788, 0.4212]], device='cuda:0');6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 152/203 [2m0:02:44 â€¢ 0:00:56[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.6724, 0.3276],â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 152/203 [2m0:02:44 â€¢ 0:00:56[0m [2;4m0.92it/s[0m  
        [0.6249, 0.3751],
        [0.6216, 0.3784],
        [0.6883, 0.3117],
        [0.6438, 0.3562],
        [0.6537, 0.3463],
        [0.6129, 0.3871],
        [0.6868, 0.3132],
        [0.6972, 0.3028],
[2K        [0.6944, 0.3056]], device='cuda:0')â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 152/203 [2m0:02:44 â€¢ 0:00:56[0m [2;4m0.92it/s[0m  
[2KAttention weights: tensor([[0.5821, 0.4179],â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 153/203 [2m0:02:47 â€¢ 0:00:59[0m [2;4m0.86it/s[0m   
        [0.5777, 0.4223],
        [0.5808, 0.4192],
        [0.5807, 0.4193],
        [0.5821, 0.4179],
        [0.5787, 0.4213],
        [0.5773, 0.4227],
        [0.5815, 0.4185],
        [0.5816, 0.4184],
[2K        [0.5832, 0.4168]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 153/203 [2m0:02:47 â€¢ 0:00:59[0m [2;4m0.86it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 153/203 [2m0:02:47 â€¢ 0:00:59[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6610, 0.3390]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 153/203 [2m0:02:47 â€¢ 0:00:59[0m [2;4m0.86it/s[0m  
[2KStep 0 - TTA Loss: 0.7883â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 153/203 [2m0:02:47 â€¢ 0:00:59[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6610, 0.3390]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 153/203 [2m0:02:47 â€¢ 0:00:59[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6610, 0.3390]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 153/203 [2m0:02:47 â€¢ 0:00:59[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6611, 0.3389]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 153/203 [2m0:02:47 â€¢ 0:00:59[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6611, 0.3389]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 153/203 [2m0:02:47 â€¢ 0:00:59[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6612, 0.3388]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 153/203 [2m0:02:47 â€¢ 0:00:59[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6614, 0.3386]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 153/203 [2m0:02:47 â€¢ 0:00:59[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6617, 0.3383]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 153/203 [2m0:02:47 â€¢ 0:00:59[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6621, 0.3379]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 153/203 [2m0:02:47 â€¢ 0:00:59[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6627, 0.3373]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 153/203 [2m0:02:47 â€¢ 0:00:59[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6633, 0.3367]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 153/203 [2m0:02:47 â€¢ 0:00:59[0m [2;4m0.86it/s[0m  
[2KStep 10 - TTA Loss: 0.7620â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 153/203 [2m0:02:47 â€¢ 0:00:59[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6642, 0.3358]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 153/203 [2m0:02:47 â€¢ 0:00:59[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6652, 0.3348]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 153/203 [2m0:02:47 â€¢ 0:00:59[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6664, 0.3336]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 153/203 [2m0:02:47 â€¢ 0:00:59[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6678, 0.3322]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 153/203 [2m0:02:47 â€¢ 0:00:59[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6694, 0.3306]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 153/203 [2m0:02:47 â€¢ 0:00:59[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6711, 0.3289]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 153/203 [2m0:02:47 â€¢ 0:00:59[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6731, 0.3269]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 153/203 [2m0:02:47 â€¢ 0:00:59[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6753, 0.3247]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 153/203 [2m0:02:47 â€¢ 0:00:59[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6778, 0.3222]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 153/203 [2m0:02:47 â€¢ 0:00:59[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6805, 0.3195]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 153/203 [2m0:02:47 â€¢ 0:00:59[0m [2;4m0.86it/s[0m  
[2KStep 20 - TTA Loss: 0.6972â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 153/203 [2m0:02:47 â€¢ 0:00:59[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6833, 0.3167]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 153/203 [2m0:02:47 â€¢ 0:00:59[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6863, 0.3137]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 153/203 [2m0:02:47 â€¢ 0:00:59[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6896, 0.3104]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 153/203 [2m0:02:47 â€¢ 0:00:59[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6931, 0.3069]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 153/203 [2m0:02:47 â€¢ 0:00:59[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6967, 0.3033]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 153/203 [2m0:02:47 â€¢ 0:00:59[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.7005, 0.2995]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 153/203 [2m0:02:47 â€¢ 0:00:59[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.7044, 0.2956]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 153/203 [2m0:02:47 â€¢ 0:00:59[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.7085, 0.2915]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 153/203 [2m0:02:47 â€¢ 0:00:59[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.7127, 0.2873]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 153/203 [2m0:02:47 â€¢ 0:00:59[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5859, 0.4141]], device='cuda:0')37mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 153/203 [2m0:02:47 â€¢ 0:00:59[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6887, 0.3113],â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 153/203 [2m0:02:47 â€¢ 0:00:59[0m [2;4m0.86it/s[0m  
        [0.6324, 0.3676],
        [0.6336, 0.3664],
        [0.6974, 0.3026],
        [0.6663, 0.3337],
        [0.6689, 0.3311],
        [0.6219, 0.3781],
        [0.6931, 0.3069],
        [0.7095, 0.2905],
[2K        [0.7172, 0.2828]], device='cuda:0')â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 153/203 [2m0:02:47 â€¢ 0:00:59[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5821, 0.4179],â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 154/203 [2m0:02:48 â€¢ 0:00:58[0m [2;4m0.86it/s[0m  
        [0.5777, 0.4223],
        [0.5808, 0.4192],
        [0.5807, 0.4193],
        [0.5821, 0.4179],
        [0.5787, 0.4213],
        [0.5772, 0.4228],
        [0.5815, 0.4185],
        [0.5815, 0.4185],
[2K        [0.5832, 0.4168]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 154/203 [2m0:02:48 â€¢ 0:00:58[0m [2;4m0.86it/s[0m  
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 154/203 [2m0:02:48 â€¢ 0:00:58[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6168, 0.3832]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 154/203 [2m0:02:48 â€¢ 0:00:58[0m [2;4m0.86it/s[0m  
[2KStep 0 - TTA Loss: 0.6955â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 154/203 [2m0:02:48 â€¢ 0:00:58[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6210, 0.3790]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 154/203 [2m0:02:48 â€¢ 0:00:58[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6252, 0.3748]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 154/203 [2m0:02:48 â€¢ 0:00:58[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6294, 0.3706]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 154/203 [2m0:02:48 â€¢ 0:00:58[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6335, 0.3665]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 154/203 [2m0:02:48 â€¢ 0:00:58[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6377, 0.3623]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 154/203 [2m0:02:48 â€¢ 0:00:58[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6418, 0.3582]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 154/203 [2m0:02:48 â€¢ 0:00:58[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6458, 0.3542]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 154/203 [2m0:02:48 â€¢ 0:00:58[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6498, 0.3502]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 154/203 [2m0:02:48 â€¢ 0:00:58[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6537, 0.3463]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 154/203 [2m0:02:48 â€¢ 0:00:58[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6575, 0.3425]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 154/203 [2m0:02:48 â€¢ 0:00:58[0m [2;4m0.86it/s[0m  
[2KStep 10 - TTA Loss: 0.5802â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 154/203 [2m0:02:48 â€¢ 0:00:58[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6611, 0.3389]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 154/203 [2m0:02:48 â€¢ 0:00:58[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6646, 0.3354]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 154/203 [2m0:02:48 â€¢ 0:00:58[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6678, 0.3322]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 154/203 [2m0:02:48 â€¢ 0:00:58[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6708, 0.3292]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 154/203 [2m0:02:48 â€¢ 0:00:58[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6736, 0.3264]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 154/203 [2m0:02:48 â€¢ 0:00:58[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6762, 0.3238]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 154/203 [2m0:02:48 â€¢ 0:00:58[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6786, 0.3214]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 154/203 [2m0:02:48 â€¢ 0:00:58[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6808, 0.3192]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 154/203 [2m0:02:48 â€¢ 0:00:58[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6827, 0.3173]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 154/203 [2m0:02:48 â€¢ 0:00:58[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6844, 0.3156]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 154/203 [2m0:02:48 â€¢ 0:00:58[0m [2;4m0.86it/s[0m  
[2KStep 20 - TTA Loss: 0.6595â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 154/203 [2m0:02:48 â€¢ 0:00:58[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6858, 0.3142]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 154/203 [2m0:02:48 â€¢ 0:00:58[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6870, 0.3130]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 154/203 [2m0:02:48 â€¢ 0:00:58[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6880, 0.3120]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 154/203 [2m0:02:48 â€¢ 0:00:58[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6887, 0.3113]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 154/203 [2m0:02:48 â€¢ 0:00:58[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6893, 0.3107]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 154/203 [2m0:02:48 â€¢ 0:00:58[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6897, 0.3103]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 154/203 [2m0:02:48 â€¢ 0:00:58[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6900, 0.3100]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 154/203 [2m0:02:48 â€¢ 0:00:58[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6901, 0.3099]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 154/203 [2m0:02:48 â€¢ 0:00:58[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6902, 0.3098]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 154/203 [2m0:02:48 â€¢ 0:00:58[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5819, 0.4181]], device='cuda:0')37mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 154/203 [2m0:02:48 â€¢ 0:00:58[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6955, 0.3045],â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 154/203 [2m0:02:48 â€¢ 0:00:58[0m [2;4m0.86it/s[0m  
        [0.6373, 0.3627],
        [0.6418, 0.3582],
        [0.7010, 0.2990],
        [0.6760, 0.3240],
        [0.6903, 0.3097],
        [0.6352, 0.3648],
        [0.7020, 0.2980],
        [0.7184, 0.2816],
[2K        [0.7238, 0.2762]], device='cuda:0')â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 154/203 [2m0:02:48 â€¢ 0:00:58[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5821, 0.4179],â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 155/203 [2m0:02:49 â€¢ 0:00:56[0m [2;4m0.86it/s[0m  
        [0.5777, 0.4223],
        [0.5809, 0.4191],
        [0.5808, 0.4192],
        [0.5823, 0.4177],
        [0.5789, 0.4211],
        [0.5774, 0.4226],
        [0.5815, 0.4185],
        [0.5815, 0.4185],
[2K        [0.5833, 0.4167]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 155/203 [2m0:02:49 â€¢ 0:00:56[0m [2;4m0.86it/s[0m  
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 155/203 [2m0:02:49 â€¢ 0:00:56[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6176, 0.3824]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 155/203 [2m0:02:49 â€¢ 0:00:56[0m [2;4m0.86it/s[0m  
[2KStep 0 - TTA Loss: 0.5192â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 155/203 [2m0:02:49 â€¢ 0:00:56[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6176, 0.3824]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 155/203 [2m0:02:49 â€¢ 0:00:56[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6176, 0.3824]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 155/203 [2m0:02:49 â€¢ 0:00:56[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6177, 0.3823]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 155/203 [2m0:02:49 â€¢ 0:00:56[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6178, 0.3822]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 155/203 [2m0:02:49 â€¢ 0:00:56[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6180, 0.3820]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 155/203 [2m0:02:49 â€¢ 0:00:56[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6184, 0.3816]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 155/203 [2m0:02:49 â€¢ 0:00:56[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6188, 0.3812]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 155/203 [2m0:02:49 â€¢ 0:00:56[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6193, 0.3807]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 155/203 [2m0:02:49 â€¢ 0:00:56[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6199, 0.3801]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 155/203 [2m0:02:49 â€¢ 0:00:56[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6207, 0.3793]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 155/203 [2m0:02:49 â€¢ 0:00:56[0m [2;4m0.86it/s[0m  
[2KStep 10 - TTA Loss: 0.5339â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 155/203 [2m0:02:49 â€¢ 0:00:56[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6216, 0.3784]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 155/203 [2m0:02:49 â€¢ 0:00:56[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6226, 0.3774]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 155/203 [2m0:02:49 â€¢ 0:00:56[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6237, 0.3763]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 155/203 [2m0:02:49 â€¢ 0:00:56[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6250, 0.3750]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 155/203 [2m0:02:49 â€¢ 0:00:56[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6264, 0.3736]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 155/203 [2m0:02:49 â€¢ 0:00:56[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6279, 0.3721]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 155/203 [2m0:02:49 â€¢ 0:00:56[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6295, 0.3705]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 155/203 [2m0:02:49 â€¢ 0:00:56[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6312, 0.3688]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 155/203 [2m0:02:49 â€¢ 0:00:56[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6330, 0.3670]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 155/203 [2m0:02:49 â€¢ 0:00:56[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6349, 0.3651]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 155/203 [2m0:02:49 â€¢ 0:00:56[0m [2;4m0.86it/s[0m  
[2KStep 20 - TTA Loss: 0.5551â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 155/203 [2m0:02:49 â€¢ 0:00:56[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6370, 0.3630]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 155/203 [2m0:02:49 â€¢ 0:00:56[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6392, 0.3608]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 155/203 [2m0:02:49 â€¢ 0:00:56[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6415, 0.3585]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 155/203 [2m0:02:49 â€¢ 0:00:56[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6439, 0.3561]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 155/203 [2m0:02:49 â€¢ 0:00:56[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6463, 0.3537]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 155/203 [2m0:02:49 â€¢ 0:00:56[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6488, 0.3512]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 155/203 [2m0:02:49 â€¢ 0:00:56[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6515, 0.3485]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 155/203 [2m0:02:49 â€¢ 0:00:56[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6542, 0.3458]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 155/203 [2m0:02:49 â€¢ 0:00:56[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6571, 0.3429]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 155/203 [2m0:02:49 â€¢ 0:00:56[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5807, 0.4193]], device='cuda:0')8;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 155/203 [2m0:02:49 â€¢ 0:00:56[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6612, 0.3388],â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 155/203 [2m0:02:49 â€¢ 0:00:56[0m [2;4m0.86it/s[0m  
        [0.6204, 0.3796],
        [0.6198, 0.3802],
        [0.6742, 0.3258],
        [0.6416, 0.3584],
        [0.6601, 0.3399],
        [0.6088, 0.3912],
        [0.6842, 0.3158],
        [0.6910, 0.3090],
[2K        [0.6890, 0.3110]], device='cuda:0')â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 155/203 [2m0:02:49 â€¢ 0:00:56[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5821, 0.4179],â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 156/203 [2m0:02:50 â€¢ 0:00:56[0m [2;4m0.85it/s[0m  
        [0.5776, 0.4224],
        [0.5808, 0.4192],
        [0.5807, 0.4193],
        [0.5822, 0.4178],
        [0.5790, 0.4210],
        [0.5773, 0.4227],
        [0.5814, 0.4186],
        [0.5814, 0.4186],
[2K        [0.5833, 0.4167]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 156/203 [2m0:02:50 â€¢ 0:00:56[0m [2;4m0.85it/s[0m  
[2KClass label: CleanAndJerkâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 156/203 [2m0:02:50 â€¢ 0:00:56[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5913, 0.4087]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 156/203 [2m0:02:50 â€¢ 0:00:56[0m [2;4m0.85it/s[0m  
[2KStep 0 - TTA Loss: 0.5168â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 156/203 [2m0:02:50 â€¢ 0:00:56[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5936, 0.4064]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 156/203 [2m0:02:50 â€¢ 0:00:56[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5961, 0.4039]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 156/203 [2m0:02:50 â€¢ 0:00:56[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5989, 0.4011]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 156/203 [2m0:02:50 â€¢ 0:00:56[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6020, 0.3980]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 156/203 [2m0:02:50 â€¢ 0:00:56[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6051, 0.3949]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 156/203 [2m0:02:50 â€¢ 0:00:56[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6084, 0.3916]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 156/203 [2m0:02:50 â€¢ 0:00:56[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6117, 0.3883]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 156/203 [2m0:02:50 â€¢ 0:00:56[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6150, 0.3850]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 156/203 [2m0:02:50 â€¢ 0:00:56[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6183, 0.3817]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 156/203 [2m0:02:50 â€¢ 0:00:56[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6215, 0.3785]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 156/203 [2m0:02:50 â€¢ 0:00:56[0m [2;4m0.85it/s[0m  
[2KStep 10 - TTA Loss: 0.5253â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 156/203 [2m0:02:50 â€¢ 0:00:56[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6247, 0.3753]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 156/203 [2m0:02:50 â€¢ 0:00:56[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6277, 0.3723]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 156/203 [2m0:02:50 â€¢ 0:00:56[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6306, 0.3694]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 156/203 [2m0:02:50 â€¢ 0:00:56[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6333, 0.3667]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 156/203 [2m0:02:50 â€¢ 0:00:56[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6359, 0.3641]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 156/203 [2m0:02:50 â€¢ 0:00:56[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6383, 0.3617]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 156/203 [2m0:02:50 â€¢ 0:00:56[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6405, 0.3595]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 156/203 [2m0:02:50 â€¢ 0:00:56[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6425, 0.3575]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 156/203 [2m0:02:50 â€¢ 0:00:56[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6442, 0.3558]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 156/203 [2m0:02:50 â€¢ 0:00:56[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6457, 0.3543]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 156/203 [2m0:02:50 â€¢ 0:00:56[0m [2;4m0.85it/s[0m  
[2KStep 20 - TTA Loss: 0.4569â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 156/203 [2m0:02:50 â€¢ 0:00:56[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6471, 0.3529]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 156/203 [2m0:02:50 â€¢ 0:00:56[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6481, 0.3519]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 156/203 [2m0:02:50 â€¢ 0:00:56[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6490, 0.3510]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 156/203 [2m0:02:50 â€¢ 0:00:56[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6497, 0.3503]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 156/203 [2m0:02:50 â€¢ 0:00:56[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6502, 0.3498]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 156/203 [2m0:02:50 â€¢ 0:00:56[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6506, 0.3494]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 156/203 [2m0:02:50 â€¢ 0:00:56[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6508, 0.3492]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 156/203 [2m0:02:50 â€¢ 0:00:56[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6510, 0.3490]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 156/203 [2m0:02:50 â€¢ 0:00:56[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6510, 0.3490]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 156/203 [2m0:02:50 â€¢ 0:00:56[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5826, 0.4174]], device='cuda:0')8;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 156/203 [2m0:02:50 â€¢ 0:00:56[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6633, 0.3367],â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 156/203 [2m0:02:50 â€¢ 0:00:56[0m [2;4m0.85it/s[0m  
        [0.6236, 0.3764],
        [0.6511, 0.3489],
        [0.6784, 0.3216],
        [0.6488, 0.3512],
        [0.6582, 0.3418],
        [0.6089, 0.3911],
        [0.6838, 0.3162],
        [0.6929, 0.3071],
[2K        [0.6937, 0.3063]], device='cuda:0')â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 156/203 [2m0:02:50 â€¢ 0:00:56[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5819, 0.4181],â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 157/203 [2m0:02:52 â€¢ 0:00:55[0m [2;4m0.84it/s[0m  
        [0.5774, 0.4226],
        [0.5804, 0.4196],
        [0.5805, 0.4195],
        [0.5820, 0.4180],
        [0.5788, 0.4212],
        [0.5771, 0.4229],
        [0.5812, 0.4188],
        [0.5812, 0.4188],
[2K        [0.5831, 0.4169]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 157/203 [2m0:02:52 â€¢ 0:00:55[0m [2;4m0.84it/s[0m  
[2KClass label: GolfSwingâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 157/203 [2m0:02:52 â€¢ 0:00:55[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6455, 0.3545]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 157/203 [2m0:02:52 â€¢ 0:00:55[0m [2;4m0.84it/s[0m  
[2KStep 0 - TTA Loss: 0.5765â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 157/203 [2m0:02:52 â€¢ 0:00:55[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6455, 0.3545]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 157/203 [2m0:02:52 â€¢ 0:00:55[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6456, 0.3544]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 157/203 [2m0:02:52 â€¢ 0:00:55[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6456, 0.3544]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 157/203 [2m0:02:52 â€¢ 0:00:55[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6456, 0.3544]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 157/203 [2m0:02:52 â€¢ 0:00:55[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6458, 0.3542]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 157/203 [2m0:02:52 â€¢ 0:00:55[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6459, 0.3541]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 157/203 [2m0:02:52 â€¢ 0:00:55[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6462, 0.3538]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 157/203 [2m0:02:52 â€¢ 0:00:55[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6465, 0.3535]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 157/203 [2m0:02:52 â€¢ 0:00:55[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6469, 0.3531]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 157/203 [2m0:02:52 â€¢ 0:00:55[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6474, 0.3526]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 157/203 [2m0:02:52 â€¢ 0:00:55[0m [2;4m0.84it/s[0m  
[2KStep 10 - TTA Loss: 0.5634â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 157/203 [2m0:02:52 â€¢ 0:00:55[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6480, 0.3520]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 157/203 [2m0:02:52 â€¢ 0:00:55[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6488, 0.3512]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 157/203 [2m0:02:52 â€¢ 0:00:55[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6498, 0.3502]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 157/203 [2m0:02:52 â€¢ 0:00:55[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6508, 0.3492]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 157/203 [2m0:02:52 â€¢ 0:00:55[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6520, 0.3480]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 157/203 [2m0:02:52 â€¢ 0:00:55[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6533, 0.3467]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 157/203 [2m0:02:52 â€¢ 0:00:55[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6547, 0.3453]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 157/203 [2m0:02:52 â€¢ 0:00:55[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6563, 0.3437]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 157/203 [2m0:02:52 â€¢ 0:00:55[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6580, 0.3420]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 157/203 [2m0:02:52 â€¢ 0:00:55[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6599, 0.3401]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 157/203 [2m0:02:52 â€¢ 0:00:55[0m [2;4m0.84it/s[0m  
[2KStep 20 - TTA Loss: 0.5745â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 157/203 [2m0:02:52 â€¢ 0:00:55[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6620, 0.3380]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 157/203 [2m0:02:52 â€¢ 0:00:55[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6641, 0.3359]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 157/203 [2m0:02:52 â€¢ 0:00:55[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6665, 0.3335]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 157/203 [2m0:02:52 â€¢ 0:00:55[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6688, 0.3312]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 157/203 [2m0:02:52 â€¢ 0:00:55[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6714, 0.3286]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 157/203 [2m0:02:52 â€¢ 0:00:55[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6741, 0.3259]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 157/203 [2m0:02:52 â€¢ 0:00:55[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6766, 0.3234]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 157/203 [2m0:02:52 â€¢ 0:00:55[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6792, 0.3208]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 157/203 [2m0:02:52 â€¢ 0:00:55[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6818, 0.3182]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 157/203 [2m0:02:52 â€¢ 0:00:55[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5824, 0.4176]], device='cuda:0')8;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 157/203 [2m0:02:52 â€¢ 0:00:55[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6649, 0.3351],â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 157/203 [2m0:02:52 â€¢ 0:00:55[0m [2;4m0.84it/s[0m  
        [0.6195, 0.3805],
        [0.6204, 0.3796],
        [0.6846, 0.3154],
        [0.6390, 0.3610],
        [0.6431, 0.3569],
        [0.5968, 0.4032],
        [0.6763, 0.3237],
        [0.6916, 0.3084],
[2K        [0.6877, 0.3123]], device='cuda:0')â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 157/203 [2m0:02:52 â€¢ 0:00:55[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5820, 0.4180],â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 158/203 [2m0:02:53 â€¢ 0:00:54[0m [2;4m0.84it/s[0m   
        [0.5775, 0.4225],
        [0.5804, 0.4196],
        [0.5807, 0.4193],
        [0.5821, 0.4179],
        [0.5788, 0.4212],
        [0.5771, 0.4229],
        [0.5813, 0.4187],
        [0.5813, 0.4187],
[2K        [0.5831, 0.4169]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[38;5;237mâ”â”â”â”â”â”â”â”[0m 158/203 [2m0:02:53 â€¢ 0:00:54[0m [2;4m0.84it/s[0m  
[2KClass label: BaseballPitchâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 158/203 [2m0:02:53 â€¢ 0:00:54[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6361, 0.3639]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 158/203 [2m0:02:53 â€¢ 0:00:54[0m [2;4m0.84it/s[0m  
[2KStep 0 - TTA Loss: 0.7846â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 158/203 [2m0:02:53 â€¢ 0:00:54[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6386, 0.3614]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 158/203 [2m0:02:53 â€¢ 0:00:54[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6413, 0.3587]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 158/203 [2m0:02:53 â€¢ 0:00:54[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6441, 0.3559]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 158/203 [2m0:02:53 â€¢ 0:00:54[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6469, 0.3531]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 158/203 [2m0:02:53 â€¢ 0:00:54[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6499, 0.3501]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 158/203 [2m0:02:53 â€¢ 0:00:54[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6526, 0.3474]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 158/203 [2m0:02:53 â€¢ 0:00:54[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6554, 0.3446]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 158/203 [2m0:02:53 â€¢ 0:00:54[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6582, 0.3418]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 158/203 [2m0:02:53 â€¢ 0:00:54[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6609, 0.3391]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 158/203 [2m0:02:53 â€¢ 0:00:54[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6636, 0.3364]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 158/203 [2m0:02:53 â€¢ 0:00:54[0m [2;4m0.84it/s[0m  
[2KStep 10 - TTA Loss: 0.9945â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 158/203 [2m0:02:53 â€¢ 0:00:54[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6660, 0.3340]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 158/203 [2m0:02:53 â€¢ 0:00:54[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6684, 0.3316]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 158/203 [2m0:02:53 â€¢ 0:00:54[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6707, 0.3293]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 158/203 [2m0:02:53 â€¢ 0:00:54[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6727, 0.3273]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 158/203 [2m0:02:53 â€¢ 0:00:54[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6746, 0.3254]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 158/203 [2m0:02:53 â€¢ 0:00:54[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6764, 0.3236]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 158/203 [2m0:02:53 â€¢ 0:00:54[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6780, 0.3220]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 158/203 [2m0:02:53 â€¢ 0:00:54[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6795, 0.3205]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 158/203 [2m0:02:53 â€¢ 0:00:54[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6809, 0.3191]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 158/203 [2m0:02:53 â€¢ 0:00:54[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6821, 0.3179]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 158/203 [2m0:02:53 â€¢ 0:00:54[0m [2;4m0.84it/s[0m  
[2KStep 20 - TTA Loss: 0.7417â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 158/203 [2m0:02:53 â€¢ 0:00:54[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6831, 0.3169]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 158/203 [2m0:02:53 â€¢ 0:00:54[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6839, 0.3161]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 158/203 [2m0:02:53 â€¢ 0:00:54[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6846, 0.3154]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 158/203 [2m0:02:53 â€¢ 0:00:54[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6851, 0.3149]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 158/203 [2m0:02:53 â€¢ 0:00:54[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6855, 0.3145]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 158/203 [2m0:02:53 â€¢ 0:00:54[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6857, 0.3143]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 158/203 [2m0:02:53 â€¢ 0:00:54[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6859, 0.3141]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 158/203 [2m0:02:53 â€¢ 0:00:54[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6860, 0.3140]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 158/203 [2m0:02:53 â€¢ 0:00:54[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6860, 0.3140]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 158/203 [2m0:02:53 â€¢ 0:00:54[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5841, 0.4159]], device='cuda:0')237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 158/203 [2m0:02:53 â€¢ 0:00:54[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6861, 0.3139],â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 158/203 [2m0:02:53 â€¢ 0:00:54[0m [2;4m0.84it/s[0m  
        [0.6234, 0.3766],
        [0.6191, 0.3809],
        [0.6935, 0.3065],
        [0.6487, 0.3513],
        [0.6510, 0.3490],
        [0.6038, 0.3962],
        [0.6844, 0.3156],
        [0.6995, 0.3005],
[2K        [0.6985, 0.3015]], device='cuda:0')â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 158/203 [2m0:02:53 â€¢ 0:00:54[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5821, 0.4179],â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 159/203 [2m0:02:54 â€¢ 0:00:53[0m [2;4m0.84it/s[0m  
        [0.5775, 0.4225],
        [0.5805, 0.4195],
        [0.5808, 0.4192],
        [0.5821, 0.4179],
        [0.5788, 0.4212],
        [0.5772, 0.4228],
        [0.5813, 0.4187],
        [0.5814, 0.4186],
[2K        [0.5831, 0.4169]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[38;5;237mâ”â”â”â”â”â”â”â”[0m 159/203 [2m0:02:54 â€¢ 0:00:53[0m [2;4m0.84it/s[0m  
[2KClass label: Billiardsâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 159/203 [2m0:02:54 â€¢ 0:00:53[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5998, 0.4002]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 159/203 [2m0:02:54 â€¢ 0:00:53[0m [2;4m0.84it/s[0m  
[2KStep 0 - TTA Loss: 0.4997â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 159/203 [2m0:02:54 â€¢ 0:00:53[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5998, 0.4002]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 159/203 [2m0:02:54 â€¢ 0:00:53[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5998, 0.4002]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 159/203 [2m0:02:54 â€¢ 0:00:53[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5998, 0.4002]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 159/203 [2m0:02:54 â€¢ 0:00:53[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5999, 0.4001]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 159/203 [2m0:02:54 â€¢ 0:00:53[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6001, 0.3999]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 159/203 [2m0:02:54 â€¢ 0:00:53[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6004, 0.3996]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 159/203 [2m0:02:54 â€¢ 0:00:53[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6008, 0.3992]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 159/203 [2m0:02:54 â€¢ 0:00:53[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6014, 0.3986]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 159/203 [2m0:02:54 â€¢ 0:00:53[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6022, 0.3978]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 159/203 [2m0:02:54 â€¢ 0:00:53[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6033, 0.3967]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 159/203 [2m0:02:54 â€¢ 0:00:53[0m [2;4m0.84it/s[0m  
[2KStep 10 - TTA Loss: 0.4941â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 159/203 [2m0:02:54 â€¢ 0:00:53[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6046, 0.3954]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 159/203 [2m0:02:54 â€¢ 0:00:53[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6062, 0.3938]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 159/203 [2m0:02:54 â€¢ 0:00:53[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6081, 0.3919]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 159/203 [2m0:02:54 â€¢ 0:00:53[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6104, 0.3896]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 159/203 [2m0:02:54 â€¢ 0:00:53[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6131, 0.3869]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 159/203 [2m0:02:54 â€¢ 0:00:53[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6161, 0.3839]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 159/203 [2m0:02:54 â€¢ 0:00:53[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6194, 0.3806]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 159/203 [2m0:02:54 â€¢ 0:00:53[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6231, 0.3769]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 159/203 [2m0:02:54 â€¢ 0:00:53[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6272, 0.3728]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 159/203 [2m0:02:54 â€¢ 0:00:53[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6317, 0.3683]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 159/203 [2m0:02:54 â€¢ 0:00:53[0m [2;4m0.84it/s[0m  
[2KStep 20 - TTA Loss: 0.5105â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 159/203 [2m0:02:54 â€¢ 0:00:53[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6364, 0.3636]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 159/203 [2m0:02:54 â€¢ 0:00:53[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6416, 0.3584]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 159/203 [2m0:02:54 â€¢ 0:00:53[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6472, 0.3528]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 159/203 [2m0:02:54 â€¢ 0:00:53[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6531, 0.3469]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 159/203 [2m0:02:54 â€¢ 0:00:53[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6594, 0.3406]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 159/203 [2m0:02:54 â€¢ 0:00:53[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6659, 0.3341]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 159/203 [2m0:02:54 â€¢ 0:00:53[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6725, 0.3275]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 159/203 [2m0:02:54 â€¢ 0:00:53[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6795, 0.3205]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 159/203 [2m0:02:54 â€¢ 0:00:53[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6865, 0.3135]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 159/203 [2m0:02:54 â€¢ 0:00:53[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5814, 0.4186]], device='cuda:0')237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 159/203 [2m0:02:54 â€¢ 0:00:53[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6710, 0.3290],â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 159/203 [2m0:02:54 â€¢ 0:00:53[0m [2;4m0.84it/s[0m  
        [0.6937, 0.3063],
        [0.6204, 0.3796],
        [0.6809, 0.3191],
        [0.6401, 0.3599],
        [0.6512, 0.3488],
        [0.6029, 0.3971],
        [0.6911, 0.3089],
        [0.7025, 0.2975],
[2K        [0.6900, 0.3100]], device='cuda:0')â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 159/203 [2m0:02:54 â€¢ 0:00:53[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5822, 0.4178],â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 160/203 [2m0:02:55 â€¢ 0:00:52[0m [2;4m0.84it/s[0m  
        [0.5779, 0.4221],
        [0.5806, 0.4194],
        [0.5810, 0.4190],
        [0.5822, 0.4178],
        [0.5789, 0.4211],
        [0.5773, 0.4227],
        [0.5814, 0.4186],
        [0.5815, 0.4185],
[2K        [0.5832, 0.4168]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 160/203 [2m0:02:55 â€¢ 0:00:52[0m [2;4m0.84it/s[0m  
[2KClass label: Billiardsâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 160/203 [2m0:02:55 â€¢ 0:00:52[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6044, 0.3956]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 160/203 [2m0:02:55 â€¢ 0:00:52[0m [2;4m0.84it/s[0m  
[2KStep 0 - TTA Loss: 0.8114â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 160/203 [2m0:02:55 â€¢ 0:00:52[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6118, 0.3882]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 160/203 [2m0:02:55 â€¢ 0:00:52[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6189, 0.3811]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 160/203 [2m0:02:55 â€¢ 0:00:52[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6257, 0.3743]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 160/203 [2m0:02:55 â€¢ 0:00:52[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6322, 0.3678]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 160/203 [2m0:02:55 â€¢ 0:00:52[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6382, 0.3618]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 160/203 [2m0:02:55 â€¢ 0:00:52[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6436, 0.3564]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 160/203 [2m0:02:55 â€¢ 0:00:52[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6489, 0.3511]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 160/203 [2m0:02:55 â€¢ 0:00:52[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6539, 0.3461]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 160/203 [2m0:02:55 â€¢ 0:00:52[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6586, 0.3414]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 160/203 [2m0:02:55 â€¢ 0:00:52[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6630, 0.3370]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 160/203 [2m0:02:55 â€¢ 0:00:52[0m [2;4m0.84it/s[0m  
[2KStep 10 - TTA Loss: 0.6987â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 160/203 [2m0:02:55 â€¢ 0:00:52[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6671, 0.3329]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 160/203 [2m0:02:55 â€¢ 0:00:52[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6710, 0.3290]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 160/203 [2m0:02:55 â€¢ 0:00:52[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6747, 0.3253]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 160/203 [2m0:02:55 â€¢ 0:00:52[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6779, 0.3221]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 160/203 [2m0:02:55 â€¢ 0:00:52[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6808, 0.3192]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 160/203 [2m0:02:55 â€¢ 0:00:52[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6835, 0.3165]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 160/203 [2m0:02:55 â€¢ 0:00:52[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6859, 0.3141]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 160/203 [2m0:02:55 â€¢ 0:00:52[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6880, 0.3120]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 160/203 [2m0:02:55 â€¢ 0:00:52[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6898, 0.3102]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 160/203 [2m0:02:55 â€¢ 0:00:52[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6913, 0.3087]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 160/203 [2m0:02:55 â€¢ 0:00:52[0m [2;4m0.84it/s[0m  
[2KStep 20 - TTA Loss: 0.8573â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 160/203 [2m0:02:55 â€¢ 0:00:52[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6926, 0.3074]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 160/203 [2m0:02:55 â€¢ 0:00:52[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6936, 0.3064]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 160/203 [2m0:02:55 â€¢ 0:00:52[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6945, 0.3055]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 160/203 [2m0:02:55 â€¢ 0:00:52[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6951, 0.3049]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 160/203 [2m0:02:55 â€¢ 0:00:52[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6956, 0.3044]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 160/203 [2m0:02:55 â€¢ 0:00:52[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6959, 0.3041]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 160/203 [2m0:02:55 â€¢ 0:00:52[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6962, 0.3038]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 160/203 [2m0:02:55 â€¢ 0:00:52[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6963, 0.3037]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 160/203 [2m0:02:55 â€¢ 0:00:52[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6963, 0.3037]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 160/203 [2m0:02:55 â€¢ 0:00:52[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5819, 0.4181]], device='cuda:0')98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 160/203 [2m0:02:55 â€¢ 0:00:52[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6668, 0.3332],â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 160/203 [2m0:02:55 â€¢ 0:00:52[0m [2;4m0.84it/s[0m  
        [0.6964, 0.3036],
        [0.6244, 0.3756],
        [0.6775, 0.3225],
        [0.6416, 0.3584],
        [0.6521, 0.3479],
        [0.6059, 0.3941],
        [0.6917, 0.3083],
        [0.7015, 0.2985],
[2K        [0.6895, 0.3105]], device='cuda:0')â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 160/203 [2m0:02:55 â€¢ 0:00:52[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5823, 0.4177],â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 161/203 [2m0:02:56 â€¢ 0:00:50[0m [2;4m0.85it/s[0m  
        [0.5783, 0.4217],
        [0.5807, 0.4193],
        [0.5812, 0.4188],
        [0.5823, 0.4177],
        [0.5790, 0.4210],
        [0.5774, 0.4226],
        [0.5815, 0.4185],
        [0.5817, 0.4183],
[2K        [0.5833, 0.4167]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 161/203 [2m0:02:56 â€¢ 0:00:50[0m [2;4m0.85it/s[0m  
[2KClass label: Billiardsâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 161/203 [2m0:02:56 â€¢ 0:00:50[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6038, 0.3962]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 161/203 [2m0:02:56 â€¢ 0:00:50[0m [2;4m0.85it/s[0m  
[2KStep 0 - TTA Loss: 0.6913â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 161/203 [2m0:02:56 â€¢ 0:00:50[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6038, 0.3962]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 161/203 [2m0:02:56 â€¢ 0:00:50[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6039, 0.3961]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 161/203 [2m0:02:56 â€¢ 0:00:50[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6039, 0.3961]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 161/203 [2m0:02:56 â€¢ 0:00:50[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6041, 0.3959]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 161/203 [2m0:02:56 â€¢ 0:00:50[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6043, 0.3957]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 161/203 [2m0:02:56 â€¢ 0:00:50[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6047, 0.3953]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 161/203 [2m0:02:56 â€¢ 0:00:50[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6052, 0.3948]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 161/203 [2m0:02:56 â€¢ 0:00:50[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6059, 0.3941]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 161/203 [2m0:02:56 â€¢ 0:00:50[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6069, 0.3931]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 161/203 [2m0:02:56 â€¢ 0:00:50[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6080, 0.3920]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 161/203 [2m0:02:56 â€¢ 0:00:50[0m [2;4m0.85it/s[0m  
[2KStep 10 - TTA Loss: 0.7435â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 161/203 [2m0:02:56 â€¢ 0:00:50[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6094, 0.3906]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 161/203 [2m0:02:56 â€¢ 0:00:50[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6111, 0.3889]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 161/203 [2m0:02:56 â€¢ 0:00:50[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6130, 0.3870]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 161/203 [2m0:02:56 â€¢ 0:00:50[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6152, 0.3848]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 161/203 [2m0:02:56 â€¢ 0:00:50[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6177, 0.3823]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 161/203 [2m0:02:56 â€¢ 0:00:50[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6205, 0.3795]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 161/203 [2m0:02:56 â€¢ 0:00:50[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6236, 0.3764]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 161/203 [2m0:02:56 â€¢ 0:00:50[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6269, 0.3731]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 161/203 [2m0:02:56 â€¢ 0:00:50[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6306, 0.3694]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 161/203 [2m0:02:56 â€¢ 0:00:50[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6346, 0.3654]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 161/203 [2m0:02:56 â€¢ 0:00:50[0m [2;4m0.85it/s[0m  
[2KStep 20 - TTA Loss: 0.6614â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 161/203 [2m0:02:56 â€¢ 0:00:50[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6389, 0.3611]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 161/203 [2m0:02:56 â€¢ 0:00:50[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6434, 0.3566]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 161/203 [2m0:02:56 â€¢ 0:00:50[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6482, 0.3518]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 161/203 [2m0:02:56 â€¢ 0:00:50[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6534, 0.3466]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 161/203 [2m0:02:56 â€¢ 0:00:50[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6587, 0.3413]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 161/203 [2m0:02:56 â€¢ 0:00:50[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6643, 0.3357]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 161/203 [2m0:02:56 â€¢ 0:00:50[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6700, 0.3300]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 161/203 [2m0:02:56 â€¢ 0:00:50[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6758, 0.3242]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 161/203 [2m0:02:56 â€¢ 0:00:50[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6816, 0.3184]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 161/203 [2m0:02:56 â€¢ 0:00:50[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5817, 0.4183]], device='cuda:0')98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 161/203 [2m0:02:56 â€¢ 0:00:50[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6641, 0.3359],â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 161/203 [2m0:02:56 â€¢ 0:00:50[0m [2;4m0.85it/s[0m  
        [0.6877, 0.3123],
        [0.6193, 0.3807],
        [0.6780, 0.3220],
        [0.6380, 0.3620],
        [0.6483, 0.3517],
        [0.6002, 0.3998],
        [0.6896, 0.3104],
        [0.6997, 0.3003],
[2K        [0.6874, 0.3126]], device='cuda:0')â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 161/203 [2m0:02:56 â€¢ 0:00:50[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5823, 0.4177],â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 162/203 [2m0:02:57 â€¢ 0:00:49[0m [2;4m0.85it/s[0m  
        [0.5784, 0.4216],
        [0.5808, 0.4192],
        [0.5813, 0.4187],
        [0.5823, 0.4177],
        [0.5790, 0.4210],
        [0.5774, 0.4226],
        [0.5816, 0.4184],
        [0.5817, 0.4183],
[2K        [0.5833, 0.4167]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 162/203 [2m0:02:57 â€¢ 0:00:49[0m [2;4m0.85it/s[0m  
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 162/203 [2m0:02:57 â€¢ 0:00:49[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6173, 0.3827]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 162/203 [2m0:02:57 â€¢ 0:00:49[0m [2;4m0.85it/s[0m  
[2KStep 0 - TTA Loss: 0.5022â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 162/203 [2m0:02:57 â€¢ 0:00:49[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6196, 0.3804]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 162/203 [2m0:02:57 â€¢ 0:00:49[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6220, 0.3780]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 162/203 [2m0:02:57 â€¢ 0:00:49[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6245, 0.3755]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 162/203 [2m0:02:57 â€¢ 0:00:49[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6270, 0.3730]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 162/203 [2m0:02:57 â€¢ 0:00:49[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6295, 0.3705]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 162/203 [2m0:02:57 â€¢ 0:00:49[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6321, 0.3679]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 162/203 [2m0:02:57 â€¢ 0:00:49[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6346, 0.3654]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 162/203 [2m0:02:57 â€¢ 0:00:49[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6372, 0.3628]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 162/203 [2m0:02:57 â€¢ 0:00:49[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6397, 0.3603]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 162/203 [2m0:02:57 â€¢ 0:00:49[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6421, 0.3579]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 162/203 [2m0:02:57 â€¢ 0:00:49[0m [2;4m0.85it/s[0m  
[2KStep 10 - TTA Loss: 0.4902â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 162/203 [2m0:02:57 â€¢ 0:00:49[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6445, 0.3555]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 162/203 [2m0:02:57 â€¢ 0:00:49[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6468, 0.3532]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 162/203 [2m0:02:57 â€¢ 0:00:49[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6489, 0.3511]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 162/203 [2m0:02:57 â€¢ 0:00:49[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6509, 0.3491]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 162/203 [2m0:02:57 â€¢ 0:00:49[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6528, 0.3472]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 162/203 [2m0:02:57 â€¢ 0:00:49[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6545, 0.3455]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 162/203 [2m0:02:57 â€¢ 0:00:49[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6560, 0.3440]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 162/203 [2m0:02:57 â€¢ 0:00:49[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6574, 0.3426]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 162/203 [2m0:02:57 â€¢ 0:00:49[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6587, 0.3413]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 162/203 [2m0:02:57 â€¢ 0:00:49[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6597, 0.3403]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 162/203 [2m0:02:57 â€¢ 0:00:49[0m [2;4m0.85it/s[0m  
[2KStep 20 - TTA Loss: 0.4828â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 162/203 [2m0:02:57 â€¢ 0:00:49[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6607, 0.3393]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 162/203 [2m0:02:57 â€¢ 0:00:49[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6614, 0.3386]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 162/203 [2m0:02:57 â€¢ 0:00:49[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6621, 0.3379]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 162/203 [2m0:02:57 â€¢ 0:00:49[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6625, 0.3375]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 162/203 [2m0:02:57 â€¢ 0:00:49[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6629, 0.3371]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 162/203 [2m0:02:57 â€¢ 0:00:49[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6632, 0.3368]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 162/203 [2m0:02:57 â€¢ 0:00:49[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6633, 0.3367]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 162/203 [2m0:02:57 â€¢ 0:00:49[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6634, 0.3366]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 162/203 [2m0:02:57 â€¢ 0:00:49[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6635, 0.3365]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 162/203 [2m0:02:57 â€¢ 0:00:49[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5811, 0.4189]], device='cuda:0')98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 162/203 [2m0:02:57 â€¢ 0:00:49[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6695, 0.3305],â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 162/203 [2m0:02:57 â€¢ 0:00:49[0m [2;4m0.85it/s[0m  
        [0.6597, 0.3403],
        [0.6237, 0.3763],
        [0.6787, 0.3213],
        [0.6461, 0.3539],
        [0.6636, 0.3364],
        [0.6122, 0.3878],
        [0.6918, 0.3082],
        [0.7015, 0.2985],
[2K        [0.6936, 0.3064]], device='cuda:0')â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 162/203 [2m0:02:57 â€¢ 0:00:49[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5824, 0.4176],â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”[0m 163/203 [2m0:02:58 â€¢ 0:00:48[0m [2;4m0.84it/s[0m   
        [0.5785, 0.4215],
        [0.5809, 0.4191],
        [0.5813, 0.4187],
        [0.5824, 0.4176],
        [0.5792, 0.4208],
        [0.5775, 0.4225],
        [0.5817, 0.4183],
        [0.5818, 0.4182],
[2K        [0.5834, 0.4166]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m[38;5;237mâ”â”â”â”â”â”â”[0m 163/203 [2m0:02:58 â€¢ 0:00:48[0m [2;4m0.84it/s[0m  
[2KClass label: BaseballPitchâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”[0m 163/203 [2m0:02:58 â€¢ 0:00:48[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6406, 0.3594]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 163/203 [2m0:02:58 â€¢ 0:00:48[0m [2;4m0.84it/s[0m  
[2KStep 0 - TTA Loss: 0.6765â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”[0m 163/203 [2m0:02:58 â€¢ 0:00:48[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6406, 0.3594]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 163/203 [2m0:02:58 â€¢ 0:00:48[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6406, 0.3594]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 163/203 [2m0:02:58 â€¢ 0:00:48[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6406, 0.3594]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 163/203 [2m0:02:58 â€¢ 0:00:48[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6407, 0.3593]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 163/203 [2m0:02:58 â€¢ 0:00:48[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6408, 0.3592]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 163/203 [2m0:02:58 â€¢ 0:00:48[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6411, 0.3589]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 163/203 [2m0:02:58 â€¢ 0:00:48[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6414, 0.3586]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 163/203 [2m0:02:58 â€¢ 0:00:48[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6419, 0.3581]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 163/203 [2m0:02:58 â€¢ 0:00:48[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6425, 0.3575]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 163/203 [2m0:02:58 â€¢ 0:00:48[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6432, 0.3568]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 163/203 [2m0:02:58 â€¢ 0:00:48[0m [2;4m0.84it/s[0m  
[2KStep 10 - TTA Loss: 0.7105â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”[0m 163/203 [2m0:02:58 â€¢ 0:00:48[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6442, 0.3558]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 163/203 [2m0:02:58 â€¢ 0:00:48[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6453, 0.3547]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 163/203 [2m0:02:58 â€¢ 0:00:48[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6467, 0.3533]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 163/203 [2m0:02:58 â€¢ 0:00:48[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6482, 0.3518]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 163/203 [2m0:02:58 â€¢ 0:00:48[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6500, 0.3500]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 163/203 [2m0:02:58 â€¢ 0:00:48[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6519, 0.3481]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 163/203 [2m0:02:58 â€¢ 0:00:48[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6541, 0.3459]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 163/203 [2m0:02:58 â€¢ 0:00:48[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6565, 0.3435]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 163/203 [2m0:02:58 â€¢ 0:00:48[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6591, 0.3409]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 163/203 [2m0:02:58 â€¢ 0:00:48[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6620, 0.3380]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 163/203 [2m0:02:58 â€¢ 0:00:48[0m [2;4m0.84it/s[0m  
[2KStep 20 - TTA Loss: 0.6621â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”[0m 163/203 [2m0:02:58 â€¢ 0:00:48[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6650, 0.3350]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 163/203 [2m0:02:58 â€¢ 0:00:48[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6683, 0.3317]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 163/203 [2m0:02:58 â€¢ 0:00:48[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6717, 0.3283]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 163/203 [2m0:02:58 â€¢ 0:00:48[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6754, 0.3246]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 163/203 [2m0:02:58 â€¢ 0:00:48[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6791, 0.3209]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 163/203 [2m0:02:58 â€¢ 0:00:48[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6831, 0.3169]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 163/203 [2m0:02:58 â€¢ 0:00:48[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6873, 0.3127]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 163/203 [2m0:02:58 â€¢ 0:00:48[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6916, 0.3084]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 163/203 [2m0:02:58 â€¢ 0:00:48[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6961, 0.3039]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 163/203 [2m0:02:58 â€¢ 0:00:48[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5847, 0.4153]], device='cuda:0');237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”[0m 163/203 [2m0:02:58 â€¢ 0:00:48[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.7006, 0.2994],â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”[0m 163/203 [2m0:02:58 â€¢ 0:00:48[0m [2;4m0.84it/s[0m  
        [0.6287, 0.3713],
        [0.6212, 0.3788],
        [0.6952, 0.3048],
        [0.6561, 0.3439],
        [0.6619, 0.3381],
        [0.6112, 0.3888],
        [0.6950, 0.3050],
        [0.7061, 0.2939],
[2K        [0.7051, 0.2949]], device='cuda:0')â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”[0m 163/203 [2m0:02:58 â€¢ 0:00:48[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5821, 0.4179],â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”[0m 164/203 [2m0:02:59 â€¢ 0:00:47[0m [2;4m0.85it/s[0m  
        [0.5784, 0.4216],
        [0.5808, 0.4192],
        [0.5812, 0.4188],
        [0.5823, 0.4177],
        [0.5791, 0.4209],
        [0.5774, 0.4226],
        [0.5815, 0.4185],
        [0.5817, 0.4183],
[2K        [0.5833, 0.4167]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m[38;5;237mâ”â”â”â”â”â”â”[0m 164/203 [2m0:02:59 â€¢ 0:00:47[0m [2;4m0.85it/s[0m  
[2KClass label: Billiardsâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”[0m 164/203 [2m0:02:59 â€¢ 0:00:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6057, 0.3943]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 164/203 [2m0:02:59 â€¢ 0:00:47[0m [2;4m0.85it/s[0m  
[2KStep 0 - TTA Loss: 0.6677â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”[0m 164/203 [2m0:02:59 â€¢ 0:00:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6078, 0.3922]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 164/203 [2m0:02:59 â€¢ 0:00:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6102, 0.3898]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 164/203 [2m0:02:59 â€¢ 0:00:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6129, 0.3871]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 164/203 [2m0:02:59 â€¢ 0:00:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6157, 0.3843]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 164/203 [2m0:02:59 â€¢ 0:00:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6187, 0.3813]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 164/203 [2m0:02:59 â€¢ 0:00:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6218, 0.3782]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 164/203 [2m0:02:59 â€¢ 0:00:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6250, 0.3750]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 164/203 [2m0:02:59 â€¢ 0:00:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6282, 0.3718]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 164/203 [2m0:02:59 â€¢ 0:00:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6314, 0.3686]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 164/203 [2m0:02:59 â€¢ 0:00:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6345, 0.3655]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 164/203 [2m0:02:59 â€¢ 0:00:47[0m [2;4m0.85it/s[0m  
[2KStep 10 - TTA Loss: 0.6692â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”[0m 164/203 [2m0:02:59 â€¢ 0:00:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6375, 0.3625]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 164/203 [2m0:02:59 â€¢ 0:00:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6405, 0.3595]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 164/203 [2m0:02:59 â€¢ 0:00:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6433, 0.3567]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 164/203 [2m0:02:59 â€¢ 0:00:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6460, 0.3540]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 164/203 [2m0:02:59 â€¢ 0:00:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6484, 0.3516]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 164/203 [2m0:02:59 â€¢ 0:00:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6508, 0.3492]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 164/203 [2m0:02:59 â€¢ 0:00:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6529, 0.3471]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 164/203 [2m0:02:59 â€¢ 0:00:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6548, 0.3452]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 164/203 [2m0:02:59 â€¢ 0:00:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6565, 0.3435]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 164/203 [2m0:02:59 â€¢ 0:00:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6579, 0.3421]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 164/203 [2m0:02:59 â€¢ 0:00:47[0m [2;4m0.85it/s[0m  
[2KStep 20 - TTA Loss: 0.6569â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”[0m 164/203 [2m0:02:59 â€¢ 0:00:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6592, 0.3408]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 164/203 [2m0:02:59 â€¢ 0:00:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6603, 0.3397]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 164/203 [2m0:02:59 â€¢ 0:00:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6612, 0.3388]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 164/203 [2m0:02:59 â€¢ 0:00:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6618, 0.3382]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 164/203 [2m0:02:59 â€¢ 0:00:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6624, 0.3376]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 164/203 [2m0:02:59 â€¢ 0:00:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6627, 0.3373]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 164/203 [2m0:02:59 â€¢ 0:00:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6630, 0.3370]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 164/203 [2m0:02:59 â€¢ 0:00:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6631, 0.3369]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 164/203 [2m0:02:59 â€¢ 0:00:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6632, 0.3368]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 164/203 [2m0:02:59 â€¢ 0:00:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5805, 0.4195]], device='cuda:0');237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”[0m 164/203 [2m0:02:59 â€¢ 0:00:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6816, 0.3184],â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”[0m 164/203 [2m0:02:59 â€¢ 0:00:47[0m [2;4m0.85it/s[0m  
        [0.6632, 0.3368],
        [0.6195, 0.3805],
        [0.6847, 0.3153],
        [0.6457, 0.3543],
        [0.6528, 0.3472],
        [0.6040, 0.3960],
        [0.6909, 0.3091],
        [0.6994, 0.3006],
[2K        [0.6943, 0.3057]], device='cuda:0')â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”[0m 164/203 [2m0:02:59 â€¢ 0:00:47[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5819, 0.4181],â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”[0m 165/203 [2m0:03:01 â€¢ 0:00:46[0m [2;4m0.84it/s[0m  
        [0.5782, 0.4218],
        [0.5807, 0.4193],
        [0.5811, 0.4189],
        [0.5822, 0.4178],
        [0.5790, 0.4210],
        [0.5773, 0.4227],
        [0.5814, 0.4186],
        [0.5815, 0.4185],
[2K        [0.5832, 0.4168]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”[0m 165/203 [2m0:03:01 â€¢ 0:00:46[0m [2;4m0.84it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”[0m 165/203 [2m0:03:01 â€¢ 0:00:46[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6593, 0.3407]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 165/203 [2m0:03:01 â€¢ 0:00:46[0m [2;4m0.84it/s[0m  
[2KStep 0 - TTA Loss: 0.5340â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”[0m 165/203 [2m0:03:01 â€¢ 0:00:46[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6593, 0.3407]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 165/203 [2m0:03:01 â€¢ 0:00:46[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6593, 0.3407]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 165/203 [2m0:03:01 â€¢ 0:00:46[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6593, 0.3407]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 165/203 [2m0:03:01 â€¢ 0:00:46[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6594, 0.3406]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 165/203 [2m0:03:01 â€¢ 0:00:46[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6595, 0.3405]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 165/203 [2m0:03:01 â€¢ 0:00:46[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6597, 0.3403]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 165/203 [2m0:03:01 â€¢ 0:00:46[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6600, 0.3400]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 165/203 [2m0:03:01 â€¢ 0:00:46[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6603, 0.3397]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 165/203 [2m0:03:01 â€¢ 0:00:46[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6608, 0.3392]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 165/203 [2m0:03:01 â€¢ 0:00:46[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6615, 0.3385]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 165/203 [2m0:03:01 â€¢ 0:00:46[0m [2;4m0.84it/s[0m  
[2KStep 10 - TTA Loss: 0.5241â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”[0m 165/203 [2m0:03:01 â€¢ 0:00:46[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6623, 0.3377]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 165/203 [2m0:03:01 â€¢ 0:00:46[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6632, 0.3368]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 165/203 [2m0:03:01 â€¢ 0:00:46[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6643, 0.3357]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 165/203 [2m0:03:01 â€¢ 0:00:46[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6657, 0.3343]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 165/203 [2m0:03:01 â€¢ 0:00:46[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6671, 0.3329]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 165/203 [2m0:03:01 â€¢ 0:00:46[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6688, 0.3312]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 165/203 [2m0:03:01 â€¢ 0:00:46[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6706, 0.3294]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 165/203 [2m0:03:01 â€¢ 0:00:46[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6726, 0.3274]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 165/203 [2m0:03:01 â€¢ 0:00:46[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6748, 0.3252]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 165/203 [2m0:03:01 â€¢ 0:00:46[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6772, 0.3228]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 165/203 [2m0:03:01 â€¢ 0:00:46[0m [2;4m0.84it/s[0m  
[2KStep 20 - TTA Loss: 0.5295â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”[0m 165/203 [2m0:03:01 â€¢ 0:00:46[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6798, 0.3202]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 165/203 [2m0:03:01 â€¢ 0:00:46[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6826, 0.3174]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 165/203 [2m0:03:01 â€¢ 0:00:46[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6855, 0.3145]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 165/203 [2m0:03:01 â€¢ 0:00:46[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6887, 0.3113]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 165/203 [2m0:03:01 â€¢ 0:00:46[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6920, 0.3080]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 165/203 [2m0:03:01 â€¢ 0:00:46[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6955, 0.3045]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 165/203 [2m0:03:01 â€¢ 0:00:46[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6991, 0.3009]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 165/203 [2m0:03:01 â€¢ 0:00:46[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.7027, 0.2973]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 165/203 [2m0:03:01 â€¢ 0:00:46[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.7065, 0.2935]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 165/203 [2m0:03:01 â€¢ 0:00:46[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5857, 0.4143]], device='cuda:0');98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”[0m 165/203 [2m0:03:01 â€¢ 0:00:46[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6832, 0.3168],â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”[0m 165/203 [2m0:03:01 â€¢ 0:00:46[0m [2;4m0.84it/s[0m  
        [0.6348, 0.3652],
        [0.6279, 0.3721],
        [0.6903, 0.3097],
        [0.6602, 0.3398],
        [0.6625, 0.3375],
        [0.6157, 0.3843],
        [0.6909, 0.3091],
        [0.7044, 0.2956],
[2K        [0.7105, 0.2895]], device='cuda:0')â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”[0m 165/203 [2m0:03:01 â€¢ 0:00:46[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5819, 0.4181],â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”[0m 166/203 [2m0:03:02 â€¢ 0:00:45[0m [2;4m0.84it/s[0m  
        [0.5782, 0.4218],
        [0.5807, 0.4193],
        [0.5810, 0.4190],
        [0.5822, 0.4178],
        [0.5790, 0.4210],
        [0.5773, 0.4227],
        [0.5814, 0.4186],
        [0.5814, 0.4186],
[2K        [0.5833, 0.4167]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”[0m 166/203 [2m0:03:02 â€¢ 0:00:45[0m [2;4m0.84it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”[0m 166/203 [2m0:03:02 â€¢ 0:00:45[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6608, 0.3392]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 166/203 [2m0:03:02 â€¢ 0:00:45[0m [2;4m0.84it/s[0m  
[2KStep 0 - TTA Loss: 0.9464â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”[0m 166/203 [2m0:03:02 â€¢ 0:00:45[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6648, 0.3352]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 166/203 [2m0:03:02 â€¢ 0:00:45[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6686, 0.3314]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 166/203 [2m0:03:02 â€¢ 0:00:45[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6722, 0.3278]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 166/203 [2m0:03:02 â€¢ 0:00:45[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6757, 0.3243]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 166/203 [2m0:03:02 â€¢ 0:00:45[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6792, 0.3208]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 166/203 [2m0:03:02 â€¢ 0:00:45[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6824, 0.3176]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 166/203 [2m0:03:02 â€¢ 0:00:45[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6854, 0.3146]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 166/203 [2m0:03:02 â€¢ 0:00:45[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6882, 0.3118]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 166/203 [2m0:03:02 â€¢ 0:00:45[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6909, 0.3091]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 166/203 [2m0:03:02 â€¢ 0:00:45[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6936, 0.3064]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 166/203 [2m0:03:02 â€¢ 0:00:45[0m [2;4m0.84it/s[0m  
[2KStep 10 - TTA Loss: 0.8570â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”[0m 166/203 [2m0:03:02 â€¢ 0:00:45[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6962, 0.3038]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 166/203 [2m0:03:02 â€¢ 0:00:45[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6985, 0.3015]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 166/203 [2m0:03:02 â€¢ 0:00:45[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.7007, 0.2993]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 166/203 [2m0:03:02 â€¢ 0:00:45[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.7028, 0.2972]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 166/203 [2m0:03:02 â€¢ 0:00:45[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.7047, 0.2953]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 166/203 [2m0:03:02 â€¢ 0:00:45[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.7066, 0.2934]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 166/203 [2m0:03:02 â€¢ 0:00:45[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.7083, 0.2917]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 166/203 [2m0:03:02 â€¢ 0:00:45[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.7097, 0.2903]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 166/203 [2m0:03:02 â€¢ 0:00:45[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.7110, 0.2890]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 166/203 [2m0:03:02 â€¢ 0:00:45[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.7121, 0.2879]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 166/203 [2m0:03:02 â€¢ 0:00:45[0m [2;4m0.84it/s[0m  
[2KStep 20 - TTA Loss: 0.7729â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”[0m 166/203 [2m0:03:02 â€¢ 0:00:45[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.7131, 0.2869]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 166/203 [2m0:03:02 â€¢ 0:00:45[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.7140, 0.2860]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 166/203 [2m0:03:02 â€¢ 0:00:45[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.7146, 0.2854]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 166/203 [2m0:03:02 â€¢ 0:00:45[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.7151, 0.2849]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 166/203 [2m0:03:02 â€¢ 0:00:45[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.7155, 0.2845]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 166/203 [2m0:03:02 â€¢ 0:00:45[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.7158, 0.2842]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 166/203 [2m0:03:02 â€¢ 0:00:45[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.7159, 0.2841]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 166/203 [2m0:03:02 â€¢ 0:00:45[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.7160, 0.2840]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 166/203 [2m0:03:02 â€¢ 0:00:45[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.7161, 0.2839]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 166/203 [2m0:03:02 â€¢ 0:00:45[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5861, 0.4139]], device='cuda:0');98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”[0m 166/203 [2m0:03:02 â€¢ 0:00:45[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6866, 0.3134],â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”[0m 166/203 [2m0:03:02 â€¢ 0:00:45[0m [2;4m0.84it/s[0m  
        [0.6322, 0.3678],
        [0.6322, 0.3678],
        [0.6957, 0.3043],
        [0.6645, 0.3355],
        [0.6681, 0.3319],
        [0.6211, 0.3789],
        [0.6921, 0.3079],
        [0.7099, 0.2901],
[2K        [0.7162, 0.2838]], device='cuda:0')â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”[0m 166/203 [2m0:03:02 â€¢ 0:00:45[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5819, 0.4181],â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”[0m 167/203 [2m0:03:03 â€¢ 0:00:44[0m [2;4m0.83it/s[0m  
        [0.5782, 0.4218],
        [0.5808, 0.4192],
        [0.5811, 0.4189],
        [0.5823, 0.4177],
        [0.5791, 0.4209],
        [0.5774, 0.4226],
        [0.5814, 0.4186],
        [0.5815, 0.4185],
[2K        [0.5835, 0.4165]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”[0m 167/203 [2m0:03:03 â€¢ 0:00:44[0m [2;4m0.83it/s[0m  
[2KClass label: BaseballPitchâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”[0m 167/203 [2m0:03:03 â€¢ 0:00:44[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6342, 0.3658]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 167/203 [2m0:03:03 â€¢ 0:00:44[0m [2;4m0.83it/s[0m  
[2KStep 0 - TTA Loss: 0.5163â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”[0m 167/203 [2m0:03:03 â€¢ 0:00:44[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6342, 0.3658]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 167/203 [2m0:03:03 â€¢ 0:00:44[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6342, 0.3658]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 167/203 [2m0:03:03 â€¢ 0:00:44[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6343, 0.3657]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 167/203 [2m0:03:03 â€¢ 0:00:44[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6344, 0.3656]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 167/203 [2m0:03:03 â€¢ 0:00:44[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6346, 0.3654]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 167/203 [2m0:03:03 â€¢ 0:00:44[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6350, 0.3650]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 167/203 [2m0:03:03 â€¢ 0:00:44[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6355, 0.3645]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 167/203 [2m0:03:03 â€¢ 0:00:44[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6361, 0.3639]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 167/203 [2m0:03:03 â€¢ 0:00:44[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6370, 0.3630]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 167/203 [2m0:03:03 â€¢ 0:00:44[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6381, 0.3619]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 167/203 [2m0:03:03 â€¢ 0:00:44[0m [2;4m0.83it/s[0m  
[2KStep 10 - TTA Loss: 0.5383â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”[0m 167/203 [2m0:03:03 â€¢ 0:00:44[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6395, 0.3605]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 167/203 [2m0:03:03 â€¢ 0:00:44[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6411, 0.3589]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 167/203 [2m0:03:03 â€¢ 0:00:44[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6430, 0.3570]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 167/203 [2m0:03:03 â€¢ 0:00:44[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6452, 0.3548]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 167/203 [2m0:03:03 â€¢ 0:00:44[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6477, 0.3523]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 167/203 [2m0:03:03 â€¢ 0:00:44[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6506, 0.3494]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 167/203 [2m0:03:03 â€¢ 0:00:44[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6537, 0.3463]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 167/203 [2m0:03:03 â€¢ 0:00:44[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6571, 0.3429]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 167/203 [2m0:03:03 â€¢ 0:00:44[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6609, 0.3391]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 167/203 [2m0:03:03 â€¢ 0:00:44[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6649, 0.3351]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 167/203 [2m0:03:03 â€¢ 0:00:44[0m [2;4m0.83it/s[0m  
[2KStep 20 - TTA Loss: 0.5843â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”[0m 167/203 [2m0:03:03 â€¢ 0:00:44[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6692, 0.3308]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 167/203 [2m0:03:03 â€¢ 0:00:44[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6738, 0.3262]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 167/203 [2m0:03:03 â€¢ 0:00:44[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6786, 0.3214]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 167/203 [2m0:03:03 â€¢ 0:00:44[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6837, 0.3163]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 167/203 [2m0:03:03 â€¢ 0:00:44[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6890, 0.3110]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 167/203 [2m0:03:03 â€¢ 0:00:44[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6944, 0.3056]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 167/203 [2m0:03:03 â€¢ 0:00:44[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.7000, 0.3000]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 167/203 [2m0:03:03 â€¢ 0:00:44[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.7058, 0.2942]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 167/203 [2m0:03:03 â€¢ 0:00:44[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.7116, 0.2884]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 167/203 [2m0:03:03 â€¢ 0:00:44[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.5854, 0.4146]], device='cuda:0');98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”[0m 167/203 [2m0:03:03 â€¢ 0:00:44[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.7176, 0.2824],â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”[0m 167/203 [2m0:03:03 â€¢ 0:00:44[0m [2;4m0.83it/s[0m  
        [0.6341, 0.3659],
        [0.6294, 0.3706],
        [0.7058, 0.2942],
        [0.6669, 0.3331],
        [0.6681, 0.3319],
        [0.6185, 0.3815],
        [0.6990, 0.3010],
        [0.7132, 0.2868],
[2K        [0.7166, 0.2834]], device='cuda:0')â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”[0m 167/203 [2m0:03:03 â€¢ 0:00:44[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.5821, 0.4179],â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”[0m 168/203 [2m0:03:04 â€¢ 0:00:42[0m [2;4m0.83it/s[0m   
        [0.5783, 0.4217],
        [0.5808, 0.4192],
        [0.5811, 0.4189],
        [0.5823, 0.4177],
        [0.5791, 0.4209],
        [0.5775, 0.4225],
        [0.5815, 0.4185],
        [0.5815, 0.4185],
[2K        [0.5835, 0.4165]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0m[38;5;237mâ”â”â”â”â”â”[0m 168/203 [2m0:03:04 â€¢ 0:00:42[0m [2;4m0.83it/s[0m  
[2KClass label: SoccerPenaltyâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”[0m 168/203 [2m0:03:04 â€¢ 0:00:42[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6594, 0.3406]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 168/203 [2m0:03:04 â€¢ 0:00:42[0m [2;4m0.83it/s[0m  
[2KStep 0 - TTA Loss: 0.7711â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”[0m 168/203 [2m0:03:04 â€¢ 0:00:42[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6633, 0.3367]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 168/203 [2m0:03:04 â€¢ 0:00:42[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6676, 0.3324]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 168/203 [2m0:03:04 â€¢ 0:00:42[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6720, 0.3280]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 168/203 [2m0:03:04 â€¢ 0:00:42[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6769, 0.3231]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 168/203 [2m0:03:04 â€¢ 0:00:42[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6818, 0.3182]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 168/203 [2m0:03:04 â€¢ 0:00:42[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6869, 0.3131]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 168/203 [2m0:03:04 â€¢ 0:00:42[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6922, 0.3078]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 168/203 [2m0:03:04 â€¢ 0:00:42[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.6973, 0.3027]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 168/203 [2m0:03:04 â€¢ 0:00:42[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.7025, 0.2975]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 168/203 [2m0:03:04 â€¢ 0:00:42[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.7076, 0.2924]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 168/203 [2m0:03:04 â€¢ 0:00:42[0m [2;4m0.83it/s[0m  
[2KStep 10 - TTA Loss: 0.7467â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”[0m 168/203 [2m0:03:04 â€¢ 0:00:42[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.7128, 0.2872]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 168/203 [2m0:03:04 â€¢ 0:00:42[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.7178, 0.2822]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 168/203 [2m0:03:04 â€¢ 0:00:42[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.7228, 0.2772]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 168/203 [2m0:03:04 â€¢ 0:00:42[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.7274, 0.2726]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 168/203 [2m0:03:04 â€¢ 0:00:42[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.7318, 0.2682]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 168/203 [2m0:03:04 â€¢ 0:00:42[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.7359, 0.2641]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 168/203 [2m0:03:04 â€¢ 0:00:42[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.7397, 0.2603]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 168/203 [2m0:03:04 â€¢ 0:00:42[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.7431, 0.2569]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 168/203 [2m0:03:04 â€¢ 0:00:42[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.7461, 0.2539]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 168/203 [2m0:03:04 â€¢ 0:00:42[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.7488, 0.2512]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 168/203 [2m0:03:04 â€¢ 0:00:42[0m [2;4m0.83it/s[0m  
[2KStep 20 - TTA Loss: 0.9939â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”[0m 168/203 [2m0:03:04 â€¢ 0:00:42[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.7510, 0.2490]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 168/203 [2m0:03:04 â€¢ 0:00:42[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.7528, 0.2472]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 168/203 [2m0:03:04 â€¢ 0:00:42[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.7544, 0.2456]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 168/203 [2m0:03:04 â€¢ 0:00:42[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.7556, 0.2444]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 168/203 [2m0:03:04 â€¢ 0:00:42[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.7565, 0.2435]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 168/203 [2m0:03:04 â€¢ 0:00:42[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.7571, 0.2429]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 168/203 [2m0:03:04 â€¢ 0:00:42[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.7575, 0.2425]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 168/203 [2m0:03:04 â€¢ 0:00:42[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.7578, 0.2422]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 168/203 [2m0:03:04 â€¢ 0:00:42[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.7579, 0.2421]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 168/203 [2m0:03:04 â€¢ 0:00:42[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.5860, 0.4140]], device='cuda:0')5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”[0m 168/203 [2m0:03:04 â€¢ 0:00:42[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.7192, 0.2808],â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”[0m 168/203 [2m0:03:04 â€¢ 0:00:42[0m [2;4m0.83it/s[0m  
        [0.6533, 0.3467],
        [0.6388, 0.3612],
        [0.7128, 0.2872],
        [0.6826, 0.3174],
        [0.6889, 0.3111],
        [0.6386, 0.3614],
        [0.7580, 0.2420],
        [0.7246, 0.2754],
[2K        [0.7251, 0.2749]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”[0m 168/203 [2m0:03:04 â€¢ 0:00:42[0m [2;4m0.83it/s[0m  
[2KAttention weights: tensor([[0.5822, 0.4178],â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”[0m 169/203 [2m0:03:05 â€¢ 0:00:41[0m [2;4m0.85it/s[0m  
        [0.5783, 0.4217],
        [0.5809, 0.4191],
        [0.5811, 0.4189],
        [0.5823, 0.4177],
        [0.5792, 0.4208],
        [0.5775, 0.4225],
        [0.5817, 0.4183],
        [0.5815, 0.4185],
[2K        [0.5835, 0.4165]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0m[38;5;237mâ”â”â”â”â”â”[0m 169/203 [2m0:03:05 â€¢ 0:00:41[0m [2;4m0.85it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”[0m 169/203 [2m0:03:05 â€¢ 0:00:41[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6562, 0.3438]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 169/203 [2m0:03:05 â€¢ 0:00:41[0m [2;4m0.85it/s[0m  
[2KStep 0 - TTA Loss: 0.4209â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”[0m 169/203 [2m0:03:05 â€¢ 0:00:41[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6562, 0.3438]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 169/203 [2m0:03:05 â€¢ 0:00:41[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6563, 0.3437]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 169/203 [2m0:03:05 â€¢ 0:00:41[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6563, 0.3437]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 169/203 [2m0:03:05 â€¢ 0:00:41[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6564, 0.3436]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 169/203 [2m0:03:05 â€¢ 0:00:41[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6567, 0.3433]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 169/203 [2m0:03:05 â€¢ 0:00:41[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6570, 0.3430]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 169/203 [2m0:03:05 â€¢ 0:00:41[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6575, 0.3425]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 169/203 [2m0:03:05 â€¢ 0:00:41[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6581, 0.3419]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 169/203 [2m0:03:05 â€¢ 0:00:41[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6589, 0.3411]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 169/203 [2m0:03:05 â€¢ 0:00:41[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6600, 0.3400]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 169/203 [2m0:03:05 â€¢ 0:00:41[0m [2;4m0.85it/s[0m  
[2KStep 10 - TTA Loss: 0.4129â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”[0m 169/203 [2m0:03:05 â€¢ 0:00:41[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6612, 0.3388]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 169/203 [2m0:03:05 â€¢ 0:00:41[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6626, 0.3374]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 169/203 [2m0:03:05 â€¢ 0:00:41[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6643, 0.3357]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 169/203 [2m0:03:05 â€¢ 0:00:41[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6662, 0.3338]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 169/203 [2m0:03:05 â€¢ 0:00:41[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6684, 0.3316]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 169/203 [2m0:03:05 â€¢ 0:00:41[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6709, 0.3291]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 169/203 [2m0:03:05 â€¢ 0:00:41[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6735, 0.3265]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 169/203 [2m0:03:05 â€¢ 0:00:41[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6765, 0.3235]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 169/203 [2m0:03:05 â€¢ 0:00:41[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6796, 0.3204]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 169/203 [2m0:03:05 â€¢ 0:00:41[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6830, 0.3170]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 169/203 [2m0:03:05 â€¢ 0:00:41[0m [2;4m0.85it/s[0m  
[2KStep 20 - TTA Loss: 0.4229â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”[0m 169/203 [2m0:03:05 â€¢ 0:00:41[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6867, 0.3133]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 169/203 [2m0:03:05 â€¢ 0:00:41[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6906, 0.3094]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 169/203 [2m0:03:05 â€¢ 0:00:41[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6947, 0.3053]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 169/203 [2m0:03:05 â€¢ 0:00:41[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6990, 0.3010]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 169/203 [2m0:03:05 â€¢ 0:00:41[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.7035, 0.2965]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 169/203 [2m0:03:05 â€¢ 0:00:41[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.7082, 0.2918]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 169/203 [2m0:03:05 â€¢ 0:00:41[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.7129, 0.2871]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 169/203 [2m0:03:05 â€¢ 0:00:41[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.7178, 0.2822]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 169/203 [2m0:03:05 â€¢ 0:00:41[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.7228, 0.2772]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 169/203 [2m0:03:05 â€¢ 0:00:41[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5865, 0.4135]], device='cuda:0')5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”[0m 169/203 [2m0:03:05 â€¢ 0:00:41[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.7002, 0.2998],â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”[0m 169/203 [2m0:03:05 â€¢ 0:00:41[0m [2;4m0.85it/s[0m  
        [0.6400, 0.3600],
        [0.6383, 0.3617],
        [0.7050, 0.2950],
        [0.6788, 0.3212],
        [0.6815, 0.3185],
        [0.6321, 0.3679],
        [0.7145, 0.2855],
        [0.7172, 0.2828],
[2K        [0.7281, 0.2719]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”[0m 169/203 [2m0:03:05 â€¢ 0:00:41[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5821, 0.4179],â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”[0m 170/203 [2m0:03:06 â€¢ 0:00:39[0m [2;4m0.86it/s[0m  
        [0.5782, 0.4218],
        [0.5807, 0.4193],
        [0.5810, 0.4190],
        [0.5822, 0.4178],
        [0.5790, 0.4210],
        [0.5774, 0.4226],
        [0.5816, 0.4184],
        [0.5813, 0.4187],
[2K        [0.5834, 0.4166]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0m[38;5;237mâ”â”â”â”â”â”[0m 170/203 [2m0:03:06 â€¢ 0:00:39[0m [2;4m0.86it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”[0m 170/203 [2m0:03:06 â€¢ 0:00:39[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6568, 0.3432]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 170/203 [2m0:03:06 â€¢ 0:00:39[0m [2;4m0.86it/s[0m  
[2KStep 0 - TTA Loss: 0.7363â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”[0m 170/203 [2m0:03:06 â€¢ 0:00:39[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6624, 0.3376]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 170/203 [2m0:03:06 â€¢ 0:00:39[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6676, 0.3324]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 170/203 [2m0:03:06 â€¢ 0:00:39[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6725, 0.3275]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 170/203 [2m0:03:06 â€¢ 0:00:39[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6772, 0.3228]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 170/203 [2m0:03:06 â€¢ 0:00:39[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6816, 0.3184]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 170/203 [2m0:03:06 â€¢ 0:00:39[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6857, 0.3143]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 170/203 [2m0:03:06 â€¢ 0:00:39[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6897, 0.3103]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 170/203 [2m0:03:06 â€¢ 0:00:39[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6933, 0.3067]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 170/203 [2m0:03:06 â€¢ 0:00:39[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6968, 0.3032]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 170/203 [2m0:03:06 â€¢ 0:00:39[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.7000, 0.3000]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 170/203 [2m0:03:06 â€¢ 0:00:39[0m [2;4m0.86it/s[0m  
[2KStep 10 - TTA Loss: 0.7636â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”[0m 170/203 [2m0:03:06 â€¢ 0:00:39[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.7030, 0.2970]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 170/203 [2m0:03:06 â€¢ 0:00:39[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.7058, 0.2942]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 170/203 [2m0:03:06 â€¢ 0:00:39[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.7084, 0.2916]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 170/203 [2m0:03:06 â€¢ 0:00:39[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.7108, 0.2892]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 170/203 [2m0:03:06 â€¢ 0:00:39[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.7130, 0.2870]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 170/203 [2m0:03:06 â€¢ 0:00:39[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.7149, 0.2851]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 170/203 [2m0:03:06 â€¢ 0:00:39[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.7167, 0.2833]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 170/203 [2m0:03:06 â€¢ 0:00:39[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.7183, 0.2817]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 170/203 [2m0:03:06 â€¢ 0:00:39[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.7197, 0.2803]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 170/203 [2m0:03:06 â€¢ 0:00:39[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.7209, 0.2791]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 170/203 [2m0:03:06 â€¢ 0:00:39[0m [2;4m0.86it/s[0m  
[2KStep 20 - TTA Loss: 0.7730â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”[0m 170/203 [2m0:03:06 â€¢ 0:00:39[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.7219, 0.2781]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 170/203 [2m0:03:06 â€¢ 0:00:39[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.7227, 0.2773]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 170/203 [2m0:03:06 â€¢ 0:00:39[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.7234, 0.2766]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 170/203 [2m0:03:06 â€¢ 0:00:39[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.7239, 0.2761]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 170/203 [2m0:03:06 â€¢ 0:00:39[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.7243, 0.2757]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 170/203 [2m0:03:06 â€¢ 0:00:39[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.7246, 0.2754]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 170/203 [2m0:03:06 â€¢ 0:00:39[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.7248, 0.2752]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 170/203 [2m0:03:06 â€¢ 0:00:39[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.7249, 0.2751]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 170/203 [2m0:03:06 â€¢ 0:00:39[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.7250, 0.2750]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 170/203 [2m0:03:06 â€¢ 0:00:39[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5860, 0.4140]], device='cuda:0')5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”[0m 170/203 [2m0:03:06 â€¢ 0:00:39[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6934, 0.3066],â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”[0m 170/203 [2m0:03:06 â€¢ 0:00:39[0m [2;4m0.86it/s[0m  
        [0.6372, 0.3628],
        [0.6411, 0.3589],
        [0.7002, 0.2998],
        [0.6742, 0.3258],
        [0.6770, 0.3230],
        [0.6272, 0.3728],
        [0.6976, 0.3024],
        [0.7125, 0.2875],
[2K        [0.7251, 0.2749]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”[0m 170/203 [2m0:03:06 â€¢ 0:00:39[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5818, 0.4182],â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”[0m 171/203 [2m0:03:07 â€¢ 0:00:38[0m [2;4m0.86it/s[0m  
        [0.5780, 0.4220],
        [0.5804, 0.4196],
        [0.5808, 0.4192],
        [0.5819, 0.4181],
        [0.5788, 0.4212],
        [0.5771, 0.4229],
        [0.5814, 0.4186],
        [0.5811, 0.4189],
[2K        [0.5829, 0.4171]], device='cuda:0', grad_fn=<SoftmaxBackward0>)4mâ•¸[0m[38;5;237mâ”â”â”â”â”â”[0m 171/203 [2m0:03:07 â€¢ 0:00:38[0m [2;4m0.86it/s[0m  
[2KClass label: BaseballPitchâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”[0m 171/203 [2m0:03:07 â€¢ 0:00:38[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6358, 0.3642]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 171/203 [2m0:03:07 â€¢ 0:00:38[0m [2;4m0.86it/s[0m  
[2KStep 0 - TTA Loss: 0.8722â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”[0m 171/203 [2m0:03:07 â€¢ 0:00:38[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6358, 0.3642]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 171/203 [2m0:03:07 â€¢ 0:00:38[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6358, 0.3642]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 171/203 [2m0:03:07 â€¢ 0:00:38[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6358, 0.3642]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 171/203 [2m0:03:07 â€¢ 0:00:38[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6359, 0.3641]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 171/203 [2m0:03:07 â€¢ 0:00:38[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6361, 0.3639]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 171/203 [2m0:03:07 â€¢ 0:00:38[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6364, 0.3636]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 171/203 [2m0:03:07 â€¢ 0:00:38[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6367, 0.3633]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 171/203 [2m0:03:07 â€¢ 0:00:38[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6372, 0.3628]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 171/203 [2m0:03:07 â€¢ 0:00:38[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6379, 0.3621]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 171/203 [2m0:03:07 â€¢ 0:00:38[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6388, 0.3612]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 171/203 [2m0:03:07 â€¢ 0:00:38[0m [2;4m0.86it/s[0m  
[2KStep 10 - TTA Loss: 0.9706â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”[0m 171/203 [2m0:03:07 â€¢ 0:00:38[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6398, 0.3602]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 171/203 [2m0:03:07 â€¢ 0:00:38[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6409, 0.3591]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 171/203 [2m0:03:07 â€¢ 0:00:38[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6422, 0.3578]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 171/203 [2m0:03:07 â€¢ 0:00:38[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6436, 0.3564]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 171/203 [2m0:03:07 â€¢ 0:00:38[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6452, 0.3548]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 171/203 [2m0:03:07 â€¢ 0:00:38[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6469, 0.3531]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 171/203 [2m0:03:07 â€¢ 0:00:38[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6488, 0.3512]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 171/203 [2m0:03:07 â€¢ 0:00:38[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6510, 0.3490]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 171/203 [2m0:03:07 â€¢ 0:00:38[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6533, 0.3467]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 171/203 [2m0:03:07 â€¢ 0:00:38[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6558, 0.3442]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 171/203 [2m0:03:07 â€¢ 0:00:38[0m [2;4m0.86it/s[0m  
[2KStep 20 - TTA Loss: 0.8409â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”[0m 171/203 [2m0:03:07 â€¢ 0:00:38[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6585, 0.3415]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 171/203 [2m0:03:07 â€¢ 0:00:38[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6614, 0.3386]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 171/203 [2m0:03:07 â€¢ 0:00:38[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6645, 0.3355]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 171/203 [2m0:03:07 â€¢ 0:00:38[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6677, 0.3323]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 171/203 [2m0:03:07 â€¢ 0:00:38[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6712, 0.3288]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 171/203 [2m0:03:07 â€¢ 0:00:38[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6748, 0.3252]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 171/203 [2m0:03:07 â€¢ 0:00:38[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6785, 0.3215]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 171/203 [2m0:03:07 â€¢ 0:00:38[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6824, 0.3176]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 171/203 [2m0:03:07 â€¢ 0:00:38[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6864, 0.3136]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 171/203 [2m0:03:07 â€¢ 0:00:38[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5835, 0.4165]], device='cuda:0')2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”[0m 171/203 [2m0:03:07 â€¢ 0:00:38[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6906, 0.3094],â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”[0m 171/203 [2m0:03:07 â€¢ 0:00:38[0m [2;4m0.86it/s[0m  
        [0.6265, 0.3735],
        [0.6175, 0.3825],
        [0.6877, 0.3123],
        [0.6498, 0.3502],
        [0.6531, 0.3469],
        [0.6043, 0.3957],
        [0.6874, 0.3126],
        [0.7009, 0.2991],
[2K        [0.6983, 0.3017]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”[0m 171/203 [2m0:03:07 â€¢ 0:00:38[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5813, 0.4187],â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”[0m 172/203 [2m0:03:08 â€¢ 0:00:37[0m [2;4m0.86it/s[0m  
        [0.5777, 0.4223],
        [0.5802, 0.4198],
        [0.5804, 0.4196],
        [0.5816, 0.4184],
        [0.5785, 0.4215],
        [0.5768, 0.4232],
        [0.5810, 0.4190],
        [0.5808, 0.4192],
[2K        [0.5825, 0.4175]], device='cuda:0', grad_fn=<SoftmaxBackward0>)4mâ•¸[0m[38;5;237mâ”â”â”â”â”â”[0m 172/203 [2m0:03:08 â€¢ 0:00:37[0m [2;4m0.86it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”[0m 172/203 [2m0:03:08 â€¢ 0:00:37[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5776, 0.4224]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 172/203 [2m0:03:08 â€¢ 0:00:37[0m [2;4m0.86it/s[0m  
[2KStep 0 - TTA Loss: 0.8313â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”[0m 172/203 [2m0:03:08 â€¢ 0:00:37[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5801, 0.4199]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 172/203 [2m0:03:08 â€¢ 0:00:37[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5827, 0.4173]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 172/203 [2m0:03:08 â€¢ 0:00:37[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5853, 0.4147]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 172/203 [2m0:03:08 â€¢ 0:00:37[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5880, 0.4120]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 172/203 [2m0:03:08 â€¢ 0:00:37[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5907, 0.4093]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 172/203 [2m0:03:08 â€¢ 0:00:37[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5935, 0.4065]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 172/203 [2m0:03:08 â€¢ 0:00:37[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5963, 0.4037]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 172/203 [2m0:03:08 â€¢ 0:00:37[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5990, 0.4010]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 172/203 [2m0:03:08 â€¢ 0:00:37[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6017, 0.3983]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 172/203 [2m0:03:08 â€¢ 0:00:37[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6043, 0.3957]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 172/203 [2m0:03:08 â€¢ 0:00:37[0m [2;4m0.86it/s[0m  
[2KStep 10 - TTA Loss: 0.7522â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”[0m 172/203 [2m0:03:08 â€¢ 0:00:37[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6069, 0.3931]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 172/203 [2m0:03:08 â€¢ 0:00:37[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6093, 0.3907]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 172/203 [2m0:03:08 â€¢ 0:00:37[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6116, 0.3884]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 172/203 [2m0:03:08 â€¢ 0:00:37[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6138, 0.3862]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 172/203 [2m0:03:08 â€¢ 0:00:37[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6158, 0.3842]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 172/203 [2m0:03:08 â€¢ 0:00:37[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6176, 0.3824]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 172/203 [2m0:03:08 â€¢ 0:00:37[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6193, 0.3807]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 172/203 [2m0:03:08 â€¢ 0:00:37[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6208, 0.3792]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 172/203 [2m0:03:08 â€¢ 0:00:37[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6221, 0.3779]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 172/203 [2m0:03:08 â€¢ 0:00:37[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6232, 0.3768]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 172/203 [2m0:03:08 â€¢ 0:00:37[0m [2;4m0.86it/s[0m  
[2KStep 20 - TTA Loss: 0.7471â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”[0m 172/203 [2m0:03:08 â€¢ 0:00:37[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6242, 0.3758]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 172/203 [2m0:03:08 â€¢ 0:00:37[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6250, 0.3750]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 172/203 [2m0:03:08 â€¢ 0:00:37[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6256, 0.3744]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 172/203 [2m0:03:08 â€¢ 0:00:37[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6262, 0.3738]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 172/203 [2m0:03:08 â€¢ 0:00:37[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6265, 0.3735]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 172/203 [2m0:03:08 â€¢ 0:00:37[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6268, 0.3732]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 172/203 [2m0:03:08 â€¢ 0:00:37[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6270, 0.3730]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 172/203 [2m0:03:08 â€¢ 0:00:37[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6271, 0.3729]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 172/203 [2m0:03:08 â€¢ 0:00:37[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6271, 0.3729]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 172/203 [2m0:03:08 â€¢ 0:00:37[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5787, 0.4213]], device='cuda:0')2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”[0m 172/203 [2m0:03:08 â€¢ 0:00:37[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6860, 0.3140],â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”[0m 172/203 [2m0:03:08 â€¢ 0:00:37[0m [2;4m0.86it/s[0m  
        [0.6285, 0.3715],
        [0.6266, 0.3734],
        [0.6898, 0.3102],
        [0.6564, 0.3436],
        [0.6665, 0.3335],
        [0.6272, 0.3728],
        [0.6939, 0.3061],
        [0.6982, 0.3018],
[2K        [0.7027, 0.2973]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”[0m 172/203 [2m0:03:08 â€¢ 0:00:37[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5810, 0.4190],â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”[0m 173/203 [2m0:03:09 â€¢ 0:00:36[0m [2;4m0.85it/s[0m   
        [0.5775, 0.4225],
        [0.5800, 0.4200],
        [0.5802, 0.4198],
        [0.5814, 0.4186],
        [0.5783, 0.4217],
        [0.5766, 0.4234],
        [0.5808, 0.4192],
        [0.5807, 0.4193],
[2K        [0.5824, 0.4176]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m[38;5;237mâ”â”â”â”â”[0m 173/203 [2m0:03:09 â€¢ 0:00:36[0m [2;4m0.85it/s[0m  
[2KClass label: HammerThrowâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”[0m 173/203 [2m0:03:09 â€¢ 0:00:36[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6057, 0.3943]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 173/203 [2m0:03:09 â€¢ 0:00:36[0m [2;4m0.85it/s[0m  
[2KStep 0 - TTA Loss: 0.8812â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”[0m 173/203 [2m0:03:09 â€¢ 0:00:36[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6057, 0.3943]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 173/203 [2m0:03:09 â€¢ 0:00:36[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6057, 0.3943]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 173/203 [2m0:03:09 â€¢ 0:00:36[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6057, 0.3943]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 173/203 [2m0:03:09 â€¢ 0:00:36[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6058, 0.3942]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 173/203 [2m0:03:09 â€¢ 0:00:36[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6059, 0.3941]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 173/203 [2m0:03:09 â€¢ 0:00:36[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6062, 0.3938]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 173/203 [2m0:03:09 â€¢ 0:00:36[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6065, 0.3935]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 173/203 [2m0:03:09 â€¢ 0:00:36[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6070, 0.3930]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 173/203 [2m0:03:09 â€¢ 0:00:36[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6076, 0.3924]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 173/203 [2m0:03:09 â€¢ 0:00:36[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6083, 0.3917]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 173/203 [2m0:03:09 â€¢ 0:00:36[0m [2;4m0.85it/s[0m  
[2KStep 10 - TTA Loss: 0.5208â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”[0m 173/203 [2m0:03:09 â€¢ 0:00:36[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6093, 0.3907]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 173/203 [2m0:03:09 â€¢ 0:00:36[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6104, 0.3896]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 173/203 [2m0:03:09 â€¢ 0:00:36[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6117, 0.3883]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 173/203 [2m0:03:09 â€¢ 0:00:36[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6132, 0.3868]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 173/203 [2m0:03:09 â€¢ 0:00:36[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6149, 0.3851]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 173/203 [2m0:03:09 â€¢ 0:00:36[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6168, 0.3832]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 173/203 [2m0:03:09 â€¢ 0:00:36[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6188, 0.3812]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 173/203 [2m0:03:09 â€¢ 0:00:36[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6209, 0.3791]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 173/203 [2m0:03:09 â€¢ 0:00:36[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6233, 0.3767]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 173/203 [2m0:03:09 â€¢ 0:00:36[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6258, 0.3742]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 173/203 [2m0:03:09 â€¢ 0:00:36[0m [2;4m0.85it/s[0m  
[2KStep 20 - TTA Loss: 0.4939â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”[0m 173/203 [2m0:03:09 â€¢ 0:00:36[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6287, 0.3713]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 173/203 [2m0:03:09 â€¢ 0:00:36[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6316, 0.3684]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 173/203 [2m0:03:09 â€¢ 0:00:36[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6347, 0.3653]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 173/203 [2m0:03:09 â€¢ 0:00:36[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6380, 0.3620]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 173/203 [2m0:03:09 â€¢ 0:00:36[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6415, 0.3585]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 173/203 [2m0:03:09 â€¢ 0:00:36[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6452, 0.3548]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 173/203 [2m0:03:09 â€¢ 0:00:36[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6490, 0.3510]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 173/203 [2m0:03:09 â€¢ 0:00:36[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6531, 0.3469]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 173/203 [2m0:03:09 â€¢ 0:00:36[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6571, 0.3429]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 173/203 [2m0:03:09 â€¢ 0:00:36[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5835, 0.4165]], device='cuda:0');5;237mâ•º[0m[38;5;237mâ”â”â”â”â”[0m 173/203 [2m0:03:09 â€¢ 0:00:36[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6723, 0.3277],â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”[0m 173/203 [2m0:03:09 â€¢ 0:00:36[0m [2;4m0.85it/s[0m  
        [0.6256, 0.3744],
        [0.6269, 0.3731],
        [0.6814, 0.3186],
        [0.6614, 0.3386],
        [0.6594, 0.3406],
        [0.6145, 0.3855],
        [0.6887, 0.3113],
        [0.6950, 0.3050],
[2K        [0.6998, 0.3002]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”[0m 173/203 [2m0:03:09 â€¢ 0:00:36[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5810, 0.4190],â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”[0m 174/203 [2m0:03:10 â€¢ 0:00:35[0m [2;4m0.85it/s[0m  
        [0.5775, 0.4225],
        [0.5799, 0.4201],
        [0.5801, 0.4199],
        [0.5814, 0.4186],
        [0.5782, 0.4218],
        [0.5765, 0.4235],
        [0.5808, 0.4192],
        [0.5806, 0.4194],
[2K        [0.5823, 0.4177]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m[38;5;237mâ”â”â”â”â”[0m 174/203 [2m0:03:10 â€¢ 0:00:35[0m [2;4m0.85it/s[0m  
[2KClass label: TennisSwingâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”[0m 174/203 [2m0:03:10 â€¢ 0:00:35[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6653, 0.3347]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 174/203 [2m0:03:10 â€¢ 0:00:35[0m [2;4m0.85it/s[0m  
[2KStep 0 - TTA Loss: 0.6766â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”[0m 174/203 [2m0:03:10 â€¢ 0:00:35[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6676, 0.3324]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 174/203 [2m0:03:10 â€¢ 0:00:35[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6698, 0.3302]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 174/203 [2m0:03:10 â€¢ 0:00:35[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6719, 0.3281]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 174/203 [2m0:03:10 â€¢ 0:00:35[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6739, 0.3261]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 174/203 [2m0:03:10 â€¢ 0:00:35[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6759, 0.3241]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 174/203 [2m0:03:10 â€¢ 0:00:35[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6777, 0.3223]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 174/203 [2m0:03:10 â€¢ 0:00:35[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6794, 0.3206]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 174/203 [2m0:03:10 â€¢ 0:00:35[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6810, 0.3190]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 174/203 [2m0:03:10 â€¢ 0:00:35[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6825, 0.3175]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 174/203 [2m0:03:10 â€¢ 0:00:35[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6839, 0.3161]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 174/203 [2m0:03:10 â€¢ 0:00:35[0m [2;4m0.85it/s[0m  
[2KStep 10 - TTA Loss: 0.6210â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”[0m 174/203 [2m0:03:10 â€¢ 0:00:35[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6853, 0.3147]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 174/203 [2m0:03:10 â€¢ 0:00:35[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6865, 0.3135]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 174/203 [2m0:03:10 â€¢ 0:00:35[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6877, 0.3123]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 174/203 [2m0:03:10 â€¢ 0:00:35[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6887, 0.3113]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 174/203 [2m0:03:10 â€¢ 0:00:35[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6897, 0.3103]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 174/203 [2m0:03:10 â€¢ 0:00:35[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6905, 0.3095]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 174/203 [2m0:03:10 â€¢ 0:00:35[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6913, 0.3087]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 174/203 [2m0:03:10 â€¢ 0:00:35[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6920, 0.3080]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 174/203 [2m0:03:10 â€¢ 0:00:35[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6926, 0.3074]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 174/203 [2m0:03:10 â€¢ 0:00:35[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6931, 0.3069]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 174/203 [2m0:03:10 â€¢ 0:00:35[0m [2;4m0.85it/s[0m  
[2KStep 20 - TTA Loss: 0.6216â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”[0m 174/203 [2m0:03:10 â€¢ 0:00:35[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6935, 0.3065]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 174/203 [2m0:03:10 â€¢ 0:00:35[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6939, 0.3061]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 174/203 [2m0:03:10 â€¢ 0:00:35[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6942, 0.3058]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 174/203 [2m0:03:10 â€¢ 0:00:35[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6944, 0.3056]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 174/203 [2m0:03:10 â€¢ 0:00:35[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6946, 0.3054]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 174/203 [2m0:03:10 â€¢ 0:00:35[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6947, 0.3053]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 174/203 [2m0:03:10 â€¢ 0:00:35[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6948, 0.3052]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 174/203 [2m0:03:10 â€¢ 0:00:35[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6948, 0.3052]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 174/203 [2m0:03:10 â€¢ 0:00:35[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6948, 0.3052]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 174/203 [2m0:03:10 â€¢ 0:00:35[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5816, 0.4184]], device='cuda:0');5;237mâ•º[0m[38;5;237mâ”â”â”â”â”[0m 174/203 [2m0:03:10 â€¢ 0:00:35[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6641, 0.3359],â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”[0m 174/203 [2m0:03:10 â€¢ 0:00:35[0m [2;4m0.85it/s[0m  
        [0.6212, 0.3788],
        [0.6176, 0.3824],
        [0.6760, 0.3240],
        [0.6455, 0.3545],
        [0.6489, 0.3511],
        [0.5993, 0.4007],
        [0.6791, 0.3209],
        [0.6949, 0.3051],
[2K        [0.6899, 0.3101]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”[0m 174/203 [2m0:03:10 â€¢ 0:00:35[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5809, 0.4191],â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”[0m 175/203 [2m0:03:12 â€¢ 0:00:33[0m [2;4m0.85it/s[0m  
        [0.5773, 0.4227],
        [0.5798, 0.4202],
        [0.5800, 0.4200],
        [0.5813, 0.4187],
        [0.5781, 0.4219],
        [0.5764, 0.4236],
        [0.5807, 0.4193],
        [0.5803, 0.4197],
[2K        [0.5822, 0.4178]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m[38;5;237mâ”â”â”â”â”[0m 175/203 [2m0:03:12 â€¢ 0:00:33[0m [2;4m0.85it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”[0m 175/203 [2m0:03:12 â€¢ 0:00:33[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6574, 0.3426]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 175/203 [2m0:03:12 â€¢ 0:00:33[0m [2;4m0.85it/s[0m  
[2KStep 0 - TTA Loss: 0.7130â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”[0m 175/203 [2m0:03:12 â€¢ 0:00:33[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6574, 0.3426]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 175/203 [2m0:03:12 â€¢ 0:00:33[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6574, 0.3426]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 175/203 [2m0:03:12 â€¢ 0:00:33[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6574, 0.3426]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 175/203 [2m0:03:12 â€¢ 0:00:33[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6575, 0.3425]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 175/203 [2m0:03:12 â€¢ 0:00:33[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6576, 0.3424]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 175/203 [2m0:03:12 â€¢ 0:00:33[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6577, 0.3423]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 175/203 [2m0:03:12 â€¢ 0:00:33[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6580, 0.3420]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 175/203 [2m0:03:12 â€¢ 0:00:33[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6583, 0.3417]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 175/203 [2m0:03:12 â€¢ 0:00:33[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6588, 0.3412]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 175/203 [2m0:03:12 â€¢ 0:00:33[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6594, 0.3406]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 175/203 [2m0:03:12 â€¢ 0:00:33[0m [2;4m0.85it/s[0m  
[2KStep 10 - TTA Loss: 0.8568â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”[0m 175/203 [2m0:03:12 â€¢ 0:00:33[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6602, 0.3398]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 175/203 [2m0:03:12 â€¢ 0:00:33[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6611, 0.3389]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 175/203 [2m0:03:12 â€¢ 0:00:33[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6622, 0.3378]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 175/203 [2m0:03:12 â€¢ 0:00:33[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6634, 0.3366]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 175/203 [2m0:03:12 â€¢ 0:00:33[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6649, 0.3351]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 175/203 [2m0:03:12 â€¢ 0:00:33[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6667, 0.3333]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 175/203 [2m0:03:12 â€¢ 0:00:33[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6686, 0.3314]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 175/203 [2m0:03:12 â€¢ 0:00:33[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6708, 0.3292]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 175/203 [2m0:03:12 â€¢ 0:00:33[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6731, 0.3269]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 175/203 [2m0:03:12 â€¢ 0:00:33[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6758, 0.3242]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 175/203 [2m0:03:12 â€¢ 0:00:33[0m [2;4m0.85it/s[0m  
[2KStep 20 - TTA Loss: 0.7273â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”[0m 175/203 [2m0:03:12 â€¢ 0:00:33[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6787, 0.3213]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 175/203 [2m0:03:12 â€¢ 0:00:33[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6818, 0.3182]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 175/203 [2m0:03:12 â€¢ 0:00:33[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6851, 0.3149]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 175/203 [2m0:03:12 â€¢ 0:00:33[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6886, 0.3114]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 175/203 [2m0:03:12 â€¢ 0:00:33[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6924, 0.3076]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 175/203 [2m0:03:12 â€¢ 0:00:33[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6963, 0.3037]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 175/203 [2m0:03:12 â€¢ 0:00:33[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.7005, 0.2995]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 175/203 [2m0:03:12 â€¢ 0:00:33[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.7048, 0.2952]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 175/203 [2m0:03:12 â€¢ 0:00:33[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.7095, 0.2905]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 175/203 [2m0:03:12 â€¢ 0:00:33[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5851, 0.4149]], device='cuda:0');5;237mâ•º[0m[38;5;237mâ”â”â”â”â”[0m 175/203 [2m0:03:12 â€¢ 0:00:33[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6863, 0.3137],â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”[0m 175/203 [2m0:03:12 â€¢ 0:00:33[0m [2;4m0.85it/s[0m  
        [0.6319, 0.3681],
        [0.6345, 0.3655],
        [0.6957, 0.3043],
        [0.6627, 0.3373],
        [0.6662, 0.3338],
        [0.6166, 0.3834],
        [0.6901, 0.3099],
        [0.7083, 0.2917],
[2K        [0.7145, 0.2855]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”[0m 175/203 [2m0:03:12 â€¢ 0:00:33[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5810, 0.4190],â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”[0m 176/203 [2m0:03:13 â€¢ 0:00:33[0m [2;4m0.84it/s[0m  
        [0.5774, 0.4226],
        [0.5799, 0.4201],
        [0.5801, 0.4199],
        [0.5814, 0.4186],
        [0.5782, 0.4218],
        [0.5765, 0.4235],
        [0.5807, 0.4193],
        [0.5804, 0.4196],
[2K        [0.5824, 0.4176]], device='cuda:0', grad_fn=<SoftmaxBackward0>)24mâ•¸[0m[38;5;237mâ”â”â”â”â”[0m 176/203 [2m0:03:13 â€¢ 0:00:33[0m [2;4m0.84it/s[0m  
[2KClass label: BaseballPitchâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”[0m 176/203 [2m0:03:13 â€¢ 0:00:33[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6357, 0.3643]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 176/203 [2m0:03:13 â€¢ 0:00:33[0m [2;4m0.84it/s[0m  
[2KStep 0 - TTA Loss: 0.7722â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”[0m 176/203 [2m0:03:13 â€¢ 0:00:33[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6404, 0.3596]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 176/203 [2m0:03:13 â€¢ 0:00:33[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6450, 0.3550]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 176/203 [2m0:03:13 â€¢ 0:00:33[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6496, 0.3504]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 176/203 [2m0:03:13 â€¢ 0:00:33[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6542, 0.3458]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 176/203 [2m0:03:13 â€¢ 0:00:33[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6587, 0.3413]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 176/203 [2m0:03:13 â€¢ 0:00:33[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6631, 0.3369]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 176/203 [2m0:03:13 â€¢ 0:00:33[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6674, 0.3326]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 176/203 [2m0:03:13 â€¢ 0:00:33[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6715, 0.3285]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 176/203 [2m0:03:13 â€¢ 0:00:33[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6755, 0.3245]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 176/203 [2m0:03:13 â€¢ 0:00:33[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6792, 0.3208]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 176/203 [2m0:03:13 â€¢ 0:00:33[0m [2;4m0.84it/s[0m  
[2KStep 10 - TTA Loss: 0.7877â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”[0m 176/203 [2m0:03:13 â€¢ 0:00:33[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6828, 0.3172]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 176/203 [2m0:03:13 â€¢ 0:00:33[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6863, 0.3137]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 176/203 [2m0:03:13 â€¢ 0:00:33[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6895, 0.3105]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 176/203 [2m0:03:13 â€¢ 0:00:33[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6926, 0.3074]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 176/203 [2m0:03:13 â€¢ 0:00:33[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6953, 0.3047]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 176/203 [2m0:03:13 â€¢ 0:00:33[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6978, 0.3022]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 176/203 [2m0:03:13 â€¢ 0:00:33[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.7001, 0.2999]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 176/203 [2m0:03:13 â€¢ 0:00:33[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.7021, 0.2979]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 176/203 [2m0:03:13 â€¢ 0:00:33[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.7039, 0.2961]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 176/203 [2m0:03:13 â€¢ 0:00:33[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.7055, 0.2945]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 176/203 [2m0:03:13 â€¢ 0:00:33[0m [2;4m0.84it/s[0m  
[2KStep 20 - TTA Loss: 0.9251â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”[0m 176/203 [2m0:03:13 â€¢ 0:00:33[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.7068, 0.2932]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 176/203 [2m0:03:13 â€¢ 0:00:33[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.7079, 0.2921]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 176/203 [2m0:03:13 â€¢ 0:00:33[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.7087, 0.2913]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 176/203 [2m0:03:13 â€¢ 0:00:33[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.7095, 0.2905]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 176/203 [2m0:03:13 â€¢ 0:00:33[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.7100, 0.2900]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 176/203 [2m0:03:13 â€¢ 0:00:33[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.7104, 0.2896]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 176/203 [2m0:03:13 â€¢ 0:00:33[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.7106, 0.2894]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 176/203 [2m0:03:13 â€¢ 0:00:33[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.7108, 0.2892]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 176/203 [2m0:03:13 â€¢ 0:00:33[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.7108, 0.2892]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 176/203 [2m0:03:13 â€¢ 0:00:33[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5845, 0.4155]], device='cuda:0');2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”[0m 176/203 [2m0:03:13 â€¢ 0:00:33[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.7109, 0.2891],â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”[0m 176/203 [2m0:03:13 â€¢ 0:00:33[0m [2;4m0.84it/s[0m  
        [0.6372, 0.3628],
        [0.6359, 0.3641],
        [0.7088, 0.2912],
        [0.6739, 0.3261],
        [0.6742, 0.3258],
        [0.6237, 0.3763],
        [0.7013, 0.2987],
        [0.7168, 0.2832],
[2K        [0.7237, 0.2763]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”[0m 176/203 [2m0:03:13 â€¢ 0:00:33[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5814, 0.4186],â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”[0m 177/203 [2m0:03:14 â€¢ 0:00:31[0m [2;4m0.84it/s[0m  
        [0.5778, 0.4222],
        [0.5802, 0.4198],
        [0.5806, 0.4194],
        [0.5817, 0.4183],
        [0.5784, 0.4216],
        [0.5767, 0.4233],
        [0.5811, 0.4189],
        [0.5808, 0.4192],
[2K        [0.5827, 0.4173]], device='cuda:0', grad_fn=<SoftmaxBackward0>)24mâ•¸[0m[38;5;237mâ”â”â”â”â”[0m 177/203 [2m0:03:14 â€¢ 0:00:31[0m [2;4m0.84it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”[0m 177/203 [2m0:03:14 â€¢ 0:00:31[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5688, 0.4312]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 177/203 [2m0:03:14 â€¢ 0:00:31[0m [2;4m0.84it/s[0m  
[2KStep 0 - TTA Loss: 0.6397â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”[0m 177/203 [2m0:03:14 â€¢ 0:00:31[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5688, 0.4312]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 177/203 [2m0:03:14 â€¢ 0:00:31[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5688, 0.4312]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 177/203 [2m0:03:14 â€¢ 0:00:31[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5688, 0.4312]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 177/203 [2m0:03:14 â€¢ 0:00:31[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5689, 0.4311]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 177/203 [2m0:03:14 â€¢ 0:00:31[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5691, 0.4309]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 177/203 [2m0:03:14 â€¢ 0:00:31[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5693, 0.4307]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 177/203 [2m0:03:14 â€¢ 0:00:31[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5696, 0.4304]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 177/203 [2m0:03:14 â€¢ 0:00:31[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5700, 0.4300]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 177/203 [2m0:03:14 â€¢ 0:00:31[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5706, 0.4294]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 177/203 [2m0:03:14 â€¢ 0:00:31[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5713, 0.4287]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 177/203 [2m0:03:14 â€¢ 0:00:31[0m [2;4m0.84it/s[0m  
[2KStep 10 - TTA Loss: 0.6435â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”[0m 177/203 [2m0:03:14 â€¢ 0:00:31[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5721, 0.4279]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 177/203 [2m0:03:14 â€¢ 0:00:31[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5731, 0.4269]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 177/203 [2m0:03:14 â€¢ 0:00:31[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5743, 0.4257]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 177/203 [2m0:03:14 â€¢ 0:00:31[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5756, 0.4244]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 177/203 [2m0:03:14 â€¢ 0:00:31[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5771, 0.4229]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 177/203 [2m0:03:14 â€¢ 0:00:31[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5787, 0.4213]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 177/203 [2m0:03:14 â€¢ 0:00:31[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5806, 0.4194]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 177/203 [2m0:03:14 â€¢ 0:00:31[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5826, 0.4174]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 177/203 [2m0:03:14 â€¢ 0:00:31[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5847, 0.4153]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 177/203 [2m0:03:14 â€¢ 0:00:31[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5871, 0.4129]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 177/203 [2m0:03:14 â€¢ 0:00:31[0m [2;4m0.84it/s[0m  
[2KStep 20 - TTA Loss: 0.7556â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”[0m 177/203 [2m0:03:14 â€¢ 0:00:31[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5896, 0.4104]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 177/203 [2m0:03:14 â€¢ 0:00:31[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5923, 0.4077]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 177/203 [2m0:03:14 â€¢ 0:00:31[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5951, 0.4049]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 177/203 [2m0:03:14 â€¢ 0:00:31[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5981, 0.4019]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 177/203 [2m0:03:14 â€¢ 0:00:31[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6012, 0.3988]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 177/203 [2m0:03:14 â€¢ 0:00:31[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6045, 0.3955]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 177/203 [2m0:03:14 â€¢ 0:00:31[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6079, 0.3921]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 177/203 [2m0:03:14 â€¢ 0:00:31[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6113, 0.3887]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 177/203 [2m0:03:14 â€¢ 0:00:31[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6149, 0.3851]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 177/203 [2m0:03:14 â€¢ 0:00:31[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5785, 0.4215]], device='cuda:0');2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”[0m 177/203 [2m0:03:14 â€¢ 0:00:31[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6686, 0.3314],â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”[0m 177/203 [2m0:03:14 â€¢ 0:00:31[0m [2;4m0.84it/s[0m  
        [0.6217, 0.3783],
        [0.6178, 0.3822],
        [0.6762, 0.3238],
        [0.6439, 0.3561],
        [0.6541, 0.3459],
        [0.6186, 0.3814],
        [0.6852, 0.3148],
        [0.6842, 0.3158],
[2K        [0.6899, 0.3101]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”[0m 177/203 [2m0:03:14 â€¢ 0:00:31[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5815, 0.4185],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”[0m 178/203 [2m0:03:15 â€¢ 0:00:28[0m [2;4m0.89it/s[0m   
        [0.5778, 0.4222],
        [0.5802, 0.4198],
        [0.5807, 0.4193],
        [0.5817, 0.4183],
        [0.5784, 0.4216],
        [0.5768, 0.4232],
        [0.5811, 0.4189],
        [0.5809, 0.4191],
[2K        [0.5828, 0.4172]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m[38;5;237mâ”â”â”â”[0m 178/203 [2m0:03:15 â€¢ 0:00:28[0m [2;4m0.89it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”[0m 178/203 [2m0:03:15 â€¢ 0:00:28[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6587, 0.3413]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 178/203 [2m0:03:15 â€¢ 0:00:28[0m [2;4m0.89it/s[0m  
[2KStep 0 - TTA Loss: 0.4159â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”[0m 178/203 [2m0:03:15 â€¢ 0:00:28[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6612, 0.3388]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 178/203 [2m0:03:15 â€¢ 0:00:28[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6639, 0.3361]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 178/203 [2m0:03:15 â€¢ 0:00:28[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6666, 0.3334]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 178/203 [2m0:03:15 â€¢ 0:00:28[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6694, 0.3306]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 178/203 [2m0:03:15 â€¢ 0:00:28[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6722, 0.3278]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 178/203 [2m0:03:15 â€¢ 0:00:28[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6751, 0.3249]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 178/203 [2m0:03:15 â€¢ 0:00:28[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6780, 0.3220]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 178/203 [2m0:03:15 â€¢ 0:00:28[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6808, 0.3192]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 178/203 [2m0:03:15 â€¢ 0:00:28[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6835, 0.3165]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 178/203 [2m0:03:15 â€¢ 0:00:28[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6862, 0.3138]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 178/203 [2m0:03:15 â€¢ 0:00:28[0m [2;4m0.89it/s[0m  
[2KStep 10 - TTA Loss: 0.4037â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”[0m 178/203 [2m0:03:15 â€¢ 0:00:28[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6888, 0.3112]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 178/203 [2m0:03:15 â€¢ 0:00:28[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6913, 0.3087]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 178/203 [2m0:03:15 â€¢ 0:00:28[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6937, 0.3063]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 178/203 [2m0:03:15 â€¢ 0:00:28[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6959, 0.3041]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 178/203 [2m0:03:15 â€¢ 0:00:28[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6979, 0.3021]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 178/203 [2m0:03:15 â€¢ 0:00:28[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6998, 0.3002]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 178/203 [2m0:03:15 â€¢ 0:00:28[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.7015, 0.2985]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 178/203 [2m0:03:15 â€¢ 0:00:28[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.7031, 0.2969]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 178/203 [2m0:03:15 â€¢ 0:00:28[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.7045, 0.2955]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 178/203 [2m0:03:15 â€¢ 0:00:28[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.7057, 0.2943]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 178/203 [2m0:03:15 â€¢ 0:00:28[0m [2;4m0.89it/s[0m  
[2KStep 20 - TTA Loss: 0.4257â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”[0m 178/203 [2m0:03:15 â€¢ 0:00:28[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.7067, 0.2933]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 178/203 [2m0:03:15 â€¢ 0:00:28[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.7076, 0.2924]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 178/203 [2m0:03:15 â€¢ 0:00:28[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.7083, 0.2917]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 178/203 [2m0:03:15 â€¢ 0:00:28[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.7089, 0.2911]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 178/203 [2m0:03:15 â€¢ 0:00:28[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.7093, 0.2907]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 178/203 [2m0:03:15 â€¢ 0:00:28[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.7096, 0.2904]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 178/203 [2m0:03:15 â€¢ 0:00:28[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.7098, 0.2902]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 178/203 [2m0:03:15 â€¢ 0:00:28[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.7099, 0.2901]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 178/203 [2m0:03:15 â€¢ 0:00:28[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.7099, 0.2901]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 178/203 [2m0:03:15 â€¢ 0:00:28[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.5849, 0.4151]], device='cuda:0')8;5;237mâ•º[0m[38;5;237mâ”â”â”â”[0m 178/203 [2m0:03:15 â€¢ 0:00:28[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6795, 0.3205],â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”[0m 178/203 [2m0:03:15 â€¢ 0:00:28[0m [2;4m0.89it/s[0m  
        [0.6337, 0.3663],
        [0.6311, 0.3689],
        [0.6895, 0.3105],
        [0.6598, 0.3402],
        [0.6662, 0.3338],
        [0.6239, 0.3761],
        [0.6913, 0.3087],
        [0.7030, 0.2970],
[2K        [0.7101, 0.2899]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”[0m 178/203 [2m0:03:15 â€¢ 0:00:28[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.5814, 0.4186],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”[0m 179/203 [2m0:03:17 â€¢ 0:00:28[0m [2;4m0.87it/s[0m  
        [0.5777, 0.4223],
        [0.5801, 0.4199],
        [0.5806, 0.4194],
        [0.5815, 0.4185],
        [0.5783, 0.4217],
        [0.5766, 0.4234],
        [0.5810, 0.4190],
        [0.5807, 0.4193],
[2K        [0.5825, 0.4175]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m[38;5;237mâ”â”â”â”[0m 179/203 [2m0:03:17 â€¢ 0:00:28[0m [2;4m0.87it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”[0m 179/203 [2m0:03:17 â€¢ 0:00:28[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5704, 0.4296]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 179/203 [2m0:03:17 â€¢ 0:00:28[0m [2;4m0.87it/s[0m  
[2KStep 0 - TTA Loss: 0.4222â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”[0m 179/203 [2m0:03:17 â€¢ 0:00:28[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5704, 0.4296]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 179/203 [2m0:03:17 â€¢ 0:00:28[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5704, 0.4296]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 179/203 [2m0:03:17 â€¢ 0:00:28[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5704, 0.4296]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 179/203 [2m0:03:17 â€¢ 0:00:28[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5705, 0.4295]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 179/203 [2m0:03:17 â€¢ 0:00:28[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5707, 0.4293]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 179/203 [2m0:03:17 â€¢ 0:00:28[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5709, 0.4291]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 179/203 [2m0:03:17 â€¢ 0:00:28[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5712, 0.4288]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 179/203 [2m0:03:17 â€¢ 0:00:28[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5716, 0.4284]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 179/203 [2m0:03:17 â€¢ 0:00:28[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5722, 0.4278]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 179/203 [2m0:03:17 â€¢ 0:00:28[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5728, 0.4272]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 179/203 [2m0:03:17 â€¢ 0:00:28[0m [2;4m0.87it/s[0m  
[2KStep 10 - TTA Loss: 0.4118â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”[0m 179/203 [2m0:03:17 â€¢ 0:00:28[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5736, 0.4264]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 179/203 [2m0:03:17 â€¢ 0:00:28[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5745, 0.4255]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 179/203 [2m0:03:17 â€¢ 0:00:28[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5755, 0.4245]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 179/203 [2m0:03:17 â€¢ 0:00:28[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5767, 0.4233]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 179/203 [2m0:03:17 â€¢ 0:00:28[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5780, 0.4220]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 179/203 [2m0:03:17 â€¢ 0:00:28[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5794, 0.4206]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 179/203 [2m0:03:17 â€¢ 0:00:28[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5810, 0.4190]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 179/203 [2m0:03:17 â€¢ 0:00:28[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5827, 0.4173]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 179/203 [2m0:03:17 â€¢ 0:00:28[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5846, 0.4154]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 179/203 [2m0:03:17 â€¢ 0:00:28[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5866, 0.4134]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 179/203 [2m0:03:17 â€¢ 0:00:28[0m [2;4m0.87it/s[0m  
[2KStep 20 - TTA Loss: 0.3996â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”[0m 179/203 [2m0:03:17 â€¢ 0:00:28[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5888, 0.4112]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 179/203 [2m0:03:17 â€¢ 0:00:28[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5911, 0.4089]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 179/203 [2m0:03:17 â€¢ 0:00:28[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5935, 0.4065]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 179/203 [2m0:03:17 â€¢ 0:00:28[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5960, 0.4040]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 179/203 [2m0:03:17 â€¢ 0:00:28[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5986, 0.4014]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 179/203 [2m0:03:17 â€¢ 0:00:28[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6013, 0.3987]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 179/203 [2m0:03:17 â€¢ 0:00:28[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6041, 0.3959]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 179/203 [2m0:03:17 â€¢ 0:00:28[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6070, 0.3930]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 179/203 [2m0:03:17 â€¢ 0:00:28[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6098, 0.3902]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 179/203 [2m0:03:17 â€¢ 0:00:28[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5778, 0.4222]], device='cuda:0')8;5;237mâ•º[0m[38;5;237mâ”â”â”â”[0m 179/203 [2m0:03:17 â€¢ 0:00:28[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6594, 0.3406],â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”[0m 179/203 [2m0:03:17 â€¢ 0:00:28[0m [2;4m0.87it/s[0m  
        [0.6167, 0.3833],
        [0.6149, 0.3851],
        [0.6704, 0.3296],
        [0.6391, 0.3609],
        [0.6519, 0.3481],
        [0.6128, 0.3872],
        [0.6811, 0.3189],
        [0.6829, 0.3171],
[2K        [0.6865, 0.3135]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”[0m 179/203 [2m0:03:17 â€¢ 0:00:28[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5812, 0.4188],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”[0m 180/203 [2m0:03:18 â€¢ 0:00:27[0m [2;4m0.87it/s[0m  
        [0.5775, 0.4225],
        [0.5799, 0.4201],
        [0.5804, 0.4196],
        [0.5813, 0.4187],
        [0.5780, 0.4220],
        [0.5763, 0.4237],
        [0.5808, 0.4192],
        [0.5805, 0.4195],
[2K        [0.5822, 0.4178]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m[38;5;237mâ”â”â”â”[0m 180/203 [2m0:03:18 â€¢ 0:00:27[0m [2;4m0.87it/s[0m  
[2KClass label: CleanAndJerkâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”[0m 180/203 [2m0:03:18 â€¢ 0:00:27[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5912, 0.4088]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 180/203 [2m0:03:18 â€¢ 0:00:27[0m [2;4m0.87it/s[0m  
[2KStep 0 - TTA Loss: 0.4114â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”[0m 180/203 [2m0:03:18 â€¢ 0:00:27[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5933, 0.4067]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 180/203 [2m0:03:18 â€¢ 0:00:27[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5959, 0.4041]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 180/203 [2m0:03:18 â€¢ 0:00:27[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5988, 0.4012]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 180/203 [2m0:03:18 â€¢ 0:00:27[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6020, 0.3980]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 180/203 [2m0:03:18 â€¢ 0:00:27[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6054, 0.3946]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 180/203 [2m0:03:18 â€¢ 0:00:27[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6089, 0.3911]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 180/203 [2m0:03:18 â€¢ 0:00:27[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6127, 0.3873]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 180/203 [2m0:03:18 â€¢ 0:00:27[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6164, 0.3836]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 180/203 [2m0:03:18 â€¢ 0:00:27[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6202, 0.3798]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 180/203 [2m0:03:18 â€¢ 0:00:27[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6240, 0.3760]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 180/203 [2m0:03:18 â€¢ 0:00:27[0m [2;4m0.87it/s[0m  
[2KStep 10 - TTA Loss: 0.4159â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”[0m 180/203 [2m0:03:18 â€¢ 0:00:27[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6276, 0.3724]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 180/203 [2m0:03:18 â€¢ 0:00:27[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6312, 0.3688]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 180/203 [2m0:03:18 â€¢ 0:00:27[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6346, 0.3654]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 180/203 [2m0:03:18 â€¢ 0:00:27[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6379, 0.3621]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 180/203 [2m0:03:18 â€¢ 0:00:27[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6410, 0.3590]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 180/203 [2m0:03:18 â€¢ 0:00:27[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6439, 0.3561]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 180/203 [2m0:03:18 â€¢ 0:00:27[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6465, 0.3535]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 180/203 [2m0:03:18 â€¢ 0:00:27[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6489, 0.3511]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 180/203 [2m0:03:18 â€¢ 0:00:27[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6510, 0.3490]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 180/203 [2m0:03:18 â€¢ 0:00:27[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6527, 0.3473]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 180/203 [2m0:03:18 â€¢ 0:00:27[0m [2;4m0.87it/s[0m  
[2KStep 20 - TTA Loss: 0.3959â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”[0m 180/203 [2m0:03:18 â€¢ 0:00:27[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6543, 0.3457]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 180/203 [2m0:03:18 â€¢ 0:00:27[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6555, 0.3445]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 180/203 [2m0:03:18 â€¢ 0:00:27[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6566, 0.3434]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 180/203 [2m0:03:18 â€¢ 0:00:27[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6574, 0.3426]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 180/203 [2m0:03:18 â€¢ 0:00:27[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6580, 0.3420]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 180/203 [2m0:03:18 â€¢ 0:00:27[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6584, 0.3416]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 180/203 [2m0:03:18 â€¢ 0:00:27[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6587, 0.3413]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 180/203 [2m0:03:18 â€¢ 0:00:27[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6589, 0.3411]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 180/203 [2m0:03:18 â€¢ 0:00:27[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6589, 0.3411]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 180/203 [2m0:03:18 â€¢ 0:00:27[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5822, 0.4178]], device='cuda:0')8;5;237mâ•º[0m[38;5;237mâ”â”â”â”[0m 180/203 [2m0:03:18 â€¢ 0:00:27[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6661, 0.3339],â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”[0m 180/203 [2m0:03:18 â€¢ 0:00:27[0m [2;4m0.87it/s[0m  
        [0.6263, 0.3737],
        [0.6590, 0.3410],
        [0.6821, 0.3179],
        [0.6514, 0.3486],
        [0.6617, 0.3383],
        [0.6171, 0.3829],
        [0.6843, 0.3157],
        [0.6938, 0.3062],
[2K        [0.6976, 0.3024]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”[0m 180/203 [2m0:03:18 â€¢ 0:00:27[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5811, 0.4189],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”[0m 181/203 [2m0:03:19 â€¢ 0:00:26[0m [2;4m0.87it/s[0m  
        [0.5774, 0.4226],
        [0.5797, 0.4203],
        [0.5802, 0.4198],
        [0.5811, 0.4189],
        [0.5778, 0.4222],
        [0.5760, 0.4240],
        [0.5807, 0.4193],
        [0.5803, 0.4197],
[2K        [0.5821, 0.4179]], device='cuda:0', grad_fn=<SoftmaxBackward0>)224mâ•¸[0m[38;5;237mâ”â”â”â”[0m 181/203 [2m0:03:19 â€¢ 0:00:26[0m [2;4m0.87it/s[0m  
[2KClass label: GolfSwingâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”[0m 181/203 [2m0:03:19 â€¢ 0:00:26[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6523, 0.3477]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 181/203 [2m0:03:19 â€¢ 0:00:26[0m [2;4m0.87it/s[0m  
[2KStep 0 - TTA Loss: 0.3698â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”[0m 181/203 [2m0:03:19 â€¢ 0:00:26[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6523, 0.3477]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 181/203 [2m0:03:19 â€¢ 0:00:26[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6523, 0.3477]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 181/203 [2m0:03:19 â€¢ 0:00:26[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6524, 0.3476]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 181/203 [2m0:03:19 â€¢ 0:00:26[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6525, 0.3475]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 181/203 [2m0:03:19 â€¢ 0:00:26[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6526, 0.3474]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 181/203 [2m0:03:19 â€¢ 0:00:26[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6529, 0.3471]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 181/203 [2m0:03:19 â€¢ 0:00:26[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6532, 0.3468]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 181/203 [2m0:03:19 â€¢ 0:00:26[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6538, 0.3462]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 181/203 [2m0:03:19 â€¢ 0:00:26[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6544, 0.3456]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 181/203 [2m0:03:19 â€¢ 0:00:26[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6553, 0.3447]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 181/203 [2m0:03:19 â€¢ 0:00:26[0m [2;4m0.87it/s[0m  
[2KStep 10 - TTA Loss: 0.3668â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”[0m 181/203 [2m0:03:19 â€¢ 0:00:26[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6563, 0.3437]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 181/203 [2m0:03:19 â€¢ 0:00:26[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6576, 0.3424]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 181/203 [2m0:03:19 â€¢ 0:00:26[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6590, 0.3410]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 181/203 [2m0:03:19 â€¢ 0:00:26[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6608, 0.3392]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 181/203 [2m0:03:19 â€¢ 0:00:26[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6627, 0.3373]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 181/203 [2m0:03:19 â€¢ 0:00:26[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6649, 0.3351]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 181/203 [2m0:03:19 â€¢ 0:00:26[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6674, 0.3326]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 181/203 [2m0:03:19 â€¢ 0:00:26[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6701, 0.3299]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 181/203 [2m0:03:19 â€¢ 0:00:26[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6730, 0.3270]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 181/203 [2m0:03:19 â€¢ 0:00:26[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6762, 0.3238]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 181/203 [2m0:03:19 â€¢ 0:00:26[0m [2;4m0.87it/s[0m  
[2KStep 20 - TTA Loss: 0.3518â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”[0m 181/203 [2m0:03:19 â€¢ 0:00:26[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6797, 0.3203]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 181/203 [2m0:03:19 â€¢ 0:00:26[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6834, 0.3166]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 181/203 [2m0:03:19 â€¢ 0:00:26[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6872, 0.3128]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 181/203 [2m0:03:19 â€¢ 0:00:26[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6913, 0.3087]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 181/203 [2m0:03:19 â€¢ 0:00:26[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6956, 0.3044]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 181/203 [2m0:03:19 â€¢ 0:00:26[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.7000, 0.3000]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 181/203 [2m0:03:19 â€¢ 0:00:26[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.7046, 0.2954]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 181/203 [2m0:03:19 â€¢ 0:00:26[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.7093, 0.2907]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 181/203 [2m0:03:19 â€¢ 0:00:26[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.7141, 0.2859]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 181/203 [2m0:03:19 â€¢ 0:00:26[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5835, 0.4165]], device='cuda:0')8;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”[0m 181/203 [2m0:03:19 â€¢ 0:00:26[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6922, 0.3078],â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”[0m 181/203 [2m0:03:19 â€¢ 0:00:26[0m [2;4m0.87it/s[0m  
        [0.6358, 0.3642],
        [0.6402, 0.3598],
        [0.7192, 0.2808],
        [0.6609, 0.3391],
        [0.6627, 0.3373],
        [0.6143, 0.3857],
        [0.6933, 0.3067],
        [0.7158, 0.2842],
[2K        [0.7102, 0.2898]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”[0m 181/203 [2m0:03:19 â€¢ 0:00:26[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5812, 0.4188],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”[0m 182/203 [2m0:03:20 â€¢ 0:00:25[0m [2;4m0.87it/s[0m  
        [0.5776, 0.4224],
        [0.5797, 0.4203],
        [0.5804, 0.4196],
        [0.5812, 0.4188],
        [0.5779, 0.4221],
        [0.5761, 0.4239],
        [0.5808, 0.4192],
        [0.5805, 0.4195],
[2K        [0.5822, 0.4178]], device='cuda:0', grad_fn=<SoftmaxBackward0>)224mâ•¸[0m[38;5;237mâ”â”â”â”[0m 182/203 [2m0:03:20 â€¢ 0:00:25[0m [2;4m0.87it/s[0m  
[2KClass label: CleanAndJerkâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”[0m 182/203 [2m0:03:20 â€¢ 0:00:25[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5912, 0.4088]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 182/203 [2m0:03:20 â€¢ 0:00:25[0m [2;4m0.87it/s[0m  
[2KStep 0 - TTA Loss: 0.7301â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”[0m 182/203 [2m0:03:20 â€¢ 0:00:25[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5942, 0.4058]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 182/203 [2m0:03:20 â€¢ 0:00:25[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5971, 0.4029]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 182/203 [2m0:03:20 â€¢ 0:00:25[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5998, 0.4002]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 182/203 [2m0:03:20 â€¢ 0:00:25[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6023, 0.3977]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 182/203 [2m0:03:20 â€¢ 0:00:25[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6047, 0.3953]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 182/203 [2m0:03:20 â€¢ 0:00:25[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6070, 0.3930]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 182/203 [2m0:03:20 â€¢ 0:00:25[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6092, 0.3908]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 182/203 [2m0:03:20 â€¢ 0:00:25[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6112, 0.3888]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 182/203 [2m0:03:20 â€¢ 0:00:25[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6132, 0.3868]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 182/203 [2m0:03:20 â€¢ 0:00:25[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6149, 0.3851]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 182/203 [2m0:03:20 â€¢ 0:00:25[0m [2;4m0.87it/s[0m  
[2KStep 10 - TTA Loss: 0.6731â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”[0m 182/203 [2m0:03:20 â€¢ 0:00:25[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6166, 0.3834]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 182/203 [2m0:03:20 â€¢ 0:00:25[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6182, 0.3818]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 182/203 [2m0:03:20 â€¢ 0:00:25[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6196, 0.3804]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 182/203 [2m0:03:20 â€¢ 0:00:25[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6210, 0.3790]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 182/203 [2m0:03:20 â€¢ 0:00:25[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6222, 0.3778]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 182/203 [2m0:03:20 â€¢ 0:00:25[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6234, 0.3766]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 182/203 [2m0:03:20 â€¢ 0:00:25[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6244, 0.3756]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 182/203 [2m0:03:20 â€¢ 0:00:25[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6252, 0.3748]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 182/203 [2m0:03:20 â€¢ 0:00:25[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6260, 0.3740]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 182/203 [2m0:03:20 â€¢ 0:00:25[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6267, 0.3733]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 182/203 [2m0:03:20 â€¢ 0:00:25[0m [2;4m0.87it/s[0m  
[2KStep 20 - TTA Loss: 0.7258â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”[0m 182/203 [2m0:03:20 â€¢ 0:00:25[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6273, 0.3727]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 182/203 [2m0:03:20 â€¢ 0:00:25[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6277, 0.3723]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 182/203 [2m0:03:20 â€¢ 0:00:25[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6281, 0.3719]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 182/203 [2m0:03:20 â€¢ 0:00:25[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6284, 0.3716]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 182/203 [2m0:03:20 â€¢ 0:00:25[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6286, 0.3714]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 182/203 [2m0:03:20 â€¢ 0:00:25[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6288, 0.3712]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 182/203 [2m0:03:20 â€¢ 0:00:25[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6289, 0.3711]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 182/203 [2m0:03:20 â€¢ 0:00:25[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6289, 0.3711]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 182/203 [2m0:03:20 â€¢ 0:00:25[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6290, 0.3710]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 182/203 [2m0:03:20 â€¢ 0:00:25[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5809, 0.4191]], device='cuda:0')8;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”[0m 182/203 [2m0:03:20 â€¢ 0:00:25[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6719, 0.3281],â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”[0m 182/203 [2m0:03:20 â€¢ 0:00:25[0m [2;4m0.87it/s[0m  
        [0.6241, 0.3759],
        [0.6290, 0.3710],
        [0.6926, 0.3074],
        [0.6440, 0.3560],
        [0.6488, 0.3512],
        [0.6006, 0.3994],
        [0.6839, 0.3161],
        [0.6991, 0.3009],
[2K        [0.6926, 0.3074]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”[0m 182/203 [2m0:03:20 â€¢ 0:00:25[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5812, 0.4188],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”[0m 183/203 [2m0:03:21 â€¢ 0:00:23[0m [2;4m0.88it/s[0m   
        [0.5775, 0.4225],
        [0.5796, 0.4204],
        [0.5804, 0.4196],
        [0.5811, 0.4189],
        [0.5778, 0.4222],
        [0.5761, 0.4239],
        [0.5808, 0.4192],
        [0.5804, 0.4196],
[2K        [0.5821, 0.4179]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â•º[0m[38;5;237mâ”â”â”[0m 183/203 [2m0:03:21 â€¢ 0:00:23[0m [2;4m0.88it/s[0m  
[2KClass label: BaseballPitchâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”[0m 183/203 [2m0:03:21 â€¢ 0:00:23[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6382, 0.3618]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 183/203 [2m0:03:21 â€¢ 0:00:23[0m [2;4m0.88it/s[0m  
[2KStep 0 - TTA Loss: 0.4876â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”[0m 183/203 [2m0:03:21 â€¢ 0:00:23[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6382, 0.3618]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 183/203 [2m0:03:21 â€¢ 0:00:23[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6382, 0.3618]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 183/203 [2m0:03:21 â€¢ 0:00:23[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6383, 0.3617]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 183/203 [2m0:03:21 â€¢ 0:00:23[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6383, 0.3617]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 183/203 [2m0:03:21 â€¢ 0:00:23[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6385, 0.3615]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 183/203 [2m0:03:21 â€¢ 0:00:23[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6387, 0.3613]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 183/203 [2m0:03:21 â€¢ 0:00:23[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6391, 0.3609]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 183/203 [2m0:03:21 â€¢ 0:00:23[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6397, 0.3603]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 183/203 [2m0:03:21 â€¢ 0:00:23[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6405, 0.3595]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 183/203 [2m0:03:21 â€¢ 0:00:23[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6415, 0.3585]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 183/203 [2m0:03:21 â€¢ 0:00:23[0m [2;4m0.88it/s[0m  
[2KStep 10 - TTA Loss: 0.5021â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”[0m 183/203 [2m0:03:21 â€¢ 0:00:23[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6427, 0.3573]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 183/203 [2m0:03:21 â€¢ 0:00:23[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6443, 0.3557]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 183/203 [2m0:03:21 â€¢ 0:00:23[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6462, 0.3538]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 183/203 [2m0:03:21 â€¢ 0:00:23[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6484, 0.3516]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 183/203 [2m0:03:21 â€¢ 0:00:23[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6509, 0.3491]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 183/203 [2m0:03:21 â€¢ 0:00:23[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6538, 0.3462]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 183/203 [2m0:03:21 â€¢ 0:00:23[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6571, 0.3429]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 183/203 [2m0:03:21 â€¢ 0:00:23[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6607, 0.3393]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 183/203 [2m0:03:21 â€¢ 0:00:23[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6647, 0.3353]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 183/203 [2m0:03:21 â€¢ 0:00:23[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6690, 0.3310]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 183/203 [2m0:03:21 â€¢ 0:00:23[0m [2;4m0.88it/s[0m  
[2KStep 20 - TTA Loss: 0.4603â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”[0m 183/203 [2m0:03:21 â€¢ 0:00:23[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6737, 0.3263]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 183/203 [2m0:03:21 â€¢ 0:00:23[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6788, 0.3212]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 183/203 [2m0:03:21 â€¢ 0:00:23[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6842, 0.3158]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 183/203 [2m0:03:21 â€¢ 0:00:23[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6900, 0.3100]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 183/203 [2m0:03:21 â€¢ 0:00:23[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6960, 0.3040]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 183/203 [2m0:03:21 â€¢ 0:00:23[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.7023, 0.2977]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 183/203 [2m0:03:21 â€¢ 0:00:23[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.7088, 0.2912]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 183/203 [2m0:03:21 â€¢ 0:00:23[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.7155, 0.2845]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 183/203 [2m0:03:21 â€¢ 0:00:23[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.7223, 0.2777]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 183/203 [2m0:03:21 â€¢ 0:00:23[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.5857, 0.4143]], device='cuda:0')38;5;237mâ•º[0m[38;5;237mâ”â”â”[0m 183/203 [2m0:03:21 â€¢ 0:00:23[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.7294, 0.2706],â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”[0m 183/203 [2m0:03:21 â€¢ 0:00:23[0m [2;4m0.88it/s[0m  
        [0.6440, 0.3560],
        [0.6368, 0.3632],
        [0.7139, 0.2861],
        [0.6741, 0.3259],
        [0.6775, 0.3225],
        [0.6250, 0.3750],
        [0.7093, 0.2907],
        [0.7219, 0.2781],
[2K        [0.7234, 0.2766]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”[0m 183/203 [2m0:03:21 â€¢ 0:00:23[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.5818, 0.4182],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”[0m 184/203 [2m0:03:22 â€¢ 0:00:22[0m [2;4m0.88it/s[0m  
        [0.5779, 0.4221],
        [0.5798, 0.4202],
        [0.5809, 0.4191],
        [0.5814, 0.4186],
        [0.5781, 0.4219],
        [0.5764, 0.4236],
        [0.5813, 0.4187],
        [0.5809, 0.4191],
[2K        [0.5824, 0.4176]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â•º[0m[38;5;237mâ”â”â”[0m 184/203 [2m0:03:22 â€¢ 0:00:22[0m [2;4m0.88it/s[0m  
[2KClass label: SoccerPenaltyâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”[0m 184/203 [2m0:03:22 â€¢ 0:00:22[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6568, 0.3432]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 184/203 [2m0:03:22 â€¢ 0:00:22[0m [2;4m0.88it/s[0m  
[2KStep 0 - TTA Loss: 0.5380â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”[0m 184/203 [2m0:03:22 â€¢ 0:00:22[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6610, 0.3390]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 184/203 [2m0:03:22 â€¢ 0:00:22[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6651, 0.3349]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 184/203 [2m0:03:22 â€¢ 0:00:22[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6692, 0.3308]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 184/203 [2m0:03:22 â€¢ 0:00:22[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6732, 0.3268]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 184/203 [2m0:03:22 â€¢ 0:00:22[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6772, 0.3228]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 184/203 [2m0:03:22 â€¢ 0:00:22[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6811, 0.3189]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 184/203 [2m0:03:22 â€¢ 0:00:22[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6849, 0.3151]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 184/203 [2m0:03:22 â€¢ 0:00:22[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6886, 0.3114]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 184/203 [2m0:03:22 â€¢ 0:00:22[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6922, 0.3078]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 184/203 [2m0:03:22 â€¢ 0:00:22[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6956, 0.3044]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 184/203 [2m0:03:22 â€¢ 0:00:22[0m [2;4m0.88it/s[0m  
[2KStep 10 - TTA Loss: 0.5135â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”[0m 184/203 [2m0:03:22 â€¢ 0:00:22[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6988, 0.3012]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 184/203 [2m0:03:22 â€¢ 0:00:22[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.7019, 0.2981]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 184/203 [2m0:03:22 â€¢ 0:00:22[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.7047, 0.2953]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 184/203 [2m0:03:22 â€¢ 0:00:22[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.7073, 0.2927]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 184/203 [2m0:03:22 â€¢ 0:00:22[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.7096, 0.2904]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 184/203 [2m0:03:22 â€¢ 0:00:22[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.7118, 0.2882]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 184/203 [2m0:03:22 â€¢ 0:00:22[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.7138, 0.2862]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 184/203 [2m0:03:22 â€¢ 0:00:22[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.7156, 0.2844]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 184/203 [2m0:03:22 â€¢ 0:00:22[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.7172, 0.2828]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 184/203 [2m0:03:22 â€¢ 0:00:22[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.7187, 0.2813]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 184/203 [2m0:03:22 â€¢ 0:00:22[0m [2;4m0.88it/s[0m  
[2KStep 20 - TTA Loss: 0.5669â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”[0m 184/203 [2m0:03:22 â€¢ 0:00:22[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.7199, 0.2801]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 184/203 [2m0:03:22 â€¢ 0:00:22[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.7209, 0.2791]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 184/203 [2m0:03:22 â€¢ 0:00:22[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.7217, 0.2783]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 184/203 [2m0:03:22 â€¢ 0:00:22[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.7224, 0.2776]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 184/203 [2m0:03:22 â€¢ 0:00:22[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.7228, 0.2772]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 184/203 [2m0:03:22 â€¢ 0:00:22[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.7232, 0.2768]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 184/203 [2m0:03:22 â€¢ 0:00:22[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.7234, 0.2766]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 184/203 [2m0:03:22 â€¢ 0:00:22[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.7235, 0.2765]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 184/203 [2m0:03:22 â€¢ 0:00:22[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.7236, 0.2764]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 184/203 [2m0:03:22 â€¢ 0:00:22[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.5842, 0.4158]], device='cuda:0')38;5;237mâ•º[0m[38;5;237mâ”â”â”[0m 184/203 [2m0:03:22 â€¢ 0:00:22[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.7042, 0.2958],â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”[0m 184/203 [2m0:03:22 â€¢ 0:00:22[0m [2;4m0.88it/s[0m  
        [0.6404, 0.3596],
        [0.6231, 0.3769],
        [0.6977, 0.3023],
        [0.6632, 0.3368],
        [0.6668, 0.3332],
        [0.6170, 0.3830],
        [0.7237, 0.2763],
        [0.7090, 0.2910],
[2K        [0.7075, 0.2925]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”[0m 184/203 [2m0:03:22 â€¢ 0:00:22[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.5819, 0.4181],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”[0m 185/203 [2m0:03:23 â€¢ 0:00:21[0m [2;4m0.88it/s[0m  
        [0.5781, 0.4219],
        [0.5798, 0.4202],
        [0.5810, 0.4190],
        [0.5815, 0.4185],
        [0.5781, 0.4219],
        [0.5764, 0.4236],
        [0.5814, 0.4186],
        [0.5810, 0.4190],
[2K        [0.5824, 0.4176]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â•º[0m[38;5;237mâ”â”â”[0m 185/203 [2m0:03:23 â€¢ 0:00:21[0m [2;4m0.88it/s[0m  
[2KClass label: Billiardsâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”[0m 185/203 [2m0:03:23 â€¢ 0:00:21[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6004, 0.3996]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 185/203 [2m0:03:23 â€¢ 0:00:21[0m [2;4m0.88it/s[0m  
[2KStep 0 - TTA Loss: 0.5682â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”[0m 185/203 [2m0:03:23 â€¢ 0:00:21[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6004, 0.3996]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 185/203 [2m0:03:23 â€¢ 0:00:21[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6004, 0.3996]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 185/203 [2m0:03:23 â€¢ 0:00:21[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6004, 0.3996]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 185/203 [2m0:03:23 â€¢ 0:00:21[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6005, 0.3995]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 185/203 [2m0:03:23 â€¢ 0:00:21[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6007, 0.3993]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 185/203 [2m0:03:23 â€¢ 0:00:21[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6010, 0.3990]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 185/203 [2m0:03:23 â€¢ 0:00:21[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6014, 0.3986]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 185/203 [2m0:03:23 â€¢ 0:00:21[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6020, 0.3980]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 185/203 [2m0:03:23 â€¢ 0:00:21[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6028, 0.3972]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 185/203 [2m0:03:23 â€¢ 0:00:21[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6039, 0.3961]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 185/203 [2m0:03:23 â€¢ 0:00:21[0m [2;4m0.88it/s[0m  
[2KStep 10 - TTA Loss: 0.5431â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”[0m 185/203 [2m0:03:23 â€¢ 0:00:21[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6052, 0.3948]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 185/203 [2m0:03:23 â€¢ 0:00:21[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6068, 0.3932]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 185/203 [2m0:03:23 â€¢ 0:00:21[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6086, 0.3914]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 185/203 [2m0:03:23 â€¢ 0:00:21[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6107, 0.3893]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 185/203 [2m0:03:23 â€¢ 0:00:21[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6132, 0.3868]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 185/203 [2m0:03:23 â€¢ 0:00:21[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6159, 0.3841]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 185/203 [2m0:03:23 â€¢ 0:00:21[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6190, 0.3810]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 185/203 [2m0:03:23 â€¢ 0:00:21[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6224, 0.3776]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 185/203 [2m0:03:23 â€¢ 0:00:21[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6261, 0.3739]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 185/203 [2m0:03:23 â€¢ 0:00:21[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6303, 0.3697]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 185/203 [2m0:03:23 â€¢ 0:00:21[0m [2;4m0.88it/s[0m  
[2KStep 20 - TTA Loss: 0.5251â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”[0m 185/203 [2m0:03:23 â€¢ 0:00:21[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6347, 0.3653]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 185/203 [2m0:03:23 â€¢ 0:00:21[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6395, 0.3605]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 185/203 [2m0:03:23 â€¢ 0:00:21[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6446, 0.3554]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 185/203 [2m0:03:23 â€¢ 0:00:21[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6500, 0.3500]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 185/203 [2m0:03:23 â€¢ 0:00:21[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6556, 0.3444]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 185/203 [2m0:03:23 â€¢ 0:00:21[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6615, 0.3385]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 185/203 [2m0:03:23 â€¢ 0:00:21[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6677, 0.3323]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 185/203 [2m0:03:23 â€¢ 0:00:21[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6741, 0.3259]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 185/203 [2m0:03:23 â€¢ 0:00:21[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6807, 0.3193]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 185/203 [2m0:03:23 â€¢ 0:00:21[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.5821, 0.4179]], device='cuda:0')38;5;237mâ•º[0m[38;5;237mâ”â”â”[0m 185/203 [2m0:03:23 â€¢ 0:00:21[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6674, 0.3326],â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”[0m 185/203 [2m0:03:23 â€¢ 0:00:21[0m [2;4m0.88it/s[0m  
        [0.6875, 0.3125],
        [0.6154, 0.3846],
        [0.6795, 0.3205],
        [0.6382, 0.3618],
        [0.6489, 0.3511],
        [0.6000, 0.4000],
        [0.6986, 0.3014],
        [0.6995, 0.3005],
[2K        [0.6870, 0.3130]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”[0m 185/203 [2m0:03:23 â€¢ 0:00:21[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.5821, 0.4179],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”[0m 186/203 [2m0:03:24 â€¢ 0:00:20[0m [2;4m0.88it/s[0m  
        [0.5788, 0.4212],
        [0.5800, 0.4200],
        [0.5813, 0.4187],
        [0.5816, 0.4184],
        [0.5782, 0.4218],
        [0.5766, 0.4234],
        [0.5816, 0.4184],
        [0.5813, 0.4187],
[2K        [0.5826, 0.4174]], device='cuda:0', grad_fn=<SoftmaxBackward0>);224mâ•¸[0m[38;5;237mâ”â”â”[0m 186/203 [2m0:03:24 â€¢ 0:00:20[0m [2;4m0.88it/s[0m  
[2KClass label: SoccerPenaltyâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”[0m 186/203 [2m0:03:24 â€¢ 0:00:20[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6570, 0.3430]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 186/203 [2m0:03:24 â€¢ 0:00:20[0m [2;4m0.88it/s[0m  
[2KStep 0 - TTA Loss: 0.6888â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”[0m 186/203 [2m0:03:24 â€¢ 0:00:20[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6598, 0.3402]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 186/203 [2m0:03:24 â€¢ 0:00:20[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6629, 0.3371]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 186/203 [2m0:03:24 â€¢ 0:00:20[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6660, 0.3340]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 186/203 [2m0:03:24 â€¢ 0:00:20[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6691, 0.3309]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 186/203 [2m0:03:24 â€¢ 0:00:20[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6726, 0.3274]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 186/203 [2m0:03:24 â€¢ 0:00:20[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6759, 0.3241]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 186/203 [2m0:03:24 â€¢ 0:00:20[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6791, 0.3209]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 186/203 [2m0:03:24 â€¢ 0:00:20[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6825, 0.3175]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 186/203 [2m0:03:24 â€¢ 0:00:20[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6858, 0.3142]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 186/203 [2m0:03:24 â€¢ 0:00:20[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6891, 0.3109]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 186/203 [2m0:03:24 â€¢ 0:00:20[0m [2;4m0.88it/s[0m  
[2KStep 10 - TTA Loss: 0.6721â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”[0m 186/203 [2m0:03:24 â€¢ 0:00:20[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6921, 0.3079]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 186/203 [2m0:03:24 â€¢ 0:00:20[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6952, 0.3048]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 186/203 [2m0:03:24 â€¢ 0:00:20[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6981, 0.3019]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 186/203 [2m0:03:24 â€¢ 0:00:20[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.7009, 0.2991]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 186/203 [2m0:03:24 â€¢ 0:00:20[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.7036, 0.2964]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 186/203 [2m0:03:24 â€¢ 0:00:20[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.7060, 0.2940]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 186/203 [2m0:03:24 â€¢ 0:00:20[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.7081, 0.2919]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 186/203 [2m0:03:24 â€¢ 0:00:20[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.7101, 0.2899]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 186/203 [2m0:03:24 â€¢ 0:00:20[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.7119, 0.2881]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 186/203 [2m0:03:24 â€¢ 0:00:20[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.7134, 0.2866]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 186/203 [2m0:03:24 â€¢ 0:00:20[0m [2;4m0.88it/s[0m  
[2KStep 20 - TTA Loss: 0.6298â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”[0m 186/203 [2m0:03:24 â€¢ 0:00:20[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.7147, 0.2853]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 186/203 [2m0:03:24 â€¢ 0:00:20[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.7159, 0.2841]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 186/203 [2m0:03:24 â€¢ 0:00:20[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.7169, 0.2831]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 186/203 [2m0:03:24 â€¢ 0:00:20[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.7176, 0.2824]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 186/203 [2m0:03:24 â€¢ 0:00:20[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.7182, 0.2818]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 186/203 [2m0:03:24 â€¢ 0:00:20[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.7186, 0.2814]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 186/203 [2m0:03:24 â€¢ 0:00:20[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.7188, 0.2812]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 186/203 [2m0:03:24 â€¢ 0:00:20[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.7189, 0.2811]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 186/203 [2m0:03:24 â€¢ 0:00:20[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.7190, 0.2810]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 186/203 [2m0:03:24 â€¢ 0:00:20[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.5845, 0.4155]], device='cuda:0')38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”[0m 186/203 [2m0:03:24 â€¢ 0:00:20[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6692, 0.3308],â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”[0m 186/203 [2m0:03:24 â€¢ 0:00:20[0m [2;4m0.88it/s[0m  
        [0.6692, 0.3308],
        [0.6182, 0.3818],
        [0.6786, 0.3214],
        [0.6448, 0.3552],
        [0.6543, 0.3457],
        [0.6064, 0.3936],
        [0.7191, 0.2809],
        [0.6983, 0.3017],
[2K        [0.6893, 0.3107]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”[0m 186/203 [2m0:03:24 â€¢ 0:00:20[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.5823, 0.4177],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”[0m 187/203 [2m0:03:25 â€¢ 0:00:19[0m [2;4m0.88it/s[0m  
        [0.5793, 0.4207],
        [0.5802, 0.4198],
        [0.5816, 0.4184],
        [0.5817, 0.4183],
        [0.5784, 0.4216],
        [0.5767, 0.4233],
        [0.5819, 0.4181],
        [0.5816, 0.4184],
[2K        [0.5827, 0.4173]], device='cuda:0', grad_fn=<SoftmaxBackward0>);224mâ•¸[0m[38;5;237mâ”â”â”[0m 187/203 [2m0:03:25 â€¢ 0:00:19[0m [2;4m0.88it/s[0m  
[2KClass label: CleanAndJerkâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”[0m 187/203 [2m0:03:25 â€¢ 0:00:19[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.5905, 0.4095]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 187/203 [2m0:03:25 â€¢ 0:00:19[0m [2;4m0.88it/s[0m  
[2KStep 0 - TTA Loss: 0.7594â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”[0m 187/203 [2m0:03:25 â€¢ 0:00:19[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.5905, 0.4095]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 187/203 [2m0:03:25 â€¢ 0:00:19[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.5905, 0.4095]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 187/203 [2m0:03:25 â€¢ 0:00:19[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.5905, 0.4095]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 187/203 [2m0:03:25 â€¢ 0:00:19[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.5905, 0.4095]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 187/203 [2m0:03:25 â€¢ 0:00:19[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.5906, 0.4094]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 187/203 [2m0:03:25 â€¢ 0:00:19[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.5907, 0.4093]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 187/203 [2m0:03:25 â€¢ 0:00:19[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.5908, 0.4092]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 187/203 [2m0:03:25 â€¢ 0:00:19[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.5910, 0.4090]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 187/203 [2m0:03:25 â€¢ 0:00:19[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.5912, 0.4088]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 187/203 [2m0:03:25 â€¢ 0:00:19[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.5914, 0.4086]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 187/203 [2m0:03:25 â€¢ 0:00:19[0m [2;4m0.88it/s[0m  
[2KStep 10 - TTA Loss: 0.7584â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”[0m 187/203 [2m0:03:25 â€¢ 0:00:19[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.5917, 0.4083]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 187/203 [2m0:03:25 â€¢ 0:00:19[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.5920, 0.4080]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 187/203 [2m0:03:25 â€¢ 0:00:19[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.5923, 0.4077]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 187/203 [2m0:03:25 â€¢ 0:00:19[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.5927, 0.4073]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 187/203 [2m0:03:25 â€¢ 0:00:19[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.5931, 0.4069]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 187/203 [2m0:03:25 â€¢ 0:00:19[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.5935, 0.4065]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 187/203 [2m0:03:25 â€¢ 0:00:19[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.5940, 0.4060]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 187/203 [2m0:03:25 â€¢ 0:00:19[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.5945, 0.4055]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 187/203 [2m0:03:25 â€¢ 0:00:19[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.5950, 0.4050]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 187/203 [2m0:03:25 â€¢ 0:00:19[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.5955, 0.4045]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 187/203 [2m0:03:25 â€¢ 0:00:19[0m [2;4m0.88it/s[0m  
[2KStep 20 - TTA Loss: 0.7343â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”[0m 187/203 [2m0:03:25 â€¢ 0:00:19[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.5961, 0.4039]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 187/203 [2m0:03:25 â€¢ 0:00:19[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.5967, 0.4033]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 187/203 [2m0:03:25 â€¢ 0:00:19[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.5974, 0.4026]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 187/203 [2m0:03:25 â€¢ 0:00:19[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.5981, 0.4019]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 187/203 [2m0:03:25 â€¢ 0:00:19[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.5988, 0.4012]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 187/203 [2m0:03:25 â€¢ 0:00:19[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.5996, 0.4004]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 187/203 [2m0:03:25 â€¢ 0:00:19[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6003, 0.3997]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 187/203 [2m0:03:25 â€¢ 0:00:19[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6011, 0.3989]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 187/203 [2m0:03:25 â€¢ 0:00:19[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6019, 0.3981]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 187/203 [2m0:03:25 â€¢ 0:00:19[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.5806, 0.4194]], device='cuda:0')38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”[0m 187/203 [2m0:03:25 â€¢ 0:00:19[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.6457, 0.3543],â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”[0m 187/203 [2m0:03:25 â€¢ 0:00:19[0m [2;4m0.88it/s[0m  
        [0.6101, 0.3899],
        [0.6027, 0.3973],
        [0.6596, 0.3404],
        [0.6196, 0.3804],
        [0.6280, 0.3720],
        [0.5809, 0.4191],
        [0.6741, 0.3259],
        [0.6747, 0.3253],
[2K        [0.6687, 0.3313]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”[0m 187/203 [2m0:03:25 â€¢ 0:00:19[0m [2;4m0.88it/s[0m  
[2KAttention weights: tensor([[0.5823, 0.4177],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”[0m 188/203 [2m0:03:26 â€¢ 0:00:17[0m [2;4m0.89it/s[0m   
        [0.5793, 0.4207],
        [0.5802, 0.4198],
        [0.5816, 0.4184],
        [0.5818, 0.4182],
        [0.5784, 0.4216],
        [0.5767, 0.4233],
        [0.5819, 0.4181],
        [0.5816, 0.4184],
[2K        [0.5827, 0.4173]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ•º[0m[38;5;237mâ”â”[0m 188/203 [2m0:03:26 â€¢ 0:00:17[0m [2;4m0.89it/s[0m  
[2KClass label: TennisSwingâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”[0m 188/203 [2m0:03:26 â€¢ 0:00:17[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6662, 0.3338]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 188/203 [2m0:03:26 â€¢ 0:00:17[0m [2;4m0.89it/s[0m  
[2KStep 0 - TTA Loss: 0.6107â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”[0m 188/203 [2m0:03:26 â€¢ 0:00:17[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6669, 0.3331]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 188/203 [2m0:03:26 â€¢ 0:00:17[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6677, 0.3323]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 188/203 [2m0:03:26 â€¢ 0:00:17[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6688, 0.3312]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 188/203 [2m0:03:26 â€¢ 0:00:17[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6700, 0.3300]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 188/203 [2m0:03:26 â€¢ 0:00:17[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6714, 0.3286]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 188/203 [2m0:03:26 â€¢ 0:00:17[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6729, 0.3271]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 188/203 [2m0:03:26 â€¢ 0:00:17[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6745, 0.3255]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 188/203 [2m0:03:26 â€¢ 0:00:17[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6761, 0.3239]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 188/203 [2m0:03:26 â€¢ 0:00:17[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6778, 0.3222]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 188/203 [2m0:03:26 â€¢ 0:00:17[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6795, 0.3205]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 188/203 [2m0:03:26 â€¢ 0:00:17[0m [2;4m0.89it/s[0m  
[2KStep 10 - TTA Loss: 0.5997â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”[0m 188/203 [2m0:03:26 â€¢ 0:00:17[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6811, 0.3189]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 188/203 [2m0:03:26 â€¢ 0:00:17[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6828, 0.3172]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 188/203 [2m0:03:26 â€¢ 0:00:17[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6844, 0.3156]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 188/203 [2m0:03:26 â€¢ 0:00:17[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6859, 0.3141]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 188/203 [2m0:03:26 â€¢ 0:00:17[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6873, 0.3127]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 188/203 [2m0:03:26 â€¢ 0:00:17[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6887, 0.3113]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 188/203 [2m0:03:26 â€¢ 0:00:17[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6899, 0.3101]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 188/203 [2m0:03:26 â€¢ 0:00:17[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6910, 0.3090]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 188/203 [2m0:03:26 â€¢ 0:00:17[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6920, 0.3080]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 188/203 [2m0:03:26 â€¢ 0:00:17[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6928, 0.3072]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 188/203 [2m0:03:26 â€¢ 0:00:17[0m [2;4m0.89it/s[0m  
[2KStep 20 - TTA Loss: 0.6061â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”[0m 188/203 [2m0:03:26 â€¢ 0:00:17[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6935, 0.3065]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 188/203 [2m0:03:26 â€¢ 0:00:17[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6941, 0.3059]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 188/203 [2m0:03:26 â€¢ 0:00:17[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6946, 0.3054]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 188/203 [2m0:03:26 â€¢ 0:00:17[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6950, 0.3050]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 188/203 [2m0:03:26 â€¢ 0:00:17[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6953, 0.3047]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 188/203 [2m0:03:26 â€¢ 0:00:17[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6955, 0.3045]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 188/203 [2m0:03:26 â€¢ 0:00:17[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6957, 0.3043]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 188/203 [2m0:03:26 â€¢ 0:00:17[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6958, 0.3042]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 188/203 [2m0:03:26 â€¢ 0:00:17[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6958, 0.3042]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 188/203 [2m0:03:26 â€¢ 0:00:17[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.5827, 0.4173]], device='cuda:0')[38;5;237mâ•º[0m[38;5;237mâ”â”[0m 188/203 [2m0:03:26 â€¢ 0:00:17[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6545, 0.3455],â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”[0m 188/203 [2m0:03:26 â€¢ 0:00:17[0m [2;4m0.89it/s[0m  
        [0.6199, 0.3801],
        [0.6093, 0.3907],
        [0.6696, 0.3304],
        [0.6259, 0.3741],
        [0.6367, 0.3633],
        [0.5843, 0.4157],
        [0.6719, 0.3281],
        [0.6958, 0.3042],
[2K        [0.6764, 0.3236]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”[0m 188/203 [2m0:03:26 â€¢ 0:00:17[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.5821, 0.4179],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”[0m 189/203 [2m0:03:27 â€¢ 0:00:16[0m [2;4m0.89it/s[0m  
        [0.5792, 0.4208],
        [0.5802, 0.4198],
        [0.5815, 0.4185],
        [0.5817, 0.4183],
        [0.5783, 0.4217],
        [0.5766, 0.4234],
        [0.5818, 0.4182],
        [0.5815, 0.4185],
[2K        [0.5826, 0.4174]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ•º[0m[38;5;237mâ”â”[0m 189/203 [2m0:03:27 â€¢ 0:00:16[0m [2;4m0.89it/s[0m  
[2KClass label: GolfSwingâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”[0m 189/203 [2m0:03:27 â€¢ 0:00:16[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6543, 0.3457]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 189/203 [2m0:03:27 â€¢ 0:00:16[0m [2;4m0.89it/s[0m  
[2KStep 0 - TTA Loss: 0.8833â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”[0m 189/203 [2m0:03:27 â€¢ 0:00:16[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6543, 0.3457]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 189/203 [2m0:03:27 â€¢ 0:00:16[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6543, 0.3457]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 189/203 [2m0:03:27 â€¢ 0:00:16[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6543, 0.3457]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 189/203 [2m0:03:27 â€¢ 0:00:16[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6544, 0.3456]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 189/203 [2m0:03:27 â€¢ 0:00:16[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6544, 0.3456]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 189/203 [2m0:03:27 â€¢ 0:00:16[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6545, 0.3455]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 189/203 [2m0:03:27 â€¢ 0:00:16[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6546, 0.3454]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 189/203 [2m0:03:27 â€¢ 0:00:16[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6548, 0.3452]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 189/203 [2m0:03:27 â€¢ 0:00:16[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6549, 0.3451]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 189/203 [2m0:03:27 â€¢ 0:00:16[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6551, 0.3449]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 189/203 [2m0:03:27 â€¢ 0:00:16[0m [2;4m0.89it/s[0m  
[2KStep 10 - TTA Loss: 0.8844â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”[0m 189/203 [2m0:03:27 â€¢ 0:00:16[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6553, 0.3447]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 189/203 [2m0:03:27 â€¢ 0:00:16[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6555, 0.3445]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 189/203 [2m0:03:27 â€¢ 0:00:16[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6557, 0.3443]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 189/203 [2m0:03:27 â€¢ 0:00:16[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6560, 0.3440]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 189/203 [2m0:03:27 â€¢ 0:00:16[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6562, 0.3438]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 189/203 [2m0:03:27 â€¢ 0:00:16[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6565, 0.3435]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 189/203 [2m0:03:27 â€¢ 0:00:16[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6568, 0.3432]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 189/203 [2m0:03:27 â€¢ 0:00:16[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6571, 0.3429]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 189/203 [2m0:03:27 â€¢ 0:00:16[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6573, 0.3427]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 189/203 [2m0:03:27 â€¢ 0:00:16[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6576, 0.3424]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 189/203 [2m0:03:27 â€¢ 0:00:16[0m [2;4m0.89it/s[0m  
[2KStep 20 - TTA Loss: 0.8852â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”[0m 189/203 [2m0:03:27 â€¢ 0:00:16[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6579, 0.3421]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 189/203 [2m0:03:27 â€¢ 0:00:16[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6582, 0.3418]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 189/203 [2m0:03:27 â€¢ 0:00:16[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6585, 0.3415]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 189/203 [2m0:03:27 â€¢ 0:00:16[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6588, 0.3412]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 189/203 [2m0:03:27 â€¢ 0:00:16[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6591, 0.3409]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 189/203 [2m0:03:27 â€¢ 0:00:16[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6593, 0.3407]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 189/203 [2m0:03:27 â€¢ 0:00:16[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6596, 0.3404]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 189/203 [2m0:03:27 â€¢ 0:00:16[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6598, 0.3402]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 189/203 [2m0:03:27 â€¢ 0:00:16[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6600, 0.3400]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 189/203 [2m0:03:27 â€¢ 0:00:16[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.5822, 0.4178]], device='cuda:0')[38;5;237mâ•º[0m[38;5;237mâ”â”[0m 189/203 [2m0:03:27 â€¢ 0:00:16[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6462, 0.3538],â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”[0m 189/203 [2m0:03:27 â€¢ 0:00:16[0m [2;4m0.89it/s[0m  
        [0.6089, 0.3911],
        [0.5957, 0.4043],
        [0.6603, 0.3397],
        [0.6147, 0.3853],
        [0.6243, 0.3757],
        [0.5772, 0.4228],
        [0.6645, 0.3355],
        [0.6765, 0.3235],
[2K        [0.6665, 0.3335]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”[0m 189/203 [2m0:03:27 â€¢ 0:00:16[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.5821, 0.4179],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”[0m 190/203 [2m0:03:29 â€¢ 0:00:15[0m [2;4m0.89it/s[0m  
        [0.5792, 0.4208],
        [0.5801, 0.4199],
        [0.5815, 0.4185],
        [0.5816, 0.4184],
        [0.5783, 0.4217],
        [0.5766, 0.4234],
        [0.5818, 0.4182],
        [0.5815, 0.4185],
[2K        [0.5826, 0.4174]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ•º[0m[38;5;237mâ”â”[0m 190/203 [2m0:03:29 â€¢ 0:00:15[0m [2;4m0.89it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”[0m 190/203 [2m0:03:29 â€¢ 0:00:15[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.5703, 0.4297]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 190/203 [2m0:03:29 â€¢ 0:00:15[0m [2;4m0.89it/s[0m  
[2KStep 0 - TTA Loss: 0.7645â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”[0m 190/203 [2m0:03:29 â€¢ 0:00:15[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.5707, 0.4293]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 190/203 [2m0:03:29 â€¢ 0:00:15[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.5714, 0.4286]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 190/203 [2m0:03:29 â€¢ 0:00:15[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.5723, 0.4277]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 190/203 [2m0:03:29 â€¢ 0:00:15[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.5734, 0.4266]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 190/203 [2m0:03:29 â€¢ 0:00:15[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.5746, 0.4254]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 190/203 [2m0:03:29 â€¢ 0:00:15[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.5760, 0.4240]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 190/203 [2m0:03:29 â€¢ 0:00:15[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.5775, 0.4225]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 190/203 [2m0:03:29 â€¢ 0:00:15[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.5791, 0.4209]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 190/203 [2m0:03:29 â€¢ 0:00:15[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.5807, 0.4193]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 190/203 [2m0:03:29 â€¢ 0:00:15[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.5824, 0.4176]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 190/203 [2m0:03:29 â€¢ 0:00:15[0m [2;4m0.89it/s[0m  
[2KStep 10 - TTA Loss: 0.7247â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”[0m 190/203 [2m0:03:29 â€¢ 0:00:15[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.5841, 0.4159]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 190/203 [2m0:03:29 â€¢ 0:00:15[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.5858, 0.4142]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 190/203 [2m0:03:29 â€¢ 0:00:15[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.5874, 0.4126]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 190/203 [2m0:03:29 â€¢ 0:00:15[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.5890, 0.4110]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 190/203 [2m0:03:29 â€¢ 0:00:15[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.5904, 0.4096]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 190/203 [2m0:03:29 â€¢ 0:00:15[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.5918, 0.4082]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 190/203 [2m0:03:29 â€¢ 0:00:15[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.5931, 0.4069]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 190/203 [2m0:03:29 â€¢ 0:00:15[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.5943, 0.4057]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 190/203 [2m0:03:29 â€¢ 0:00:15[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.5953, 0.4047]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 190/203 [2m0:03:29 â€¢ 0:00:15[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.5962, 0.4038]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 190/203 [2m0:03:29 â€¢ 0:00:15[0m [2;4m0.89it/s[0m  
[2KStep 20 - TTA Loss: 0.6878â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”[0m 190/203 [2m0:03:29 â€¢ 0:00:15[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.5969, 0.4031]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 190/203 [2m0:03:29 â€¢ 0:00:15[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.5976, 0.4024]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 190/203 [2m0:03:29 â€¢ 0:00:15[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.5981, 0.4019]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 190/203 [2m0:03:29 â€¢ 0:00:15[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.5985, 0.4015]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 190/203 [2m0:03:29 â€¢ 0:00:15[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.5988, 0.4012]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 190/203 [2m0:03:29 â€¢ 0:00:15[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.5991, 0.4009]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 190/203 [2m0:03:29 â€¢ 0:00:15[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.5992, 0.4008]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 190/203 [2m0:03:29 â€¢ 0:00:15[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.5993, 0.4007]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 190/203 [2m0:03:29 â€¢ 0:00:15[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.5993, 0.4007]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 190/203 [2m0:03:29 â€¢ 0:00:15[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.5777, 0.4223]], device='cuda:0')[38;5;237mâ•º[0m[38;5;237mâ”â”[0m 190/203 [2m0:03:29 â€¢ 0:00:15[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.6514, 0.3486],â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”[0m 190/203 [2m0:03:29 â€¢ 0:00:15[0m [2;4m0.89it/s[0m  
        [0.6139, 0.3861],
        [0.6048, 0.3952],
        [0.6660, 0.3340],
        [0.6285, 0.3715],
        [0.6389, 0.3611],
        [0.5994, 0.4006],
        [0.6730, 0.3270],
        [0.6776, 0.3224],
[2K        [0.6772, 0.3228]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”[0m 190/203 [2m0:03:29 â€¢ 0:00:15[0m [2;4m0.89it/s[0m  
[2KAttention weights: tensor([[0.5821, 0.4179],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”[0m 191/203 [2m0:03:31 â€¢ 0:00:14[0m [2;4m0.87it/s[0m  
        [0.5792, 0.4208],
        [0.5802, 0.4198],
        [0.5816, 0.4184],
        [0.5817, 0.4183],
        [0.5783, 0.4217],
        [0.5767, 0.4233],
        [0.5818, 0.4182],
        [0.5816, 0.4184],
[2K        [0.5827, 0.4173]], device='cuda:0', grad_fn=<SoftmaxBackward0>)6;224mâ•¸[0m[38;5;237mâ”â”[0m 191/203 [2m0:03:31 â€¢ 0:00:14[0m [2;4m0.87it/s[0m  
[2KClass label: CleanAndJerkâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”[0m 191/203 [2m0:03:31 â€¢ 0:00:14[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5949, 0.4051]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 191/203 [2m0:03:31 â€¢ 0:00:14[0m [2;4m0.87it/s[0m  
[2KStep 0 - TTA Loss: 0.7515â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”[0m 191/203 [2m0:03:31 â€¢ 0:00:14[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5949, 0.4051]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 191/203 [2m0:03:31 â€¢ 0:00:14[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5950, 0.4050]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 191/203 [2m0:03:31 â€¢ 0:00:14[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5950, 0.4050]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 191/203 [2m0:03:31 â€¢ 0:00:14[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5950, 0.4050]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 191/203 [2m0:03:31 â€¢ 0:00:14[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5951, 0.4049]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 191/203 [2m0:03:31 â€¢ 0:00:14[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5952, 0.4048]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 191/203 [2m0:03:31 â€¢ 0:00:14[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5954, 0.4046]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 191/203 [2m0:03:31 â€¢ 0:00:14[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5957, 0.4043]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 191/203 [2m0:03:31 â€¢ 0:00:14[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5960, 0.4040]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 191/203 [2m0:03:31 â€¢ 0:00:14[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5965, 0.4035]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 191/203 [2m0:03:31 â€¢ 0:00:14[0m [2;4m0.87it/s[0m  
[2KStep 10 - TTA Loss: 0.7469â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”[0m 191/203 [2m0:03:31 â€¢ 0:00:14[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5970, 0.4030]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 191/203 [2m0:03:31 â€¢ 0:00:14[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5976, 0.4024]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 191/203 [2m0:03:31 â€¢ 0:00:14[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5983, 0.4017]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 191/203 [2m0:03:31 â€¢ 0:00:14[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5992, 0.4008]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 191/203 [2m0:03:31 â€¢ 0:00:14[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6001, 0.3999]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 191/203 [2m0:03:31 â€¢ 0:00:14[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6012, 0.3988]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 191/203 [2m0:03:31 â€¢ 0:00:14[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6025, 0.3975]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 191/203 [2m0:03:31 â€¢ 0:00:14[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6040, 0.3960]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 191/203 [2m0:03:31 â€¢ 0:00:14[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6055, 0.3945]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 191/203 [2m0:03:31 â€¢ 0:00:14[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6072, 0.3928]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 191/203 [2m0:03:31 â€¢ 0:00:14[0m [2;4m0.87it/s[0m  
[2KStep 20 - TTA Loss: 0.7835â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”[0m 191/203 [2m0:03:31 â€¢ 0:00:14[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6089, 0.3911]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 191/203 [2m0:03:31 â€¢ 0:00:14[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6107, 0.3893]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 191/203 [2m0:03:31 â€¢ 0:00:14[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6127, 0.3873]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 191/203 [2m0:03:31 â€¢ 0:00:14[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6149, 0.3851]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 191/203 [2m0:03:31 â€¢ 0:00:14[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6171, 0.3829]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 191/203 [2m0:03:31 â€¢ 0:00:14[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6196, 0.3804]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 191/203 [2m0:03:31 â€¢ 0:00:14[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6220, 0.3780]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 191/203 [2m0:03:31 â€¢ 0:00:14[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6245, 0.3755]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 191/203 [2m0:03:31 â€¢ 0:00:14[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6270, 0.3730]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 191/203 [2m0:03:31 â€¢ 0:00:14[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5815, 0.4185]], device='cuda:0')[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”[0m 191/203 [2m0:03:31 â€¢ 0:00:14[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6522, 0.3478],â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”[0m 191/203 [2m0:03:31 â€¢ 0:00:14[0m [2;4m0.87it/s[0m  
        [0.6207, 0.3793],
        [0.6297, 0.3703],
        [0.6691, 0.3309],
        [0.6314, 0.3686],
        [0.6418, 0.3582],
        [0.5957, 0.4043],
        [0.6720, 0.3280],
        [0.6848, 0.3152],
[2K        [0.6790, 0.3210]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”[0m 191/203 [2m0:03:31 â€¢ 0:00:14[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5821, 0.4179],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”[0m 192/203 [2m0:03:32 â€¢ 0:00:13[0m [2;4m0.86it/s[0m  
        [0.5792, 0.4208],
        [0.5802, 0.4198],
        [0.5816, 0.4184],
        [0.5818, 0.4182],
        [0.5784, 0.4216],
        [0.5766, 0.4234],
        [0.5818, 0.4182],
        [0.5816, 0.4184],
[2K        [0.5828, 0.4172]], device='cuda:0', grad_fn=<SoftmaxBackward0>)6;224mâ•¸[0m[38;5;237mâ”â”[0m 192/203 [2m0:03:32 â€¢ 0:00:13[0m [2;4m0.86it/s[0m  
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”[0m 192/203 [2m0:03:32 â€¢ 0:00:13[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6185, 0.3815]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 192/203 [2m0:03:32 â€¢ 0:00:13[0m [2;4m0.86it/s[0m  
[2KStep 0 - TTA Loss: 0.7332â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”[0m 192/203 [2m0:03:32 â€¢ 0:00:13[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6202, 0.3798]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 192/203 [2m0:03:32 â€¢ 0:00:13[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6220, 0.3780]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 192/203 [2m0:03:32 â€¢ 0:00:13[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6240, 0.3760]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 192/203 [2m0:03:32 â€¢ 0:00:13[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6262, 0.3738]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 192/203 [2m0:03:32 â€¢ 0:00:13[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6284, 0.3716]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 192/203 [2m0:03:32 â€¢ 0:00:13[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6307, 0.3693]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 192/203 [2m0:03:32 â€¢ 0:00:13[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6329, 0.3671]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 192/203 [2m0:03:32 â€¢ 0:00:13[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6352, 0.3648]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 192/203 [2m0:03:32 â€¢ 0:00:13[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6375, 0.3625]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 192/203 [2m0:03:32 â€¢ 0:00:13[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6397, 0.3603]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 192/203 [2m0:03:32 â€¢ 0:00:13[0m [2;4m0.86it/s[0m  
[2KStep 10 - TTA Loss: 0.7501â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”[0m 192/203 [2m0:03:32 â€¢ 0:00:13[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6418, 0.3582]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 192/203 [2m0:03:32 â€¢ 0:00:13[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6439, 0.3561]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 192/203 [2m0:03:32 â€¢ 0:00:13[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6458, 0.3542]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 192/203 [2m0:03:32 â€¢ 0:00:13[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6476, 0.3524]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 192/203 [2m0:03:32 â€¢ 0:00:13[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6494, 0.3506]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 192/203 [2m0:03:32 â€¢ 0:00:13[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6509, 0.3491]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 192/203 [2m0:03:32 â€¢ 0:00:13[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6524, 0.3476]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 192/203 [2m0:03:32 â€¢ 0:00:13[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6537, 0.3463]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 192/203 [2m0:03:32 â€¢ 0:00:13[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6549, 0.3451]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 192/203 [2m0:03:32 â€¢ 0:00:13[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6559, 0.3441]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 192/203 [2m0:03:32 â€¢ 0:00:13[0m [2;4m0.86it/s[0m  
[2KStep 20 - TTA Loss: 0.7443â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”[0m 192/203 [2m0:03:32 â€¢ 0:00:13[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6568, 0.3432]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 192/203 [2m0:03:32 â€¢ 0:00:13[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6575, 0.3425]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 192/203 [2m0:03:32 â€¢ 0:00:13[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6580, 0.3420]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 192/203 [2m0:03:32 â€¢ 0:00:13[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6585, 0.3415]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 192/203 [2m0:03:32 â€¢ 0:00:13[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6588, 0.3412]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 192/203 [2m0:03:32 â€¢ 0:00:13[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6590, 0.3410]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 192/203 [2m0:03:32 â€¢ 0:00:13[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6592, 0.3408]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 192/203 [2m0:03:32 â€¢ 0:00:13[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6593, 0.3407]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 192/203 [2m0:03:32 â€¢ 0:00:13[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6593, 0.3407]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 192/203 [2m0:03:32 â€¢ 0:00:13[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5801, 0.4199]], device='cuda:0')[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”[0m 192/203 [2m0:03:32 â€¢ 0:00:13[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6644, 0.3356],â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”[0m 192/203 [2m0:03:32 â€¢ 0:00:13[0m [2;4m0.86it/s[0m  
        [0.6191, 0.3809],
        [0.6267, 0.3733],
        [0.6768, 0.3232],
        [0.6442, 0.3558],
        [0.6594, 0.3406],
        [0.6080, 0.3920],
        [0.6846, 0.3154],
        [0.6930, 0.3070],
[2K        [0.6902, 0.3098]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”[0m 192/203 [2m0:03:32 â€¢ 0:00:13[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5820, 0.4180],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”[0m 193/203 [2m0:03:33 â€¢ 0:00:12[0m [2;4m0.86it/s[0m   
        [0.5791, 0.4209],
        [0.5801, 0.4199],
        [0.5815, 0.4185],
        [0.5817, 0.4183],
        [0.5783, 0.4217],
        [0.5766, 0.4234],
        [0.5818, 0.4182],
        [0.5816, 0.4184],
[2K        [0.5828, 0.4172]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ•º[0m[38;5;237mâ”[0m 193/203 [2m0:03:33 â€¢ 0:00:12[0m [2;4m0.86it/s[0m  
[2KClass label: BaseballPitchâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”[0m 193/203 [2m0:03:33 â€¢ 0:00:12[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6387, 0.3613]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 193/203 [2m0:03:33 â€¢ 0:00:12[0m [2;4m0.86it/s[0m  
[2KStep 0 - TTA Loss: 0.8260â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”[0m 193/203 [2m0:03:33 â€¢ 0:00:12[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6387, 0.3613]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 193/203 [2m0:03:33 â€¢ 0:00:12[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6387, 0.3613]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 193/203 [2m0:03:33 â€¢ 0:00:12[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6388, 0.3612]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 193/203 [2m0:03:33 â€¢ 0:00:12[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6388, 0.3612]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 193/203 [2m0:03:33 â€¢ 0:00:12[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6389, 0.3611]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 193/203 [2m0:03:33 â€¢ 0:00:12[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6390, 0.3610]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 193/203 [2m0:03:33 â€¢ 0:00:12[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6392, 0.3608]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 193/203 [2m0:03:33 â€¢ 0:00:12[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6395, 0.3605]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 193/203 [2m0:03:33 â€¢ 0:00:12[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6399, 0.3601]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 193/203 [2m0:03:33 â€¢ 0:00:12[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6403, 0.3597]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 193/203 [2m0:03:33 â€¢ 0:00:12[0m [2;4m0.86it/s[0m  
[2KStep 10 - TTA Loss: 1.1215â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”[0m 193/203 [2m0:03:33 â€¢ 0:00:12[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6408, 0.3592]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 193/203 [2m0:03:33 â€¢ 0:00:12[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6414, 0.3586]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 193/203 [2m0:03:33 â€¢ 0:00:12[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6421, 0.3579]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 193/203 [2m0:03:33 â€¢ 0:00:12[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6430, 0.3570]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 193/203 [2m0:03:33 â€¢ 0:00:12[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6439, 0.3561]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 193/203 [2m0:03:33 â€¢ 0:00:12[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6451, 0.3549]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 193/203 [2m0:03:33 â€¢ 0:00:12[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6463, 0.3537]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 193/203 [2m0:03:33 â€¢ 0:00:12[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6478, 0.3522]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 193/203 [2m0:03:33 â€¢ 0:00:12[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6494, 0.3506]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 193/203 [2m0:03:33 â€¢ 0:00:12[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6512, 0.3488]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 193/203 [2m0:03:33 â€¢ 0:00:12[0m [2;4m0.86it/s[0m  
[2KStep 20 - TTA Loss: 0.8410â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”[0m 193/203 [2m0:03:33 â€¢ 0:00:12[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6531, 0.3469]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 193/203 [2m0:03:33 â€¢ 0:00:12[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6551, 0.3449]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 193/203 [2m0:03:33 â€¢ 0:00:12[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6574, 0.3426]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 193/203 [2m0:03:33 â€¢ 0:00:12[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6595, 0.3405]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 193/203 [2m0:03:33 â€¢ 0:00:12[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6618, 0.3382]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 193/203 [2m0:03:33 â€¢ 0:00:12[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6639, 0.3361]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 193/203 [2m0:03:33 â€¢ 0:00:12[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6663, 0.3337]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 193/203 [2m0:03:33 â€¢ 0:00:12[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6688, 0.3312]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 193/203 [2m0:03:33 â€¢ 0:00:12[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6715, 0.3285]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 193/203 [2m0:03:33 â€¢ 0:00:12[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5834, 0.4166]], device='cuda:0')[38;5;237mâ•º[0m[38;5;237mâ”[0m 193/203 [2m0:03:33 â€¢ 0:00:12[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6743, 0.3257],â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”[0m 193/203 [2m0:03:33 â€¢ 0:00:12[0m [2;4m0.86it/s[0m  
        [0.6214, 0.3786],
        [0.6114, 0.3886],
        [0.6770, 0.3230],
        [0.6393, 0.3607],
        [0.6460, 0.3540],
        [0.5965, 0.4035],
        [0.6811, 0.3189],
        [0.6914, 0.3086],
[2K        [0.6885, 0.3115]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”[0m 193/203 [2m0:03:33 â€¢ 0:00:12[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5819, 0.4181],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”[0m 194/203 [2m0:03:34 â€¢ 0:00:11[0m [2;4m0.87it/s[0m  
        [0.5790, 0.4210],
        [0.5801, 0.4199],
        [0.5815, 0.4185],
        [0.5817, 0.4183],
        [0.5783, 0.4217],
        [0.5765, 0.4235],
        [0.5816, 0.4184],
        [0.5815, 0.4185],
[2K        [0.5827, 0.4173]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ•º[0m[38;5;237mâ”[0m 194/203 [2m0:03:34 â€¢ 0:00:11[0m [2;4m0.87it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”[0m 194/203 [2m0:03:34 â€¢ 0:00:11[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6636, 0.3364]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 194/203 [2m0:03:34 â€¢ 0:00:11[0m [2;4m0.87it/s[0m  
[2KStep 0 - TTA Loss: 0.5148â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”[0m 194/203 [2m0:03:34 â€¢ 0:00:11[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6655, 0.3345]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 194/203 [2m0:03:34 â€¢ 0:00:11[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6674, 0.3326]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 194/203 [2m0:03:34 â€¢ 0:00:11[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6693, 0.3307]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 194/203 [2m0:03:34 â€¢ 0:00:11[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6711, 0.3289]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 194/203 [2m0:03:34 â€¢ 0:00:11[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6728, 0.3272]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 194/203 [2m0:03:34 â€¢ 0:00:11[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6745, 0.3255]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 194/203 [2m0:03:34 â€¢ 0:00:11[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6762, 0.3238]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 194/203 [2m0:03:34 â€¢ 0:00:11[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6778, 0.3222]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 194/203 [2m0:03:34 â€¢ 0:00:11[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6793, 0.3207]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 194/203 [2m0:03:34 â€¢ 0:00:11[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6807, 0.3193]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 194/203 [2m0:03:34 â€¢ 0:00:11[0m [2;4m0.87it/s[0m  
[2KStep 10 - TTA Loss: 0.4654â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”[0m 194/203 [2m0:03:34 â€¢ 0:00:11[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6821, 0.3179]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 194/203 [2m0:03:34 â€¢ 0:00:11[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6834, 0.3166]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 194/203 [2m0:03:34 â€¢ 0:00:11[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6847, 0.3153]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 194/203 [2m0:03:34 â€¢ 0:00:11[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6859, 0.3141]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 194/203 [2m0:03:34 â€¢ 0:00:11[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6870, 0.3130]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 194/203 [2m0:03:34 â€¢ 0:00:11[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6880, 0.3120]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 194/203 [2m0:03:34 â€¢ 0:00:11[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6889, 0.3111]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 194/203 [2m0:03:34 â€¢ 0:00:11[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6898, 0.3102]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 194/203 [2m0:03:34 â€¢ 0:00:11[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6905, 0.3095]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 194/203 [2m0:03:34 â€¢ 0:00:11[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6912, 0.3088]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 194/203 [2m0:03:34 â€¢ 0:00:11[0m [2;4m0.87it/s[0m  
[2KStep 20 - TTA Loss: 0.4718â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”[0m 194/203 [2m0:03:34 â€¢ 0:00:11[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6917, 0.3083]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 194/203 [2m0:03:34 â€¢ 0:00:11[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6922, 0.3078]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 194/203 [2m0:03:34 â€¢ 0:00:11[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6926, 0.3074]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 194/203 [2m0:03:34 â€¢ 0:00:11[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6929, 0.3071]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 194/203 [2m0:03:34 â€¢ 0:00:11[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6931, 0.3069]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 194/203 [2m0:03:34 â€¢ 0:00:11[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6933, 0.3067]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 194/203 [2m0:03:34 â€¢ 0:00:11[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6934, 0.3066]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 194/203 [2m0:03:34 â€¢ 0:00:11[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6935, 0.3065]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 194/203 [2m0:03:34 â€¢ 0:00:11[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6935, 0.3065]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 194/203 [2m0:03:34 â€¢ 0:00:11[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5843, 0.4157]], device='cuda:0')[38;5;237mâ•º[0m[38;5;237mâ”[0m 194/203 [2m0:03:34 â€¢ 0:00:11[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.6753, 0.3247],â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”[0m 194/203 [2m0:03:34 â€¢ 0:00:11[0m [2;4m0.87it/s[0m  
        [0.6221, 0.3779],
        [0.6150, 0.3850],
        [0.6820, 0.3180],
        [0.6434, 0.3566],
        [0.6482, 0.3518],
        [0.6004, 0.3996],
        [0.6820, 0.3180],
        [0.6942, 0.3058],
[2K        [0.6936, 0.3064]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”[0m 194/203 [2m0:03:34 â€¢ 0:00:11[0m [2;4m0.87it/s[0m  
[2KAttention weights: tensor([[0.5817, 0.4183],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”[0m 195/203 [2m0:03:35 â€¢ 0:00:10[0m [2;4m0.85it/s[0m  
        [0.5789, 0.4211],
        [0.5800, 0.4200],
        [0.5814, 0.4186],
        [0.5816, 0.4184],
        [0.5781, 0.4219],
        [0.5765, 0.4235],
        [0.5815, 0.4185],
        [0.5813, 0.4187],
[2K        [0.5826, 0.4174]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ•º[0m[38;5;237mâ”[0m 195/203 [2m0:03:35 â€¢ 0:00:10[0m [2;4m0.85it/s[0m  
[2KClass label: GolfSwingâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”[0m 195/203 [2m0:03:35 â€¢ 0:00:10[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6494, 0.3506]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 195/203 [2m0:03:35 â€¢ 0:00:10[0m [2;4m0.85it/s[0m  
[2KStep 0 - TTA Loss: 0.4973â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”[0m 195/203 [2m0:03:35 â€¢ 0:00:10[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6494, 0.3506]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 195/203 [2m0:03:35 â€¢ 0:00:10[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6495, 0.3505]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 195/203 [2m0:03:35 â€¢ 0:00:10[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6495, 0.3505]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 195/203 [2m0:03:35 â€¢ 0:00:10[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6496, 0.3504]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 195/203 [2m0:03:35 â€¢ 0:00:10[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6497, 0.3503]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 195/203 [2m0:03:35 â€¢ 0:00:10[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6499, 0.3501]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 195/203 [2m0:03:35 â€¢ 0:00:10[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6502, 0.3498]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 195/203 [2m0:03:35 â€¢ 0:00:10[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6506, 0.3494]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 195/203 [2m0:03:35 â€¢ 0:00:10[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6511, 0.3489]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 195/203 [2m0:03:35 â€¢ 0:00:10[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6517, 0.3483]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 195/203 [2m0:03:35 â€¢ 0:00:10[0m [2;4m0.85it/s[0m  
[2KStep 10 - TTA Loss: 0.5861â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”[0m 195/203 [2m0:03:35 â€¢ 0:00:10[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6525, 0.3475]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 195/203 [2m0:03:35 â€¢ 0:00:10[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6534, 0.3466]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 195/203 [2m0:03:35 â€¢ 0:00:10[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6546, 0.3454]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 195/203 [2m0:03:35 â€¢ 0:00:10[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6559, 0.3441]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 195/203 [2m0:03:35 â€¢ 0:00:10[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6575, 0.3425]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 195/203 [2m0:03:35 â€¢ 0:00:10[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6593, 0.3407]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 195/203 [2m0:03:35 â€¢ 0:00:10[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6612, 0.3388]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 195/203 [2m0:03:35 â€¢ 0:00:10[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6635, 0.3365]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 195/203 [2m0:03:35 â€¢ 0:00:10[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6659, 0.3341]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 195/203 [2m0:03:35 â€¢ 0:00:10[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6685, 0.3315]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 195/203 [2m0:03:35 â€¢ 0:00:10[0m [2;4m0.85it/s[0m  
[2KStep 20 - TTA Loss: 0.5901â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”[0m 195/203 [2m0:03:35 â€¢ 0:00:10[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6712, 0.3288]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 195/203 [2m0:03:35 â€¢ 0:00:10[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6739, 0.3261]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 195/203 [2m0:03:35 â€¢ 0:00:10[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6768, 0.3232]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 195/203 [2m0:03:35 â€¢ 0:00:10[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6800, 0.3200]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 195/203 [2m0:03:35 â€¢ 0:00:10[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6832, 0.3168]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 195/203 [2m0:03:35 â€¢ 0:00:10[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6868, 0.3132]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 195/203 [2m0:03:35 â€¢ 0:00:10[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6905, 0.3095]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 195/203 [2m0:03:35 â€¢ 0:00:10[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6945, 0.3055]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 195/203 [2m0:03:35 â€¢ 0:00:10[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6984, 0.3016]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 195/203 [2m0:03:35 â€¢ 0:00:10[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5838, 0.4162]], device='cuda:0')[38;5;237mâ•º[0m[38;5;237mâ”[0m 195/203 [2m0:03:35 â€¢ 0:00:10[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6778, 0.3222],â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”[0m 195/203 [2m0:03:35 â€¢ 0:00:10[0m [2;4m0.85it/s[0m  
        [0.6260, 0.3740],
        [0.6200, 0.3800],
        [0.7024, 0.2976],
        [0.6470, 0.3530],
        [0.6507, 0.3493],
        [0.6027, 0.3973],
        [0.6827, 0.3173],
        [0.7041, 0.2959],
[2K        [0.6980, 0.3020]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”[0m 195/203 [2m0:03:35 â€¢ 0:00:10[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5817, 0.4183],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”[0m 196/203 [2m0:03:36 â€¢ 0:00:09[0m [2;4m0.85it/s[0m  
        [0.5789, 0.4211],
        [0.5800, 0.4200],
        [0.5814, 0.4186],
        [0.5816, 0.4184],
        [0.5781, 0.4219],
        [0.5765, 0.4235],
        [0.5814, 0.4186],
        [0.5813, 0.4187],
[2K        [0.5826, 0.4174]], device='cuda:0', grad_fn=<SoftmaxBackward0>);6;224mâ•¸[0m[38;5;237mâ”[0m 196/203 [2m0:03:36 â€¢ 0:00:09[0m [2;4m0.85it/s[0m  
[2KClass label: TennisSwingâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”[0m 196/203 [2m0:03:36 â€¢ 0:00:09[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6672, 0.3328]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 196/203 [2m0:03:36 â€¢ 0:00:09[0m [2;4m0.85it/s[0m  
[2KStep 0 - TTA Loss: 1.3757â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”[0m 196/203 [2m0:03:36 â€¢ 0:00:09[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6704, 0.3296]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 196/203 [2m0:03:36 â€¢ 0:00:09[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6740, 0.3260]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 196/203 [2m0:03:36 â€¢ 0:00:09[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6777, 0.3223]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 196/203 [2m0:03:36 â€¢ 0:00:09[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6817, 0.3183]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 196/203 [2m0:03:36 â€¢ 0:00:09[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6859, 0.3141]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 196/203 [2m0:03:36 â€¢ 0:00:09[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6901, 0.3099]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 196/203 [2m0:03:36 â€¢ 0:00:09[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6945, 0.3055]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 196/203 [2m0:03:36 â€¢ 0:00:09[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6990, 0.3010]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 196/203 [2m0:03:36 â€¢ 0:00:09[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.7034, 0.2966]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 196/203 [2m0:03:36 â€¢ 0:00:09[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.7078, 0.2922]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 196/203 [2m0:03:36 â€¢ 0:00:09[0m [2;4m0.85it/s[0m  
[2KStep 10 - TTA Loss: 1.2834â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”[0m 196/203 [2m0:03:36 â€¢ 0:00:09[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.7122, 0.2878]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 196/203 [2m0:03:36 â€¢ 0:00:09[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.7164, 0.2836]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 196/203 [2m0:03:36 â€¢ 0:00:09[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.7204, 0.2796]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 196/203 [2m0:03:36 â€¢ 0:00:09[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.7242, 0.2758]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 196/203 [2m0:03:36 â€¢ 0:00:09[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.7279, 0.2721]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 196/203 [2m0:03:36 â€¢ 0:00:09[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.7312, 0.2688]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 196/203 [2m0:03:36 â€¢ 0:00:09[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.7343, 0.2657]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 196/203 [2m0:03:36 â€¢ 0:00:09[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.7372, 0.2628]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 196/203 [2m0:03:36 â€¢ 0:00:09[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.7397, 0.2603]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 196/203 [2m0:03:36 â€¢ 0:00:09[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.7419, 0.2581]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 196/203 [2m0:03:36 â€¢ 0:00:09[0m [2;4m0.85it/s[0m  
[2KStep 20 - TTA Loss: 1.1604â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”[0m 196/203 [2m0:03:36 â€¢ 0:00:09[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.7438, 0.2562]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 196/203 [2m0:03:36 â€¢ 0:00:09[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.7455, 0.2545]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 196/203 [2m0:03:36 â€¢ 0:00:09[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.7468, 0.2532]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 196/203 [2m0:03:36 â€¢ 0:00:09[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.7479, 0.2521]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 196/203 [2m0:03:36 â€¢ 0:00:09[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.7487, 0.2513]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 196/203 [2m0:03:36 â€¢ 0:00:09[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.7492, 0.2508]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 196/203 [2m0:03:36 â€¢ 0:00:09[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.7496, 0.2504]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 196/203 [2m0:03:36 â€¢ 0:00:09[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.7498, 0.2502]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 196/203 [2m0:03:36 â€¢ 0:00:09[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.7499, 0.2501]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 196/203 [2m0:03:36 â€¢ 0:00:09[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5848, 0.4152]], device='cuda:0')[38;2;98;6;224mâ•¸[0m[38;5;237mâ”[0m 196/203 [2m0:03:36 â€¢ 0:00:09[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.7011, 0.2989],â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”[0m 196/203 [2m0:03:36 â€¢ 0:00:09[0m [2;4m0.85it/s[0m  
        [0.6521, 0.3479],
        [0.6406, 0.3594],
        [0.7223, 0.2777],
        [0.6696, 0.3304],
        [0.6775, 0.3225],
        [0.6197, 0.3803],
        [0.7021, 0.2979],
        [0.7500, 0.2500],
[2K        [0.7206, 0.2794]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”[0m 196/203 [2m0:03:36 â€¢ 0:00:09[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5817, 0.4183],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”[0m 197/203 [2m0:03:37 â€¢ 0:00:08[0m [2;4m0.85it/s[0m  
        [0.5789, 0.4211],
        [0.5798, 0.4202],
        [0.5813, 0.4187],
        [0.5815, 0.4185],
        [0.5780, 0.4220],
        [0.5765, 0.4235],
        [0.5813, 0.4187],
        [0.5812, 0.4188],
[2K        [0.5826, 0.4174]], device='cuda:0', grad_fn=<SoftmaxBackward0>);6;224mâ•¸[0m[38;5;237mâ”[0m 197/203 [2m0:03:37 â€¢ 0:00:08[0m [2;4m0.85it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”[0m 197/203 [2m0:03:37 â€¢ 0:00:08[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5732, 0.4268]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 197/203 [2m0:03:37 â€¢ 0:00:08[0m [2;4m0.85it/s[0m  
[2KStep 0 - TTA Loss: 1.0414â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”[0m 197/203 [2m0:03:37 â€¢ 0:00:08[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5732, 0.4268]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 197/203 [2m0:03:37 â€¢ 0:00:08[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5732, 0.4268]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 197/203 [2m0:03:37 â€¢ 0:00:08[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5732, 0.4268]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 197/203 [2m0:03:37 â€¢ 0:00:08[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5733, 0.4267]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 197/203 [2m0:03:37 â€¢ 0:00:08[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5735, 0.4265]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 197/203 [2m0:03:37 â€¢ 0:00:08[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5737, 0.4263]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 197/203 [2m0:03:37 â€¢ 0:00:08[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5739, 0.4261]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 197/203 [2m0:03:37 â€¢ 0:00:08[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5743, 0.4257]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 197/203 [2m0:03:37 â€¢ 0:00:08[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5747, 0.4253]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 197/203 [2m0:03:37 â€¢ 0:00:08[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5752, 0.4248]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 197/203 [2m0:03:37 â€¢ 0:00:08[0m [2;4m0.85it/s[0m  
[2KStep 10 - TTA Loss: 0.9587â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”[0m 197/203 [2m0:03:37 â€¢ 0:00:08[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5759, 0.4241]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 197/203 [2m0:03:37 â€¢ 0:00:08[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5765, 0.4235]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 197/203 [2m0:03:37 â€¢ 0:00:08[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5773, 0.4227]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 197/203 [2m0:03:37 â€¢ 0:00:08[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5781, 0.4219]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 197/203 [2m0:03:37 â€¢ 0:00:08[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5791, 0.4209]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 197/203 [2m0:03:37 â€¢ 0:00:08[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5801, 0.4199]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 197/203 [2m0:03:37 â€¢ 0:00:08[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5811, 0.4189]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 197/203 [2m0:03:37 â€¢ 0:00:08[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5823, 0.4177]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 197/203 [2m0:03:37 â€¢ 0:00:08[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5835, 0.4165]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 197/203 [2m0:03:37 â€¢ 0:00:08[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5848, 0.4152]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 197/203 [2m0:03:37 â€¢ 0:00:08[0m [2;4m0.85it/s[0m  
[2KStep 20 - TTA Loss: 1.0617â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”[0m 197/203 [2m0:03:37 â€¢ 0:00:08[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5861, 0.4139]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 197/203 [2m0:03:37 â€¢ 0:00:08[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5875, 0.4125]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 197/203 [2m0:03:37 â€¢ 0:00:08[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5890, 0.4110]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 197/203 [2m0:03:37 â€¢ 0:00:08[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5904, 0.4096]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 197/203 [2m0:03:37 â€¢ 0:00:08[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5920, 0.4080]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 197/203 [2m0:03:37 â€¢ 0:00:08[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5936, 0.4064]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 197/203 [2m0:03:37 â€¢ 0:00:08[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5954, 0.4046]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 197/203 [2m0:03:37 â€¢ 0:00:08[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5972, 0.4028]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 197/203 [2m0:03:37 â€¢ 0:00:08[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5989, 0.4011]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 197/203 [2m0:03:37 â€¢ 0:00:08[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5775, 0.4225]], device='cuda:0')[38;2;98;6;224mâ•¸[0m[38;5;237mâ”[0m 197/203 [2m0:03:37 â€¢ 0:00:08[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6592, 0.3408],â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”[0m 197/203 [2m0:03:37 â€¢ 0:00:08[0m [2;4m0.85it/s[0m  
        [0.6220, 0.3780],
        [0.6137, 0.3863],
        [0.6741, 0.3259],
        [0.6340, 0.3660],
        [0.6460, 0.3540],
        [0.6007, 0.3993],
        [0.6777, 0.3223],
        [0.6933, 0.3067],
[2K        [0.6833, 0.3167]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”[0m 197/203 [2m0:03:37 â€¢ 0:00:08[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5817, 0.4183],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m 198/203 [2m0:03:39 â€¢ 0:00:06[0m [2;4m0.85it/s[0m  4m0.85it/s[0m  
        [0.5789, 0.4211],
        [0.5798, 0.4202],
        [0.5813, 0.4187],
        [0.5815, 0.4185],
        [0.5780, 0.4220],
        [0.5764, 0.4236],
        [0.5813, 0.4187],
        [0.5812, 0.4188],
[2K        [0.5826, 0.4174]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ•º[0m 198/203 [2m0:03:39 â€¢ 0:00:06[0m [2;4m0.85it/s[0m  
[2KClass label: SoccerPenaltyâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m 198/203 [2m0:03:39 â€¢ 0:00:06[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6569, 0.3431]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:39 â€¢ 0:00:06[0m [2;4m0.85it/s[0m  
[2KStep 0 - TTA Loss: 0.4694â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m 198/203 [2m0:03:39 â€¢ 0:00:06[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6586, 0.3414]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:39 â€¢ 0:00:06[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6608, 0.3392]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:39 â€¢ 0:00:06[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6633, 0.3367]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:39 â€¢ 0:00:06[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6661, 0.3339]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:39 â€¢ 0:00:06[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6693, 0.3307]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:39 â€¢ 0:00:06[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6727, 0.3273]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:39 â€¢ 0:00:06[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6764, 0.3236]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:39 â€¢ 0:00:06[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6802, 0.3198]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:39 â€¢ 0:00:06[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6841, 0.3159]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:39 â€¢ 0:00:06[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6879, 0.3121]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:39 â€¢ 0:00:06[0m [2;4m0.85it/s[0m  
[2KStep 10 - TTA Loss: 0.4664â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m 198/203 [2m0:03:39 â€¢ 0:00:06[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6917, 0.3083]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:39 â€¢ 0:00:06[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6955, 0.3045]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:39 â€¢ 0:00:06[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6991, 0.3009]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:39 â€¢ 0:00:06[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.7026, 0.2974]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:39 â€¢ 0:00:06[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.7060, 0.2940]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:39 â€¢ 0:00:06[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.7090, 0.2910]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:39 â€¢ 0:00:06[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.7118, 0.2882]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:39 â€¢ 0:00:06[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.7144, 0.2856]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:39 â€¢ 0:00:06[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.7167, 0.2833]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:39 â€¢ 0:00:06[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.7186, 0.2814]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:39 â€¢ 0:00:06[0m [2;4m0.85it/s[0m  
[2KStep 20 - TTA Loss: 0.4792â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m 198/203 [2m0:03:39 â€¢ 0:00:06[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.7204, 0.2796]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:39 â€¢ 0:00:06[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.7218, 0.2782]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:39 â€¢ 0:00:06[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.7230, 0.2770]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:39 â€¢ 0:00:06[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.7239, 0.2761]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:39 â€¢ 0:00:06[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.7246, 0.2754]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:39 â€¢ 0:00:06[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.7251, 0.2749]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:39 â€¢ 0:00:06[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.7254, 0.2746]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:39 â€¢ 0:00:06[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.7256, 0.2744]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:39 â€¢ 0:00:06[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.7257, 0.2743]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:39 â€¢ 0:00:06[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5843, 0.4157]], device='cuda:0')m[38;5;237mâ•º[0m 198/203 [2m0:03:39 â€¢ 0:00:06[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6716, 0.3284],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m 198/203 [2m0:03:39 â€¢ 0:00:06[0m [2;4m0.85it/s[0m  
        [0.6314, 0.3686],
        [0.6190, 0.3810],
        [0.6821, 0.3179],
        [0.6508, 0.3492],
        [0.6595, 0.3405],
        [0.6134, 0.3866],
        [0.7258, 0.2742],
        [0.6966, 0.3034],
[2K        [0.6935, 0.3065]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m 198/203 [2m0:03:39 â€¢ 0:00:06[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5818, 0.4182],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m 199/203 [2m0:03:40 â€¢ 0:00:05[0m [2;4m0.86it/s[0m  
        [0.5790, 0.4210],
        [0.5798, 0.4202],
        [0.5814, 0.4186],
        [0.5816, 0.4184],
        [0.5781, 0.4219],
        [0.5764, 0.4236],
        [0.5814, 0.4186],
        [0.5813, 0.4187],
[2K        [0.5827, 0.4173]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ•º[0m 199/203 [2m0:03:40 â€¢ 0:00:05[0m [2;4m0.86it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m 199/203 [2m0:03:40 â€¢ 0:00:05[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5723, 0.4277]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:40 â€¢ 0:00:05[0m [2;4m0.86it/s[0m  
[2KStep 0 - TTA Loss: 0.6529â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m 199/203 [2m0:03:40 â€¢ 0:00:05[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5723, 0.4277]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:40 â€¢ 0:00:05[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5723, 0.4277]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:40 â€¢ 0:00:05[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5724, 0.4276]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:40 â€¢ 0:00:05[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5725, 0.4275]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:40 â€¢ 0:00:05[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5726, 0.4274]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:40 â€¢ 0:00:05[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5729, 0.4271]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:40 â€¢ 0:00:05[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5732, 0.4268]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:40 â€¢ 0:00:05[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5737, 0.4263]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:40 â€¢ 0:00:05[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5743, 0.4257]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:40 â€¢ 0:00:05[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5750, 0.4250]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:40 â€¢ 0:00:05[0m [2;4m0.86it/s[0m  
[2KStep 10 - TTA Loss: 0.6066â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m 199/203 [2m0:03:40 â€¢ 0:00:05[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5758, 0.4242]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:40 â€¢ 0:00:05[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5767, 0.4233]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:40 â€¢ 0:00:05[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5778, 0.4222]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:40 â€¢ 0:00:05[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5790, 0.4210]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:40 â€¢ 0:00:05[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5803, 0.4197]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:40 â€¢ 0:00:05[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5818, 0.4182]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:40 â€¢ 0:00:05[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5834, 0.4166]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:40 â€¢ 0:00:05[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5852, 0.4148]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:40 â€¢ 0:00:05[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5871, 0.4129]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:40 â€¢ 0:00:05[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5892, 0.4108]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:40 â€¢ 0:00:05[0m [2;4m0.86it/s[0m  
[2KStep 20 - TTA Loss: 0.5993â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m 199/203 [2m0:03:40 â€¢ 0:00:05[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5913, 0.4087]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:40 â€¢ 0:00:05[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5936, 0.4064]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:40 â€¢ 0:00:05[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5961, 0.4039]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:40 â€¢ 0:00:05[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5987, 0.4013]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:40 â€¢ 0:00:05[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6014, 0.3986]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:40 â€¢ 0:00:05[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6043, 0.3957]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:40 â€¢ 0:00:05[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6072, 0.3928]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:40 â€¢ 0:00:05[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6102, 0.3898]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:40 â€¢ 0:00:05[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6134, 0.3866]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:40 â€¢ 0:00:05[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5780, 0.4220]], device='cuda:0')m[38;5;237mâ•º[0m 199/203 [2m0:03:40 â€¢ 0:00:05[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.6615, 0.3385],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m 199/203 [2m0:03:40 â€¢ 0:00:05[0m [2;4m0.86it/s[0m  
        [0.6243, 0.3757],
        [0.6178, 0.3822],
        [0.6739, 0.3261],
        [0.6435, 0.3565],
        [0.6537, 0.3463],
        [0.6166, 0.3834],
        [0.6920, 0.3080],
        [0.6859, 0.3141],
[2K        [0.6894, 0.3106]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m 199/203 [2m0:03:40 â€¢ 0:00:05[0m [2;4m0.86it/s[0m  
[2KAttention weights: tensor([[0.5818, 0.4182],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m 200/203 [2m0:03:41 â€¢ 0:00:04[0m [2;4m0.84it/s[0m  
        [0.5790, 0.4210],
        [0.5798, 0.4202],
        [0.5814, 0.4186],
        [0.5815, 0.4185],
        [0.5779, 0.4221],
        [0.5764, 0.4236],
        [0.5814, 0.4186],
        [0.5813, 0.4187],
[2K        [0.5826, 0.4174]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ•º[0m 200/203 [2m0:03:41 â€¢ 0:00:04[0m [2;4m0.84it/s[0m  
[2KClass label: CleanAndJerkâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m 200/203 [2m0:03:41 â€¢ 0:00:04[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5911, 0.4089]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:41 â€¢ 0:00:04[0m [2;4m0.84it/s[0m  
[2KStep 0 - TTA Loss: 0.8007â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m 200/203 [2m0:03:41 â€¢ 0:00:04[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5927, 0.4073]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:41 â€¢ 0:00:04[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5942, 0.4058]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:41 â€¢ 0:00:04[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5956, 0.4044]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:41 â€¢ 0:00:04[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5968, 0.4032]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:41 â€¢ 0:00:04[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5978, 0.4022]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:41 â€¢ 0:00:04[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5988, 0.4012]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:41 â€¢ 0:00:04[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5996, 0.4004]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:41 â€¢ 0:00:04[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6003, 0.3997]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:41 â€¢ 0:00:04[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6010, 0.3990]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:41 â€¢ 0:00:04[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6015, 0.3985]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:41 â€¢ 0:00:04[0m [2;4m0.84it/s[0m  
[2KStep 10 - TTA Loss: 0.8466â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m 200/203 [2m0:03:41 â€¢ 0:00:04[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6020, 0.3980]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:41 â€¢ 0:00:04[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6024, 0.3976]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:41 â€¢ 0:00:04[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6028, 0.3972]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:41 â€¢ 0:00:04[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6031, 0.3969]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:41 â€¢ 0:00:04[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6033, 0.3967]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:41 â€¢ 0:00:04[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6035, 0.3965]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:41 â€¢ 0:00:04[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6037, 0.3963]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:41 â€¢ 0:00:04[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6038, 0.3962]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:41 â€¢ 0:00:04[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6039, 0.3961]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:41 â€¢ 0:00:04[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6040, 0.3960]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:41 â€¢ 0:00:04[0m [2;4m0.84it/s[0m  
[2KStep 20 - TTA Loss: 0.8465â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m 200/203 [2m0:03:41 â€¢ 0:00:04[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6040, 0.3960]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:41 â€¢ 0:00:04[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6041, 0.3959]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:41 â€¢ 0:00:04[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6041, 0.3959]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:41 â€¢ 0:00:04[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6041, 0.3959]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:41 â€¢ 0:00:04[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6041, 0.3959]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:41 â€¢ 0:00:04[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6042, 0.3958]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:41 â€¢ 0:00:04[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6042, 0.3958]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:41 â€¢ 0:00:04[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6042, 0.3958]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:41 â€¢ 0:00:04[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6042, 0.3958]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:03:41 â€¢ 0:00:04[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5802, 0.4198]], device='cuda:0')m[38;5;237mâ•º[0m 200/203 [2m0:03:41 â€¢ 0:00:04[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.6459, 0.3541],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m 200/203 [2m0:03:41 â€¢ 0:00:04[0m [2;4m0.84it/s[0m  
        [0.6110, 0.3890],
        [0.6042, 0.3958],
        [0.6621, 0.3379],
        [0.6234, 0.3766],
        [0.6337, 0.3663],
        [0.5914, 0.4086],
        [0.6698, 0.3302],
        [0.6751, 0.3249],
[2K        [0.6716, 0.3284]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m 200/203 [2m0:03:41 â€¢ 0:00:04[0m [2;4m0.84it/s[0m  
[2KAttention weights: tensor([[0.5818, 0.4182],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m 201/203 [2m0:03:42 â€¢ 0:00:03[0m [2;4m0.85it/s[0m  
        [0.5789, 0.4211],
        [0.5797, 0.4203],
        [0.5814, 0.4186],
        [0.5815, 0.4185],
        [0.5778, 0.4222],
        [0.5763, 0.4237],
        [0.5814, 0.4186],
        [0.5812, 0.4188],
[2K        [0.5826, 0.4174]], device='cuda:0', grad_fn=<SoftmaxBackward0>)8;6;224mâ•¸[0m 201/203 [2m0:03:42 â€¢ 0:00:03[0m [2;4m0.85it/s[0m  
[2KClass label: CleanAndJerkâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m 201/203 [2m0:03:42 â€¢ 0:00:03[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5908, 0.4092]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:42 â€¢ 0:00:03[0m [2;4m0.85it/s[0m  
[2KStep 0 - TTA Loss: 0.7399â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m 201/203 [2m0:03:42 â€¢ 0:00:03[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5908, 0.4092]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:42 â€¢ 0:00:03[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5908, 0.4092]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:42 â€¢ 0:00:03[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5908, 0.4092]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:42 â€¢ 0:00:03[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5908, 0.4092]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:42 â€¢ 0:00:03[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5908, 0.4092]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:42 â€¢ 0:00:03[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5908, 0.4092]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:42 â€¢ 0:00:03[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5908, 0.4092]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:42 â€¢ 0:00:03[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5909, 0.4091]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:42 â€¢ 0:00:03[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5909, 0.4091]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:42 â€¢ 0:00:03[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5910, 0.4090]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:42 â€¢ 0:00:03[0m [2;4m0.85it/s[0m  
[2KStep 10 - TTA Loss: 0.7467â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m 201/203 [2m0:03:42 â€¢ 0:00:03[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5910, 0.4090]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:42 â€¢ 0:00:03[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5911, 0.4089]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:42 â€¢ 0:00:03[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5913, 0.4087]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:42 â€¢ 0:00:03[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5914, 0.4086]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:42 â€¢ 0:00:03[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5916, 0.4084]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:42 â€¢ 0:00:03[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5918, 0.4082]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:42 â€¢ 0:00:03[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5920, 0.4080]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:42 â€¢ 0:00:03[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5922, 0.4078]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:42 â€¢ 0:00:03[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5925, 0.4075]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:42 â€¢ 0:00:03[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5927, 0.4073]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:42 â€¢ 0:00:03[0m [2;4m0.85it/s[0m  
[2KStep 20 - TTA Loss: 0.7427â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m 201/203 [2m0:03:42 â€¢ 0:00:03[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5930, 0.4070]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:42 â€¢ 0:00:03[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5933, 0.4067]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:42 â€¢ 0:00:03[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5936, 0.4064]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:42 â€¢ 0:00:03[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5939, 0.4061]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:42 â€¢ 0:00:03[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5942, 0.4058]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:42 â€¢ 0:00:03[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5946, 0.4054]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:42 â€¢ 0:00:03[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5949, 0.4051]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:42 â€¢ 0:00:03[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5953, 0.4047]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:42 â€¢ 0:00:03[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5957, 0.4043]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:42 â€¢ 0:00:03[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5800, 0.4200]], device='cuda:0')m[38;2;98;6;224mâ•¸[0m 201/203 [2m0:03:42 â€¢ 0:00:03[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6373, 0.3627],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m 201/203 [2m0:03:42 â€¢ 0:00:03[0m [2;4m0.85it/s[0m  
        [0.6037, 0.3963],
        [0.5961, 0.4039],
        [0.6503, 0.3497],
        [0.6100, 0.3900],
        [0.6178, 0.3822],
        [0.5700, 0.4300],
        [0.6581, 0.3419],
        [0.6665, 0.3335],
[2K        [0.6595, 0.3405]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m 201/203 [2m0:03:42 â€¢ 0:00:03[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5818, 0.4182],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m 202/203 [2m0:03:43 â€¢ 0:00:02[0m [2;4m0.85it/s[0m  
        [0.5790, 0.4210],
        [0.5798, 0.4202],
        [0.5814, 0.4186],
        [0.5815, 0.4185],
        [0.5779, 0.4221],
        [0.5763, 0.4237],
        [0.5814, 0.4186],
        [0.5813, 0.4187],
[2K        [0.5827, 0.4173]], device='cuda:0', grad_fn=<SoftmaxBackward0>)8;6;224mâ•¸[0m 202/203 [2m0:03:43 â€¢ 0:00:02[0m [2;4m0.85it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m 202/203 [2m0:03:43 â€¢ 0:00:02[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6588, 0.3412]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:43 â€¢ 0:00:02[0m [2;4m0.85it/s[0m  
[2KStep 0 - TTA Loss: 0.3745â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m 202/203 [2m0:03:43 â€¢ 0:00:02[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6593, 0.3407]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:43 â€¢ 0:00:02[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6603, 0.3397]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:43 â€¢ 0:00:02[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6615, 0.3385]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:43 â€¢ 0:00:02[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6631, 0.3369]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:43 â€¢ 0:00:02[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6649, 0.3351]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:43 â€¢ 0:00:02[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6668, 0.3332]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:43 â€¢ 0:00:02[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6689, 0.3311]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:43 â€¢ 0:00:02[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6710, 0.3290]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:43 â€¢ 0:00:02[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6733, 0.3267]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:43 â€¢ 0:00:02[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6755, 0.3245]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:43 â€¢ 0:00:02[0m [2;4m0.85it/s[0m  
[2KStep 10 - TTA Loss: 0.3230â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m 202/203 [2m0:03:43 â€¢ 0:00:02[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6778, 0.3222]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:43 â€¢ 0:00:02[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6800, 0.3200]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:43 â€¢ 0:00:02[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6821, 0.3179]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:43 â€¢ 0:00:02[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6842, 0.3158]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:43 â€¢ 0:00:02[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6861, 0.3139]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:43 â€¢ 0:00:02[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6879, 0.3121]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:43 â€¢ 0:00:02[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6896, 0.3104]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:43 â€¢ 0:00:02[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6911, 0.3089]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:43 â€¢ 0:00:02[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6924, 0.3076]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:43 â€¢ 0:00:02[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6936, 0.3064]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:43 â€¢ 0:00:02[0m [2;4m0.85it/s[0m  
[2KStep 20 - TTA Loss: 0.3269â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m 202/203 [2m0:03:43 â€¢ 0:00:02[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6946, 0.3054]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:43 â€¢ 0:00:02[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6955, 0.3045]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:43 â€¢ 0:00:02[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6961, 0.3039]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:43 â€¢ 0:00:02[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6967, 0.3033]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:43 â€¢ 0:00:02[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6971, 0.3029]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:43 â€¢ 0:00:02[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6974, 0.3026]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:43 â€¢ 0:00:02[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6976, 0.3024]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:43 â€¢ 0:00:02[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6977, 0.3023]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:43 â€¢ 0:00:02[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6977, 0.3023]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:03:43 â€¢ 0:00:02[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.5848, 0.4152]], device='cuda:0')m[38;2;98;6;224mâ•¸[0m 202/203 [2m0:03:43 â€¢ 0:00:02[0m [2;4m0.85it/s[0m  
[2KAttention weights: tensor([[0.6694, 0.3306],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m 202/203 [2m0:03:43 â€¢ 0:00:02[0m [2;4m0.85it/s[0m  
        [0.6255, 0.3745],
        [0.6213, 0.3787],
        [0.6821, 0.3179],
        [0.6475, 0.3525],
        [0.6520, 0.3480],
        [0.6027, 0.3973],
        [0.6823, 0.3177],
        [0.6956, 0.3044],
[2K        [0.6979, 0.3021]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m 202/203 [2m0:03:43 â€¢ 0:00:02[0m [2;4m0.85it/s[0m  
[2K                   video-id  t-start  t-end     labelâ”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:03:44 â€¢ 0:00:00[0m [2;4m0.85it/s[0m  4m0.85it/s[0m  
0  video_validation_0000365     18.1   24.3  HighJump
1  video_validation_0000365     29.6   33.3  HighJump
2  video_validation_0000365     69.7   77.3  HighJump
3  video_validation_0000365     80.8   84.3  HighJump
[2K4  video_validation_0000365    110.4  116.2  HighJumpâ”â”â”â”â”â”â”[0m 203/203 [2m0:03:44 â€¢ 0:00:00[0m [2;4m0.85it/s[0m  
[2K                   video-id     t-start       t-end       score     label[2m0:03:44 â€¢ 0:00:00[0m [2;4m0.85it/s[0m  
0  video_validation_0000365   19.066667   36.633333   1.3982303  HighJump
1  video_validation_0000365   37.533333   37.633333  0.10916153  HighJump
2  video_validation_0000365   66.866667   89.033333   1.6766406  HighJump
3  video_validation_0000365  111.533333  118.733333   1.6405214  HighJump
[2K4  video_validation_0000365  119.433333  122.733333   1.1498401  HighJump[2m0:03:44 â€¢ 0:00:00[0m [2;4m0.85it/s[0m  
[2KGround truth labels:  ['HighJump' 'PoleVault' 'TennisSwing' 'GolfSwing' 'HammerThrow'â€¢ 0:00:00[0m [2;4m0.85it/s[0m  
[2K 'Billiards' 'BaseballPitch' 'CleanAndJerk' 'ThrowDiscus' 'SoccerPenalty'][2m0:03:44 â€¢ 0:00:00[0m [2;4m0.85it/s[0m  
[2KPredicted labels:  ['HighJump' 'TennisSwing' 'GolfSwing' 'HammerThrow' 'Billiards'44 â€¢ 0:00:00[0m [2;4m0.85it/s[0m  
[2K 'BaseballPitch' 'CleanAndJerk' 'ThrowDiscus' 'SoccerPenalty' 'PoleVault'][2m0:03:44 â€¢ 0:00:00[0m [2;4m0.85it/s[0m  
[2King [38;2;98;6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:03:44 â€¢ 0:00:00[0m [2;4m0.85it/s[0m  
[2King [38;2;98;6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:03:44 â€¢ 0:00:00[0m [2;4m0.85it/s[0m  
[2KBaseballPitch [6.7, 2.0, 0.8, 0.4, 0.3]â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:03:44 â€¢ 0:00:00[0m [2;4m0.85it/s[0m  
[2King [38;2;98;6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:03:44 â€¢ 0:00:00[0m [2;4m0.85it/s[0m  
[2King [38;2;98;6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:03:44 â€¢ 0:00:00[0m [2;4m0.85it/s[0m  
[2KBilliards [4.1, 2.5, 1.0, 0.5, 0.1]â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:03:44 â€¢ 0:00:00[0m [2;4m0.85it/s[0m  
[2King [38;2;98;6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:03:44 â€¢ 0:00:00[0m [2;4m0.85it/s[0m  
[2King [38;2;98;6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:03:44 â€¢ 0:00:00[0m [2;4m0.85it/s[0m  
[2KCleanAndJerk [32.5, 22.9, 17.5, 13.3, 7.5]â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:03:44 â€¢ 0:00:00[0m [2;4m0.85it/s[0m  
[2King [38;2;98;6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:03:44 â€¢ 0:00:00[0m [2;4m0.85it/s[0m  
[2King [38;2;98;6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:03:44 â€¢ 0:00:00[0m [2;4m0.85it/s[0m  
[2KGolfSwing [21.4, 11.6, 4.1, 0.8, 0.7]â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:03:44 â€¢ 0:00:00[0m [2;4m0.85it/s[0m  
[2King [38;2;98;6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:03:44 â€¢ 0:00:00[0m [2;4m0.85it/s[0m  
[2King [38;2;98;6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:03:44 â€¢ 0:00:00[0m [2;4m0.85it/s[0m  
[2KHammerThrow [28.5, 21.6, 14.2, 10.4, 7.0]â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:03:44 â€¢ 0:00:00[0m [2;4m0.85it/s[0m  
[2King [38;2;98;6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:03:44 â€¢ 0:00:00[0m [2;4m0.85it/s[0m  
[2King [38;2;98;6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:03:44 â€¢ 0:00:00[0m [2;4m0.85it/s[0m  
[2KHighJump [28.8, 15.2, 9.8, 5.1, 2.2]â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:03:44 â€¢ 0:00:00[0m [2;4m0.85it/s[0m  
[2King [38;2;98;6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:03:44 â€¢ 0:00:00[0m [2;4m0.85it/s[0m  
[2King [38;2;98;6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:03:44 â€¢ 0:00:00[0m [2;4m0.85it/s[0m  
[2KPoleVault [45.6, 39.6, 28.5, 22.4, 13.9]â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:03:44 â€¢ 0:00:00[0m [2;4m0.85it/s[0m  
[2King [38;2;98;6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:03:44 â€¢ 0:00:00[0m [2;4m0.85it/s[0m  
[2King [38;2;98;6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:03:44 â€¢ 0:00:00[0m [2;4m0.85it/s[0m  
[2KSoccerPenalty [26.7, 14.6, 6.5, 2.3, 0.6]â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:03:44 â€¢ 0:00:00[0m [2;4m0.85it/s[0m  
[2King [38;2;98;6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:03:44 â€¢ 0:00:00[0m [2;4m0.85it/s[0m  
[2King [38;2;98;6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:03:44 â€¢ 0:00:00[0m [2;4m0.85it/s[0m  
[2KTennisSwing [1.4, 0.6, 0.4, 0.1, 0.0]â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:03:44 â€¢ 0:00:00[0m [2;4m0.85it/s[0m  
[2King [38;2;98;6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:03:44 â€¢ 0:00:00[0m [2;4m0.85it/s[0m  
[2King [38;2;98;6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:03:44 â€¢ 0:00:00[0m [2;4m0.85it/s[0m  
[2KThrowDiscus [4.5, 3.5, 2.0, 1.5, 0.5]â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:03:44 â€¢ 0:00:00[0m [2;4m0.85it/s[0m  
[2King [38;2;98;6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:03:44 â€¢ 0:00:00[0m [2;4m0.85it/s[0m  
[2King [38;2;98;6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:03:44 â€¢ 0:00:00[0m [2;4m0.85it/s[0m  
[2K\begin{tabular}{lrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr}
\hline
 Class         &     AP@0% &   AP@0% &      AP@0% &      AP@0% &     AP@0% \\
\hline
 BaseballPitch &  6.7      &     2   &  0.8       &  0.4       &  0.3      \\
 Billiards     &  4.1      &     2.5 &  1         &  0.5       &  0.1      \\
 CleanAndJerk  & 32.5      &    22.9 & 17.5       & 13.3       &  7.5      \\
 GolfSwing     & 21.4      &    11.6 &  4.1       &  0.8       &  0.7      \\
 HammerThrow   & 28.5      &    21.6 & 14.2       & 10.4       &  7        \\
 HighJump      & 28.8      &    15.2 &  9.8       &  5.1       &  2.2      \\
 PoleVault     & 45.6      &    39.6 & 28.5       & 22.4       & 13.9      \\
 SoccerPenalty & 26.7      &    14.6 &  6.5       &  2.3       &  0.6      \\
 TennisSwing   &  1.4      &     0.6 &  0.4       &  0.1       &  0        \\
 ThrowDiscus   &  4.5      &     3.5 &  2         &  1.5       &  0.5      \\
 IoU           &  0.231544 &     0   &  0.0741627 &  0.0717703 &  0.386139 \\
\hline
[2K\end{tabular};6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:03:44 â€¢ 0:00:00[0m [2;4m0.85it/s[0m  
[2K[20.02 13.41  8.48  5.68  3.28]â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:03:44 â€¢ 0:00:00[0m [2;4m0.85it/s[0m  
[2KAverage TIOU:  0.008518114404385881â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:03:44 â€¢ 0:00:00[0m [2;4m0.85it/s[0m  
[2Kpreds_tensor: torch.Size([203, 10])â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:03:44 â€¢ 0:00:00[0m [2;4m0.85it/s[0m  
[2KTop-1 accuracy: 0.8226600985221675â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:03:44 â€¢ 0:00:00[0m [2;4m0.85it/s[0m  
[2Kpreds_tensor: torch.Size([203, 10])â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:03:44 â€¢ 0:00:00[0m [2;4m0.85it/s[0m  
[2KTop-3 accuracy: 0.9458128078817734â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:03:44 â€¢ 0:00:00[0m [2;4m0.85it/s[0m  
[2Kpreds_tensor: torch.Size([203, 10])â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:03:44 â€¢ 0:00:00[0m [2;4m0.85it/s[0m  
[2KTop-5 accuracy: 0.9950738916256158â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:03:44 â€¢ 0:00:00[0m [2;4m0.85it/s[0m  
[2Kâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“â”â”â”[0m 203/203 [2m0:03:44 â€¢ 0:00:00[0m [2;4m0.85it/s[0m  
â”ƒ[1m [0m[1m       Test metric       [0m[1m [0mâ”ƒ[1m [0m[1m      DataLoader 0       [0m[1m [0mâ”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚[36m [0m[36m          AP_0           [0m[36m [0mâ”‚[35m [0m[35m   20.020000457763672    [0m[35m [0mâ”‚
â”‚[36m [0m[36m         avg_AP          [0m[36m [0mâ”‚[35m [0m[35m   10.173999786376953    [0m[35m [0mâ”‚
â”‚[36m [0m[36m  label top-1 accuracy   [0m[36m [0mâ”‚[35m [0m[35m   0.8226600885391235    [0m[35m [0mâ”‚
â”‚[36m [0m[36m  label top-3 accuracy   [0m[36m [0mâ”‚[35m [0m[35m   0.9458128213882446    [0m[35m [0mâ”‚
â”‚[36m [0m[36m  label top-5 accuracy   [0m[36m [0mâ”‚[35m [0m[35m   0.9950739145278931    [0m[35m [0mâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
[2KTesting [38;2;98;6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:03:44 â€¢ 0:00:00[0m [2;4m0.85it/s[0m
[?25h[[36m2025-02-14 00:58:29,039[0m][[34m__main__[0m][[32mINFO[0m] - Best ckpt path: None[0m
[[36m2025-02-14 00:58:29,050[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/def/fewshot/logs/train/runs/2025-02-14_00-54-28[0m
[[36m2025-02-14 00:58:29,051[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
