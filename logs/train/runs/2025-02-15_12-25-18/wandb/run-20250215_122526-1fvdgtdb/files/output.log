[[36m2025-02-15 12:25:27,638[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
You are using a CUDA device ('NVIDIA GeForce RTX 4070 SUPER') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
No of videos in train is 214
Loading train Video Information ...
No of class 10
No of videos in validation is 203
Loading validation Video Information ...
No of class 10
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”“
â”ƒ[1;35m [0m[1;35m   [0m[1;35m [0mâ”ƒ[1;35m [0m[1;35mName                                                   [0m[1;35m [0mâ”ƒ[1;35m [0m[1;35mType                           [0m[1;35m [0mâ”ƒ[1;35m [0m[1;35mParams[0m[1;35m [0mâ”ƒ[1;35m [0m[1;35mMode [0m[1;35m [0mâ”ƒ
â”¡â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”©
â”‚[2m [0m[2m0  [0m[2m [0mâ”‚ net                                                     â”‚ T3ALNet                         â”‚  639 M â”‚ train â”‚
â”‚[2m [0m[2m1  [0m[2m [0mâ”‚ net.model                                               â”‚ CoCa                            â”‚  638 M â”‚ train â”‚
â”‚[2m [0m[2m2  [0m[2m [0mâ”‚ net.model.text                                          â”‚ TextTransformer                 â”‚  123 M â”‚ train â”‚
â”‚[2m [0m[2m3  [0m[2m [0mâ”‚ net.model.text.token_embedding                          â”‚ Embedding                       â”‚ 37.9 M â”‚ train â”‚
â”‚[2m [0m[2m4  [0m[2m [0mâ”‚ net.model.text.transformer                              â”‚ Transformer                     â”‚ 85.1 M â”‚ train â”‚
â”‚[2m [0m[2m5  [0m[2m [0mâ”‚ net.model.text.transformer.resblocks                    â”‚ ModuleList                      â”‚ 85.1 M â”‚ train â”‚
â”‚[2m [0m[2m6  [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m7  [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m8  [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m9  [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m10 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m11 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m12 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m13 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m14 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m15 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m16 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m17 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m18 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m19 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m20 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m21 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m22 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m23 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m24 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m25 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m26 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m27 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m28 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m29 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m30 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m31 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m32 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m33 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m34 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m35 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m36 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m37 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m38 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m39 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m40 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m41 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m42 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m43 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m44 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m45 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m46 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m47 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m48 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m49 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m50 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m51 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m52 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m53 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m54 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m55 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m56 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m57 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m58 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m59 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m60 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m61 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m62 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m63 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m64 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m65 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m66 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m67 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m68 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m69 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m70 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m71 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m72 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m73 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m74 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m75 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m76 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m77 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m78 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m79 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m80 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m81 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m82 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m83 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m84 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m85 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m86 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m87 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m88 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m89 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m90 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m91 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m92 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m93 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m94 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m95 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m96 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m97 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m98 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m99 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m100[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m101[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m102[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m103[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m104[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m105[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m106[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m107[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m108[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m109[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m110[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m111[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m112[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m113[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m114[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m115[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m116[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10                 â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m117[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.ln_1            â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m118[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.attn            â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m119[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.attn.out_proj   â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m120[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.ls_1            â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m121[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.ln_2            â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m122[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.mlp             â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m123[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.mlp.c_fc        â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m124[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.mlp.gelu        â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m125[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.mlp.c_proj      â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m126[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.ls_2            â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m127[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11                 â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m128[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.ln_1            â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m129[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.attn            â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m130[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.attn.out_proj   â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m131[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.ls_1            â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m132[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.ln_2            â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m133[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.mlp             â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m134[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.mlp.c_fc        â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m135[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.mlp.gelu        â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m136[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.mlp.c_proj      â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m137[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.ls_2            â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m138[0m[2m [0mâ”‚ net.model.text.ln_final                                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m139[0m[2m [0mâ”‚ net.model.visual                                        â”‚ VisionTransformer               â”‚  306 M â”‚ train â”‚
â”‚[2m [0m[2m140[0m[2m [0mâ”‚ net.model.visual.conv1                                  â”‚ Conv2d                          â”‚  602 K â”‚ train â”‚
â”‚[2m [0m[2m141[0m[2m [0mâ”‚ net.model.visual.patch_dropout                          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m142[0m[2m [0mâ”‚ net.model.visual.ln_pre                                 â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m143[0m[2m [0mâ”‚ net.model.visual.transformer                            â”‚ Transformer                     â”‚  302 M â”‚ train â”‚
â”‚[2m [0m[2m144[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks                  â”‚ ModuleList                      â”‚  302 M â”‚ train â”‚
â”‚[2m [0m[2m145[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m146[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m147[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m148[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m149[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m150[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m151[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m152[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m153[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m154[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m155[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m156[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m157[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m158[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m159[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m160[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m161[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m162[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m163[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m164[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m165[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m166[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m167[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m168[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m169[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m170[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m171[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m172[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m173[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m174[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m175[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m176[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m177[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m178[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m179[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m180[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m181[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m182[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m183[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m184[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m185[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m186[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m187[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m188[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m189[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m190[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m191[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m192[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m193[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m194[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m195[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m196[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m197[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m198[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m199[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m200[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m201[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m202[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m203[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m204[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m205[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m206[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m207[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m208[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m209[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m210[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m211[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m212[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m213[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m214[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m215[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m216[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m217[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m218[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m219[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m220[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m221[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m222[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m223[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m224[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m225[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m226[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m227[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m228[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m229[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m230[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m231[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m232[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m233[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m234[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m235[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m236[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m237[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m238[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m239[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m240[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m241[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m242[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m243[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m244[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m245[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m246[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m247[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m248[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m249[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m250[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m251[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m252[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m253[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m254[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m255[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m256[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m257[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m258[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m259[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m260[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m261[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m262[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m263[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m264[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m265[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m266[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m267[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m268[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m269[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m270[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m271[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m272[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m273[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m274[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m275[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m276[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m277[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m278[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m279[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m280[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m281[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m282[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m283[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m284[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m285[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m286[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m287[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m288[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m289[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m290[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m291[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m292[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m293[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m294[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m295[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m296[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m297[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m298[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m299[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m300[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m301[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m302[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m303[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m304[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m305[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m306[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m307[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m308[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m309[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m310[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m311[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m312[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m313[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m314[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m315[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m316[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m317[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m318[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m319[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m320[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m321[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m322[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m323[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m324[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m325[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m326[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m327[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m328[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m329[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m330[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m331[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m332[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m333[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m334[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m335[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m336[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m337[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m338[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m339[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m340[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m341[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m342[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m343[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m344[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m345[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m346[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m347[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m348[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m349[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m350[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m351[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m352[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m353[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m354[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m355[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m356[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m357[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m358[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m359[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m360[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m361[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m362[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m363[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m364[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m365[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m366[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m367[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m368[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m369[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m370[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m371[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m372[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m373[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m374[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m375[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m376[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m377[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m378[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m379[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m380[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m381[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m382[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m383[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m384[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m385[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m386[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m387[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m388[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m389[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m390[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m391[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m392[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m393[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m394[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m395[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m396[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m397[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m398[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m399[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m400[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m401[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m402[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m403[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m404[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m405[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m406[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m407[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m408[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m409[0m[2m [0mâ”‚ net.model.visual.attn_pool                              â”‚ AttentionalPooler               â”‚  3.0 M â”‚ train â”‚
â”‚[2m [0m[2m410[0m[2m [0mâ”‚ net.model.visual.attn_pool.attn                         â”‚ MultiheadAttention              â”‚  2.8 M â”‚ train â”‚
â”‚[2m [0m[2m411[0m[2m [0mâ”‚ net.model.visual.attn_pool.attn.out_proj                â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m412[0m[2m [0mâ”‚ net.model.visual.attn_pool.ln_q                         â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m413[0m[2m [0mâ”‚ net.model.visual.attn_pool.ln_k                         â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m414[0m[2m [0mâ”‚ net.model.visual.ln_post                                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m415[0m[2m [0mâ”‚ net.model.text_decoder                                  â”‚ MultimodalTransformer           â”‚  208 M â”‚ train â”‚
â”‚[2m [0m[2m416[0m[2m [0mâ”‚ net.model.text_decoder.resblocks                        â”‚ ModuleList                      â”‚ 85.1 M â”‚ train â”‚
â”‚[2m [0m[2m417[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m418[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m419[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m420[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m421[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m422[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m423[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m424[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m425[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m426[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m427[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m428[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m429[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m430[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m431[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m432[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m433[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m434[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m435[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m436[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m437[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m438[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m439[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m440[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m441[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m442[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m443[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m444[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m445[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m446[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m447[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m448[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m449[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m450[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m451[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m452[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m453[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m454[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m455[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m456[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m457[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m458[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m459[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m460[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m461[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m462[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m463[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m464[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m465[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m466[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m467[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m468[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m469[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m470[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m471[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m472[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m473[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m474[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m475[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m476[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m477[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m478[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m479[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m480[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m481[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m482[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m483[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m484[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m485[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m486[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m487[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m488[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m489[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m490[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m491[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m492[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m493[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m494[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m495[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m496[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m497[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m498[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m499[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m500[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m501[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m502[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m503[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m504[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m505[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m506[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m507[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m508[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m509[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m510[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m511[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m512[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m513[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m514[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m515[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m516[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m517[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m518[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m519[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m520[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m521[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m522[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m523[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m524[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m525[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m526[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m527[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m528[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m529[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m530[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m531[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m532[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m533[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m534[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m535[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m536[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m537[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m538[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m539[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m540[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m541[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m542[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m543[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m544[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m545[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m546[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m547[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m548[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m549[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn                       â”‚ ModuleList                      â”‚ 85.1 M â”‚ train â”‚
â”‚[2m [0m[2m550[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m551[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m552[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m553[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m554[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m555[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m556[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m557[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m558[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m559[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m560[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m561[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m562[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m563[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m564[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m565[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m566[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m567[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m568[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m569[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m570[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m571[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m572[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m573[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m574[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m575[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m576[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m577[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m578[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m579[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m580[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m581[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m582[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m583[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m584[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m585[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m586[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m587[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m588[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m589[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m590[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m591[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m592[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m593[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m594[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m595[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m596[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m597[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m598[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m599[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m600[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m601[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m602[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m603[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m604[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m605[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m606[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m607[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m608[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m609[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m610[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m611[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m612[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m613[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m614[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m615[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m616[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m617[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m618[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m619[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m620[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m621[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m622[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m623[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m624[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m625[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m626[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m627[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m628[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m629[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m630[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m631[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m632[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m633[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m634[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m635[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m636[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m637[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m638[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m639[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m640[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m641[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m642[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m643[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m644[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m645[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m646[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m647[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m648[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m649[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m650[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m651[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m652[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m653[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m654[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m655[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m656[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m657[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m658[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m659[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m660[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m661[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m662[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m663[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m664[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m665[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m666[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m667[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m668[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m669[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m670[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10                    â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m671[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.ln_1               â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m672[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.attn               â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m673[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.attn.out_proj      â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m674[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.ls_1               â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m675[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.ln_1_kv            â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m676[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.ln_2               â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m677[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.mlp                â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m678[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.mlp.c_fc           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m679[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.mlp.gelu           â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m680[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.mlp.c_proj         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m681[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.ls_2               â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m682[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11                    â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m683[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.ln_1               â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m684[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.attn               â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m685[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.attn.out_proj      â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m686[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.ls_1               â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m687[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.ln_1_kv            â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m688[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.ln_2               â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m689[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.mlp                â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m690[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.mlp.c_fc           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m691[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.mlp.gelu           â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m692[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.mlp.c_proj         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m693[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.ls_2               â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m694[0m[2m [0mâ”‚ net.model.text_decoder.ln_final                         â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m695[0m[2m [0mâ”‚ net.tta_loss                                            â”‚ ByolLoss                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m696[0m[2m [0mâ”‚ net.video_proj                                          â”‚ VideoProjector                  â”‚  591 K â”‚ train â”‚
â”‚[2m [0m[2m697[0m[2m [0mâ”‚ net.video_proj.transform                                â”‚ Sequential                      â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m698[0m[2m [0mâ”‚ net.video_proj.transform.0                              â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m699[0m[2m [0mâ”‚ net.video_proj.transform.1                              â”‚ Dropout                         â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m700[0m[2m [0mâ”‚ net.fusion                                              â”‚ Fusion                          â”‚  6.2 K â”‚ train â”‚
â”‚[2m [0m[2m701[0m[2m [0mâ”‚ net.fusion.attn                                         â”‚ Sequential                      â”‚  6.2 K â”‚ train â”‚
â”‚[2m [0m[2m702[0m[2m [0mâ”‚ net.fusion.attn.0                                       â”‚ Linear                          â”‚  6.1 K â”‚ train â”‚
â”‚[2m [0m[2m703[0m[2m [0mâ”‚ net.fusion.attn.1                                       â”‚ Linear                          â”‚     10 â”‚ train â”‚
â”‚[2m [0m[2m704[0m[2m [0mâ”‚ net.fusion.attn.2                                       â”‚ Softmax                         â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m705[0m[2m [0mâ”‚ binary_acc                                              â”‚ BinaryAccuracy                  â”‚      0 â”‚ train â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜
[1mTrainable params[0m: 1.2 M
[1mNon-trainable params[0m: 637 M
[1mTotal params[0m: 639 M
[1mTotal estimated model params size (MB)[0m: 2.6 K
[1mModules in train mode[0m: 706
[1mModules in eval mode[0m: 0
/home/def/miniforge3/envs/fewshot/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.
`Trainer.fit` stopped: `max_epochs=0` reached.
[[36m2025-02-15 12:25:29,293[0m][[34m__main__[0m][[32mINFO[0m] - Starting testing![0m
[[36m2025-02-15 12:25:29,293[0m][[34m__main__[0m][[33mWARNING[0m] - Best ckpt not found! Using current weights for testing...[0m
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/def/miniforge3/envs/fewshot/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.
[2KStart testing...
[2KAttention weights: tensor([[0.5592, 0.4408],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 0/203 [2m0:00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
        [0.5551, 0.4449],
        [0.5531, 0.4469],
        [0.5589, 0.4411],
[2K        [0.5552, 0.4448]], device='cuda:0', grad_fn=<SoftmaxBackward0>):00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2K/home/def/fewshot/src/models/components/tt_method.py:317: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or
`x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3683.)
  dot_product = (x @ y.T)
[2KClass label: HammerThrowâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 0/203 [2m0:00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5594, 0.4406]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KStep 0 - TTA Loss: 0.7116â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 0/203 [2m0:00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5624, 0.4376]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5642, 0.4358]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5668, 0.4332]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5686, 0.4314]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5706, 0.4294]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5720, 0.4280]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5730, 0.4270]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5735, 0.4265]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5738, 0.4262]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5556, 0.4444]], device='cuda:0')203 [2m0:00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.6501, 0.3499],â”â”â”â”â”â”â”â”â”â”â”[0m 0/203 [2m0:00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
        [0.5738, 0.4262],
        [0.5741, 0.4259],
        [0.6755, 0.3245],
[2K        [0.6365, 0.3635]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”[0m 0/203 [2m0:00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5592, 0.4408],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 1/203 [2m0:00:03 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
        [0.5550, 0.4450],
        [0.5531, 0.4469],
        [0.5589, 0.4411],
[2K        [0.5552, 0.4448]], device='cuda:0', grad_fn=<SoftmaxBackward0>):00:03 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KClass label: HammerThrowâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 1/203 [2m0:00:03 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5559, 0.4441]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KStep 0 - TTA Loss: 0.9387â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 1/203 [2m0:00:03 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5559, 0.4441]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5559, 0.4441]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5560, 0.4440]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5562, 0.4438]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5567, 0.4433]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5575, 0.4425]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5588, 0.4412]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5603, 0.4397]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5621, 0.4379]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5551, 0.4449]], device='cuda:0')203 [2m0:00:03 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.6430, 0.3570],â”â”â”â”â”â”â”â”â”â”â”[0m 1/203 [2m0:00:03 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
        [0.5641, 0.4359],
        [0.5668, 0.4332],
        [0.6696, 0.3304],
[2K        [0.6283, 0.3717]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”[0m 1/203 [2m0:00:03 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5591, 0.4409],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 2/203 [2m0:00:04 â€¢ 0:05:17[0m [2;4m0.64it/s[0m  
        [0.5549, 0.4451],
        [0.5530, 0.4470],
        [0.5588, 0.4412],
[2K        [0.5551, 0.4449]], device='cuda:0', grad_fn=<SoftmaxBackward0>):00:04 â€¢ 0:05:17[0m [2;4m0.64it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 2/203 [2m0:00:04 â€¢ 0:05:17[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6259, 0.3741]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.64it/s[0m  
[2KStep 0 - TTA Loss: 0.7807â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 2/203 [2m0:00:04 â€¢ 0:05:17[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6264, 0.3736]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6264, 0.3736]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6263, 0.3737]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6265, 0.3735]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6265, 0.3735]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6266, 0.3734]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6267, 0.3733]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6267, 0.3733]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6267, 0.3733]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5552, 0.4448]], device='cuda:0')203 [2m0:00:04 â€¢ 0:05:17[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6419, 0.3581],â”â”â”â”â”â”â”â”â”â”â”[0m 2/203 [2m0:00:04 â€¢ 0:05:17[0m [2;4m0.64it/s[0m  
        [0.5641, 0.4359],
        [0.5661, 0.4339],
        [0.6680, 0.3320],
[2K        [0.6267, 0.3733]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”[0m 2/203 [2m0:00:04 â€¢ 0:05:17[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5589, 0.4411],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 3/203 [2m0:00:05 â€¢ 0:04:23[0m [2;4m0.76it/s[0m  
        [0.5547, 0.4453],
        [0.5528, 0.4472],
        [0.5587, 0.4413],
[2K        [0.5549, 0.4451]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”[0m 3/203 [2m0:00:05 â€¢ 0:04:23[0m [2;4m0.76it/s[0m  
[2KClass label: PoleVault[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 3/203 [2m0:00:05 â€¢ 0:04:23[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.5628, 0.4372]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:23[0m [2;4m0.76it/s[0m  
[2KStep 0 - TTA Loss: 0.6670[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 3/203 [2m0:00:05 â€¢ 0:04:23[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.5628, 0.4372]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:23[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.5628, 0.4372]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:23[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.5630, 0.4370]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:23[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.5637, 0.4363]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:23[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.5650, 0.4350]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:23[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.5671, 0.4329]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:23[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.5702, 0.4298]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:23[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.5741, 0.4259]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:23[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.5787, 0.4213]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:23[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.5536, 0.4464]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 3/203 [2m0:00:05 â€¢ 0:04:23[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.6478, 0.3522],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 3/203 [2m0:00:05 â€¢ 0:04:23[0m [2;4m0.76it/s[0m  
        [0.5711, 0.4289],
        [0.5839, 0.4161],
        [0.6771, 0.3229],
[2K        [0.6377, 0.3623]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 3/203 [2m0:00:05 â€¢ 0:04:23[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.5589, 0.4411],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 4/203 [2m0:00:12 â€¢ 0:10:17[0m [2;4m0.32it/s[0m  
        [0.5546, 0.4454],
        [0.5528, 0.4472],
        [0.5587, 0.4413],
[2K        [0.5548, 0.4452]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”[0m 4/203 [2m0:00:12 â€¢ 0:10:17[0m [2;4m0.32it/s[0m  
[2KClass label: BaseballPitch[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 4/203 [2m0:00:12 â€¢ 0:10:17[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.6424, 0.3576]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:12 â€¢ 0:10:17[0m [2;4m0.32it/s[0m  
[2KStep 0 - TTA Loss: 1.2463[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 4/203 [2m0:00:12 â€¢ 0:10:17[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.6449, 0.3551]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:12 â€¢ 0:10:17[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.6474, 0.3526]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:12 â€¢ 0:10:17[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.6500, 0.3500]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:12 â€¢ 0:10:17[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.6522, 0.3478]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:12 â€¢ 0:10:17[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.6545, 0.3455]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:12 â€¢ 0:10:17[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.6562, 0.3438]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:12 â€¢ 0:10:17[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.6575, 0.3425]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:12 â€¢ 0:10:17[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.6585, 0.3415]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:12 â€¢ 0:10:17[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.6589, 0.3411]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:12 â€¢ 0:10:17[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.5597, 0.4403]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 4/203 [2m0:00:12 â€¢ 0:10:17[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.6591, 0.3409],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 4/203 [2m0:00:12 â€¢ 0:10:17[0m [2;4m0.32it/s[0m  
        [0.5792, 0.4208],
        [0.5906, 0.4094],
        [0.6821, 0.3179],
[2K        [0.6454, 0.3546]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 4/203 [2m0:00:12 â€¢ 0:10:17[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.5588, 0.4412],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 5/203 [2m0:00:13 â€¢ 0:08:14[0m [2;4m0.40it/s[0m  
        [0.5546, 0.4454],
        [0.5527, 0.4473],
        [0.5586, 0.4414],
[2K        [0.5547, 0.4453]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”[0m 5/203 [2m0:00:13 â€¢ 0:08:14[0m [2;4m0.40it/s[0m  
[2KClass label: HammerThrowm[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 5/203 [2m0:00:13 â€¢ 0:08:14[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.5617, 0.4383]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:13 â€¢ 0:08:14[0m [2;4m0.40it/s[0m  
[2KStep 0 - TTA Loss: 0.6117[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 5/203 [2m0:00:13 â€¢ 0:08:14[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.5617, 0.4383]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:13 â€¢ 0:08:14[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.5617, 0.4383]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:13 â€¢ 0:08:14[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.5619, 0.4381]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:13 â€¢ 0:08:14[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.5622, 0.4378]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:13 â€¢ 0:08:14[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.5626, 0.4374]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:13 â€¢ 0:08:14[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.5632, 0.4368]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:13 â€¢ 0:08:14[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.5637, 0.4363]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:13 â€¢ 0:08:14[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.5643, 0.4357]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:13 â€¢ 0:08:14[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.5648, 0.4352]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:13 â€¢ 0:08:14[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.5549, 0.4451]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 5/203 [2m0:00:13 â€¢ 0:08:14[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.6493, 0.3507],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 5/203 [2m0:00:13 â€¢ 0:08:14[0m [2;4m0.40it/s[0m  
        [0.5652, 0.4348],
        [0.5717, 0.4283],
        [0.6726, 0.3274],
[2K        [0.6304, 0.3696]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 5/203 [2m0:00:13 â€¢ 0:08:14[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.5588, 0.4412],m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:14 â€¢ 0:07:47[0m [2;4m0.42it/s[0m  
        [0.5546, 0.4454],
        [0.5528, 0.4472],
        [0.5587, 0.4413],
[2K        [0.5548, 0.4452]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:14 â€¢ 0:07:47[0m [2;4m0.42it/s[0m  
[2KClass label: HammerThrowm[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:14 â€¢ 0:07:47[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.5509, 0.4491]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:14 â€¢ 0:07:47[0m [2;4m0.42it/s[0m  
[2KStep 0 - TTA Loss: 0.6297[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:14 â€¢ 0:07:47[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.5511, 0.4489]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:14 â€¢ 0:07:47[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.5517, 0.4483]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:14 â€¢ 0:07:47[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.5525, 0.4475]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:14 â€¢ 0:07:47[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.5535, 0.4465]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:14 â€¢ 0:07:47[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.5543, 0.4457]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:14 â€¢ 0:07:47[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.5550, 0.4450]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:14 â€¢ 0:07:47[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.5556, 0.4444]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:14 â€¢ 0:07:47[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.5558, 0.4442]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:14 â€¢ 0:07:47[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.5560, 0.4440]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:14 â€¢ 0:07:47[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.5547, 0.4453]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:14 â€¢ 0:07:47[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.6404, 0.3596],8;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:14 â€¢ 0:07:47[0m [2;4m0.42it/s[0m  
        [0.5560, 0.4440],
        [0.5626, 0.4374],
        [0.6642, 0.3358],
[2K        [0.6230, 0.3770]], device='cuda:0')38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:14 â€¢ 0:07:47[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.5589, 0.4411],m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:23 â€¢ 0:10:58[0m [2;4m0.30it/s[0m  
        [0.5547, 0.4453],
        [0.5528, 0.4472],
        [0.5587, 0.4413],
[2K        [0.5549, 0.4451]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:23 â€¢ 0:10:58[0m [2;4m0.30it/s[0m  
[2KClass label: SoccerPenalty[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:23 â€¢ 0:10:58[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.6627, 0.3373]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:23 â€¢ 0:10:58[0m [2;4m0.30it/s[0m  
[2KStep 0 - TTA Loss: 1.1582[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:23 â€¢ 0:10:58[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.6627, 0.3373]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:23 â€¢ 0:10:58[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.6626, 0.3374]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:23 â€¢ 0:10:58[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.6622, 0.3378]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:23 â€¢ 0:10:58[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.6610, 0.3390]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:23 â€¢ 0:10:58[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.6586, 0.3414]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:23 â€¢ 0:10:58[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.6546, 0.3454]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:23 â€¢ 0:10:58[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.6488, 0.3512]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:23 â€¢ 0:10:58[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.6412, 0.3588]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:23 â€¢ 0:10:58[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.6320, 0.3680]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:23 â€¢ 0:10:58[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.5570, 0.4430]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:23 â€¢ 0:10:58[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.6156, 0.3844],8;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:23 â€¢ 0:10:58[0m [2;4m0.30it/s[0m  
        [0.5378, 0.4622],
        [0.5446, 0.4554],
        [0.6214, 0.3786],
[2K        [0.6091, 0.3909]], device='cuda:0')38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:23 â€¢ 0:10:58[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.5589, 0.4411],mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:25 â€¢ 0:10:15[0m [2;4m0.32it/s[0m  
        [0.5548, 0.4452],
        [0.5528, 0.4472],
        [0.5587, 0.4413],
[2K        [0.5549, 0.4451]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:25 â€¢ 0:10:15[0m [2;4m0.32it/s[0m  
[2KClass label: BaseballPitch[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:25 â€¢ 0:10:15[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.6381, 0.3619]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:25 â€¢ 0:10:15[0m [2;4m0.32it/s[0m  
[2KStep 0 - TTA Loss: 0.5708[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:25 â€¢ 0:10:15[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.6340, 0.3660]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:25 â€¢ 0:10:15[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.6314, 0.3686]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:25 â€¢ 0:10:15[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.6304, 0.3696]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:25 â€¢ 0:10:15[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.6300, 0.3700]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:25 â€¢ 0:10:15[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.6300, 0.3700]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:25 â€¢ 0:10:15[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.6306, 0.3694]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:25 â€¢ 0:10:15[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.6312, 0.3688]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:25 â€¢ 0:10:15[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.6317, 0.3683]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:25 â€¢ 0:10:15[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.6321, 0.3679]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:25 â€¢ 0:10:15[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.5586, 0.4414]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:25 â€¢ 0:10:15[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.6322, 0.3678],0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:25 â€¢ 0:10:15[0m [2;4m0.32it/s[0m  
        [0.5458, 0.4542],
        [0.5509, 0.4491],
        [0.6345, 0.3655],
[2K        [0.6157, 0.3843]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:25 â€¢ 0:10:15[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.5589, 0.4411],mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 9/203 [2m0:00:26 â€¢ 0:09:30[0m [2;4m0.34it/s[0m  
        [0.5547, 0.4453],
        [0.5528, 0.4472],
        [0.5586, 0.4414],
[2K        [0.5548, 0.4452]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 9/203 [2m0:00:26 â€¢ 0:09:30[0m [2;4m0.34it/s[0m  
[2KClass label: BaseballPitch[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 9/203 [2m0:00:26 â€¢ 0:09:30[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6396, 0.3604]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:26 â€¢ 0:09:30[0m [2;4m0.34it/s[0m  
[2KStep 0 - TTA Loss: 0.5471[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 9/203 [2m0:00:26 â€¢ 0:09:30[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6396, 0.3604]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:26 â€¢ 0:09:30[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6397, 0.3603]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:26 â€¢ 0:09:30[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6401, 0.3599]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:26 â€¢ 0:09:30[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6411, 0.3589]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:26 â€¢ 0:09:30[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6426, 0.3574]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:26 â€¢ 0:09:30[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6448, 0.3552]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:26 â€¢ 0:09:30[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6478, 0.3522]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:26 â€¢ 0:09:30[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6509, 0.3491]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:26 â€¢ 0:09:30[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6546, 0.3454]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:26 â€¢ 0:09:30[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.5595, 0.4405]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 9/203 [2m0:00:26 â€¢ 0:09:30[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6583, 0.3417],0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 9/203 [2m0:00:26 â€¢ 0:09:30[0m [2;4m0.34it/s[0m  
        [0.5665, 0.4335],
        [0.5657, 0.4343],
        [0.6702, 0.3298],
[2K        [0.6309, 0.3691]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 9/203 [2m0:00:26 â€¢ 0:09:30[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.5588, 0.4412],mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 10/203 [2m0:00:27 â€¢ 0:08:42[0m [2;4m0.37it/s[0m  
        [0.5547, 0.4453],
        [0.5528, 0.4472],
        [0.5586, 0.4414],
[2K        [0.5548, 0.4452]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 10/203 [2m0:00:27 â€¢ 0:08:42[0m [2;4m0.37it/s[0m  
[2KClass label: ThrowDiscusm[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 10/203 [2m0:00:27 â€¢ 0:08:42[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.6250, 0.3750]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:27 â€¢ 0:08:42[0m [2;4m0.37it/s[0m  
[2KStep 0 - TTA Loss: 0.8062[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 10/203 [2m0:00:27 â€¢ 0:08:42[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.6265, 0.3735]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:27 â€¢ 0:08:42[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.6274, 0.3726]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:27 â€¢ 0:08:42[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.6280, 0.3720]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:27 â€¢ 0:08:42[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.6281, 0.3719]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:27 â€¢ 0:08:42[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.6281, 0.3719]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:27 â€¢ 0:08:42[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.6278, 0.3722]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:27 â€¢ 0:08:42[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.6274, 0.3726]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:27 â€¢ 0:08:42[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.6269, 0.3731]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:27 â€¢ 0:08:42[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.6267, 0.3733]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:27 â€¢ 0:08:42[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5552, 0.4448]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 10/203 [2m0:00:27 â€¢ 0:08:42[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.6514, 0.3486],0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 10/203 [2m0:00:27 â€¢ 0:08:42[0m [2;4m0.37it/s[0m  
        [0.5636, 0.4364],
        [0.5662, 0.4338],
        [0.6680, 0.3320],
[2K        [0.6267, 0.3733]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 10/203 [2m0:00:27 â€¢ 0:08:42[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5587, 0.4413],0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 11/203 [2m0:00:28 â€¢ 0:08:03[0m [2;4m0.40it/s[0m   
        [0.5547, 0.4453],
        [0.5528, 0.4472],
        [0.5585, 0.4415],
[2K        [0.5548, 0.4452]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 11/203 [2m0:00:28 â€¢ 0:08:03[0m [2;4m0.40it/s[0m  
[2KClass label: HammerThrow0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 11/203 [2m0:00:28 â€¢ 0:08:03[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.5563, 0.4437]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:28 â€¢ 0:08:03[0m [2;4m0.40it/s[0m  
[2KStep 0 - TTA Loss: 0.6160m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 11/203 [2m0:00:28 â€¢ 0:08:03[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.5563, 0.4437]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:28 â€¢ 0:08:03[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.5563, 0.4437]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:28 â€¢ 0:08:03[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.5563, 0.4437]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:28 â€¢ 0:08:03[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.5564, 0.4436]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:28 â€¢ 0:08:03[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.5567, 0.4433]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:28 â€¢ 0:08:03[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.5574, 0.4426]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:28 â€¢ 0:08:03[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.5585, 0.4415]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:28 â€¢ 0:08:03[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.5599, 0.4401]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:28 â€¢ 0:08:03[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.5616, 0.4384]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:28 â€¢ 0:08:03[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.5549, 0.4451]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 11/203 [2m0:00:28 â€¢ 0:08:03[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.6426, 0.3574],38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 11/203 [2m0:00:28 â€¢ 0:08:03[0m [2;4m0.40it/s[0m  
        [0.5633, 0.4367],
        [0.5633, 0.4367],
        [0.6683, 0.3317],
[2K        [0.6253, 0.3747]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 11/203 [2m0:00:28 â€¢ 0:08:03[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.5587, 0.4413],0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 12/203 [2m0:00:29 â€¢ 0:07:41[0m [2;4m0.41it/s[0m  
        [0.5547, 0.4453],
        [0.5528, 0.4472],
        [0.5585, 0.4415],
[2K        [0.5548, 0.4452]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 12/203 [2m0:00:29 â€¢ 0:07:41[0m [2;4m0.41it/s[0m  
[2KClass label: ThrowDiscus0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 12/203 [2m0:00:29 â€¢ 0:07:41[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.6284, 0.3716]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:29 â€¢ 0:07:41[0m [2;4m0.41it/s[0m  
[2KStep 0 - TTA Loss: 1.2709m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 12/203 [2m0:00:29 â€¢ 0:07:41[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.6287, 0.3713]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:29 â€¢ 0:07:41[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.6285, 0.3715]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:29 â€¢ 0:07:41[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.6280, 0.3720]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:29 â€¢ 0:07:41[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.6275, 0.3725]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:29 â€¢ 0:07:41[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.6268, 0.3732]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:29 â€¢ 0:07:41[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.6263, 0.3737]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:29 â€¢ 0:07:41[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.6258, 0.3742]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:29 â€¢ 0:07:41[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.6255, 0.3745]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:29 â€¢ 0:07:41[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.6253, 0.3747]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:29 â€¢ 0:07:41[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.5551, 0.4449]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 12/203 [2m0:00:29 â€¢ 0:07:41[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.6445, 0.3555],38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 12/203 [2m0:00:29 â€¢ 0:07:41[0m [2;4m0.41it/s[0m  
        [0.5681, 0.4319],
        [0.5652, 0.4348],
        [0.6727, 0.3273],
[2K        [0.6253, 0.3747]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 12/203 [2m0:00:29 â€¢ 0:07:41[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.5587, 0.4413],4mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 13/203 [2m0:00:30 â€¢ 0:07:13[0m [2;4m0.44it/s[0m  
        [0.5547, 0.4453],
        [0.5528, 0.4472],
        [0.5585, 0.4415],
[2K        [0.5548, 0.4452]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 13/203 [2m0:00:30 â€¢ 0:07:13[0m [2;4m0.44it/s[0m  
[2KClass label: BaseballPitch[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 13/203 [2m0:00:30 â€¢ 0:07:13[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.6384, 0.3616]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:30 â€¢ 0:07:13[0m [2;4m0.44it/s[0m  
[2KStep 0 - TTA Loss: 0.6222m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 13/203 [2m0:00:30 â€¢ 0:07:13[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.6384, 0.3616]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:30 â€¢ 0:07:13[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.6385, 0.3615]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:30 â€¢ 0:07:13[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.6393, 0.3607]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:30 â€¢ 0:07:13[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.6415, 0.3585]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:30 â€¢ 0:07:13[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.6457, 0.3543]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:30 â€¢ 0:07:13[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.6527, 0.3473]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:30 â€¢ 0:07:13[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.6625, 0.3375]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:30 â€¢ 0:07:13[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.6750, 0.3250]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:30 â€¢ 0:07:13[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.6896, 0.3104]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:30 â€¢ 0:07:13[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.5614, 0.4386]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 13/203 [2m0:00:30 â€¢ 0:07:13[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.7052, 0.2948],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 13/203 [2m0:00:30 â€¢ 0:07:13[0m [2;4m0.44it/s[0m  
        [0.5989, 0.4011],
        [0.5891, 0.4109],
        [0.7028, 0.2972],
[2K        [0.6535, 0.3465]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 13/203 [2m0:00:30 â€¢ 0:07:13[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.5588, 0.4412],4mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:31 â€¢ 0:06:56[0m [2;4m0.46it/s[0m  
        [0.5547, 0.4453],
        [0.5528, 0.4472],
        [0.5586, 0.4414],
[2K        [0.5548, 0.4452]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:31 â€¢ 0:06:56[0m [2;4m0.46it/s[0m  
[2KClass label: ThrowDiscus0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:31 â€¢ 0:06:56[0m [2;4m0.46it/s[0m  
[2KAttention weights: tensor([[0.6252, 0.3748]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:31 â€¢ 0:06:56[0m [2;4m0.46it/s[0m  
[2KStep 0 - TTA Loss: 0.9173m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:31 â€¢ 0:06:56[0m [2;4m0.46it/s[0m  
[2KAttention weights: tensor([[0.6323, 0.3677]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:31 â€¢ 0:06:56[0m [2;4m0.46it/s[0m  
[2KAttention weights: tensor([[0.6385, 0.3615]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:31 â€¢ 0:06:56[0m [2;4m0.46it/s[0m  
[2KAttention weights: tensor([[0.6438, 0.3562]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:31 â€¢ 0:06:56[0m [2;4m0.46it/s[0m  
[2KAttention weights: tensor([[0.6479, 0.3521]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:31 â€¢ 0:06:56[0m [2;4m0.46it/s[0m  
[2KAttention weights: tensor([[0.6510, 0.3490]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:31 â€¢ 0:06:56[0m [2;4m0.46it/s[0m  
[2KAttention weights: tensor([[0.6531, 0.3469]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:31 â€¢ 0:06:56[0m [2;4m0.46it/s[0m  
[2KAttention weights: tensor([[0.6545, 0.3455]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:31 â€¢ 0:06:56[0m [2;4m0.46it/s[0m  
[2KAttention weights: tensor([[0.6553, 0.3447]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:31 â€¢ 0:06:56[0m [2;4m0.46it/s[0m  
[2KAttention weights: tensor([[0.6556, 0.3444]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:31 â€¢ 0:06:56[0m [2;4m0.46it/s[0m  
[2KAttention weights: tensor([[0.5564, 0.4436]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:31 â€¢ 0:06:56[0m [2;4m0.46it/s[0m  
[2KAttention weights: tensor([[0.7054, 0.2946],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:31 â€¢ 0:06:56[0m [2;4m0.46it/s[0m  
        [0.6000, 0.4000],
        [0.5921, 0.4079],
        [0.7030, 0.2970],
[2K        [0.6558, 0.3442]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:31 â€¢ 0:06:56[0m [2;4m0.46it/s[0m  
[2KAttention weights: tensor([[0.5589, 0.4411],4mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 15/203 [2m0:00:32 â€¢ 0:06:35[0m [2;4m0.48it/s[0m  
        [0.5548, 0.4452],
        [0.5529, 0.4471],
        [0.5587, 0.4413],
[2K        [0.5549, 0.4451]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 15/203 [2m0:00:32 â€¢ 0:06:35[0m [2;4m0.48it/s[0m  
[2KClass label: PoleVault[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 15/203 [2m0:00:32 â€¢ 0:06:35[0m [2;4m0.48it/s[0m  
[2KAttention weights: tensor([[0.5636, 0.4364]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:32 â€¢ 0:06:35[0m [2;4m0.48it/s[0m  
[2KStep 0 - TTA Loss: 1.2590m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 15/203 [2m0:00:32 â€¢ 0:06:35[0m [2;4m0.48it/s[0m  
[2KAttention weights: tensor([[0.5636, 0.4364]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:32 â€¢ 0:06:35[0m [2;4m0.48it/s[0m  
[2KAttention weights: tensor([[0.5637, 0.4363]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:32 â€¢ 0:06:35[0m [2;4m0.48it/s[0m  
[2KAttention weights: tensor([[0.5640, 0.4360]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:32 â€¢ 0:06:35[0m [2;4m0.48it/s[0m  
[2KAttention weights: tensor([[0.5648, 0.4352]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:32 â€¢ 0:06:35[0m [2;4m0.48it/s[0m  
[2KAttention weights: tensor([[0.5661, 0.4339]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:32 â€¢ 0:06:35[0m [2;4m0.48it/s[0m  
[2KAttention weights: tensor([[0.5678, 0.4322]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:32 â€¢ 0:06:35[0m [2;4m0.48it/s[0m  
[2KAttention weights: tensor([[0.5700, 0.4300]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:32 â€¢ 0:06:35[0m [2;4m0.48it/s[0m  
[2KAttention weights: tensor([[0.5725, 0.4275]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:32 â€¢ 0:06:35[0m [2;4m0.48it/s[0m  
[2KAttention weights: tensor([[0.5755, 0.4245]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:32 â€¢ 0:06:35[0m [2;4m0.48it/s[0m  
[2KAttention weights: tensor([[0.5535, 0.4465]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 15/203 [2m0:00:32 â€¢ 0:06:35[0m [2;4m0.48it/s[0m  
[2KAttention weights: tensor([[0.6559, 0.3441],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 15/203 [2m0:00:32 â€¢ 0:06:35[0m [2;4m0.48it/s[0m  
        [0.5745, 0.4255],
        [0.5785, 0.4215],
        [0.6769, 0.3231],
[2K        [0.6404, 0.3596]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 15/203 [2m0:00:32 â€¢ 0:06:35[0m [2;4m0.48it/s[0m  
[2KAttention weights: tensor([[0.5589, 0.4411],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 16/203 [2m0:00:33 â€¢ 0:06:13[0m [2;4m0.50it/s[0m   
        [0.5548, 0.4452],
        [0.5528, 0.4472],
        [0.5587, 0.4413],
[2K        [0.5549, 0.4451]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 16/203 [2m0:00:33 â€¢ 0:06:13[0m [2;4m0.50it/s[0m  
[2KClass label: PoleVaultâ”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 16/203 [2m0:00:33 â€¢ 0:06:13[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.5612, 0.4388]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:33 â€¢ 0:06:13[0m [2;4m0.50it/s[0m  
[2KStep 0 - TTA Loss: 0.76730m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 16/203 [2m0:00:33 â€¢ 0:06:13[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.5641, 0.4359]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:33 â€¢ 0:06:13[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.5670, 0.4330]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:33 â€¢ 0:06:13[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.5694, 0.4306]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:33 â€¢ 0:06:13[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.5715, 0.4285]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:33 â€¢ 0:06:13[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.5732, 0.4268]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:33 â€¢ 0:06:13[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.5745, 0.4255]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:33 â€¢ 0:06:13[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.5754, 0.4246]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:33 â€¢ 0:06:13[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.5760, 0.4240]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:33 â€¢ 0:06:13[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.5762, 0.4238]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:33 â€¢ 0:06:13[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.5534, 0.4466]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 16/203 [2m0:00:33 â€¢ 0:06:13[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.6503, 0.3497],[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 16/203 [2m0:00:33 â€¢ 0:06:13[0m [2;4m0.50it/s[0m  
        [0.5701, 0.4299],
        [0.5763, 0.4237],
        [0.6734, 0.3266],
[2K        [0.6341, 0.3659]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 16/203 [2m0:00:33 â€¢ 0:06:13[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.5589, 0.4411],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 17/203 [2m0:00:34 â€¢ 0:06:07[0m [2;4m0.51it/s[0m  
        [0.5548, 0.4452],
        [0.5528, 0.4472],
        [0.5587, 0.4413],
[2K        [0.5549, 0.4451]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 17/203 [2m0:00:34 â€¢ 0:06:07[0m [2;4m0.51it/s[0m  
[2KClass label: BaseballPitchm[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 17/203 [2m0:00:34 â€¢ 0:06:07[0m [2;4m0.51it/s[0m  
[2KAttention weights: tensor([[0.6341, 0.3659]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:34 â€¢ 0:06:07[0m [2;4m0.51it/s[0m  
[2KStep 0 - TTA Loss: 1.04940m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 17/203 [2m0:00:34 â€¢ 0:06:07[0m [2;4m0.51it/s[0m  
[2KAttention weights: tensor([[0.6341, 0.3659]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:34 â€¢ 0:06:07[0m [2;4m0.51it/s[0m  
[2KAttention weights: tensor([[0.6342, 0.3658]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:34 â€¢ 0:06:07[0m [2;4m0.51it/s[0m  
[2KAttention weights: tensor([[0.6342, 0.3658]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:34 â€¢ 0:06:07[0m [2;4m0.51it/s[0m  
[2KAttention weights: tensor([[0.6343, 0.3657]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:34 â€¢ 0:06:07[0m [2;4m0.51it/s[0m  
[2KAttention weights: tensor([[0.6343, 0.3657]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:34 â€¢ 0:06:07[0m [2;4m0.51it/s[0m  
[2KAttention weights: tensor([[0.6341, 0.3659]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:34 â€¢ 0:06:07[0m [2;4m0.51it/s[0m  
[2KAttention weights: tensor([[0.6337, 0.3663]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:34 â€¢ 0:06:07[0m [2;4m0.51it/s[0m  
[2KAttention weights: tensor([[0.6330, 0.3670]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:34 â€¢ 0:06:07[0m [2;4m0.51it/s[0m  
[2KAttention weights: tensor([[0.6320, 0.3680]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:34 â€¢ 0:06:07[0m [2;4m0.51it/s[0m  
[2KAttention weights: tensor([[0.5585, 0.4415]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 17/203 [2m0:00:34 â€¢ 0:06:07[0m [2;4m0.51it/s[0m  
[2KAttention weights: tensor([[0.6306, 0.3694],[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 17/203 [2m0:00:34 â€¢ 0:06:07[0m [2;4m0.51it/s[0m  
        [0.5544, 0.4456],
        [0.5608, 0.4392],
        [0.6590, 0.3410],
[2K        [0.6229, 0.3771]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 17/203 [2m0:00:34 â€¢ 0:06:07[0m [2;4m0.51it/s[0m  
[2KAttention weights: tensor([[0.5589, 0.4411],24mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 18/203 [2m0:00:35 â€¢ 0:06:10[0m [2;4m0.50it/s[0m  
        [0.5548, 0.4452],
        [0.5528, 0.4472],
        [0.5586, 0.4414],
[2K        [0.5548, 0.4452]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 18/203 [2m0:00:35 â€¢ 0:06:10[0m [2;4m0.50it/s[0m  
[2KClass label: ThrowDiscus[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 18/203 [2m0:00:35 â€¢ 0:06:10[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.6273, 0.3727]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:35 â€¢ 0:06:10[0m [2;4m0.50it/s[0m  
[2KStep 0 - TTA Loss: 0.48640m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 18/203 [2m0:00:35 â€¢ 0:06:10[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.6256, 0.3744]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:35 â€¢ 0:06:10[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.6226, 0.3774]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:35 â€¢ 0:06:10[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.6188, 0.3812]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:35 â€¢ 0:06:10[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.6146, 0.3854]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:35 â€¢ 0:06:10[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.6105, 0.3895]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:35 â€¢ 0:06:10[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.6070, 0.3930]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:35 â€¢ 0:06:10[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.6043, 0.3957]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:35 â€¢ 0:06:10[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.6026, 0.3974]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:35 â€¢ 0:06:10[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.6019, 0.3981]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:35 â€¢ 0:06:10[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.5541, 0.4459]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 18/203 [2m0:00:35 â€¢ 0:06:10[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.6194, 0.3806],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 18/203 [2m0:00:35 â€¢ 0:06:10[0m [2;4m0.50it/s[0m  
        [0.5390, 0.4610],
        [0.5463, 0.4537],
        [0.6527, 0.3473],
[2K        [0.6017, 0.3983]], device='cuda:0')â•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 18/203 [2m0:00:35 â€¢ 0:06:10[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.5587, 0.4413],24mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 19/203 [2m0:00:36 â€¢ 0:05:00[0m [2;4m0.61it/s[0m  
        [0.5546, 0.4454],
        [0.5526, 0.4474],
        [0.5585, 0.4415],
[2K        [0.5546, 0.4454]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 19/203 [2m0:00:36 â€¢ 0:05:00[0m [2;4m0.61it/s[0m  
[2KClass label: SoccerPenaltym[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 19/203 [2m0:00:36 â€¢ 0:05:00[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.6694, 0.3306]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:36 â€¢ 0:05:00[0m [2;4m0.61it/s[0m  
[2KStep 0 - TTA Loss: 0.52860m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 19/203 [2m0:00:36 â€¢ 0:05:00[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.6694, 0.3306]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:36 â€¢ 0:05:00[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.6694, 0.3306]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:36 â€¢ 0:05:00[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.6693, 0.3307]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:36 â€¢ 0:05:00[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.6693, 0.3307]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:36 â€¢ 0:05:00[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.6695, 0.3305]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:36 â€¢ 0:05:00[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.6700, 0.3300]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:36 â€¢ 0:05:00[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.6711, 0.3289]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:36 â€¢ 0:05:00[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.6725, 0.3275]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:36 â€¢ 0:05:00[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.6741, 0.3259]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:36 â€¢ 0:05:00[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.5588, 0.4412]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 19/203 [2m0:00:36 â€¢ 0:05:00[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.6399, 0.3601],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 19/203 [2m0:00:36 â€¢ 0:05:00[0m [2;4m0.61it/s[0m  
        [0.5582, 0.4418],
        [0.5608, 0.4392],
        [0.6757, 0.3243],
[2K        [0.6201, 0.3799]], device='cuda:0')â•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 19/203 [2m0:00:36 â€¢ 0:05:00[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.5586, 0.4414],24mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 20/203 [2m0:00:37 â€¢ 0:04:44[0m [2;4m0.64it/s[0m  
        [0.5545, 0.4455],
        [0.5526, 0.4474],
        [0.5584, 0.4416],
[2K        [0.5545, 0.4455]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 20/203 [2m0:00:37 â€¢ 0:04:44[0m [2;4m0.64it/s[0m  
[2KClass label: PoleVaultâ”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 20/203 [2m0:00:37 â€¢ 0:04:44[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5620, 0.4380]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:37 â€¢ 0:04:44[0m [2;4m0.64it/s[0m  
[2KStep 0 - TTA Loss: 0.68960m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 20/203 [2m0:00:37 â€¢ 0:04:44[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5624, 0.4376]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:37 â€¢ 0:04:44[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5638, 0.4362]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:37 â€¢ 0:04:44[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5659, 0.4341]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:37 â€¢ 0:04:44[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5683, 0.4317]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:37 â€¢ 0:04:44[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5706, 0.4294]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:37 â€¢ 0:04:44[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5725, 0.4275]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:37 â€¢ 0:04:44[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5739, 0.4261]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:37 â€¢ 0:04:44[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5748, 0.4252]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:37 â€¢ 0:04:44[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5752, 0.4248]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:37 â€¢ 0:04:44[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5531, 0.4469]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 20/203 [2m0:00:37 â€¢ 0:04:44[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6457, 0.3543],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 20/203 [2m0:00:37 â€¢ 0:04:44[0m [2;4m0.64it/s[0m  
        [0.5670, 0.4330],
        [0.5754, 0.4246],
        [0.6779, 0.3221],
[2K        [0.6294, 0.3706]], device='cuda:0')â•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 20/203 [2m0:00:37 â€¢ 0:04:44[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5585, 0.4415],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 21/203 [2m0:00:42 â€¢ 0:05:35[0m [2;4m0.54it/s[0m   
        [0.5545, 0.4455],
        [0.5526, 0.4474],
        [0.5583, 0.4417],
[2K        [0.5545, 0.4455]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 21/203 [2m0:00:42 â€¢ 0:05:35[0m [2;4m0.54it/s[0m  
[2KClass label: ThrowDiscus[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 21/203 [2m0:00:42 â€¢ 0:05:35[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.6246, 0.3754]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:42 â€¢ 0:05:35[0m [2;4m0.54it/s[0m  
[2KStep 0 - TTA Loss: 0.7945[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 21/203 [2m0:00:42 â€¢ 0:05:35[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.6246, 0.3754]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:42 â€¢ 0:05:35[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.6246, 0.3754]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:42 â€¢ 0:05:35[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.6248, 0.3752]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:42 â€¢ 0:05:35[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.6253, 0.3747]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:42 â€¢ 0:05:35[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.6260, 0.3740]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:42 â€¢ 0:05:35[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.6272, 0.3728]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:42 â€¢ 0:05:35[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.6286, 0.3714]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:42 â€¢ 0:05:35[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.6305, 0.3695]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:42 â€¢ 0:05:35[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.6323, 0.3677]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:42 â€¢ 0:05:35[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.5550, 0.4450]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 21/203 [2m0:00:42 â€¢ 0:05:35[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.6453, 0.3547],[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 21/203 [2m0:00:42 â€¢ 0:05:35[0m [2;4m0.54it/s[0m  
        [0.5661, 0.4339],
        [0.5756, 0.4244],
        [0.6748, 0.3252],
[2K        [0.6337, 0.3663]], device='cuda:0')m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 21/203 [2m0:00:42 â€¢ 0:05:35[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.5585, 0.4415],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 22/203 [2m0:00:43 â€¢ 0:05:26[0m [2;4m0.56it/s[0m  
        [0.5545, 0.4455],
        [0.5526, 0.4474],
        [0.5583, 0.4417],
[2K        [0.5544, 0.4456]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 22/203 [2m0:00:43 â€¢ 0:05:26[0m [2;4m0.56it/s[0m  
[2KClass label: HammerThrow[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 22/203 [2m0:00:43 â€¢ 0:05:26[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.5579, 0.4421]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:43 â€¢ 0:05:26[0m [2;4m0.56it/s[0m  
[2KStep 0 - TTA Loss: 1.2405[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 22/203 [2m0:00:43 â€¢ 0:05:26[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.5596, 0.4404]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:43 â€¢ 0:05:26[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.5613, 0.4387]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:43 â€¢ 0:05:26[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.5631, 0.4369]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:43 â€¢ 0:05:26[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.5647, 0.4353]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:43 â€¢ 0:05:26[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.5660, 0.4340]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:43 â€¢ 0:05:26[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.5671, 0.4329]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:43 â€¢ 0:05:26[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.5679, 0.4321]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:43 â€¢ 0:05:26[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.5684, 0.4316]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:43 â€¢ 0:05:26[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.5686, 0.4314]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:43 â€¢ 0:05:26[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.5548, 0.4452]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 22/203 [2m0:00:43 â€¢ 0:05:26[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.6449, 0.3551],[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 22/203 [2m0:00:43 â€¢ 0:05:26[0m [2;4m0.56it/s[0m  
        [0.5687, 0.4313],
        [0.5727, 0.4273],
        [0.6732, 0.3268],
[2K        [0.6336, 0.3664]], device='cuda:0')m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 22/203 [2m0:00:43 â€¢ 0:05:26[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.5585, 0.4415],224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 23/203 [2m0:00:44 â€¢ 0:05:13[0m [2;4m0.58it/s[0m  
        [0.5544, 0.4456],
        [0.5526, 0.4474],
        [0.5583, 0.4417],
[2K        [0.5544, 0.4456]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 23/203 [2m0:00:44 â€¢ 0:05:13[0m [2;4m0.58it/s[0m  
[2KClass label: HammerThrow[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 23/203 [2m0:00:44 â€¢ 0:05:13[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5569, 0.4431]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:44 â€¢ 0:05:13[0m [2;4m0.58it/s[0m  
[2KStep 0 - TTA Loss: 0.9962[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 23/203 [2m0:00:44 â€¢ 0:05:13[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5569, 0.4431]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:44 â€¢ 0:05:13[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5569, 0.4431]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:44 â€¢ 0:05:13[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5569, 0.4431]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:44 â€¢ 0:05:13[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5569, 0.4431]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:44 â€¢ 0:05:13[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5567, 0.4433]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:44 â€¢ 0:05:13[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5566, 0.4434]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:44 â€¢ 0:05:13[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5561, 0.4439]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:44 â€¢ 0:05:13[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5550, 0.4450]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:44 â€¢ 0:05:13[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5536, 0.4464]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:44 â€¢ 0:05:13[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5540, 0.4460]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 23/203 [2m0:00:44 â€¢ 0:05:13[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.6338, 0.3662],â•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 23/203 [2m0:00:44 â€¢ 0:05:13[0m [2;4m0.58it/s[0m  
        [0.5519, 0.4481],
        [0.5598, 0.4402],
        [0.6632, 0.3368],
[2K        [0.6195, 0.3805]], device='cuda:0')mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 23/203 [2m0:00:44 â€¢ 0:05:13[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5584, 0.4416],224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 24/203 [2m0:00:45 â€¢ 0:03:56[0m [2;4m0.76it/s[0m  
        [0.5543, 0.4457],
        [0.5525, 0.4475],
        [0.5582, 0.4418],
[2K        [0.5543, 0.4457]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 24/203 [2m0:00:45 â€¢ 0:03:56[0m [2;4m0.76it/s[0m  
[2KClass label: SoccerPenalty0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 24/203 [2m0:00:45 â€¢ 0:03:56[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.6653, 0.3347]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:45 â€¢ 0:03:56[0m [2;4m0.76it/s[0m  
[2KStep 0 - TTA Loss: 1.4914[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 24/203 [2m0:00:45 â€¢ 0:03:56[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.6645, 0.3355]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:45 â€¢ 0:03:56[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.6639, 0.3361]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:45 â€¢ 0:03:56[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.6634, 0.3366]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:45 â€¢ 0:03:56[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.6632, 0.3368]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:45 â€¢ 0:03:56[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.6631, 0.3369]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:45 â€¢ 0:03:56[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.6631, 0.3369]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:45 â€¢ 0:03:56[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.6631, 0.3369]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:45 â€¢ 0:03:56[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.6630, 0.3370]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:45 â€¢ 0:03:56[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.6630, 0.3370]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:45 â€¢ 0:03:56[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.5582, 0.4418]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 24/203 [2m0:00:45 â€¢ 0:03:56[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.6364, 0.3636],â•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 24/203 [2m0:00:45 â€¢ 0:03:56[0m [2;4m0.76it/s[0m  
        [0.5526, 0.4474],
        [0.5598, 0.4402],
        [0.6631, 0.3369],
[2K        [0.6200, 0.3800]], device='cuda:0')mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 24/203 [2m0:00:45 â€¢ 0:03:56[0m [2;4m0.76it/s[0m  
[2KAttention weights: tensor([[0.5583, 0.4417],224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 25/203 [2m0:00:47 â€¢ 0:04:03[0m [2;4m0.73it/s[0m  
        [0.5541, 0.4459],
        [0.5524, 0.4476],
        [0.5582, 0.4418],
[2K        [0.5541, 0.4459]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 25/203 [2m0:00:47 â€¢ 0:04:03[0m [2;4m0.73it/s[0m  
[2KClass label: ThrowDiscus[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 25/203 [2m0:00:47 â€¢ 0:04:03[0m [2;4m0.73it/s[0m  
[2KAttention weights: tensor([[0.6200, 0.3800]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:47 â€¢ 0:04:03[0m [2;4m0.73it/s[0m  
[2KStep 0 - TTA Loss: 0.9379[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 25/203 [2m0:00:47 â€¢ 0:04:03[0m [2;4m0.73it/s[0m  
[2KAttention weights: tensor([[0.6200, 0.3800]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:47 â€¢ 0:04:03[0m [2;4m0.73it/s[0m  
[2KAttention weights: tensor([[0.6200, 0.3800]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:47 â€¢ 0:04:03[0m [2;4m0.73it/s[0m  
[2KAttention weights: tensor([[0.6201, 0.3799]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:47 â€¢ 0:04:03[0m [2;4m0.73it/s[0m  
[2KAttention weights: tensor([[0.6202, 0.3798]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:47 â€¢ 0:04:03[0m [2;4m0.73it/s[0m  
[2KAttention weights: tensor([[0.6203, 0.3797]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:47 â€¢ 0:04:03[0m [2;4m0.73it/s[0m  
[2KAttention weights: tensor([[0.6206, 0.3794]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:47 â€¢ 0:04:03[0m [2;4m0.73it/s[0m  
[2KAttention weights: tensor([[0.6209, 0.3791]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:47 â€¢ 0:04:03[0m [2;4m0.73it/s[0m  
[2KAttention weights: tensor([[0.6213, 0.3787]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:47 â€¢ 0:04:03[0m [2;4m0.73it/s[0m  
[2KAttention weights: tensor([[0.6221, 0.3779]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:47 â€¢ 0:04:03[0m [2;4m0.73it/s[0m  
[2KAttention weights: tensor([[0.5544, 0.4456]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 25/203 [2m0:00:47 â€¢ 0:04:03[0m [2;4m0.73it/s[0m  
[2KAttention weights: tensor([[0.6353, 0.3647],â•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 25/203 [2m0:00:47 â€¢ 0:04:03[0m [2;4m0.73it/s[0m  
        [0.5560, 0.4440],
        [0.5629, 0.4371],
        [0.6638, 0.3362],
[2K        [0.6229, 0.3771]], device='cuda:0')mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 25/203 [2m0:00:47 â€¢ 0:04:03[0m [2;4m0.73it/s[0m  
[2KAttention weights: tensor([[0.5583, 0.4417],â•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 26/203 [2m0:00:48 â€¢ 0:04:00[0m [2;4m0.74it/s[0m   
        [0.5541, 0.4459],
        [0.5524, 0.4476],
        [0.5582, 0.4418],
[2K        [0.5541, 0.4459]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 26/203 [2m0:00:48 â€¢ 0:04:00[0m [2;4m0.74it/s[0m  
[2KClass label: PoleVaultâ”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 26/203 [2m0:00:48 â€¢ 0:04:00[0m [2;4m0.74it/s[0m  
[2KAttention weights: tensor([[0.5607, 0.4393]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:48 â€¢ 0:04:00[0m [2;4m0.74it/s[0m  
[2KStep 0 - TTA Loss: 0.6195[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 26/203 [2m0:00:48 â€¢ 0:04:00[0m [2;4m0.74it/s[0m  
[2KAttention weights: tensor([[0.5631, 0.4369]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:48 â€¢ 0:04:00[0m [2;4m0.74it/s[0m  
[2KAttention weights: tensor([[0.5667, 0.4333]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:48 â€¢ 0:04:00[0m [2;4m0.74it/s[0m  
[2KAttention weights: tensor([[0.5711, 0.4289]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:48 â€¢ 0:04:00[0m [2;4m0.74it/s[0m  
[2KAttention weights: tensor([[0.5756, 0.4244]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:48 â€¢ 0:04:00[0m [2;4m0.74it/s[0m  
[2KAttention weights: tensor([[0.5797, 0.4203]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:48 â€¢ 0:04:00[0m [2;4m0.74it/s[0m  
[2KAttention weights: tensor([[0.5832, 0.4168]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:48 â€¢ 0:04:00[0m [2;4m0.74it/s[0m  
[2KAttention weights: tensor([[0.5859, 0.4141]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:48 â€¢ 0:04:00[0m [2;4m0.74it/s[0m  
[2KAttention weights: tensor([[0.5876, 0.4124]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:48 â€¢ 0:04:00[0m [2;4m0.74it/s[0m  
[2KAttention weights: tensor([[0.5884, 0.4116]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:48 â€¢ 0:04:00[0m [2;4m0.74it/s[0m  
[2KAttention weights: tensor([[0.5535, 0.4465]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 26/203 [2m0:00:48 â€¢ 0:04:00[0m [2;4m0.74it/s[0m  
[2KAttention weights: tensor([[0.6512, 0.3488],m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 26/203 [2m0:00:48 â€¢ 0:04:00[0m [2;4m0.74it/s[0m  
        [0.5776, 0.4224],
        [0.5886, 0.4114],
        [0.6786, 0.3214],
[2K        [0.6446, 0.3554]], device='cuda:0')0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 26/203 [2m0:00:48 â€¢ 0:04:00[0m [2;4m0.74it/s[0m  
[2KAttention weights: tensor([[0.5583, 0.4417],â•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 27/203 [2m0:00:54 â€¢ 0:04:33[0m [2;4m0.65it/s[0m  
        [0.5541, 0.4459],
        [0.5524, 0.4476],
        [0.5582, 0.4418],
[2K        [0.5542, 0.4458]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 27/203 [2m0:00:54 â€¢ 0:04:33[0m [2;4m0.65it/s[0m  
[2KClass label: ThrowDiscusâ”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 27/203 [2m0:00:54 â€¢ 0:04:33[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.6241, 0.3759]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:54 â€¢ 0:04:33[0m [2;4m0.65it/s[0m  
[2KStep 0 - TTA Loss: 1.0478[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 27/203 [2m0:00:54 â€¢ 0:04:33[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.6241, 0.3759]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:54 â€¢ 0:04:33[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.6242, 0.3758]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:54 â€¢ 0:04:33[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.6246, 0.3754]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:54 â€¢ 0:04:33[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.6256, 0.3744]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:54 â€¢ 0:04:33[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.6272, 0.3728]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:54 â€¢ 0:04:33[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.6291, 0.3709]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:54 â€¢ 0:04:33[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.6316, 0.3684]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:54 â€¢ 0:04:33[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.6347, 0.3653]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:54 â€¢ 0:04:33[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.6378, 0.3622]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:54 â€¢ 0:04:33[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.5552, 0.4448]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 27/203 [2m0:00:54 â€¢ 0:04:33[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.6500, 0.3500],m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 27/203 [2m0:00:54 â€¢ 0:04:33[0m [2;4m0.65it/s[0m  
        [0.5740, 0.4260],
        [0.5861, 0.4139],
        [0.6779, 0.3221],
[2K        [0.6411, 0.3589]], device='cuda:0')0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 27/203 [2m0:00:54 â€¢ 0:04:33[0m [2;4m0.65it/s[0m  
[2KAttention weights: tensor([[0.5584, 0.4416],;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 28/203 [2m0:00:56 â€¢ 0:04:35[0m [2;4m0.64it/s[0m  
        [0.5542, 0.4458],
        [0.5525, 0.4475],
        [0.5583, 0.4417],
[2K        [0.5542, 0.4458]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 28/203 [2m0:00:56 â€¢ 0:04:35[0m [2;4m0.64it/s[0m  
[2KClass label: HammerThrowâ”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 28/203 [2m0:00:56 â€¢ 0:04:35[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5557, 0.4443]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:56 â€¢ 0:04:35[0m [2;4m0.64it/s[0m  
[2KStep 0 - TTA Loss: 1.0350[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 28/203 [2m0:00:56 â€¢ 0:04:35[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5600, 0.4400]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:56 â€¢ 0:04:35[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5655, 0.4345]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:56 â€¢ 0:04:35[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5716, 0.4284]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:56 â€¢ 0:04:35[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5776, 0.4224]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:56 â€¢ 0:04:35[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5830, 0.4170]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:56 â€¢ 0:04:35[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5874, 0.4126]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:56 â€¢ 0:04:35[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5905, 0.4095]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:56 â€¢ 0:04:35[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5925, 0.4075]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:56 â€¢ 0:04:35[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5935, 0.4065]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:56 â€¢ 0:04:35[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5556, 0.4444]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 28/203 [2m0:00:56 â€¢ 0:04:35[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6625, 0.3375],mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 28/203 [2m0:00:56 â€¢ 0:04:35[0m [2;4m0.64it/s[0m  
        [0.5938, 0.4062],
        [0.5921, 0.4079],
        [0.6852, 0.3148],
[2K        [0.6527, 0.3473]], device='cuda:0')4mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 28/203 [2m0:00:56 â€¢ 0:04:35[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5585, 0.4415],;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 29/203 [2m0:00:57 â€¢ 0:04:35[0m [2;4m0.63it/s[0m  
        [0.5543, 0.4457],
        [0.5526, 0.4474],
        [0.5583, 0.4417],
[2K        [0.5544, 0.4456]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 29/203 [2m0:00:57 â€¢ 0:04:35[0m [2;4m0.63it/s[0m  
[2KClass label: ThrowDiscusâ”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 29/203 [2m0:00:57 â€¢ 0:04:35[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6224, 0.3776]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:57 â€¢ 0:04:35[0m [2;4m0.63it/s[0m  
[2KStep 0 - TTA Loss: 1.1830[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 29/203 [2m0:00:57 â€¢ 0:04:35[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6224, 0.3776]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:57 â€¢ 0:04:35[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6225, 0.3775]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:57 â€¢ 0:04:35[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6230, 0.3770]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:57 â€¢ 0:04:35[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6237, 0.3763]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:57 â€¢ 0:04:35[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6247, 0.3753]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:57 â€¢ 0:04:35[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6259, 0.3741]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:57 â€¢ 0:04:35[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6270, 0.3730]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:57 â€¢ 0:04:35[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6281, 0.3719]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:57 â€¢ 0:04:35[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6289, 0.3711]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:57 â€¢ 0:04:35[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.5548, 0.4452]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 29/203 [2m0:00:57 â€¢ 0:04:35[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6494, 0.3506],mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 29/203 [2m0:00:57 â€¢ 0:04:35[0m [2;4m0.63it/s[0m  
        [0.5730, 0.4270],
        [0.5719, 0.4281],
        [0.6760, 0.3240],
[2K        [0.6296, 0.3704]], device='cuda:0')4mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 29/203 [2m0:00:57 â€¢ 0:04:35[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.5585, 0.4415],;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 30/203 [2m0:00:58 â€¢ 0:04:42[0m [2;4m0.61it/s[0m  
        [0.5543, 0.4457],
        [0.5526, 0.4474],
        [0.5583, 0.4417],
[2K        [0.5544, 0.4456]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 30/203 [2m0:00:58 â€¢ 0:04:42[0m [2;4m0.61it/s[0m  
[2KClass label: ThrowDiscusâ”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 30/203 [2m0:00:58 â€¢ 0:04:42[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.6237, 0.3763]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:58 â€¢ 0:04:42[0m [2;4m0.61it/s[0m  
[2KStep 0 - TTA Loss: 0.8120[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 30/203 [2m0:00:58 â€¢ 0:04:42[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.6242, 0.3758]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:58 â€¢ 0:04:42[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.6243, 0.3757]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:58 â€¢ 0:04:42[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.6244, 0.3756]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:58 â€¢ 0:04:42[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.6242, 0.3758]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:58 â€¢ 0:04:42[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.6239, 0.3761]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:58 â€¢ 0:04:42[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.6234, 0.3766]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:58 â€¢ 0:04:42[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.6230, 0.3770]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:58 â€¢ 0:04:42[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.6226, 0.3774]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:58 â€¢ 0:04:42[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.6224, 0.3776]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:58 â€¢ 0:04:42[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.5545, 0.4455]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 30/203 [2m0:00:58 â€¢ 0:04:42[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.6395, 0.3605],mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 30/203 [2m0:00:58 â€¢ 0:04:42[0m [2;4m0.61it/s[0m  
        [0.5646, 0.4354],
        [0.5673, 0.4327],
        [0.6702, 0.3298],
[2K        [0.6224, 0.3776]], device='cuda:0')4mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 30/203 [2m0:00:58 â€¢ 0:04:42[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.5585, 0.4415],mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 31/203 [2m0:01:00 â€¢ 0:04:56[0m [2;4m0.58it/s[0m   
        [0.5543, 0.4457],
        [0.5526, 0.4474],
        [0.5583, 0.4417],
[2K        [0.5543, 0.4457]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 31/203 [2m0:01:00 â€¢ 0:04:56[0m [2;4m0.58it/s[0m  
[2KClass label: ThrowDiscusâ”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 31/203 [2m0:01:00 â€¢ 0:04:56[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.6260, 0.3740]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:01:00 â€¢ 0:04:56[0m [2;4m0.58it/s[0m  
[2KStep 0 - TTA Loss: 1.1156â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 31/203 [2m0:01:00 â€¢ 0:04:56[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.6260, 0.3740]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:01:00 â€¢ 0:04:56[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.6260, 0.3740]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:01:00 â€¢ 0:04:56[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.6259, 0.3741]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:01:00 â€¢ 0:04:56[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.6258, 0.3742]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:01:00 â€¢ 0:04:56[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.6258, 0.3742]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:01:00 â€¢ 0:04:56[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.6258, 0.3742]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:01:00 â€¢ 0:04:56[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.6261, 0.3739]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:01:00 â€¢ 0:04:56[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.6266, 0.3734]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:01:00 â€¢ 0:04:56[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.6274, 0.3726]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:01:00 â€¢ 0:04:56[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5548, 0.4452]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 31/203 [2m0:01:00 â€¢ 0:04:56[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.6425, 0.3575],0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 31/203 [2m0:01:00 â€¢ 0:04:56[0m [2;4m0.58it/s[0m  
        [0.5626, 0.4374],
        [0.5693, 0.4307],
        [0.6733, 0.3267],
[2K        [0.6291, 0.3709]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 31/203 [2m0:01:00 â€¢ 0:04:56[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5585, 0.4415],mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 32/203 [2m0:01:03 â€¢ 0:05:32[0m [2;4m0.52it/s[0m  
        [0.5543, 0.4457],
        [0.5526, 0.4474],
        [0.5583, 0.4417],
[2K        [0.5543, 0.4457]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 32/203 [2m0:01:03 â€¢ 0:05:32[0m [2;4m0.52it/s[0m  
[2KClass label: ThrowDiscusâ”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 32/203 [2m0:01:03 â€¢ 0:05:32[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.6239, 0.3761]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:01:03 â€¢ 0:05:32[0m [2;4m0.52it/s[0m  
[2KStep 0 - TTA Loss: 0.7653â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 32/203 [2m0:01:03 â€¢ 0:05:32[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.6247, 0.3753]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:01:03 â€¢ 0:05:32[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.6245, 0.3755]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:01:03 â€¢ 0:05:32[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.6234, 0.3766]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:01:03 â€¢ 0:05:32[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.6217, 0.3783]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:01:03 â€¢ 0:05:32[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.6198, 0.3802]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:01:03 â€¢ 0:05:32[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.6180, 0.3820]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:01:03 â€¢ 0:05:32[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.6166, 0.3834]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:01:03 â€¢ 0:05:32[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.6157, 0.3843]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:01:03 â€¢ 0:05:32[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.6152, 0.3848]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:01:03 â€¢ 0:05:32[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.5541, 0.4459]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 32/203 [2m0:01:03 â€¢ 0:05:32[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.6357, 0.3643],0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 32/203 [2m0:01:03 â€¢ 0:05:32[0m [2;4m0.52it/s[0m  
        [0.5537, 0.4463],
        [0.5551, 0.4449],
        [0.6643, 0.3357],
[2K        [0.6151, 0.3849]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 32/203 [2m0:01:03 â€¢ 0:05:32[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.5584, 0.4416],6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 33/203 [2m0:01:04 â€¢ 0:05:29[0m [2;4m0.52it/s[0m  
        [0.5542, 0.4458],
        [0.5526, 0.4474],
        [0.5583, 0.4417],
[2K        [0.5542, 0.4458]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 33/203 [2m0:01:04 â€¢ 0:05:29[0m [2;4m0.52it/s[0m  
[2KClass label: ThrowDiscusâ”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 33/203 [2m0:01:04 â€¢ 0:05:29[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.6247, 0.3753]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:01:04 â€¢ 0:05:29[0m [2;4m0.52it/s[0m  
[2KStep 0 - TTA Loss: 0.8577â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 33/203 [2m0:01:04 â€¢ 0:05:29[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.6247, 0.3753]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:01:04 â€¢ 0:05:29[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.6246, 0.3754]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:01:04 â€¢ 0:05:29[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.6244, 0.3756]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:01:04 â€¢ 0:05:29[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.6241, 0.3759]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:01:04 â€¢ 0:05:29[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.6238, 0.3762]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:01:04 â€¢ 0:05:29[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.6239, 0.3761]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:01:04 â€¢ 0:05:29[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.6241, 0.3759]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:01:04 â€¢ 0:05:29[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.6248, 0.3752]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:01:04 â€¢ 0:05:29[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.6259, 0.3741]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:01:04 â€¢ 0:05:29[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.5545, 0.4455]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 33/203 [2m0:01:04 â€¢ 0:05:29[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.6419, 0.3581],4mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 33/203 [2m0:01:04 â€¢ 0:05:29[0m [2;4m0.52it/s[0m  
        [0.5617, 0.4383],
        [0.5665, 0.4335],
        [0.6688, 0.3312],
[2K        [0.6275, 0.3725]], device='cuda:0')24mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 33/203 [2m0:01:04 â€¢ 0:05:29[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.5584, 0.4416],6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 34/203 [2m0:01:05 â€¢ 0:05:29[0m [2;4m0.52it/s[0m  
        [0.5542, 0.4458],
        [0.5525, 0.4475],
        [0.5582, 0.4418],
[2K        [0.5541, 0.4459]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 34/203 [2m0:01:05 â€¢ 0:05:29[0m [2;4m0.52it/s[0m  
[2KClass label: HammerThrowâ”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 34/203 [2m0:01:05 â€¢ 0:05:29[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.5623, 0.4377]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:01:05 â€¢ 0:05:29[0m [2;4m0.52it/s[0m  
[2KStep 0 - TTA Loss: 0.5451â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 34/203 [2m0:01:05 â€¢ 0:05:29[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.5638, 0.4362]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:01:05 â€¢ 0:05:29[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.5655, 0.4345]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:01:05 â€¢ 0:05:29[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.5672, 0.4328]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:01:05 â€¢ 0:05:29[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.5689, 0.4311]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:01:05 â€¢ 0:05:29[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.5702, 0.4298]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:01:05 â€¢ 0:05:29[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.5712, 0.4288]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:01:05 â€¢ 0:05:29[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.5718, 0.4282]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:01:05 â€¢ 0:05:29[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.5722, 0.4278]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:01:05 â€¢ 0:05:29[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.5724, 0.4276]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:01:05 â€¢ 0:05:29[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.5547, 0.4453]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 34/203 [2m0:01:05 â€¢ 0:05:29[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.6479, 0.3521],4mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 34/203 [2m0:01:05 â€¢ 0:05:29[0m [2;4m0.52it/s[0m  
        [0.5724, 0.4276],
        [0.5737, 0.4263],
        [0.6749, 0.3251],
[2K        [0.6380, 0.3620]], device='cuda:0')24mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 34/203 [2m0:01:05 â€¢ 0:05:29[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.5583, 0.4417],6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 35/203 [2m0:01:07 â€¢ 0:05:34[0m [2;4m0.50it/s[0m  
        [0.5541, 0.4459],
        [0.5525, 0.4475],
        [0.5582, 0.4418],
[2K        [0.5540, 0.4460]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 35/203 [2m0:01:07 â€¢ 0:05:34[0m [2;4m0.50it/s[0m  
[2KClass label: ThrowDiscusâ”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 35/203 [2m0:01:07 â€¢ 0:05:34[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.6234, 0.3766]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:01:07 â€¢ 0:05:34[0m [2;4m0.50it/s[0m  
[2KStep 0 - TTA Loss: 0.9217â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 35/203 [2m0:01:07 â€¢ 0:05:34[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.6234, 0.3766]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:01:07 â€¢ 0:05:34[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.6234, 0.3766]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:01:07 â€¢ 0:05:34[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.6235, 0.3765]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:01:07 â€¢ 0:05:34[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.6238, 0.3762]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:01:07 â€¢ 0:05:34[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.6241, 0.3759]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:01:07 â€¢ 0:05:34[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.6246, 0.3754]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:01:07 â€¢ 0:05:34[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.6245, 0.3755]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:01:07 â€¢ 0:05:34[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.6237, 0.3763]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:01:07 â€¢ 0:05:34[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.6219, 0.3781]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:01:07 â€¢ 0:05:34[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.5541, 0.4459]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 35/203 [2m0:01:07 â€¢ 0:05:34[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.6351, 0.3649],4mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 35/203 [2m0:01:07 â€¢ 0:05:34[0m [2;4m0.50it/s[0m  
        [0.5572, 0.4428],
        [0.5607, 0.4393],
        [0.6627, 0.3373],
[2K        [0.6192, 0.3808]], device='cuda:0')24mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 35/203 [2m0:01:07 â€¢ 0:05:34[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.5583, 0.4417],7mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 36/203 [2m0:01:07 â€¢ 0:04:38[0m [2;4m0.60it/s[0m   
        [0.5541, 0.4459],
        [0.5525, 0.4475],
        [0.5582, 0.4418],
[2K        [0.5540, 0.4460]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 36/203 [2m0:01:07 â€¢ 0:04:38[0m [2;4m0.60it/s[0m  
[2KClass label: HammerThrowâ”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 36/203 [2m0:01:07 â€¢ 0:04:38[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.5600, 0.4400]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:01:07 â€¢ 0:04:38[0m [2;4m0.60it/s[0m  
[2KStep 0 - TTA Loss: 1.1369â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 36/203 [2m0:01:07 â€¢ 0:04:38[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.5566, 0.4434]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:01:07 â€¢ 0:04:38[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.5542, 0.4458]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:01:07 â€¢ 0:04:38[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.5522, 0.4478]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:01:07 â€¢ 0:04:38[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.5514, 0.4486]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:01:07 â€¢ 0:04:38[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.5510, 0.4490]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:01:07 â€¢ 0:04:38[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.5507, 0.4493]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:01:07 â€¢ 0:04:38[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.5502, 0.4498]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:01:07 â€¢ 0:04:38[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.5501, 0.4499]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:01:07 â€¢ 0:04:38[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.5500, 0.4500]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:01:07 â€¢ 0:04:38[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.5538, 0.4462]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 36/203 [2m0:01:07 â€¢ 0:04:38[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.6319, 0.3681],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 36/203 [2m0:01:07 â€¢ 0:04:38[0m [2;4m0.60it/s[0m  
        [0.5500, 0.4500],
        [0.5556, 0.4444],
        [0.6608, 0.3392],
[2K        [0.6148, 0.3852]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 36/203 [2m0:01:07 â€¢ 0:04:38[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.5583, 0.4417],7mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 37/203 [2m0:01:08 â€¢ 0:04:26[0m [2;4m0.63it/s[0m  
        [0.5541, 0.4459],
        [0.5525, 0.4475],
        [0.5582, 0.4418],
[2K        [0.5540, 0.4460]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 37/203 [2m0:01:08 â€¢ 0:04:26[0m [2;4m0.63it/s[0m  
[2KClass label: SoccerPenaltyâ”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 37/203 [2m0:01:08 â€¢ 0:04:26[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6666, 0.3334]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:01:08 â€¢ 0:04:26[0m [2;4m0.63it/s[0m  
[2KStep 0 - TTA Loss: 1.0079â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 37/203 [2m0:01:08 â€¢ 0:04:26[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6666, 0.3334]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:01:08 â€¢ 0:04:26[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6667, 0.3333]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:01:08 â€¢ 0:04:26[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6673, 0.3327]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:01:08 â€¢ 0:04:26[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6690, 0.3310]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:01:08 â€¢ 0:04:26[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6724, 0.3276]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:01:08 â€¢ 0:04:26[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6781, 0.3219]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:01:08 â€¢ 0:04:26[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6863, 0.3137]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:01:08 â€¢ 0:04:26[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6969, 0.3031]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:01:08 â€¢ 0:04:26[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.7097, 0.2903]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:01:08 â€¢ 0:04:26[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.5608, 0.4392]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 37/203 [2m0:01:08 â€¢ 0:04:26[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6691, 0.3309],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 37/203 [2m0:01:08 â€¢ 0:04:26[0m [2;4m0.63it/s[0m  
        [0.5839, 0.4161],
        [0.5864, 0.4136],
        [0.7239, 0.2761],
[2K        [0.6430, 0.3570]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 37/203 [2m0:01:08 â€¢ 0:04:26[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.5584, 0.4416],7mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 38/203 [2m0:01:09 â€¢ 0:04:22[0m [2;4m0.63it/s[0m  
        [0.5541, 0.4459],
        [0.5525, 0.4475],
        [0.5583, 0.4417],
[2K        [0.5540, 0.4460]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 38/203 [2m0:01:09 â€¢ 0:04:22[0m [2;4m0.63it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 38/203 [2m0:01:09 â€¢ 0:04:22[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6269, 0.3731]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:01:09 â€¢ 0:04:22[0m [2;4m0.63it/s[0m  
[2KStep 0 - TTA Loss: 0.8975â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 38/203 [2m0:01:09 â€¢ 0:04:22[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6323, 0.3677]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:01:09 â€¢ 0:04:22[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6382, 0.3618]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:01:09 â€¢ 0:04:22[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6444, 0.3556]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:01:09 â€¢ 0:04:22[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6504, 0.3496]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:01:09 â€¢ 0:04:22[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6557, 0.3443]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:01:09 â€¢ 0:04:22[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6599, 0.3401]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:01:09 â€¢ 0:04:22[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6629, 0.3371]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:01:09 â€¢ 0:04:22[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6648, 0.3352]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:01:09 â€¢ 0:04:22[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6658, 0.3342]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:01:09 â€¢ 0:04:22[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.5561, 0.4439]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 38/203 [2m0:01:09 â€¢ 0:04:22[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6869, 0.3131],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 38/203 [2m0:01:09 â€¢ 0:04:22[0m [2;4m0.63it/s[0m  
        [0.6047, 0.3953],
        [0.6052, 0.3948],
        [0.7363, 0.2637],
[2K        [0.6661, 0.3339]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 38/203 [2m0:01:09 â€¢ 0:04:22[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.5585, 0.4415],;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 39/203 [2m0:01:12 â€¢ 0:04:32[0m [2;4m0.60it/s[0m  
        [0.5543, 0.4457],
        [0.5526, 0.4474],
        [0.5585, 0.4415],
[2K        [0.5541, 0.4459]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 39/203 [2m0:01:12 â€¢ 0:04:32[0m [2;4m0.60it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 39/203 [2m0:01:12 â€¢ 0:04:32[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.6230, 0.3770]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:12 â€¢ 0:04:32[0m [2;4m0.60it/s[0m  
[2KStep 0 - TTA Loss: 0.8506â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 39/203 [2m0:01:12 â€¢ 0:04:32[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.6230, 0.3770]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:12 â€¢ 0:04:32[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.6233, 0.3767]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:12 â€¢ 0:04:32[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.6241, 0.3759]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:12 â€¢ 0:04:32[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.6255, 0.3745]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:12 â€¢ 0:04:32[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.6278, 0.3722]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:12 â€¢ 0:04:32[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.6307, 0.3693]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:12 â€¢ 0:04:32[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.6342, 0.3658]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:12 â€¢ 0:04:32[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.6379, 0.3621]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:12 â€¢ 0:04:32[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.6414, 0.3586]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:12 â€¢ 0:04:32[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.5553, 0.4447]], device='cuda:0')mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 39/203 [2m0:01:12 â€¢ 0:04:32[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.6543, 0.3457],24mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 39/203 [2m0:01:12 â€¢ 0:04:32[0m [2;4m0.60it/s[0m  
        [0.5785, 0.4215],
        [0.5806, 0.4194],
        [0.6848, 0.3152],
[2K        [0.6448, 0.3552]], device='cuda:0')224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 39/203 [2m0:01:12 â€¢ 0:04:32[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.5586, 0.4414],;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 40/203 [2m0:01:13 â€¢ 0:04:32[0m [2;4m0.60it/s[0m  
        [0.5543, 0.4457],
        [0.5527, 0.4473],
        [0.5586, 0.4414],
[2K        [0.5542, 0.4458]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 40/203 [2m0:01:13 â€¢ 0:04:32[0m [2;4m0.60it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 40/203 [2m0:01:13 â€¢ 0:04:32[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.5650, 0.4350]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:13 â€¢ 0:04:32[0m [2;4m0.60it/s[0m  
[2KStep 0 - TTA Loss: 0.6916â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 40/203 [2m0:01:13 â€¢ 0:04:32[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.5674, 0.4326]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:13 â€¢ 0:04:32[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.5689, 0.4311]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:13 â€¢ 0:04:32[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.5698, 0.4302]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:13 â€¢ 0:04:32[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.5704, 0.4296]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:13 â€¢ 0:04:32[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.5705, 0.4295]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:13 â€¢ 0:04:32[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.5704, 0.4296]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:13 â€¢ 0:04:32[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.5703, 0.4297]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:13 â€¢ 0:04:32[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.5702, 0.4298]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:13 â€¢ 0:04:32[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.5702, 0.4298]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:13 â€¢ 0:04:32[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.5530, 0.4470]], device='cuda:0')mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 40/203 [2m0:01:13 â€¢ 0:04:32[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.6502, 0.3498],24mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 40/203 [2m0:01:13 â€¢ 0:04:32[0m [2;4m0.60it/s[0m  
        [0.5697, 0.4303],
        [0.5702, 0.4298],
        [0.6767, 0.3233],
[2K        [0.6361, 0.3639]], device='cuda:0')224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 40/203 [2m0:01:13 â€¢ 0:04:32[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.5586, 0.4414],37mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 41/203 [2m0:01:17 â€¢ 0:04:58[0m [2;4m0.55it/s[0m   
        [0.5543, 0.4457],
        [0.5526, 0.4474],
        [0.5586, 0.4414],
[2K        [0.5542, 0.4458]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 41/203 [2m0:01:17 â€¢ 0:04:58[0m [2;4m0.55it/s[0m  
[2KClass label: HammerThrowâ”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 41/203 [2m0:01:17 â€¢ 0:04:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.5563, 0.4437]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:01:17 â€¢ 0:04:58[0m [2;4m0.55it/s[0m  
[2KStep 0 - TTA Loss: 0.4910â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 41/203 [2m0:01:17 â€¢ 0:04:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.5563, 0.4437]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:01:17 â€¢ 0:04:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.5563, 0.4437]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:01:17 â€¢ 0:04:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.5566, 0.4434]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:01:17 â€¢ 0:04:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.5572, 0.4428]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:01:17 â€¢ 0:04:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.5586, 0.4414]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:01:17 â€¢ 0:04:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.5607, 0.4393]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:01:17 â€¢ 0:04:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.5635, 0.4365]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:01:17 â€¢ 0:04:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.5670, 0.4330]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:01:17 â€¢ 0:04:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.5714, 0.4286]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:01:17 â€¢ 0:04:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.5550, 0.4450]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 41/203 [2m0:01:17 â€¢ 0:04:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.6484, 0.3516],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 41/203 [2m0:01:17 â€¢ 0:04:58[0m [2;4m0.55it/s[0m  
        [0.5762, 0.4238],
        [0.5700, 0.4300],
        [0.6750, 0.3250],
[2K        [0.6334, 0.3666]], device='cuda:0')â•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 41/203 [2m0:01:17 â€¢ 0:04:58[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.5586, 0.4414],37mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 42/203 [2m0:01:18 â€¢ 0:05:00[0m [2;4m0.54it/s[0m  
        [0.5543, 0.4457],
        [0.5526, 0.4474],
        [0.5586, 0.4414],
[2K        [0.5542, 0.4458]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 42/203 [2m0:01:18 â€¢ 0:05:00[0m [2;4m0.54it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 42/203 [2m0:01:18 â€¢ 0:05:00[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.6203, 0.3797]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:01:18 â€¢ 0:05:00[0m [2;4m0.54it/s[0m  
[2KStep 0 - TTA Loss: 0.9092â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 42/203 [2m0:01:18 â€¢ 0:05:00[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.6235, 0.3765]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:01:18 â€¢ 0:05:00[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.6267, 0.3733]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:01:18 â€¢ 0:05:00[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.6296, 0.3704]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:01:18 â€¢ 0:05:00[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.6321, 0.3679]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:01:18 â€¢ 0:05:00[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.6343, 0.3657]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:01:18 â€¢ 0:05:00[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.6360, 0.3640]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:01:18 â€¢ 0:05:00[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.6371, 0.3629]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:01:18 â€¢ 0:05:00[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.6378, 0.3622]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:01:18 â€¢ 0:05:00[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.6382, 0.3618]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:01:18 â€¢ 0:05:00[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.5551, 0.4449]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 42/203 [2m0:01:18 â€¢ 0:05:00[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.6515, 0.3485],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 42/203 [2m0:01:18 â€¢ 0:05:00[0m [2;4m0.54it/s[0m  
        [0.5780, 0.4220],
        [0.5746, 0.4254],
        [0.6793, 0.3207],
[2K        [0.6383, 0.3617]], device='cuda:0')â•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 42/203 [2m0:01:18 â€¢ 0:05:00[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.5586, 0.4414],37mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 43/203 [2m0:01:19 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
        [0.5543, 0.4457],
        [0.5526, 0.4474],
        [0.5586, 0.4414],
[2K        [0.5543, 0.4457]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 43/203 [2m0:01:19 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KClass label: HammerThrowâ”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 43/203 [2m0:01:19 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5549, 0.4451]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:01:19 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KStep 0 - TTA Loss: 0.7181â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 43/203 [2m0:01:19 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5549, 0.4451]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:01:19 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5550, 0.4450]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:01:19 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5555, 0.4445]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:01:19 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5566, 0.4434]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:01:19 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5585, 0.4415]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:01:19 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5614, 0.4386]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:01:19 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5652, 0.4348]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:01:19 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5699, 0.4301]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:01:19 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5754, 0.4246]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:01:19 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5553, 0.4447]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 43/203 [2m0:01:19 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6552, 0.3448],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 43/203 [2m0:01:19 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
        [0.5811, 0.4189],
        [0.5775, 0.4225],
        [0.6774, 0.3226],
[2K        [0.6403, 0.3597]], device='cuda:0')â•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 43/203 [2m0:01:19 â€¢ 0:04:09[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5586, 0.4414],8;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 44/203 [2m0:01:21 â€¢ 0:04:10[0m [2;4m0.64it/s[0m  
        [0.5544, 0.4456],
        [0.5527, 0.4473],
        [0.5587, 0.4413],
[2K        [0.5543, 0.4457]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 44/203 [2m0:01:21 â€¢ 0:04:10[0m [2;4m0.64it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 44/203 [2m0:01:21 â€¢ 0:04:10[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5591, 0.4409]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:01:21 â€¢ 0:04:10[0m [2;4m0.64it/s[0m  
[2KStep 0 - TTA Loss: 0.5833â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 44/203 [2m0:01:21 â€¢ 0:04:10[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5635, 0.4365]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:01:21 â€¢ 0:04:10[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5680, 0.4320]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:01:21 â€¢ 0:04:10[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5725, 0.4275]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:01:21 â€¢ 0:04:10[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5764, 0.4236]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:01:21 â€¢ 0:04:10[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5798, 0.4202]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:01:21 â€¢ 0:04:10[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5823, 0.4177]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:01:21 â€¢ 0:04:10[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5841, 0.4159]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:01:21 â€¢ 0:04:10[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5851, 0.4149]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:01:21 â€¢ 0:04:10[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5856, 0.4144]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:01:21 â€¢ 0:04:10[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5537, 0.4463]], device='cuda:0')7mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 44/203 [2m0:01:21 â€¢ 0:04:10[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.6584, 0.3416],224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 44/203 [2m0:01:21 â€¢ 0:04:10[0m [2;4m0.64it/s[0m  
        [0.5867, 0.4133],
        [0.5858, 0.4142],
        [0.6830, 0.3170],
[2K        [0.6446, 0.3554]], device='cuda:0');224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 44/203 [2m0:01:21 â€¢ 0:04:10[0m [2;4m0.64it/s[0m  
[2KAttention weights: tensor([[0.5587, 0.4413],8;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 45/203 [2m0:01:25 â€¢ 0:04:35[0m [2;4m0.58it/s[0m  
        [0.5546, 0.4454],
        [0.5528, 0.4472],
        [0.5587, 0.4413],
[2K        [0.5544, 0.4456]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 45/203 [2m0:01:25 â€¢ 0:04:35[0m [2;4m0.58it/s[0m  
[2KClass label: BaseballPitchâ”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 45/203 [2m0:01:25 â€¢ 0:04:35[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.6340, 0.3660]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:01:25 â€¢ 0:04:35[0m [2;4m0.58it/s[0m  
[2KStep 0 - TTA Loss: 1.2249â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 45/203 [2m0:01:25 â€¢ 0:04:35[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.6340, 0.3660]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:01:25 â€¢ 0:04:35[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.6341, 0.3659]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:01:25 â€¢ 0:04:35[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.6345, 0.3655]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:01:25 â€¢ 0:04:35[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.6351, 0.3649]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:01:25 â€¢ 0:04:35[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.6363, 0.3637]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:01:25 â€¢ 0:04:35[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.6379, 0.3621]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:01:25 â€¢ 0:04:35[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.6405, 0.3595]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:01:25 â€¢ 0:04:35[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.6443, 0.3557]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:01:25 â€¢ 0:04:35[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.6485, 0.3515]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:01:25 â€¢ 0:04:35[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5592, 0.4408]], device='cuda:0')7mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 45/203 [2m0:01:25 â€¢ 0:04:35[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.6528, 0.3472],224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 45/203 [2m0:01:25 â€¢ 0:04:35[0m [2;4m0.58it/s[0m  
        [0.5720, 0.4280],
        [0.5759, 0.4241],
        [0.6733, 0.3267],
[2K        [0.6359, 0.3641]], device='cuda:0');224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 45/203 [2m0:01:25 â€¢ 0:04:35[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5587, 0.4413],237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 46/203 [2m0:01:27 â€¢ 0:04:43[0m [2;4m0.56it/s[0m   
        [0.5546, 0.4454],
        [0.5528, 0.4472],
        [0.5587, 0.4413],
[2K        [0.5545, 0.4455]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 46/203 [2m0:01:27 â€¢ 0:04:43[0m [2;4m0.56it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 46/203 [2m0:01:27 â€¢ 0:04:43[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.6192, 0.3808]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 46/203 [2m0:01:27 â€¢ 0:04:43[0m [2;4m0.56it/s[0m  
[2KStep 0 - TTA Loss: 0.8428â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 46/203 [2m0:01:27 â€¢ 0:04:43[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.6221, 0.3779]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 46/203 [2m0:01:27 â€¢ 0:04:43[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.6248, 0.3752]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 46/203 [2m0:01:27 â€¢ 0:04:43[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.6271, 0.3729]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 46/203 [2m0:01:27 â€¢ 0:04:43[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.6291, 0.3709]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 46/203 [2m0:01:27 â€¢ 0:04:43[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.6306, 0.3694]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 46/203 [2m0:01:27 â€¢ 0:04:43[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.6317, 0.3683]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 46/203 [2m0:01:27 â€¢ 0:04:43[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.6323, 0.3677]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 46/203 [2m0:01:27 â€¢ 0:04:43[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.6327, 0.3673]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 46/203 [2m0:01:27 â€¢ 0:04:43[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.6329, 0.3671]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 46/203 [2m0:01:27 â€¢ 0:04:43[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.5551, 0.4449]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 46/203 [2m0:01:27 â€¢ 0:04:43[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.6530, 0.3470],â•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 46/203 [2m0:01:27 â€¢ 0:04:43[0m [2;4m0.56it/s[0m  
        [0.5697, 0.4303],
        [0.5730, 0.4270],
        [0.6730, 0.3270],
[2K        [0.6330, 0.3670]], device='cuda:0')mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 46/203 [2m0:01:27 â€¢ 0:04:43[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.5588, 0.4412],237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 47/203 [2m0:01:29 â€¢ 0:04:36[0m [2;4m0.57it/s[0m  
        [0.5546, 0.4454],
        [0.5528, 0.4472],
        [0.5588, 0.4412],
[2K        [0.5545, 0.4455]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 47/203 [2m0:01:29 â€¢ 0:04:36[0m [2;4m0.57it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 47/203 [2m0:01:29 â€¢ 0:04:36[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.5599, 0.4401]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 47/203 [2m0:01:29 â€¢ 0:04:36[0m [2;4m0.57it/s[0m  
[2KStep 0 - TTA Loss: 0.7509â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 47/203 [2m0:01:29 â€¢ 0:04:36[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.5599, 0.4401]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 47/203 [2m0:01:29 â€¢ 0:04:36[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.5600, 0.4400]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 47/203 [2m0:01:29 â€¢ 0:04:36[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.5605, 0.4395]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 47/203 [2m0:01:29 â€¢ 0:04:36[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.5620, 0.4380]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 47/203 [2m0:01:29 â€¢ 0:04:36[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.5647, 0.4353]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 47/203 [2m0:01:29 â€¢ 0:04:36[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.5692, 0.4308]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 47/203 [2m0:01:29 â€¢ 0:04:36[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.5754, 0.4246]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 47/203 [2m0:01:29 â€¢ 0:04:36[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.5832, 0.4168]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 47/203 [2m0:01:29 â€¢ 0:04:36[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.5927, 0.4073]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 47/203 [2m0:01:29 â€¢ 0:04:36[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.5545, 0.4455]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 47/203 [2m0:01:29 â€¢ 0:04:36[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.6618, 0.3382],â•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 47/203 [2m0:01:29 â€¢ 0:04:36[0m [2;4m0.57it/s[0m  
        [0.5897, 0.4103],
        [0.6034, 0.3966],
        [0.6882, 0.3118],
[2K        [0.6534, 0.3466]], device='cuda:0')mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 47/203 [2m0:01:29 â€¢ 0:04:36[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.5588, 0.4412],237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 48/203 [2m0:01:33 â€¢ 0:05:02[0m [2;4m0.51it/s[0m  
        [0.5547, 0.4453],
        [0.5529, 0.4471],
        [0.5588, 0.4412],
[2K        [0.5545, 0.4455]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 48/203 [2m0:01:33 â€¢ 0:05:02[0m [2;4m0.51it/s[0m  
[2KClass label: HammerThrowâ”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 48/203 [2m0:01:33 â€¢ 0:05:02[0m [2;4m0.51it/s[0m  
[2KAttention weights: tensor([[0.5623, 0.4377]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 48/203 [2m0:01:33 â€¢ 0:05:02[0m [2;4m0.51it/s[0m  
[2KStep 0 - TTA Loss: 1.0587â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 48/203 [2m0:01:33 â€¢ 0:05:02[0m [2;4m0.51it/s[0m  
[2KAttention weights: tensor([[0.5701, 0.4299]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 48/203 [2m0:01:33 â€¢ 0:05:02[0m [2;4m0.51it/s[0m  
[2KAttention weights: tensor([[0.5773, 0.4227]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 48/203 [2m0:01:33 â€¢ 0:05:02[0m [2;4m0.51it/s[0m  
[2KAttention weights: tensor([[0.5838, 0.4162]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 48/203 [2m0:01:33 â€¢ 0:05:02[0m [2;4m0.51it/s[0m  
[2KAttention weights: tensor([[0.5892, 0.4108]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 48/203 [2m0:01:33 â€¢ 0:05:02[0m [2;4m0.51it/s[0m  
[2KAttention weights: tensor([[0.5935, 0.4065]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 48/203 [2m0:01:33 â€¢ 0:05:02[0m [2;4m0.51it/s[0m  
[2KAttention weights: tensor([[0.5966, 0.4034]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 48/203 [2m0:01:33 â€¢ 0:05:02[0m [2;4m0.51it/s[0m  
[2KAttention weights: tensor([[0.5987, 0.4013]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 48/203 [2m0:01:33 â€¢ 0:05:02[0m [2;4m0.51it/s[0m  
[2KAttention weights: tensor([[0.5999, 0.4001]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 48/203 [2m0:01:33 â€¢ 0:05:02[0m [2;4m0.51it/s[0m  
[2KAttention weights: tensor([[0.6005, 0.3995]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 48/203 [2m0:01:33 â€¢ 0:05:02[0m [2;4m0.51it/s[0m  
[2KAttention weights: tensor([[0.5563, 0.4437]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 48/203 [2m0:01:33 â€¢ 0:05:02[0m [2;4m0.51it/s[0m  
[2KAttention weights: tensor([[0.6657, 0.3343],â•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 48/203 [2m0:01:33 â€¢ 0:05:02[0m [2;4m0.51it/s[0m  
        [0.6007, 0.3993],
        [0.6133, 0.3867],
        [0.6927, 0.3073],
[2K        [0.6620, 0.3380]], device='cuda:0')mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 48/203 [2m0:01:33 â€¢ 0:05:02[0m [2;4m0.51it/s[0m  
[2KAttention weights: tensor([[0.5589, 0.4411],98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 49/203 [2m0:01:34 â€¢ 0:04:48[0m [2;4m0.53it/s[0m  
        [0.5548, 0.4452],
        [0.5531, 0.4469],
        [0.5589, 0.4411],
[2K        [0.5546, 0.4454]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 49/203 [2m0:01:34 â€¢ 0:04:48[0m [2;4m0.53it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 49/203 [2m0:01:34 â€¢ 0:04:48[0m [2;4m0.53it/s[0m  
[2KAttention weights: tensor([[0.5625, 0.4375]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 49/203 [2m0:01:34 â€¢ 0:04:48[0m [2;4m0.53it/s[0m  
[2KStep 0 - TTA Loss: 1.0643â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 49/203 [2m0:01:34 â€¢ 0:04:48[0m [2;4m0.53it/s[0m  
[2KAttention weights: tensor([[0.5625, 0.4375]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 49/203 [2m0:01:34 â€¢ 0:04:48[0m [2;4m0.53it/s[0m  
[2KAttention weights: tensor([[0.5627, 0.4373]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 49/203 [2m0:01:34 â€¢ 0:04:48[0m [2;4m0.53it/s[0m  
[2KAttention weights: tensor([[0.5634, 0.4366]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 49/203 [2m0:01:34 â€¢ 0:04:48[0m [2;4m0.53it/s[0m  
[2KAttention weights: tensor([[0.5649, 0.4351]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 49/203 [2m0:01:34 â€¢ 0:04:48[0m [2;4m0.53it/s[0m  
[2KAttention weights: tensor([[0.5677, 0.4323]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 49/203 [2m0:01:34 â€¢ 0:04:48[0m [2;4m0.53it/s[0m  
[2KAttention weights: tensor([[0.5719, 0.4281]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 49/203 [2m0:01:34 â€¢ 0:04:48[0m [2;4m0.53it/s[0m  
[2KAttention weights: tensor([[0.5774, 0.4226]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 49/203 [2m0:01:34 â€¢ 0:04:48[0m [2;4m0.53it/s[0m  
[2KAttention weights: tensor([[0.5847, 0.4153]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 49/203 [2m0:01:34 â€¢ 0:04:48[0m [2;4m0.53it/s[0m  
[2KAttention weights: tensor([[0.5931, 0.4069]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 49/203 [2m0:01:34 â€¢ 0:04:48[0m [2;4m0.53it/s[0m  
[2KAttention weights: tensor([[0.5548, 0.4452]], device='cuda:0')37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 49/203 [2m0:01:34 â€¢ 0:04:48[0m [2;4m0.53it/s[0m  
[2KAttention weights: tensor([[0.6601, 0.3399],;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 49/203 [2m0:01:34 â€¢ 0:04:48[0m [2;4m0.53it/s[0m  
        [0.5896, 0.4104],
        [0.6024, 0.3976],
        [0.6875, 0.3125],
[2K        [0.6522, 0.3478]], device='cuda:0')6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 49/203 [2m0:01:34 â€¢ 0:04:48[0m [2;4m0.53it/s[0m  
[2KAttention weights: tensor([[0.5590, 0.4410],98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 50/203 [2m0:01:36 â€¢ 0:05:01[0m [2;4m0.51it/s[0m  
        [0.5549, 0.4451],
        [0.5533, 0.4467],
        [0.5589, 0.4411],
[2K        [0.5546, 0.4454]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 50/203 [2m0:01:36 â€¢ 0:05:01[0m [2;4m0.51it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 50/203 [2m0:01:36 â€¢ 0:05:01[0m [2;4m0.51it/s[0m  
[2KAttention weights: tensor([[0.6276, 0.3724]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 50/203 [2m0:01:36 â€¢ 0:05:01[0m [2;4m0.51it/s[0m  
[2KStep 0 - TTA Loss: 0.6208â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 50/203 [2m0:01:36 â€¢ 0:05:01[0m [2;4m0.51it/s[0m  
[2KAttention weights: tensor([[0.6344, 0.3656]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 50/203 [2m0:01:36 â€¢ 0:05:01[0m [2;4m0.51it/s[0m  
[2KAttention weights: tensor([[0.6411, 0.3589]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 50/203 [2m0:01:36 â€¢ 0:05:01[0m [2;4m0.51it/s[0m  
[2KAttention weights: tensor([[0.6475, 0.3525]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 50/203 [2m0:01:36 â€¢ 0:05:01[0m [2;4m0.51it/s[0m  
[2KAttention weights: tensor([[0.6530, 0.3470]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 50/203 [2m0:01:36 â€¢ 0:05:01[0m [2;4m0.51it/s[0m  
[2KAttention weights: tensor([[0.6576, 0.3424]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 50/203 [2m0:01:36 â€¢ 0:05:01[0m [2;4m0.51it/s[0m  
[2KAttention weights: tensor([[0.6610, 0.3390]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 50/203 [2m0:01:36 â€¢ 0:05:01[0m [2;4m0.51it/s[0m  
[2KAttention weights: tensor([[0.6634, 0.3366]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 50/203 [2m0:01:36 â€¢ 0:05:01[0m [2;4m0.51it/s[0m  
[2KAttention weights: tensor([[0.6647, 0.3353]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 50/203 [2m0:01:36 â€¢ 0:05:01[0m [2;4m0.51it/s[0m  
[2KAttention weights: tensor([[0.6653, 0.3347]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 50/203 [2m0:01:36 â€¢ 0:05:01[0m [2;4m0.51it/s[0m  
[2KAttention weights: tensor([[0.5566, 0.4434]], device='cuda:0')37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 50/203 [2m0:01:36 â€¢ 0:05:01[0m [2;4m0.51it/s[0m  
[2KAttention weights: tensor([[0.6687, 0.3313],;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 50/203 [2m0:01:36 â€¢ 0:05:01[0m [2;4m0.51it/s[0m  
        [0.6016, 0.3984],
        [0.6139, 0.3861],
        [0.6932, 0.3068],
[2K        [0.6656, 0.3344]], device='cuda:0')6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 50/203 [2m0:01:36 â€¢ 0:05:01[0m [2;4m0.51it/s[0m  
[2KAttention weights: tensor([[0.5590, 0.4410],;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 51/203 [2m0:01:38 â€¢ 0:05:36[0m [2;4m0.45it/s[0m   
        [0.5549, 0.4451],
        [0.5534, 0.4466],
        [0.5590, 0.4410],
[2K        [0.5547, 0.4453]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 51/203 [2m0:01:38 â€¢ 0:05:36[0m [2;4m0.45it/s[0m  
[2KClass label: BaseballPitchâ”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 51/203 [2m0:01:38 â€¢ 0:05:36[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.6385, 0.3615]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 51/203 [2m0:01:38 â€¢ 0:05:36[0m [2;4m0.45it/s[0m  
[2KStep 0 - TTA Loss: 0.9015â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 51/203 [2m0:01:38 â€¢ 0:05:36[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.6385, 0.3615]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 51/203 [2m0:01:38 â€¢ 0:05:36[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.6386, 0.3614]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 51/203 [2m0:01:38 â€¢ 0:05:36[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.6390, 0.3610]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 51/203 [2m0:01:38 â€¢ 0:05:36[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.6398, 0.3602]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 51/203 [2m0:01:38 â€¢ 0:05:36[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.6411, 0.3589]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 51/203 [2m0:01:38 â€¢ 0:05:36[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.6427, 0.3573]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 51/203 [2m0:01:38 â€¢ 0:05:36[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.6451, 0.3549]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 51/203 [2m0:01:38 â€¢ 0:05:36[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.6481, 0.3519]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 51/203 [2m0:01:38 â€¢ 0:05:36[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.6514, 0.3486]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 51/203 [2m0:01:38 â€¢ 0:05:36[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.5596, 0.4404]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 51/203 [2m0:01:38 â€¢ 0:05:36[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.6546, 0.3454],mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 51/203 [2m0:01:38 â€¢ 0:05:36[0m [2;4m0.45it/s[0m  
        [0.5748, 0.4252],
        [0.5792, 0.4208],
        [0.6785, 0.3215],
[2K        [0.6404, 0.3596]], device='cuda:0')7mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 51/203 [2m0:01:38 â€¢ 0:05:36[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.5590, 0.4410],;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 52/203 [2m0:01:38 â€¢ 0:05:17[0m [2;4m0.48it/s[0m  
        [0.5549, 0.4451],
        [0.5534, 0.4466],
        [0.5590, 0.4410],
[2K        [0.5547, 0.4453]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 52/203 [2m0:01:38 â€¢ 0:05:17[0m [2;4m0.48it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 52/203 [2m0:01:38 â€¢ 0:05:17[0m [2;4m0.48it/s[0m  
[2KAttention weights: tensor([[0.6222, 0.3778]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 52/203 [2m0:01:38 â€¢ 0:05:17[0m [2;4m0.48it/s[0m  
[2KStep 0 - TTA Loss: 1.1534â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 52/203 [2m0:01:38 â€¢ 0:05:17[0m [2;4m0.48it/s[0m  
[2KAttention weights: tensor([[0.6264, 0.3736]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 52/203 [2m0:01:38 â€¢ 0:05:17[0m [2;4m0.48it/s[0m  
[2KAttention weights: tensor([[0.6309, 0.3691]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 52/203 [2m0:01:38 â€¢ 0:05:17[0m [2;4m0.48it/s[0m  
[2KAttention weights: tensor([[0.6354, 0.3646]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 52/203 [2m0:01:38 â€¢ 0:05:17[0m [2;4m0.48it/s[0m  
[2KAttention weights: tensor([[0.6400, 0.3600]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 52/203 [2m0:01:38 â€¢ 0:05:17[0m [2;4m0.48it/s[0m  
[2KAttention weights: tensor([[0.6439, 0.3561]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 52/203 [2m0:01:38 â€¢ 0:05:17[0m [2;4m0.48it/s[0m  
[2KAttention weights: tensor([[0.6472, 0.3528]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 52/203 [2m0:01:38 â€¢ 0:05:17[0m [2;4m0.48it/s[0m  
[2KAttention weights: tensor([[0.6496, 0.3504]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 52/203 [2m0:01:38 â€¢ 0:05:17[0m [2;4m0.48it/s[0m  
[2KAttention weights: tensor([[0.6512, 0.3488]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 52/203 [2m0:01:38 â€¢ 0:05:17[0m [2;4m0.48it/s[0m  
[2KAttention weights: tensor([[0.6520, 0.3480]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 52/203 [2m0:01:38 â€¢ 0:05:17[0m [2;4m0.48it/s[0m  
[2KAttention weights: tensor([[0.5560, 0.4440]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 52/203 [2m0:01:38 â€¢ 0:05:17[0m [2;4m0.48it/s[0m  
[2KAttention weights: tensor([[0.6619, 0.3381],mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 52/203 [2m0:01:38 â€¢ 0:05:17[0m [2;4m0.48it/s[0m  
        [0.5836, 0.4164],
        [0.5877, 0.4123],
        [0.6796, 0.3204],
[2K        [0.6523, 0.3477]], device='cuda:0')7mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 52/203 [2m0:01:38 â€¢ 0:05:17[0m [2;4m0.48it/s[0m  
[2KAttention weights: tensor([[0.5589, 0.4411],;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 53/203 [2m0:01:39 â€¢ 0:04:51[0m [2;4m0.52it/s[0m  
        [0.5549, 0.4451],
        [0.5534, 0.4466],
        [0.5589, 0.4411],
[2K        [0.5547, 0.4453]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 53/203 [2m0:01:39 â€¢ 0:04:51[0m [2;4m0.52it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 53/203 [2m0:01:39 â€¢ 0:04:51[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.5637, 0.4363]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 53/203 [2m0:01:39 â€¢ 0:04:51[0m [2;4m0.52it/s[0m  
[2KStep 0 - TTA Loss: 1.1738â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 53/203 [2m0:01:39 â€¢ 0:04:51[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.5637, 0.4363]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 53/203 [2m0:01:39 â€¢ 0:04:51[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.5638, 0.4362]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 53/203 [2m0:01:39 â€¢ 0:04:51[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.5644, 0.4356]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 53/203 [2m0:01:39 â€¢ 0:04:51[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.5655, 0.4345]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 53/203 [2m0:01:39 â€¢ 0:04:51[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.5675, 0.4325]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 53/203 [2m0:01:39 â€¢ 0:04:51[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.5701, 0.4299]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 53/203 [2m0:01:39 â€¢ 0:04:51[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.5735, 0.4265]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 53/203 [2m0:01:39 â€¢ 0:04:51[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.5775, 0.4225]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 53/203 [2m0:01:39 â€¢ 0:04:51[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.5816, 0.4184]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 53/203 [2m0:01:39 â€¢ 0:04:51[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.5543, 0.4457]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 53/203 [2m0:01:39 â€¢ 0:04:51[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.6535, 0.3465],mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 53/203 [2m0:01:39 â€¢ 0:04:51[0m [2;4m0.52it/s[0m  
        [0.5788, 0.4212],
        [0.5859, 0.4141],
        [0.6814, 0.3186],
[2K        [0.6463, 0.3537]], device='cuda:0')7mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 53/203 [2m0:01:39 â€¢ 0:04:51[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.5589, 0.4411],;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 54/203 [2m0:01:41 â€¢ 0:04:48[0m [2;4m0.52it/s[0m  
        [0.5549, 0.4451],
        [0.5534, 0.4466],
        [0.5590, 0.4410],
[2K        [0.5547, 0.4453]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 54/203 [2m0:01:41 â€¢ 0:04:48[0m [2;4m0.52it/s[0m  
[2KClass label: SoccerPenaltyâ”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 54/203 [2m0:01:41 â€¢ 0:04:48[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.6667, 0.3333]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 54/203 [2m0:01:41 â€¢ 0:04:48[0m [2;4m0.52it/s[0m  
[2KStep 0 - TTA Loss: 0.4760â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 54/203 [2m0:01:41 â€¢ 0:04:48[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.6716, 0.3284]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 54/203 [2m0:01:41 â€¢ 0:04:48[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.6783, 0.3217]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 54/203 [2m0:01:41 â€¢ 0:04:48[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.6858, 0.3142]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 54/203 [2m0:01:41 â€¢ 0:04:48[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.6935, 0.3065]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 54/203 [2m0:01:41 â€¢ 0:04:48[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.7004, 0.2996]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 54/203 [2m0:01:41 â€¢ 0:04:48[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.7059, 0.2941]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 54/203 [2m0:01:41 â€¢ 0:04:48[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.7098, 0.2902]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 54/203 [2m0:01:41 â€¢ 0:04:48[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.7123, 0.2877]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 54/203 [2m0:01:41 â€¢ 0:04:48[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.7134, 0.2866]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 54/203 [2m0:01:41 â€¢ 0:04:48[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.5610, 0.4390]], device='cuda:0')237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 54/203 [2m0:01:41 â€¢ 0:04:48[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.6683, 0.3317],6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 54/203 [2m0:01:41 â€¢ 0:04:48[0m [2;4m0.52it/s[0m  
        [0.5909, 0.4091],
        [0.5954, 0.4046],
        [0.7138, 0.2862],
[2K        [0.6529, 0.3471]], device='cuda:0');6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 54/203 [2m0:01:41 â€¢ 0:04:48[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.5589, 0.4411],;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 55/203 [2m0:01:43 â€¢ 0:04:55[0m [2;4m0.50it/s[0m  
        [0.5550, 0.4450],
        [0.5535, 0.4465],
        [0.5590, 0.4410],
[2K        [0.5547, 0.4453]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 55/203 [2m0:01:43 â€¢ 0:04:55[0m [2;4m0.50it/s[0m  
[2KClass label: HammerThrowâ”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 55/203 [2m0:01:43 â€¢ 0:04:55[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.5568, 0.4432]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 55/203 [2m0:01:43 â€¢ 0:04:55[0m [2;4m0.50it/s[0m  
[2KStep 0 - TTA Loss: 0.6539â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 55/203 [2m0:01:43 â€¢ 0:04:55[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.5568, 0.4432]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 55/203 [2m0:01:43 â€¢ 0:04:55[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.5570, 0.4430]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 55/203 [2m0:01:43 â€¢ 0:04:55[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.5580, 0.4420]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 55/203 [2m0:01:43 â€¢ 0:04:55[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.5602, 0.4398]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 55/203 [2m0:01:43 â€¢ 0:04:55[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.5642, 0.4358]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 55/203 [2m0:01:43 â€¢ 0:04:55[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.5702, 0.4298]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 55/203 [2m0:01:43 â€¢ 0:04:55[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.5784, 0.4216]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 55/203 [2m0:01:43 â€¢ 0:04:55[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.5887, 0.4113]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 55/203 [2m0:01:43 â€¢ 0:04:55[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.6009, 0.3991]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 55/203 [2m0:01:43 â€¢ 0:04:55[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.5572, 0.4428]], device='cuda:0')237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 55/203 [2m0:01:43 â€¢ 0:04:55[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.6806, 0.3194],6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 55/203 [2m0:01:43 â€¢ 0:04:55[0m [2;4m0.50it/s[0m  
        [0.6139, 0.3861],
        [0.6018, 0.3982],
        [0.7143, 0.2857],
[2K        [0.6612, 0.3388]], device='cuda:0');6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 55/203 [2m0:01:43 â€¢ 0:04:55[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.5591, 0.4409],5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 56/203 [2m0:01:44 â€¢ 0:04:29[0m [2;4m0.55it/s[0m   
        [0.5551, 0.4449],
        [0.5536, 0.4464],
        [0.5591, 0.4409],
[2K        [0.5549, 0.4451]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 56/203 [2m0:01:44 â€¢ 0:04:29[0m [2;4m0.55it/s[0m  
[2KClass label: HammerThrowâ”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 56/203 [2m0:01:44 â€¢ 0:04:29[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.5591, 0.4409]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 56/203 [2m0:01:44 â€¢ 0:04:29[0m [2;4m0.55it/s[0m  
[2KStep 0 - TTA Loss: 1.3059â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 56/203 [2m0:01:44 â€¢ 0:04:29[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.5719, 0.4281]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 56/203 [2m0:01:44 â€¢ 0:04:29[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.5839, 0.4161]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 56/203 [2m0:01:44 â€¢ 0:04:29[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.5938, 0.4062]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 56/203 [2m0:01:44 â€¢ 0:04:29[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.6020, 0.3980]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 56/203 [2m0:01:44 â€¢ 0:04:29[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.6086, 0.3914]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 56/203 [2m0:01:44 â€¢ 0:04:29[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.6134, 0.3866]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 56/203 [2m0:01:44 â€¢ 0:04:29[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.6166, 0.3834]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 56/203 [2m0:01:44 â€¢ 0:04:29[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.6184, 0.3816]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 56/203 [2m0:01:44 â€¢ 0:04:29[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.6192, 0.3808]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 56/203 [2m0:01:44 â€¢ 0:04:29[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.5576, 0.4424]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 56/203 [2m0:01:44 â€¢ 0:04:29[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.6830, 0.3170],7mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 56/203 [2m0:01:44 â€¢ 0:04:29[0m [2;4m0.55it/s[0m  
        [0.6195, 0.3805],
        [0.6050, 0.3950],
        [0.7100, 0.2900],
[2K        [0.6637, 0.3363]], device='cuda:0')37mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 56/203 [2m0:01:44 â€¢ 0:04:29[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.5592, 0.4408],5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 57/203 [2m0:01:45 â€¢ 0:04:15[0m [2;4m0.57it/s[0m  
        [0.5554, 0.4446],
        [0.5538, 0.4462],
        [0.5592, 0.4408],
[2K        [0.5550, 0.4450]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 57/203 [2m0:01:45 â€¢ 0:04:15[0m [2;4m0.57it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 57/203 [2m0:01:45 â€¢ 0:04:15[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.5625, 0.4375]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 57/203 [2m0:01:45 â€¢ 0:04:15[0m [2;4m0.57it/s[0m  
[2KStep 0 - TTA Loss: 0.6563â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 57/203 [2m0:01:45 â€¢ 0:04:15[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.5625, 0.4375]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 57/203 [2m0:01:45 â€¢ 0:04:15[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.5627, 0.4373]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 57/203 [2m0:01:45 â€¢ 0:04:15[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.5632, 0.4368]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 57/203 [2m0:01:45 â€¢ 0:04:15[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.5642, 0.4358]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 57/203 [2m0:01:45 â€¢ 0:04:15[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.5659, 0.4341]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 57/203 [2m0:01:45 â€¢ 0:04:15[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.5683, 0.4317]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 57/203 [2m0:01:45 â€¢ 0:04:15[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.5712, 0.4288]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 57/203 [2m0:01:45 â€¢ 0:04:15[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.5747, 0.4253]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 57/203 [2m0:01:45 â€¢ 0:04:15[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.5783, 0.4217]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 57/203 [2m0:01:45 â€¢ 0:04:15[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.5545, 0.4455]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 57/203 [2m0:01:45 â€¢ 0:04:15[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.6565, 0.3435],7mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 57/203 [2m0:01:45 â€¢ 0:04:15[0m [2;4m0.57it/s[0m  
        [0.5798, 0.4202],
        [0.5819, 0.4181],
        [0.6794, 0.3206],
[2K        [0.6419, 0.3581]], device='cuda:0')37mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 57/203 [2m0:01:45 â€¢ 0:04:15[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.5592, 0.4408],5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 58/203 [2m0:01:47 â€¢ 0:04:19[0m [2;4m0.56it/s[0m  
        [0.5554, 0.4446],
        [0.5538, 0.4462],
        [0.5592, 0.4408],
[2K        [0.5550, 0.4450]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 58/203 [2m0:01:47 â€¢ 0:04:19[0m [2;4m0.56it/s[0m  
[2KClass label: SoccerPenaltyâ”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 58/203 [2m0:01:47 â€¢ 0:04:19[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.6681, 0.3319]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 58/203 [2m0:01:47 â€¢ 0:04:19[0m [2;4m0.56it/s[0m  
[2KStep 0 - TTA Loss: 0.8389â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 58/203 [2m0:01:47 â€¢ 0:04:19[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.6736, 0.3264]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 58/203 [2m0:01:47 â€¢ 0:04:19[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.6810, 0.3190]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 58/203 [2m0:01:47 â€¢ 0:04:19[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.6890, 0.3110]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 58/203 [2m0:01:47 â€¢ 0:04:19[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.6966, 0.3034]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 58/203 [2m0:01:47 â€¢ 0:04:19[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.7033, 0.2967]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 58/203 [2m0:01:47 â€¢ 0:04:19[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.7086, 0.2914]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 58/203 [2m0:01:47 â€¢ 0:04:19[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.7122, 0.2878]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 58/203 [2m0:01:47 â€¢ 0:04:19[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.7144, 0.2856]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 58/203 [2m0:01:47 â€¢ 0:04:19[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.7154, 0.2846]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 58/203 [2m0:01:47 â€¢ 0:04:19[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.5613, 0.4387]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 58/203 [2m0:01:47 â€¢ 0:04:19[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.6727, 0.3273],7mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 58/203 [2m0:01:47 â€¢ 0:04:19[0m [2;4m0.56it/s[0m  
        [0.5938, 0.4062],
        [0.5938, 0.4062],
        [0.7157, 0.2843],
[2K        [0.6510, 0.3490]], device='cuda:0')37mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 58/203 [2m0:01:47 â€¢ 0:04:19[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.5593, 0.4407],2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 59/203 [2m0:01:48 â€¢ 0:04:12[0m [2;4m0.57it/s[0m  
        [0.5554, 0.4446],
        [0.5538, 0.4462],
        [0.5593, 0.4407],
[2K        [0.5551, 0.4449]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 59/203 [2m0:01:48 â€¢ 0:04:12[0m [2;4m0.57it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 59/203 [2m0:01:48 â€¢ 0:04:12[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.6228, 0.3772]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 59/203 [2m0:01:48 â€¢ 0:04:12[0m [2;4m0.57it/s[0m  
[2KStep 0 - TTA Loss: 1.4257â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 59/203 [2m0:01:48 â€¢ 0:04:12[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.6228, 0.3772]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 59/203 [2m0:01:48 â€¢ 0:04:12[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.6229, 0.3771]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 59/203 [2m0:01:48 â€¢ 0:04:12[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.6233, 0.3767]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 59/203 [2m0:01:48 â€¢ 0:04:12[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.6243, 0.3757]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 59/203 [2m0:01:48 â€¢ 0:04:12[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.6259, 0.3741]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 59/203 [2m0:01:48 â€¢ 0:04:12[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.6284, 0.3716]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 59/203 [2m0:01:48 â€¢ 0:04:12[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.6315, 0.3685]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 59/203 [2m0:01:48 â€¢ 0:04:12[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.6353, 0.3647]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 59/203 [2m0:01:48 â€¢ 0:04:12[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.6396, 0.3604]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 59/203 [2m0:01:48 â€¢ 0:04:12[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.5563, 0.4437]], device='cuda:0');237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 59/203 [2m0:01:48 â€¢ 0:04:12[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.6570, 0.3430],;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 59/203 [2m0:01:48 â€¢ 0:04:12[0m [2;4m0.57it/s[0m  
        [0.5803, 0.4197],
        [0.5848, 0.4152],
        [0.6943, 0.3057],
[2K        [0.6444, 0.3556]], device='cuda:0')8;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 59/203 [2m0:01:48 â€¢ 0:04:12[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.5593, 0.4407],2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 60/203 [2m0:01:49 â€¢ 0:04:16[0m [2;4m0.56it/s[0m  
        [0.5555, 0.4445],
        [0.5538, 0.4462],
        [0.5594, 0.4406],
[2K        [0.5552, 0.4448]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 60/203 [2m0:01:49 â€¢ 0:04:16[0m [2;4m0.56it/s[0m  
[2KClass label: HammerThrowâ”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 60/203 [2m0:01:49 â€¢ 0:04:16[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.5613, 0.4387]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 60/203 [2m0:01:49 â€¢ 0:04:16[0m [2;4m0.56it/s[0m  
[2KStep 0 - TTA Loss: 0.9035â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 60/203 [2m0:01:49 â€¢ 0:04:16[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.5679, 0.4321]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 60/203 [2m0:01:49 â€¢ 0:04:16[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.5757, 0.4243]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 60/203 [2m0:01:49 â€¢ 0:04:16[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.5843, 0.4157]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 60/203 [2m0:01:49 â€¢ 0:04:16[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.5931, 0.4069]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 60/203 [2m0:01:49 â€¢ 0:04:16[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.6010, 0.3990]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 60/203 [2m0:01:49 â€¢ 0:04:16[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.6073, 0.3927]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 60/203 [2m0:01:49 â€¢ 0:04:16[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.6119, 0.3881]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 60/203 [2m0:01:49 â€¢ 0:04:16[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.6148, 0.3852]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 60/203 [2m0:01:49 â€¢ 0:04:16[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.6163, 0.3837]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 60/203 [2m0:01:49 â€¢ 0:04:16[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.5579, 0.4421]], device='cuda:0');237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 60/203 [2m0:01:49 â€¢ 0:04:16[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.6844, 0.3156],;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 60/203 [2m0:01:49 â€¢ 0:04:16[0m [2;4m0.56it/s[0m  
        [0.6167, 0.3833],
        [0.6052, 0.3948],
        [0.7077, 0.2923],
[2K        [0.6673, 0.3327]], device='cuda:0')8;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 60/203 [2m0:01:49 â€¢ 0:04:16[0m [2;4m0.56it/s[0m  
[2KAttention weights: tensor([[0.5595, 0.4405],;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 61/203 [2m0:01:52 â€¢ 0:03:57[0m [2;4m0.60it/s[0m   
        [0.5558, 0.4442],
        [0.5540, 0.4460],
        [0.5596, 0.4404],
[2K        [0.5554, 0.4446]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 61/203 [2m0:01:52 â€¢ 0:03:57[0m [2;4m0.60it/s[0m  
[2KClass label: SoccerPenaltyâ”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 61/203 [2m0:01:52 â€¢ 0:03:57[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.6656, 0.3344]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 61/203 [2m0:01:52 â€¢ 0:03:57[0m [2;4m0.60it/s[0m  
[2KStep 0 - TTA Loss: 1.5006â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 61/203 [2m0:01:52 â€¢ 0:03:57[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.6656, 0.3344]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 61/203 [2m0:01:52 â€¢ 0:03:57[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.6657, 0.3343]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 61/203 [2m0:01:52 â€¢ 0:03:57[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.6661, 0.3339]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 61/203 [2m0:01:52 â€¢ 0:03:57[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.6666, 0.3334]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 61/203 [2m0:01:52 â€¢ 0:03:57[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.6670, 0.3330]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 61/203 [2m0:01:52 â€¢ 0:03:57[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.6675, 0.3325]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 61/203 [2m0:01:52 â€¢ 0:03:57[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.6673, 0.3327]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 61/203 [2m0:01:52 â€¢ 0:03:57[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.6651, 0.3349]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 61/203 [2m0:01:52 â€¢ 0:03:57[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.6604, 0.3396]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 61/203 [2m0:01:52 â€¢ 0:03:57[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.5591, 0.4409]], device='cuda:0')mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 61/203 [2m0:01:52 â€¢ 0:03:57[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.6458, 0.3542],37mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 61/203 [2m0:01:52 â€¢ 0:03:57[0m [2;4m0.60it/s[0m  
        [0.5771, 0.4229],
        [0.5726, 0.4274],
        [0.6550, 0.3450],
[2K        [0.6373, 0.3627]], device='cuda:0')237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 61/203 [2m0:01:52 â€¢ 0:03:57[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.5595, 0.4405],;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 62/203 [2m0:01:53 â€¢ 0:03:53[0m [2;4m0.61it/s[0m  
        [0.5558, 0.4442],
        [0.5540, 0.4460],
        [0.5595, 0.4405],
[2K        [0.5554, 0.4446]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 62/203 [2m0:01:53 â€¢ 0:03:53[0m [2;4m0.61it/s[0m  
[2KClass label: SoccerPenaltyâ”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 62/203 [2m0:01:53 â€¢ 0:03:53[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.6636, 0.3364]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 62/203 [2m0:01:53 â€¢ 0:03:53[0m [2;4m0.61it/s[0m  
[2KStep 0 - TTA Loss: 1.1731â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 62/203 [2m0:01:53 â€¢ 0:03:53[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.6581, 0.3419]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 62/203 [2m0:01:53 â€¢ 0:03:53[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.6532, 0.3468]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 62/203 [2m0:01:53 â€¢ 0:03:53[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.6492, 0.3508]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 62/203 [2m0:01:53 â€¢ 0:03:53[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.6456, 0.3544]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 62/203 [2m0:01:53 â€¢ 0:03:53[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.6422, 0.3578]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 62/203 [2m0:01:53 â€¢ 0:03:53[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.6396, 0.3604]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 62/203 [2m0:01:53 â€¢ 0:03:53[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.6381, 0.3619]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 62/203 [2m0:01:53 â€¢ 0:03:53[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.6373, 0.3627]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 62/203 [2m0:01:53 â€¢ 0:03:53[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.6369, 0.3631]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 62/203 [2m0:01:53 â€¢ 0:03:53[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.5582, 0.4418]], device='cuda:0')mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 62/203 [2m0:01:53 â€¢ 0:03:53[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.6311, 0.3689],37mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 62/203 [2m0:01:53 â€¢ 0:03:53[0m [2;4m0.61it/s[0m  
        [0.5595, 0.4405],
        [0.5593, 0.4407],
        [0.6369, 0.3631],
[2K        [0.6224, 0.3776]], device='cuda:0')237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 62/203 [2m0:01:53 â€¢ 0:03:53[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.5594, 0.4406],;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 63/203 [2m0:01:57 â€¢ 0:04:06[0m [2;4m0.57it/s[0m  
        [0.5558, 0.4442],
        [0.5540, 0.4460],
        [0.5593, 0.4407],
[2K        [0.5554, 0.4446]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 63/203 [2m0:01:57 â€¢ 0:04:06[0m [2;4m0.57it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 63/203 [2m0:01:57 â€¢ 0:04:06[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.6325, 0.3675]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 63/203 [2m0:01:57 â€¢ 0:04:06[0m [2;4m0.57it/s[0m  
[2KStep 0 - TTA Loss: 0.9207â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 63/203 [2m0:01:57 â€¢ 0:04:06[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.6325, 0.3675]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 63/203 [2m0:01:57 â€¢ 0:04:06[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.6326, 0.3674]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 63/203 [2m0:01:57 â€¢ 0:04:06[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.6331, 0.3669]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 63/203 [2m0:01:57 â€¢ 0:04:06[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.6343, 0.3657]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 63/203 [2m0:01:57 â€¢ 0:04:06[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.6369, 0.3631]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 63/203 [2m0:01:57 â€¢ 0:04:06[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.6408, 0.3592]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 63/203 [2m0:01:57 â€¢ 0:04:06[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.6465, 0.3535]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 63/203 [2m0:01:57 â€¢ 0:04:06[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.6540, 0.3460]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 63/203 [2m0:01:57 â€¢ 0:04:06[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.6634, 0.3366]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 63/203 [2m0:01:57 â€¢ 0:04:06[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.5577, 0.4423]], device='cuda:0')mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 63/203 [2m0:01:57 â€¢ 0:04:06[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.6724, 0.3276],37mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 63/203 [2m0:01:57 â€¢ 0:04:06[0m [2;4m0.57it/s[0m  
        [0.6024, 0.3976],
        [0.6046, 0.3954],
        [0.6865, 0.3135],
[2K        [0.6741, 0.3259]], device='cuda:0')237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 63/203 [2m0:01:57 â€¢ 0:04:06[0m [2;4m0.57it/s[0m  
[2KAttention weights: tensor([[0.5594, 0.4406],;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 64/203 [2m0:02:06 â€¢ 0:05:05[0m [2;4m0.46it/s[0m  
        [0.5558, 0.4442],
        [0.5540, 0.4460],
        [0.5593, 0.4407],
[2K        [0.5555, 0.4445]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 64/203 [2m0:02:06 â€¢ 0:05:05[0m [2;4m0.46it/s[0m  
[2KClass label: BaseballPitchâ”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 64/203 [2m0:02:06 â€¢ 0:05:05[0m [2;4m0.46it/s[0m  
[2KAttention weights: tensor([[0.6385, 0.3615]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 64/203 [2m0:02:06 â€¢ 0:05:05[0m [2;4m0.46it/s[0m  
[2KStep 0 - TTA Loss: 0.4564â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 64/203 [2m0:02:06 â€¢ 0:05:05[0m [2;4m0.46it/s[0m  
[2KAttention weights: tensor([[0.6477, 0.3523]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 64/203 [2m0:02:06 â€¢ 0:05:05[0m [2;4m0.46it/s[0m  
[2KAttention weights: tensor([[0.6580, 0.3420]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 64/203 [2m0:02:06 â€¢ 0:05:05[0m [2;4m0.46it/s[0m  
[2KAttention weights: tensor([[0.6685, 0.3315]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 64/203 [2m0:02:06 â€¢ 0:05:05[0m [2;4m0.46it/s[0m  
[2KAttention weights: tensor([[0.6784, 0.3216]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 64/203 [2m0:02:06 â€¢ 0:05:05[0m [2;4m0.46it/s[0m  
[2KAttention weights: tensor([[0.6870, 0.3130]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 64/203 [2m0:02:06 â€¢ 0:05:05[0m [2;4m0.46it/s[0m  
[2KAttention weights: tensor([[0.6938, 0.3062]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 64/203 [2m0:02:06 â€¢ 0:05:05[0m [2;4m0.46it/s[0m  
[2KAttention weights: tensor([[0.6986, 0.3014]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 64/203 [2m0:02:06 â€¢ 0:05:05[0m [2;4m0.46it/s[0m  
[2KAttention weights: tensor([[0.7015, 0.2985]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 64/203 [2m0:02:06 â€¢ 0:05:05[0m [2;4m0.46it/s[0m  
[2KAttention weights: tensor([[0.7029, 0.2971]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 64/203 [2m0:02:06 â€¢ 0:05:05[0m [2;4m0.46it/s[0m  
[2KAttention weights: tensor([[0.5621, 0.4379]], device='cuda:0')5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 64/203 [2m0:02:06 â€¢ 0:05:05[0m [2;4m0.46it/s[0m  
[2KAttention weights: tensor([[0.7033, 0.2967],8;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 64/203 [2m0:02:06 â€¢ 0:05:05[0m [2;4m0.46it/s[0m  
        [0.6158, 0.3842],
        [0.6112, 0.3888],
        [0.7021, 0.2979],
[2K        [0.6841, 0.3159]], device='cuda:0')98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 64/203 [2m0:02:06 â€¢ 0:05:05[0m [2;4m0.46it/s[0m  
[2KAttention weights: tensor([[0.5596, 0.4404],;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 65/203 [2m0:02:10 â€¢ 0:06:13[0m [2;4m0.37it/s[0m  
        [0.5559, 0.4441],
        [0.5541, 0.4459],
        [0.5594, 0.4406],
[2K        [0.5557, 0.4443]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 65/203 [2m0:02:10 â€¢ 0:06:13[0m [2;4m0.37it/s[0m  
[2KClass label: BaseballPitchâ”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 65/203 [2m0:02:10 â€¢ 0:06:13[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.6403, 0.3597]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 65/203 [2m0:02:10 â€¢ 0:06:13[0m [2;4m0.37it/s[0m  
[2KStep 0 - TTA Loss: 0.8106â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 65/203 [2m0:02:10 â€¢ 0:06:13[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.6403, 0.3597]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 65/203 [2m0:02:10 â€¢ 0:06:13[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.6406, 0.3594]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 65/203 [2m0:02:10 â€¢ 0:06:13[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.6420, 0.3580]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 65/203 [2m0:02:10 â€¢ 0:06:13[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.6448, 0.3552]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 65/203 [2m0:02:10 â€¢ 0:06:13[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.6495, 0.3505]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 65/203 [2m0:02:10 â€¢ 0:06:13[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.6560, 0.3439]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 65/203 [2m0:02:10 â€¢ 0:06:13[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.6642, 0.3358]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 65/203 [2m0:02:10 â€¢ 0:06:13[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.6738, 0.3262]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 65/203 [2m0:02:10 â€¢ 0:06:13[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.6842, 0.3158]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 65/203 [2m0:02:10 â€¢ 0:06:13[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5619, 0.4381]], device='cuda:0')5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 65/203 [2m0:02:10 â€¢ 0:06:13[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.6950, 0.3050],8;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 65/203 [2m0:02:10 â€¢ 0:06:13[0m [2;4m0.37it/s[0m  
        [0.5997, 0.4003],
        [0.5917, 0.4083],
        [0.6975, 0.3025],
[2K        [0.6567, 0.3433]], device='cuda:0')98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 65/203 [2m0:02:10 â€¢ 0:06:13[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5596, 0.4404],8;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 66/203 [2m0:02:11 â€¢ 0:05:51[0m [2;4m0.39it/s[0m   
        [0.5560, 0.4440],
        [0.5541, 0.4459],
        [0.5594, 0.4406],
[2K        [0.5557, 0.4443]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 66/203 [2m0:02:11 â€¢ 0:05:51[0m [2;4m0.39it/s[0m  
[2KClass label: SoccerPenaltyâ”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 66/203 [2m0:02:11 â€¢ 0:05:51[0m [2;4m0.39it/s[0m  
[2KAttention weights: tensor([[0.6679, 0.3321]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 66/203 [2m0:02:11 â€¢ 0:05:51[0m [2;4m0.39it/s[0m  
[2KStep 0 - TTA Loss: 0.8349â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 66/203 [2m0:02:11 â€¢ 0:05:51[0m [2;4m0.39it/s[0m  
[2KAttention weights: tensor([[0.6734, 0.3266]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 66/203 [2m0:02:11 â€¢ 0:05:51[0m [2;4m0.39it/s[0m  
[2KAttention weights: tensor([[0.6787, 0.3213]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 66/203 [2m0:02:11 â€¢ 0:05:51[0m [2;4m0.39it/s[0m  
[2KAttention weights: tensor([[0.6836, 0.3164]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 66/203 [2m0:02:11 â€¢ 0:05:51[0m [2;4m0.39it/s[0m  
[2KAttention weights: tensor([[0.6880, 0.3120]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 66/203 [2m0:02:11 â€¢ 0:05:51[0m [2;4m0.39it/s[0m  
[2KAttention weights: tensor([[0.6916, 0.3084]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 66/203 [2m0:02:11 â€¢ 0:05:51[0m [2;4m0.39it/s[0m  
[2KAttention weights: tensor([[0.6943, 0.3057]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 66/203 [2m0:02:11 â€¢ 0:05:51[0m [2;4m0.39it/s[0m  
[2KAttention weights: tensor([[0.6961, 0.3039]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 66/203 [2m0:02:11 â€¢ 0:05:51[0m [2;4m0.39it/s[0m  
[2KAttention weights: tensor([[0.6973, 0.3027]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 66/203 [2m0:02:11 â€¢ 0:05:51[0m [2;4m0.39it/s[0m  
[2KAttention weights: tensor([[0.6978, 0.3022]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 66/203 [2m0:02:11 â€¢ 0:05:51[0m [2;4m0.39it/s[0m  
[2KAttention weights: tensor([[0.5609, 0.4391]], device='cuda:0')7mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 66/203 [2m0:02:11 â€¢ 0:05:51[0m [2;4m0.39it/s[0m  
[2KAttention weights: tensor([[0.6883, 0.3117],237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 66/203 [2m0:02:11 â€¢ 0:05:51[0m [2;4m0.39it/s[0m  
        [0.5920, 0.4080],
        [0.5892, 0.4108],
        [0.6980, 0.3020],
[2K        [0.6517, 0.3483]], device='cuda:0');237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 66/203 [2m0:02:11 â€¢ 0:05:51[0m [2;4m0.39it/s[0m  
[2KAttention weights: tensor([[0.5598, 0.4402],8;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 67/203 [2m0:02:12 â€¢ 0:05:31[0m [2;4m0.41it/s[0m  
        [0.5561, 0.4439],
        [0.5542, 0.4458],
        [0.5595, 0.4405],
[2K        [0.5558, 0.4442]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 67/203 [2m0:02:12 â€¢ 0:05:31[0m [2;4m0.41it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 67/203 [2m0:02:12 â€¢ 0:05:31[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.5627, 0.4373]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 67/203 [2m0:02:12 â€¢ 0:05:31[0m [2;4m0.41it/s[0m  
[2KStep 0 - TTA Loss: 0.6572â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 67/203 [2m0:02:12 â€¢ 0:05:31[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.5627, 0.4373]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 67/203 [2m0:02:12 â€¢ 0:05:31[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.5629, 0.4371]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 67/203 [2m0:02:12 â€¢ 0:05:31[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.5635, 0.4365]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 67/203 [2m0:02:12 â€¢ 0:05:31[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.5652, 0.4348]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 67/203 [2m0:02:12 â€¢ 0:05:31[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.5686, 0.4314]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 67/203 [2m0:02:12 â€¢ 0:05:31[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.5741, 0.4259]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 67/203 [2m0:02:12 â€¢ 0:05:31[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.5818, 0.4182]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 67/203 [2m0:02:12 â€¢ 0:05:31[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.5916, 0.4084]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 67/203 [2m0:02:12 â€¢ 0:05:31[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.6032, 0.3968]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 67/203 [2m0:02:12 â€¢ 0:05:31[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.5563, 0.4437]], device='cuda:0')7mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 67/203 [2m0:02:12 â€¢ 0:05:31[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.6736, 0.3264],237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 67/203 [2m0:02:12 â€¢ 0:05:31[0m [2;4m0.41it/s[0m  
        [0.6005, 0.3995],
        [0.6163, 0.3837],
        [0.6996, 0.3004],
[2K        [0.6617, 0.3383]], device='cuda:0');237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 67/203 [2m0:02:12 â€¢ 0:05:31[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.5598, 0.4402],8;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 68/203 [2m0:02:32 â€¢ 0:14:41[0m [2;4m0.15it/s[0m  
        [0.5561, 0.4439],
        [0.5543, 0.4457],
        [0.5595, 0.4405],
[2K        [0.5558, 0.4442]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 68/203 [2m0:02:32 â€¢ 0:14:41[0m [2;4m0.15it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 68/203 [2m0:02:32 â€¢ 0:14:41[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.6281, 0.3719]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 68/203 [2m0:02:32 â€¢ 0:14:41[0m [2;4m0.15it/s[0m  
[2KStep 0 - TTA Loss: 0.9818â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 68/203 [2m0:02:32 â€¢ 0:14:41[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.6382, 0.3618]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 68/203 [2m0:02:32 â€¢ 0:14:41[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.6480, 0.3520]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 68/203 [2m0:02:32 â€¢ 0:14:41[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.6571, 0.3429]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 68/203 [2m0:02:32 â€¢ 0:14:41[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.6650, 0.3350]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 68/203 [2m0:02:32 â€¢ 0:14:41[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.6716, 0.3284]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 68/203 [2m0:02:32 â€¢ 0:14:41[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.6767, 0.3233]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 68/203 [2m0:02:32 â€¢ 0:14:41[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.6803, 0.3197]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 68/203 [2m0:02:32 â€¢ 0:14:41[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.6824, 0.3176]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 68/203 [2m0:02:32 â€¢ 0:14:41[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.6834, 0.3166]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 68/203 [2m0:02:32 â€¢ 0:14:41[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.5585, 0.4415]], device='cuda:0')7mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 68/203 [2m0:02:32 â€¢ 0:14:41[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.6860, 0.3140],237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 68/203 [2m0:02:32 â€¢ 0:14:41[0m [2;4m0.15it/s[0m  
        [0.6190, 0.3810],
        [0.6327, 0.3673],
        [0.7073, 0.2927],
[2K        [0.6838, 0.3162]], device='cuda:0');237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 68/203 [2m0:02:32 â€¢ 0:14:41[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.5599, 0.4401],8;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 69/203 [2m0:02:34 â€¢ 0:12:24[0m [2;4m0.18it/s[0m  
        [0.5562, 0.4438],
        [0.5544, 0.4456],
        [0.5596, 0.4404],
[2K        [0.5560, 0.4440]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 69/203 [2m0:02:34 â€¢ 0:12:24[0m [2;4m0.18it/s[0m  
[2KClass label: SoccerPenaltyâ”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 69/203 [2m0:02:34 â€¢ 0:12:24[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.6629, 0.3371]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 69/203 [2m0:02:34 â€¢ 0:12:24[0m [2;4m0.18it/s[0m  
[2KStep 0 - TTA Loss: 1.5298â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 69/203 [2m0:02:34 â€¢ 0:12:24[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.6629, 0.3371]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 69/203 [2m0:02:34 â€¢ 0:12:24[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.6631, 0.3369]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 69/203 [2m0:02:34 â€¢ 0:12:24[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.6641, 0.3359]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 69/203 [2m0:02:34 â€¢ 0:12:24[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.6665, 0.3335]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 69/203 [2m0:02:34 â€¢ 0:12:24[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.6707, 0.3293]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 69/203 [2m0:02:34 â€¢ 0:12:24[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.6765, 0.3235]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 69/203 [2m0:02:34 â€¢ 0:12:24[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.6837, 0.3163]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 69/203 [2m0:02:34 â€¢ 0:12:24[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.6921, 0.3079]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 69/203 [2m0:02:34 â€¢ 0:12:24[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.7008, 0.2992]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 69/203 [2m0:02:34 â€¢ 0:12:24[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.5614, 0.4386]], device='cuda:0');5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 69/203 [2m0:02:34 â€¢ 0:12:24[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.5599, 0.4401],8;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 70/203 [2m0:02:37 â€¢ 0:11:44[0m [2;4m0.19it/s[0m  
        [0.5563, 0.4437],
        [0.5545, 0.4455],
        [0.5597, 0.4403],
[2K        [0.5560, 0.4440]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 70/203 [2m0:02:37 â€¢ 0:11:44[0m [2;4m0.19it/s[0m  
[2KClass label: HammerThrowâ”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 70/203 [2m0:02:37 â€¢ 0:11:44[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.5634, 0.4366]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 70/203 [2m0:02:37 â€¢ 0:11:44[0m [2;4m0.19it/s[0m  
[2KStep 0 - TTA Loss: 0.5449â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 70/203 [2m0:02:37 â€¢ 0:11:44[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.5722, 0.4278]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 70/203 [2m0:02:37 â€¢ 0:11:44[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.5820, 0.4180]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 70/203 [2m0:02:37 â€¢ 0:11:44[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.5920, 0.4080]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 70/203 [2m0:02:37 â€¢ 0:11:44[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.6015, 0.3985]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 70/203 [2m0:02:37 â€¢ 0:11:44[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.6096, 0.3904]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 70/203 [2m0:02:37 â€¢ 0:11:44[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.6164, 0.3836]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 70/203 [2m0:02:37 â€¢ 0:11:44[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.6213, 0.3787]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 70/203 [2m0:02:37 â€¢ 0:11:44[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.6244, 0.3756]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 70/203 [2m0:02:37 â€¢ 0:11:44[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.6258, 0.3742]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 70/203 [2m0:02:37 â€¢ 0:11:44[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.5589, 0.4411]], device='cuda:0');5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 70/203 [2m0:02:37 â€¢ 0:11:44[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.6936, 0.3064],98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 70/203 [2m0:02:37 â€¢ 0:11:44[0m [2;4m0.19it/s[0m  
        [0.6263, 0.3737],
        [0.6202, 0.3798],
        [0.7263, 0.2737],
[2K        [0.6785, 0.3215]], device='cuda:0');98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 70/203 [2m0:02:37 â€¢ 0:11:44[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.5600, 0.4400],8;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 71/203 [2m0:02:41 â€¢ 0:13:09[0m [2;4m0.17it/s[0m  
        [0.5564, 0.4436],
        [0.5546, 0.4454],
        [0.5597, 0.4403],
[2K        [0.5561, 0.4439]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 71/203 [2m0:02:41 â€¢ 0:13:09[0m [2;4m0.17it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 71/203 [2m0:02:41 â€¢ 0:13:09[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.5668, 0.4332]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 71/203 [2m0:02:41 â€¢ 0:13:09[0m [2;4m0.17it/s[0m  
[2KStep 0 - TTA Loss: 0.6516â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 71/203 [2m0:02:41 â€¢ 0:13:09[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.5668, 0.4332]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 71/203 [2m0:02:41 â€¢ 0:13:09[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.5671, 0.4329]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 71/203 [2m0:02:41 â€¢ 0:13:09[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.5682, 0.4318]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 71/203 [2m0:02:41 â€¢ 0:13:09[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.5705, 0.4295]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 71/203 [2m0:02:41 â€¢ 0:13:09[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.5743, 0.4257]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 71/203 [2m0:02:41 â€¢ 0:13:09[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.5795, 0.4205]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 71/203 [2m0:02:41 â€¢ 0:13:09[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.5863, 0.4137]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 71/203 [2m0:02:41 â€¢ 0:13:09[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.5947, 0.4053]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 71/203 [2m0:02:41 â€¢ 0:13:09[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.6041, 0.3959]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 71/203 [2m0:02:41 â€¢ 0:13:09[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.5566, 0.4434]], device='cuda:0');5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 71/203 [2m0:02:41 â€¢ 0:13:09[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.6771, 0.3229],98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 71/203 [2m0:02:41 â€¢ 0:13:09[0m [2;4m0.17it/s[0m  
        [0.6129, 0.3871],
        [0.6144, 0.3856],
        [0.7000, 0.3000],
[2K        [0.6685, 0.3315]], device='cuda:0');98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 71/203 [2m0:02:41 â€¢ 0:13:09[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.5601, 0.4399],38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 72/203 [2m0:02:47 â€¢ 0:07:50[0m [2;4m0.28it/s[0m   
        [0.5565, 0.4435],
        [0.5547, 0.4453],
        [0.5598, 0.4402],
[2K        [0.5562, 0.4438]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 72/203 [2m0:02:47 â€¢ 0:07:50[0m [2;4m0.28it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 72/203 [2m0:02:47 â€¢ 0:07:50[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6259, 0.3741]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 72/203 [2m0:02:47 â€¢ 0:07:50[0m [2;4m0.28it/s[0m  
[2KStep 0 - TTA Loss: 0.5952â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 72/203 [2m0:02:47 â€¢ 0:07:50[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6364, 0.3636]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 72/203 [2m0:02:47 â€¢ 0:07:50[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6480, 0.3520]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 72/203 [2m0:02:47 â€¢ 0:07:50[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6602, 0.3398]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 72/203 [2m0:02:47 â€¢ 0:07:50[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6721, 0.3279]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 72/203 [2m0:02:47 â€¢ 0:07:50[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6827, 0.3173]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 72/203 [2m0:02:47 â€¢ 0:07:50[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6914, 0.3086]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 72/203 [2m0:02:47 â€¢ 0:07:50[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6977, 0.3023]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 72/203 [2m0:02:47 â€¢ 0:07:50[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.7018, 0.2982]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 72/203 [2m0:02:47 â€¢ 0:07:50[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.7039, 0.2961]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 72/203 [2m0:02:47 â€¢ 0:07:50[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.5599, 0.4401]], device='cuda:0')37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 72/203 [2m0:02:47 â€¢ 0:07:50[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.7000, 0.3000],;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 72/203 [2m0:02:47 â€¢ 0:07:50[0m [2;4m0.28it/s[0m  
        [0.6398, 0.3602],
        [0.6455, 0.3545],
        [0.7166, 0.2834],
[2K        [0.7046, 0.2954]], device='cuda:0')5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 72/203 [2m0:02:47 â€¢ 0:07:50[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.5603, 0.4397],38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 73/203 [2m0:02:55 â€¢ 0:09:56[0m [2;4m0.22it/s[0m  
        [0.5568, 0.4432],
        [0.5549, 0.4451],
        [0.5599, 0.4401],
[2K        [0.5565, 0.4435]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 73/203 [2m0:02:55 â€¢ 0:09:56[0m [2;4m0.22it/s[0m  
[2KClass label: BaseballPitchâ”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 73/203 [2m0:02:55 â€¢ 0:09:56[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.6377, 0.3623]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 73/203 [2m0:02:55 â€¢ 0:09:56[0m [2;4m0.22it/s[0m  
[2KStep 0 - TTA Loss: 0.9743â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 73/203 [2m0:02:55 â€¢ 0:09:56[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.6377, 0.3623]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 73/203 [2m0:02:55 â€¢ 0:09:56[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.6381, 0.3619]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 73/203 [2m0:02:55 â€¢ 0:09:56[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.6396, 0.3604]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 73/203 [2m0:02:55 â€¢ 0:09:56[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.6427, 0.3573]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 73/203 [2m0:02:55 â€¢ 0:09:56[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.6477, 0.3523]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 73/203 [2m0:02:55 â€¢ 0:09:56[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.6548, 0.3452]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 73/203 [2m0:02:55 â€¢ 0:09:56[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.6636, 0.3364]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 73/203 [2m0:02:55 â€¢ 0:09:56[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.6736, 0.3264]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 73/203 [2m0:02:55 â€¢ 0:09:56[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.6845, 0.3155]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 73/203 [2m0:02:55 â€¢ 0:09:56[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.5626, 0.4374]], device='cuda:0')37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 73/203 [2m0:02:55 â€¢ 0:09:56[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.6956, 0.3044],;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 73/203 [2m0:02:55 â€¢ 0:09:56[0m [2;4m0.22it/s[0m  
        [0.6185, 0.3815],
        [0.6175, 0.3825],
        [0.7036, 0.2964],
[2K        [0.6853, 0.3147]], device='cuda:0')5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 73/203 [2m0:02:55 â€¢ 0:09:56[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.5604, 0.4396],38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 74/203 [2m0:02:57 â€¢ 0:08:49[0m [2;4m0.24it/s[0m  
        [0.5569, 0.4431],
        [0.5550, 0.4450],
        [0.5600, 0.4400],
[2K        [0.5566, 0.4434]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 74/203 [2m0:02:57 â€¢ 0:08:49[0m [2;4m0.24it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 74/203 [2m0:02:57 â€¢ 0:08:49[0m [2;4m0.24it/s[0m  
[2KAttention weights: tensor([[0.6303, 0.3697]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 74/203 [2m0:02:57 â€¢ 0:08:49[0m [2;4m0.24it/s[0m  
[2KStep 0 - TTA Loss: 0.7711â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 74/203 [2m0:02:57 â€¢ 0:08:49[0m [2;4m0.24it/s[0m  
[2KAttention weights: tensor([[0.6412, 0.3588]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 74/203 [2m0:02:57 â€¢ 0:08:49[0m [2;4m0.24it/s[0m  
[2KAttention weights: tensor([[0.6513, 0.3487]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 74/203 [2m0:02:57 â€¢ 0:08:49[0m [2;4m0.24it/s[0m  
[2KAttention weights: tensor([[0.6601, 0.3399]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 74/203 [2m0:02:57 â€¢ 0:08:49[0m [2;4m0.24it/s[0m  
[2KAttention weights: tensor([[0.6673, 0.3327]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 74/203 [2m0:02:57 â€¢ 0:08:49[0m [2;4m0.24it/s[0m  
[2KAttention weights: tensor([[0.6732, 0.3268]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 74/203 [2m0:02:57 â€¢ 0:08:49[0m [2;4m0.24it/s[0m  
[2KAttention weights: tensor([[0.6777, 0.3223]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 74/203 [2m0:02:57 â€¢ 0:08:49[0m [2;4m0.24it/s[0m  
[2KAttention weights: tensor([[0.6807, 0.3193]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 74/203 [2m0:02:57 â€¢ 0:08:49[0m [2;4m0.24it/s[0m  
[2KAttention weights: tensor([[0.6826, 0.3174]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 74/203 [2m0:02:57 â€¢ 0:08:49[0m [2;4m0.24it/s[0m  
[2KAttention weights: tensor([[0.6835, 0.3165]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 74/203 [2m0:02:57 â€¢ 0:08:49[0m [2;4m0.24it/s[0m  
[2KAttention weights: tensor([[0.5594, 0.4406]], device='cuda:0')8;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 74/203 [2m0:02:57 â€¢ 0:08:49[0m [2;4m0.24it/s[0m  
[2KAttention weights: tensor([[0.6930, 0.3070],;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 74/203 [2m0:02:57 â€¢ 0:08:49[0m [2;4m0.24it/s[0m  
        [0.6168, 0.3832],
        [0.6157, 0.3843],
        [0.7052, 0.2948],
[2K        [0.6838, 0.3162]], device='cuda:0')2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 74/203 [2m0:02:57 â€¢ 0:08:49[0m [2;4m0.24it/s[0m  
[2KAttention weights: tensor([[0.5605, 0.4395],38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 75/203 [2m0:02:59 â€¢ 0:08:04[0m [2;4m0.26it/s[0m  
        [0.5570, 0.4430],
        [0.5551, 0.4449],
        [0.5601, 0.4399],
[2K        [0.5568, 0.4432]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 75/203 [2m0:02:59 â€¢ 0:08:04[0m [2;4m0.26it/s[0m  
[2KClass label: BaseballPitchâ”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 75/203 [2m0:02:59 â€¢ 0:08:04[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.6376, 0.3624]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 75/203 [2m0:02:59 â€¢ 0:08:04[0m [2;4m0.26it/s[0m  
[2KStep 0 - TTA Loss: 0.8779â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 75/203 [2m0:02:59 â€¢ 0:08:04[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.6376, 0.3624]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 75/203 [2m0:02:59 â€¢ 0:08:04[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.6379, 0.3621]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 75/203 [2m0:02:59 â€¢ 0:08:04[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.6392, 0.3608]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 75/203 [2m0:02:59 â€¢ 0:08:04[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.6423, 0.3577]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 75/203 [2m0:02:59 â€¢ 0:08:04[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.6481, 0.3519]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 75/203 [2m0:02:59 â€¢ 0:08:04[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.6568, 0.3432]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 75/203 [2m0:02:59 â€¢ 0:08:04[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.6685, 0.3315]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 75/203 [2m0:02:59 â€¢ 0:08:04[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.6828, 0.3172]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 75/203 [2m0:02:59 â€¢ 0:08:04[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.6991, 0.3009]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 75/203 [2m0:02:59 â€¢ 0:08:04[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.5636, 0.4364]], device='cuda:0')8;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 75/203 [2m0:02:59 â€¢ 0:08:04[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.7166, 0.2834],;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 75/203 [2m0:02:59 â€¢ 0:08:04[0m [2;4m0.26it/s[0m  
        [0.6137, 0.3863],
        [0.6019, 0.3981],
        [0.7061, 0.2939],
[2K        [0.6701, 0.3299]], device='cuda:0')2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 75/203 [2m0:02:59 â€¢ 0:08:04[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.5606, 0.4394],38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 76/203 [2m0:03:00 â€¢ 0:07:28[0m [2;4m0.28it/s[0m  
        [0.5571, 0.4429],
        [0.5552, 0.4448],
        [0.5602, 0.4398],
[2K        [0.5569, 0.4431]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 76/203 [2m0:03:00 â€¢ 0:07:28[0m [2;4m0.28it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 76/203 [2m0:03:00 â€¢ 0:07:28[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6259, 0.3741]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 76/203 [2m0:03:00 â€¢ 0:07:28[0m [2;4m0.28it/s[0m  
[2KStep 0 - TTA Loss: 0.9879â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 76/203 [2m0:03:00 â€¢ 0:07:28[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6362, 0.3638]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 76/203 [2m0:03:00 â€¢ 0:07:28[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6456, 0.3544]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 76/203 [2m0:03:00 â€¢ 0:07:28[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6535, 0.3465]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 76/203 [2m0:03:00 â€¢ 0:07:28[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6596, 0.3404]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 76/203 [2m0:03:00 â€¢ 0:07:28[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6644, 0.3356]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 76/203 [2m0:03:00 â€¢ 0:07:28[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6677, 0.3323]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 76/203 [2m0:03:00 â€¢ 0:07:28[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6699, 0.3301]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 76/203 [2m0:03:00 â€¢ 0:07:28[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6712, 0.3288]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 76/203 [2m0:03:00 â€¢ 0:07:28[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6719, 0.3281]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 76/203 [2m0:03:00 â€¢ 0:07:28[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.5592, 0.4408]], device='cuda:0')8;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 76/203 [2m0:03:00 â€¢ 0:07:28[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.7131, 0.2869],;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 76/203 [2m0:03:00 â€¢ 0:07:28[0m [2;4m0.28it/s[0m  
        [0.6148, 0.3852],
        [0.6032, 0.3968],
        [0.7087, 0.2913],
[2K        [0.6722, 0.3278]], device='cuda:0')2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 76/203 [2m0:03:00 â€¢ 0:07:28[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.5609, 0.4391],[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 77/203 [2m0:03:04 â€¢ 0:07:50[0m [2;4m0.27it/s[0m   
        [0.5573, 0.4427],
        [0.5553, 0.4447],
        [0.5603, 0.4397],
[2K        [0.5571, 0.4429]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 77/203 [2m0:03:04 â€¢ 0:07:50[0m [2;4m0.27it/s[0m  
[2KClass label: SoccerPenaltyâ”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 77/203 [2m0:03:04 â€¢ 0:07:50[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6728, 0.3272]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 77/203 [2m0:03:04 â€¢ 0:07:50[0m [2;4m0.27it/s[0m  
[2KStep 0 - TTA Loss: 0.6237â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 77/203 [2m0:03:04 â€¢ 0:07:50[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6728, 0.3272]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 77/203 [2m0:03:04 â€¢ 0:07:50[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6730, 0.3270]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 77/203 [2m0:03:04 â€¢ 0:07:50[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6740, 0.3260]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 77/203 [2m0:03:04 â€¢ 0:07:50[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6764, 0.3236]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 77/203 [2m0:03:04 â€¢ 0:07:50[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6808, 0.3192]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 77/203 [2m0:03:04 â€¢ 0:07:50[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6877, 0.3123]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 77/203 [2m0:03:04 â€¢ 0:07:50[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6975, 0.3025]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 77/203 [2m0:03:04 â€¢ 0:07:50[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.7097, 0.2903]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 77/203 [2m0:03:04 â€¢ 0:07:50[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.7241, 0.2759]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 77/203 [2m0:03:04 â€¢ 0:07:50[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.5634, 0.4366]], device='cuda:0')237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 77/203 [2m0:03:04 â€¢ 0:07:50[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6943, 0.3057],5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 77/203 [2m0:03:04 â€¢ 0:07:50[0m [2;4m0.27it/s[0m  
        [0.6106, 0.3894],
        [0.6083, 0.3917],
        [0.7397, 0.2603],
[2K        [0.6678, 0.3322]], device='cuda:0');5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 77/203 [2m0:03:04 â€¢ 0:07:50[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.5609, 0.4391],[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 78/203 [2m0:03:05 â€¢ 0:07:23[0m [2;4m0.28it/s[0m  
        [0.5573, 0.4427],
        [0.5554, 0.4446],
        [0.5603, 0.4397],
[2K        [0.5572, 0.4428]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 78/203 [2m0:03:05 â€¢ 0:07:23[0m [2;4m0.28it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 78/203 [2m0:03:05 â€¢ 0:07:23[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.5663, 0.4337]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 78/203 [2m0:03:05 â€¢ 0:07:23[0m [2;4m0.28it/s[0m  
[2KStep 0 - TTA Loss: 0.6883â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 78/203 [2m0:03:05 â€¢ 0:07:23[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.5755, 0.4245]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 78/203 [2m0:03:05 â€¢ 0:07:23[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.5849, 0.4151]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 78/203 [2m0:03:05 â€¢ 0:07:23[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.5940, 0.4060]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 78/203 [2m0:03:05 â€¢ 0:07:23[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6023, 0.3977]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 78/203 [2m0:03:05 â€¢ 0:07:23[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6093, 0.3907]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 78/203 [2m0:03:05 â€¢ 0:07:23[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6150, 0.3850]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 78/203 [2m0:03:05 â€¢ 0:07:23[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6190, 0.3810]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 78/203 [2m0:03:05 â€¢ 0:07:23[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6215, 0.3785]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 78/203 [2m0:03:05 â€¢ 0:07:23[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6227, 0.3773]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 78/203 [2m0:03:05 â€¢ 0:07:23[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.5576, 0.4424]], device='cuda:0')237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 78/203 [2m0:03:05 â€¢ 0:07:23[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6958, 0.3042],5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 78/203 [2m0:03:05 â€¢ 0:07:23[0m [2;4m0.28it/s[0m  
        [0.6203, 0.3797],
        [0.6231, 0.3769],
        [0.7434, 0.2566],
[2K        [0.6749, 0.3251]], device='cuda:0');5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 78/203 [2m0:03:05 â€¢ 0:07:23[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.5609, 0.4391],[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 79/203 [2m0:03:14 â€¢ 0:08:08[0m [2;4m0.25it/s[0m  
        [0.5574, 0.4426],
        [0.5554, 0.4446],
        [0.5603, 0.4397],
[2K        [0.5572, 0.4428]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 79/203 [2m0:03:14 â€¢ 0:08:08[0m [2;4m0.25it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 79/203 [2m0:03:14 â€¢ 0:08:08[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6285, 0.3715]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 79/203 [2m0:03:14 â€¢ 0:08:08[0m [2;4m0.25it/s[0m  
[2KStep 0 - TTA Loss: 0.5395â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 79/203 [2m0:03:14 â€¢ 0:08:08[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6285, 0.3715]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 79/203 [2m0:03:14 â€¢ 0:08:08[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6287, 0.3713]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 79/203 [2m0:03:14 â€¢ 0:08:08[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6295, 0.3705]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 79/203 [2m0:03:14 â€¢ 0:08:08[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6311, 0.3689]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 79/203 [2m0:03:14 â€¢ 0:08:08[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6337, 0.3663]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 79/203 [2m0:03:14 â€¢ 0:08:08[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6372, 0.3628]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 79/203 [2m0:03:14 â€¢ 0:08:08[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6416, 0.3584]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 79/203 [2m0:03:14 â€¢ 0:08:08[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6466, 0.3534]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 79/203 [2m0:03:14 â€¢ 0:08:08[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6521, 0.3479]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 79/203 [2m0:03:14 â€¢ 0:08:08[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.5586, 0.4414]], device='cuda:0')38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 79/203 [2m0:03:14 â€¢ 0:08:08[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6665, 0.3335],2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 79/203 [2m0:03:14 â€¢ 0:08:08[0m [2;4m0.25it/s[0m  
        [0.5910, 0.4090],
        [0.6022, 0.3978],
        [0.6977, 0.3023],
[2K        [0.6576, 0.3424]], device='cuda:0');2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 79/203 [2m0:03:14 â€¢ 0:08:08[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.5609, 0.4391],[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 80/203 [2m0:03:17 â€¢ 0:06:20[0m [2;4m0.32it/s[0m  
        [0.5574, 0.4426],
        [0.5555, 0.4445],
        [0.5603, 0.4397],
[2K        [0.5572, 0.4428]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 80/203 [2m0:03:17 â€¢ 0:06:20[0m [2;4m0.32it/s[0m  
[2KClass label: BaseballPitchâ”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 80/203 [2m0:03:17 â€¢ 0:06:20[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.6423, 0.3577]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 80/203 [2m0:03:17 â€¢ 0:06:20[0m [2;4m0.32it/s[0m  
[2KStep 0 - TTA Loss: 1.1667â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 80/203 [2m0:03:17 â€¢ 0:06:20[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.6484, 0.3516]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 80/203 [2m0:03:17 â€¢ 0:06:20[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.6544, 0.3456]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 80/203 [2m0:03:17 â€¢ 0:06:20[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.6611, 0.3389]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 80/203 [2m0:03:17 â€¢ 0:06:20[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.6675, 0.3325]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 80/203 [2m0:03:17 â€¢ 0:06:20[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.6729, 0.3271]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 80/203 [2m0:03:17 â€¢ 0:06:20[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.6769, 0.3231]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 80/203 [2m0:03:17 â€¢ 0:06:20[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.6800, 0.3200]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 80/203 [2m0:03:17 â€¢ 0:06:20[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.6820, 0.3180]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 80/203 [2m0:03:17 â€¢ 0:06:20[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.6830, 0.3170]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 80/203 [2m0:03:17 â€¢ 0:06:20[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.5627, 0.4373]], device='cuda:0')38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 80/203 [2m0:03:17 â€¢ 0:06:20[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.6833, 0.3167],2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 80/203 [2m0:03:17 â€¢ 0:06:20[0m [2;4m0.32it/s[0m  
        [0.6006, 0.3994],
        [0.6038, 0.3962],
        [0.6990, 0.3010],
[2K        [0.6631, 0.3369]], device='cuda:0');2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 80/203 [2m0:03:17 â€¢ 0:06:20[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.5610, 0.4390],[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 81/203 [2m0:03:18 â€¢ 0:05:44[0m [2;4m0.35it/s[0m  
        [0.5574, 0.4426],
        [0.5555, 0.4445],
        [0.5603, 0.4397],
[2K        [0.5572, 0.4428]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 81/203 [2m0:03:18 â€¢ 0:05:44[0m [2;4m0.35it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 81/203 [2m0:03:18 â€¢ 0:05:44[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6315, 0.3685]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 81/203 [2m0:03:18 â€¢ 0:05:44[0m [2;4m0.35it/s[0m  
[2KStep 0 - TTA Loss: 1.0901â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 81/203 [2m0:03:18 â€¢ 0:05:44[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6315, 0.3685]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 81/203 [2m0:03:18 â€¢ 0:05:44[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6316, 0.3684]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 81/203 [2m0:03:18 â€¢ 0:05:44[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6321, 0.3679]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 81/203 [2m0:03:18 â€¢ 0:05:44[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6329, 0.3671]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 81/203 [2m0:03:18 â€¢ 0:05:44[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6341, 0.3659]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 81/203 [2m0:03:18 â€¢ 0:05:44[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6356, 0.3644]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 81/203 [2m0:03:18 â€¢ 0:05:44[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6374, 0.3626]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 81/203 [2m0:03:18 â€¢ 0:05:44[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6392, 0.3608]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 81/203 [2m0:03:18 â€¢ 0:05:44[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6410, 0.3590]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 81/203 [2m0:03:18 â€¢ 0:05:44[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5581, 0.4419]], device='cuda:0')38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 81/203 [2m0:03:18 â€¢ 0:05:44[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6643, 0.3357],2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 81/203 [2m0:03:18 â€¢ 0:05:44[0m [2;4m0.35it/s[0m  
        [0.5803, 0.4197],
        [0.5813, 0.4187],
        [0.6831, 0.3169],
[2K        [0.6427, 0.3573]], device='cuda:0');2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 81/203 [2m0:03:18 â€¢ 0:05:44[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5610, 0.4390],[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 82/203 [2m0:03:19 â€¢ 0:05:17[0m [2;4m0.38it/s[0m   
        [0.5574, 0.4426],
        [0.5555, 0.4445],
        [0.5603, 0.4397],
[2K        [0.5572, 0.4428]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 82/203 [2m0:03:19 â€¢ 0:05:17[0m [2;4m0.38it/s[0m  
[2KClass label: SoccerPenaltyâ”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 82/203 [2m0:03:19 â€¢ 0:05:17[0m [2;4m0.38it/s[0m  
[2KAttention weights: tensor([[0.6703, 0.3297]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 82/203 [2m0:03:19 â€¢ 0:05:17[0m [2;4m0.38it/s[0m  
[2KStep 0 - TTA Loss: 0.9118â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 82/203 [2m0:03:19 â€¢ 0:05:17[0m [2;4m0.38it/s[0m  
[2KAttention weights: tensor([[0.6724, 0.3276]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 82/203 [2m0:03:19 â€¢ 0:05:17[0m [2;4m0.38it/s[0m  
[2KAttention weights: tensor([[0.6747, 0.3253]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 82/203 [2m0:03:19 â€¢ 0:05:17[0m [2;4m0.38it/s[0m  
[2KAttention weights: tensor([[0.6768, 0.3232]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 82/203 [2m0:03:19 â€¢ 0:05:17[0m [2;4m0.38it/s[0m  
[2KAttention weights: tensor([[0.6788, 0.3212]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 82/203 [2m0:03:19 â€¢ 0:05:17[0m [2;4m0.38it/s[0m  
[2KAttention weights: tensor([[0.6803, 0.3197]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 82/203 [2m0:03:19 â€¢ 0:05:17[0m [2;4m0.38it/s[0m  
[2KAttention weights: tensor([[0.6815, 0.3185]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 82/203 [2m0:03:19 â€¢ 0:05:17[0m [2;4m0.38it/s[0m  
[2KAttention weights: tensor([[0.6823, 0.3177]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 82/203 [2m0:03:19 â€¢ 0:05:17[0m [2;4m0.38it/s[0m  
[2KAttention weights: tensor([[0.6828, 0.3172]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 82/203 [2m0:03:19 â€¢ 0:05:17[0m [2;4m0.38it/s[0m  
[2KAttention weights: tensor([[0.6831, 0.3169]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 82/203 [2m0:03:19 â€¢ 0:05:17[0m [2;4m0.38it/s[0m  
[2KAttention weights: tensor([[0.5611, 0.4389]], device='cuda:0');237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 82/203 [2m0:03:19 â€¢ 0:05:17[0m [2;4m0.38it/s[0m  
[2KAttention weights: tensor([[0.6622, 0.3378],;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 82/203 [2m0:03:19 â€¢ 0:05:17[0m [2;4m0.38it/s[0m  
        [0.5765, 0.4235],
        [0.5766, 0.4234],
        [0.6832, 0.3168],
[2K        [0.6377, 0.3623]], device='cuda:0')8;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 82/203 [2m0:03:19 â€¢ 0:05:17[0m [2;4m0.38it/s[0m  
[2KAttention weights: tensor([[0.5610, 0.4390],[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 83/203 [2m0:03:20 â€¢ 0:04:58[0m [2;4m0.40it/s[0m  
        [0.5574, 0.4426],
        [0.5555, 0.4445],
        [0.5603, 0.4397],
[2K        [0.5572, 0.4428]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 83/203 [2m0:03:20 â€¢ 0:04:58[0m [2;4m0.40it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 83/203 [2m0:03:20 â€¢ 0:04:58[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.5629, 0.4371]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 83/203 [2m0:03:20 â€¢ 0:04:58[0m [2;4m0.40it/s[0m  
[2KStep 0 - TTA Loss: 0.6253â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 83/203 [2m0:03:20 â€¢ 0:04:58[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.5629, 0.4371]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 83/203 [2m0:03:20 â€¢ 0:04:58[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.5630, 0.4370]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 83/203 [2m0:03:20 â€¢ 0:04:58[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.5634, 0.4366]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 83/203 [2m0:03:20 â€¢ 0:04:58[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.5646, 0.4354]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 83/203 [2m0:03:20 â€¢ 0:04:58[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.5666, 0.4334]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 83/203 [2m0:03:20 â€¢ 0:04:58[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.5700, 0.4300]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 83/203 [2m0:03:20 â€¢ 0:04:58[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.5752, 0.4248]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 83/203 [2m0:03:20 â€¢ 0:04:58[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.5818, 0.4182]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 83/203 [2m0:03:20 â€¢ 0:04:58[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.5894, 0.4106]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 83/203 [2m0:03:20 â€¢ 0:04:58[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.5568, 0.4432]], device='cuda:0');237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 83/203 [2m0:03:20 â€¢ 0:04:58[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.6617, 0.3383],;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 83/203 [2m0:03:20 â€¢ 0:04:58[0m [2;4m0.40it/s[0m  
        [0.5866, 0.4134],
        [0.5980, 0.4020],
        [0.6886, 0.3114],
[2K        [0.6511, 0.3489]], device='cuda:0')8;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 83/203 [2m0:03:20 â€¢ 0:04:58[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.5610, 0.4390],[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 84/203 [2m0:03:22 â€¢ 0:04:53[0m [2;4m0.41it/s[0m  
        [0.5574, 0.4426],
        [0.5555, 0.4445],
        [0.5604, 0.4396],
[2K        [0.5572, 0.4428]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 84/203 [2m0:03:22 â€¢ 0:04:53[0m [2;4m0.41it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 84/203 [2m0:03:22 â€¢ 0:04:53[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.5650, 0.4350]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 84/203 [2m0:03:22 â€¢ 0:04:53[0m [2;4m0.41it/s[0m  
[2KStep 0 - TTA Loss: 0.6324â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 84/203 [2m0:03:22 â€¢ 0:04:53[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.5738, 0.4262]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 84/203 [2m0:03:22 â€¢ 0:04:53[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.5823, 0.4177]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 84/203 [2m0:03:22 â€¢ 0:04:53[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.5902, 0.4098]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 84/203 [2m0:03:22 â€¢ 0:04:53[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.5972, 0.4028]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 84/203 [2m0:03:22 â€¢ 0:04:53[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.6026, 0.3974]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 84/203 [2m0:03:22 â€¢ 0:04:53[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.6067, 0.3933]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 84/203 [2m0:03:22 â€¢ 0:04:53[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.6095, 0.3905]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 84/203 [2m0:03:22 â€¢ 0:04:53[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.6112, 0.3888]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 84/203 [2m0:03:22 â€¢ 0:04:53[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.6120, 0.3880]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 84/203 [2m0:03:22 â€¢ 0:04:53[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.5574, 0.4426]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 84/203 [2m0:03:22 â€¢ 0:04:53[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.6664, 0.3336],;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 84/203 [2m0:03:22 â€¢ 0:04:53[0m [2;4m0.41it/s[0m  
        [0.5973, 0.4027],
        [0.6123, 0.3877],
        [0.6937, 0.3063],
[2K        [0.6600, 0.3400]], device='cuda:0')8;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 84/203 [2m0:03:22 â€¢ 0:04:53[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.5610, 0.4390],[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 85/203 [2m0:03:25 â€¢ 0:04:52[0m [2;4m0.40it/s[0m  
        [0.5574, 0.4426],
        [0.5556, 0.4444],
        [0.5604, 0.4396],
[2K        [0.5573, 0.4427]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 85/203 [2m0:03:25 â€¢ 0:04:52[0m [2;4m0.40it/s[0m  
[2KClass label: HammerThrowâ”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 85/203 [2m0:03:25 â€¢ 0:04:52[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.5609, 0.4391]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 85/203 [2m0:03:25 â€¢ 0:04:52[0m [2;4m0.40it/s[0m  
[2KStep 0 - TTA Loss: 0.7186â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 85/203 [2m0:03:25 â€¢ 0:04:52[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.5609, 0.4391]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 85/203 [2m0:03:25 â€¢ 0:04:52[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.5610, 0.4390]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 85/203 [2m0:03:25 â€¢ 0:04:52[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.5617, 0.4383]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 85/203 [2m0:03:25 â€¢ 0:04:52[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.5631, 0.4369]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 85/203 [2m0:03:25 â€¢ 0:04:52[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.5657, 0.4343]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 85/203 [2m0:03:25 â€¢ 0:04:52[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.5693, 0.4307]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 85/203 [2m0:03:25 â€¢ 0:04:52[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.5739, 0.4261]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 85/203 [2m0:03:25 â€¢ 0:04:52[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.5788, 0.4212]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 85/203 [2m0:03:25 â€¢ 0:04:52[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.5844, 0.4156]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 85/203 [2m0:03:25 â€¢ 0:04:52[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.5585, 0.4415]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 85/203 [2m0:03:25 â€¢ 0:04:52[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.6649, 0.3351],;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 85/203 [2m0:03:25 â€¢ 0:04:52[0m [2;4m0.40it/s[0m  
        [0.5897, 0.4103],
        [0.5925, 0.4075],
        [0.6819, 0.3181],
[2K        [0.6491, 0.3509]], device='cuda:0')8;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 85/203 [2m0:03:25 â€¢ 0:04:52[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.5610, 0.4390],[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 86/203 [2m0:03:28 â€¢ 0:05:14[0m [2;4m0.37it/s[0m  
        [0.5575, 0.4425],
        [0.5557, 0.4443],
        [0.5604, 0.4396],
[2K        [0.5573, 0.4427]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 86/203 [2m0:03:28 â€¢ 0:05:14[0m [2;4m0.37it/s[0m  
[2KClass label: HammerThrowâ”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 86/203 [2m0:03:28 â€¢ 0:05:14[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5605, 0.4395]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 86/203 [2m0:03:28 â€¢ 0:05:14[0m [2;4m0.37it/s[0m  
[2KStep 0 - TTA Loss: 0.5878â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 86/203 [2m0:03:28 â€¢ 0:05:14[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5681, 0.4319]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 86/203 [2m0:03:28 â€¢ 0:05:14[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5776, 0.4224]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 86/203 [2m0:03:28 â€¢ 0:05:14[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5881, 0.4119]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 86/203 [2m0:03:28 â€¢ 0:05:14[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5985, 0.4015]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 86/203 [2m0:03:28 â€¢ 0:05:14[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.6080, 0.3920]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 86/203 [2m0:03:28 â€¢ 0:05:14[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.6160, 0.3840]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 86/203 [2m0:03:28 â€¢ 0:05:14[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.6218, 0.3782]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 86/203 [2m0:03:28 â€¢ 0:05:14[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.6256, 0.3744]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 86/203 [2m0:03:28 â€¢ 0:05:14[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.6274, 0.3726]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 86/203 [2m0:03:28 â€¢ 0:05:14[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5601, 0.4399]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 86/203 [2m0:03:28 â€¢ 0:05:14[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.6874, 0.3126],;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 86/203 [2m0:03:28 â€¢ 0:05:14[0m [2;4m0.37it/s[0m  
        [0.6280, 0.3720],
        [0.6139, 0.3861],
        [0.7062, 0.2938],
[2K        [0.6721, 0.3279]], device='cuda:0')8;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 86/203 [2m0:03:28 â€¢ 0:05:14[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5611, 0.4389],m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 87/203 [2m0:03:31 â€¢ 0:05:13[0m [2;4m0.37it/s[0m   
        [0.5576, 0.4424],
        [0.5558, 0.4442],
        [0.5605, 0.4395],
[2K        [0.5574, 0.4426]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 87/203 [2m0:03:31 â€¢ 0:05:13[0m [2;4m0.37it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 87/203 [2m0:03:31 â€¢ 0:05:13[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5683, 0.4317]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 87/203 [2m0:03:31 â€¢ 0:05:13[0m [2;4m0.37it/s[0m  
[2KStep 0 - TTA Loss: 0.9153â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 87/203 [2m0:03:31 â€¢ 0:05:13[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5683, 0.4317]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 87/203 [2m0:03:31 â€¢ 0:05:13[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5686, 0.4314]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 87/203 [2m0:03:31 â€¢ 0:05:13[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5701, 0.4299]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 87/203 [2m0:03:31 â€¢ 0:05:13[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5733, 0.4267]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 87/203 [2m0:03:31 â€¢ 0:05:13[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5790, 0.4210]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 87/203 [2m0:03:31 â€¢ 0:05:13[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5873, 0.4127]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 87/203 [2m0:03:31 â€¢ 0:05:13[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5985, 0.4015]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 87/203 [2m0:03:31 â€¢ 0:05:13[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.6122, 0.3878]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 87/203 [2m0:03:31 â€¢ 0:05:13[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.6281, 0.3719]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 87/203 [2m0:03:31 â€¢ 0:05:13[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5590, 0.4410]], device='cuda:0')5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 87/203 [2m0:03:31 â€¢ 0:05:13[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.6968, 0.3032],8;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 87/203 [2m0:03:31 â€¢ 0:05:13[0m [2;4m0.37it/s[0m  
        [0.6412, 0.3588],
        [0.6453, 0.3547],
        [0.7151, 0.2849],
[2K        [0.6901, 0.3099]], device='cuda:0')38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 87/203 [2m0:03:31 â€¢ 0:05:13[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5612, 0.4388],m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 88/203 [2m0:03:34 â€¢ 0:05:37[0m [2;4m0.34it/s[0m  
        [0.5578, 0.4422],
        [0.5559, 0.4441],
        [0.5606, 0.4394],
[2K        [0.5575, 0.4425]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 88/203 [2m0:03:34 â€¢ 0:05:37[0m [2;4m0.34it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 88/203 [2m0:03:34 â€¢ 0:05:37[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.5649, 0.4351]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 88/203 [2m0:03:34 â€¢ 0:05:37[0m [2;4m0.34it/s[0m  
[2KStep 0 - TTA Loss: 0.9149â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 88/203 [2m0:03:34 â€¢ 0:05:37[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.5830, 0.4170]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 88/203 [2m0:03:34 â€¢ 0:05:37[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.5999, 0.4001]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 88/203 [2m0:03:34 â€¢ 0:05:37[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6148, 0.3852]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 88/203 [2m0:03:34 â€¢ 0:05:37[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6275, 0.3725]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 88/203 [2m0:03:34 â€¢ 0:05:37[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6373, 0.3627]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 88/203 [2m0:03:34 â€¢ 0:05:37[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6446, 0.3554]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 88/203 [2m0:03:34 â€¢ 0:05:37[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6497, 0.3503]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 88/203 [2m0:03:34 â€¢ 0:05:37[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6527, 0.3473]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 88/203 [2m0:03:34 â€¢ 0:05:37[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6541, 0.3459]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 88/203 [2m0:03:34 â€¢ 0:05:37[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.5596, 0.4404]], device='cuda:0')5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 88/203 [2m0:03:34 â€¢ 0:05:37[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6907, 0.3093],8;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 88/203 [2m0:03:34 â€¢ 0:05:37[0m [2;4m0.34it/s[0m  
        [0.6380, 0.3620],
        [0.6545, 0.3455],
        [0.7143, 0.2857],
[2K        [0.6918, 0.3082]], device='cuda:0')38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 88/203 [2m0:03:34 â€¢ 0:05:37[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.5614, 0.4386],m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 89/203 [2m0:03:42 â€¢ 0:05:21[0m [2;4m0.36it/s[0m  
        [0.5580, 0.4420],
        [0.5562, 0.4438],
        [0.5607, 0.4393],
[2K        [0.5577, 0.4423]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 89/203 [2m0:03:42 â€¢ 0:05:21[0m [2;4m0.36it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 89/203 [2m0:03:42 â€¢ 0:05:21[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.5638, 0.4362]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 89/203 [2m0:03:42 â€¢ 0:05:21[0m [2;4m0.36it/s[0m  
[2KStep 0 - TTA Loss: 0.7447â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 89/203 [2m0:03:42 â€¢ 0:05:21[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.5638, 0.4362]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 89/203 [2m0:03:42 â€¢ 0:05:21[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.5641, 0.4359]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 89/203 [2m0:03:42 â€¢ 0:05:21[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.5655, 0.4345]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 89/203 [2m0:03:42 â€¢ 0:05:21[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.5683, 0.4317]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 89/203 [2m0:03:42 â€¢ 0:05:21[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.5731, 0.4269]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 89/203 [2m0:03:42 â€¢ 0:05:21[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.5800, 0.4200]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 89/203 [2m0:03:42 â€¢ 0:05:21[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.5888, 0.4112]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 89/203 [2m0:03:42 â€¢ 0:05:21[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.5996, 0.4004]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 89/203 [2m0:03:42 â€¢ 0:05:21[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.6120, 0.3880]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 89/203 [2m0:03:42 â€¢ 0:05:21[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.5587, 0.4413]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 89/203 [2m0:03:42 â€¢ 0:05:21[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.6745, 0.3255],8;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 89/203 [2m0:03:42 â€¢ 0:05:21[0m [2;4m0.36it/s[0m  
        [0.6091, 0.3909],
        [0.6252, 0.3748],
        [0.6991, 0.3009],
[2K        [0.6698, 0.3302]], device='cuda:0')38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 89/203 [2m0:03:42 â€¢ 0:05:21[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.5615, 0.4385],m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 90/203 [2m0:03:52 â€¢ 0:10:25[0m [2;4m0.18it/s[0m  
        [0.5581, 0.4419],
        [0.5564, 0.4436],
        [0.5608, 0.4392],
[2K        [0.5577, 0.4423]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 90/203 [2m0:03:52 â€¢ 0:10:25[0m [2;4m0.18it/s[0m  
[2KClass label: HammerThrowâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 90/203 [2m0:03:52 â€¢ 0:10:25[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.5593, 0.4407]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 90/203 [2m0:03:52 â€¢ 0:10:25[0m [2;4m0.18it/s[0m  
[2KStep 0 - TTA Loss: 0.7321â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 90/203 [2m0:03:52 â€¢ 0:10:25[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.5692, 0.4308]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 90/203 [2m0:03:52 â€¢ 0:10:25[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.5781, 0.4219]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 90/203 [2m0:03:52 â€¢ 0:10:25[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.5859, 0.4141]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 90/203 [2m0:03:52 â€¢ 0:10:25[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.5923, 0.4077]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 90/203 [2m0:03:52 â€¢ 0:10:25[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.5975, 0.4025]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 90/203 [2m0:03:52 â€¢ 0:10:25[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.6013, 0.3987]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 90/203 [2m0:03:52 â€¢ 0:10:25[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.6038, 0.3962]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 90/203 [2m0:03:52 â€¢ 0:10:25[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.6053, 0.3947]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 90/203 [2m0:03:52 â€¢ 0:10:25[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.6060, 0.3940]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 90/203 [2m0:03:52 â€¢ 0:10:25[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.5599, 0.4401]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 90/203 [2m0:03:52 â€¢ 0:10:25[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.6708, 0.3292],8;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 90/203 [2m0:03:52 â€¢ 0:10:25[0m [2;4m0.18it/s[0m  
        [0.6062, 0.3938],
        [0.6194, 0.3806],
        [0.6971, 0.3029],
[2K        [0.6657, 0.3343]], device='cuda:0')38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 90/203 [2m0:03:52 â€¢ 0:10:25[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.5615, 0.4385],m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 91/203 [2m0:03:55 â€¢ 0:10:02[0m [2;4m0.19it/s[0m  
        [0.5582, 0.4418],
        [0.5566, 0.4434],
        [0.5608, 0.4392],
[2K        [0.5578, 0.4422]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 91/203 [2m0:03:55 â€¢ 0:10:02[0m [2;4m0.19it/s[0m  
[2KClass label: SoccerPenaltyâ”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 91/203 [2m0:03:55 â€¢ 0:10:02[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.6696, 0.3304]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 91/203 [2m0:03:55 â€¢ 0:10:02[0m [2;4m0.19it/s[0m  
[2KStep 0 - TTA Loss: 1.2227â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 91/203 [2m0:03:55 â€¢ 0:10:02[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.6696, 0.3304]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 91/203 [2m0:03:55 â€¢ 0:10:02[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.6697, 0.3303]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 91/203 [2m0:03:55 â€¢ 0:10:02[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.6699, 0.3301]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 91/203 [2m0:03:55 â€¢ 0:10:02[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.6704, 0.3296]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 91/203 [2m0:03:55 â€¢ 0:10:02[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.6711, 0.3289]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 91/203 [2m0:03:55 â€¢ 0:10:02[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.6720, 0.3280]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 91/203 [2m0:03:55 â€¢ 0:10:02[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.6733, 0.3267]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 91/203 [2m0:03:55 â€¢ 0:10:02[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.6745, 0.3255]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 91/203 [2m0:03:55 â€¢ 0:10:02[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.6757, 0.3243]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 91/203 [2m0:03:55 â€¢ 0:10:02[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.5613, 0.4387]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 91/203 [2m0:03:55 â€¢ 0:10:02[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.6558, 0.3442],8;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 91/203 [2m0:03:55 â€¢ 0:10:02[0m [2;4m0.19it/s[0m  
        [0.5806, 0.4194],
        [0.5851, 0.4149],
        [0.6768, 0.3232],
[2K        [0.6427, 0.3573]], device='cuda:0')38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 91/203 [2m0:03:55 â€¢ 0:10:02[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.5616, 0.4384],0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 92/203 [2m0:03:56 â€¢ 0:08:31[0m [2;4m0.22it/s[0m   
        [0.5582, 0.4418],
        [0.5566, 0.4434],
        [0.5608, 0.4392],
[2K        [0.5578, 0.4422]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 92/203 [2m0:03:56 â€¢ 0:08:31[0m [2;4m0.22it/s[0m  
[2KClass label: BaseballPitchâ”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 92/203 [2m0:03:56 â€¢ 0:08:31[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.6445, 0.3555]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 92/203 [2m0:03:56 â€¢ 0:08:31[0m [2;4m0.22it/s[0m  
[2KStep 0 - TTA Loss: 0.9870â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 92/203 [2m0:03:56 â€¢ 0:08:31[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.6458, 0.3542]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 92/203 [2m0:03:56 â€¢ 0:08:31[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.6474, 0.3526]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 92/203 [2m0:03:56 â€¢ 0:08:31[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.6485, 0.3515]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 92/203 [2m0:03:56 â€¢ 0:08:31[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.6493, 0.3507]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 92/203 [2m0:03:56 â€¢ 0:08:31[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.6501, 0.3499]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 92/203 [2m0:03:56 â€¢ 0:08:31[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.6508, 0.3492]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 92/203 [2m0:03:56 â€¢ 0:08:31[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.6514, 0.3486]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 92/203 [2m0:03:56 â€¢ 0:08:31[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.6517, 0.3483]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 92/203 [2m0:03:56 â€¢ 0:08:31[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.6518, 0.3482]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 92/203 [2m0:03:56 â€¢ 0:08:31[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.5620, 0.4380]], device='cuda:0');5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 92/203 [2m0:03:56 â€¢ 0:08:31[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.6519, 0.3481],38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 92/203 [2m0:03:56 â€¢ 0:08:31[0m [2;4m0.22it/s[0m  
        [0.5755, 0.4245],
        [0.5772, 0.4228],
        [0.6740, 0.3260],
[2K        [0.6369, 0.3631]], device='cuda:0')[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 92/203 [2m0:03:56 â€¢ 0:08:31[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.5616, 0.4384],0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 93/203 [2m0:03:57 â€¢ 0:07:29[0m [2;4m0.25it/s[0m  
        [0.5583, 0.4417],
        [0.5566, 0.4434],
        [0.5608, 0.4392],
[2K        [0.5579, 0.4421]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 93/203 [2m0:03:57 â€¢ 0:07:29[0m [2;4m0.25it/s[0m  
[2KClass label: SoccerPenaltyâ”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 93/203 [2m0:03:57 â€¢ 0:07:29[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6734, 0.3266]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 93/203 [2m0:03:57 â€¢ 0:07:29[0m [2;4m0.25it/s[0m  
[2KStep 0 - TTA Loss: 0.8301â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 93/203 [2m0:03:57 â€¢ 0:07:29[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6734, 0.3266]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 93/203 [2m0:03:57 â€¢ 0:07:29[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6735, 0.3265]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 93/203 [2m0:03:57 â€¢ 0:07:29[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6740, 0.3260]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 93/203 [2m0:03:57 â€¢ 0:07:29[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6752, 0.3248]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 93/203 [2m0:03:57 â€¢ 0:07:29[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6774, 0.3226]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 93/203 [2m0:03:57 â€¢ 0:07:29[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6814, 0.3186]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 93/203 [2m0:03:57 â€¢ 0:07:29[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6869, 0.3131]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 93/203 [2m0:03:57 â€¢ 0:07:29[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6938, 0.3062]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 93/203 [2m0:03:57 â€¢ 0:07:29[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.7016, 0.2984]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 93/203 [2m0:03:57 â€¢ 0:07:29[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.5627, 0.4373]], device='cuda:0');5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 93/203 [2m0:03:57 â€¢ 0:07:29[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6668, 0.3332],38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 93/203 [2m0:03:57 â€¢ 0:07:29[0m [2;4m0.25it/s[0m  
        [0.5859, 0.4141],
        [0.5867, 0.4133],
        [0.7104, 0.2896],
[2K        [0.6451, 0.3549]], device='cuda:0')[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 93/203 [2m0:03:57 â€¢ 0:07:29[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.5616, 0.4384],0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 94/203 [2m0:03:58 â€¢ 0:07:12[0m [2;4m0.25it/s[0m  
        [0.5583, 0.4417],
        [0.5567, 0.4433],
        [0.5609, 0.4391],
[2K        [0.5579, 0.4421]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 94/203 [2m0:03:58 â€¢ 0:07:12[0m [2;4m0.25it/s[0m  
[2KClass label: SoccerPenaltyâ”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 94/203 [2m0:03:58 â€¢ 0:07:12[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6662, 0.3338]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 94/203 [2m0:03:58 â€¢ 0:07:12[0m [2;4m0.25it/s[0m  
[2KStep 0 - TTA Loss: 0.6994â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 94/203 [2m0:03:58 â€¢ 0:07:12[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6768, 0.3232]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 94/203 [2m0:03:58 â€¢ 0:07:12[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6868, 0.3132]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 94/203 [2m0:03:58 â€¢ 0:07:12[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6966, 0.3034]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 94/203 [2m0:03:58 â€¢ 0:07:12[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.7057, 0.2943]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 94/203 [2m0:03:58 â€¢ 0:07:12[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.7136, 0.2864]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 94/203 [2m0:03:58 â€¢ 0:07:12[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.7197, 0.2803]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 94/203 [2m0:03:58 â€¢ 0:07:12[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.7242, 0.2758]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 94/203 [2m0:03:58 â€¢ 0:07:12[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.7269, 0.2731]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 94/203 [2m0:03:58 â€¢ 0:07:12[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.7282, 0.2718]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 94/203 [2m0:03:58 â€¢ 0:07:12[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.5636, 0.4364]], device='cuda:0')m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 94/203 [2m0:03:58 â€¢ 0:07:12[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6749, 0.3251],38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 94/203 [2m0:03:58 â€¢ 0:07:12[0m [2;4m0.25it/s[0m  
        [0.5920, 0.4080],
        [0.5916, 0.4084],
        [0.7286, 0.2714],
[2K        [0.6493, 0.3507]], device='cuda:0')[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 94/203 [2m0:03:58 â€¢ 0:07:12[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.5617, 0.4383],0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 95/203 [2m0:04:00 â€¢ 0:06:39[0m [2;4m0.27it/s[0m  
        [0.5584, 0.4416],
        [0.5567, 0.4433],
        [0.5610, 0.4390],
[2K        [0.5579, 0.4421]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 95/203 [2m0:04:00 â€¢ 0:06:39[0m [2;4m0.27it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 95/203 [2m0:04:00 â€¢ 0:06:39[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6237, 0.3763]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 95/203 [2m0:04:00 â€¢ 0:06:39[0m [2;4m0.27it/s[0m  
[2KStep 0 - TTA Loss: 0.6643â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 95/203 [2m0:04:00 â€¢ 0:06:39[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6237, 0.3763]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 95/203 [2m0:04:00 â€¢ 0:06:39[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6238, 0.3762]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 95/203 [2m0:04:00 â€¢ 0:06:39[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6244, 0.3756]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 95/203 [2m0:04:00 â€¢ 0:06:39[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6257, 0.3743]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 95/203 [2m0:04:00 â€¢ 0:06:39[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6281, 0.3719]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 95/203 [2m0:04:00 â€¢ 0:06:39[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6318, 0.3682]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 95/203 [2m0:04:00 â€¢ 0:06:39[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6372, 0.3628]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 95/203 [2m0:04:00 â€¢ 0:06:39[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6436, 0.3564]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 95/203 [2m0:04:00 â€¢ 0:06:39[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6506, 0.3494]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 95/203 [2m0:04:00 â€¢ 0:06:39[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.5596, 0.4404]], device='cuda:0')m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 95/203 [2m0:04:00 â€¢ 0:06:39[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6721, 0.3279],38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 95/203 [2m0:04:00 â€¢ 0:06:39[0m [2;4m0.27it/s[0m  
        [0.5939, 0.4061],
        [0.5955, 0.4045],
        [0.7083, 0.2917],
[2K        [0.6581, 0.3419]], device='cuda:0')[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 95/203 [2m0:04:00 â€¢ 0:06:39[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.5618, 0.4382],0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 96/203 [2m0:04:04 â€¢ 0:06:37[0m [2;4m0.27it/s[0m  
        [0.5584, 0.4416],
        [0.5568, 0.4432],
        [0.5611, 0.4389],
[2K        [0.5581, 0.4419]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 96/203 [2m0:04:04 â€¢ 0:06:37[0m [2;4m0.27it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 96/203 [2m0:04:04 â€¢ 0:06:37[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.5708, 0.4292]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 96/203 [2m0:04:04 â€¢ 0:06:37[0m [2;4m0.27it/s[0m  
[2KStep 0 - TTA Loss: 0.5915â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 96/203 [2m0:04:04 â€¢ 0:06:37[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.5783, 0.4217]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 96/203 [2m0:04:04 â€¢ 0:06:37[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.5863, 0.4137]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 96/203 [2m0:04:04 â€¢ 0:06:37[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.5942, 0.4058]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 96/203 [2m0:04:04 â€¢ 0:06:37[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6015, 0.3985]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 96/203 [2m0:04:04 â€¢ 0:06:37[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6079, 0.3921]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 96/203 [2m0:04:04 â€¢ 0:06:37[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6130, 0.3870]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 96/203 [2m0:04:04 â€¢ 0:06:37[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6166, 0.3834]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 96/203 [2m0:04:04 â€¢ 0:06:37[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6189, 0.3811]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 96/203 [2m0:04:04 â€¢ 0:06:37[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6200, 0.3800]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 96/203 [2m0:04:04 â€¢ 0:06:37[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.5590, 0.4410]], device='cuda:0')m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 96/203 [2m0:04:04 â€¢ 0:06:37[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6824, 0.3176],38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 96/203 [2m0:04:04 â€¢ 0:06:37[0m [2;4m0.27it/s[0m  
        [0.6103, 0.3897],
        [0.6203, 0.3797],
        [0.7118, 0.2882],
[2K        [0.6766, 0.3234]], device='cuda:0')[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 96/203 [2m0:04:04 â€¢ 0:06:37[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.5619, 0.4381],[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 97/203 [2m0:04:06 â€¢ 0:05:20[0m [2;4m0.33it/s[0m   
        [0.5586, 0.4414],
        [0.5569, 0.4431],
        [0.5612, 0.4388],
[2K        [0.5582, 0.4418]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 97/203 [2m0:04:06 â€¢ 0:05:20[0m [2;4m0.33it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 97/203 [2m0:04:06 â€¢ 0:05:20[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.6293, 0.3707]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 97/203 [2m0:04:06 â€¢ 0:05:20[0m [2;4m0.33it/s[0m  
[2KStep 0 - TTA Loss: 0.6792â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 97/203 [2m0:04:06 â€¢ 0:05:20[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.6293, 0.3707]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 97/203 [2m0:04:06 â€¢ 0:05:20[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.6295, 0.3705]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 97/203 [2m0:04:06 â€¢ 0:05:20[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.6303, 0.3697]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 97/203 [2m0:04:06 â€¢ 0:05:20[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.6319, 0.3681]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 97/203 [2m0:04:06 â€¢ 0:05:20[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.6345, 0.3655]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 97/203 [2m0:04:06 â€¢ 0:05:20[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.6379, 0.3621]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 97/203 [2m0:04:06 â€¢ 0:05:20[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.6421, 0.3579]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 97/203 [2m0:04:06 â€¢ 0:05:20[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.6473, 0.3527]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 97/203 [2m0:04:06 â€¢ 0:05:20[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.6531, 0.3469]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 97/203 [2m0:04:06 â€¢ 0:05:20[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.5598, 0.4402]], device='cuda:0')8;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 97/203 [2m0:04:06 â€¢ 0:05:20[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.6634, 0.3366],[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 97/203 [2m0:04:06 â€¢ 0:05:20[0m [2;4m0.33it/s[0m  
        [0.5912, 0.4088],
        [0.5999, 0.4001],
        [0.6901, 0.3099],
[2K        [0.6595, 0.3405]], device='cuda:0')[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 97/203 [2m0:04:06 â€¢ 0:05:20[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.5620, 0.4380],[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 98/203 [2m0:04:09 â€¢ 0:05:08[0m [2;4m0.34it/s[0m  
        [0.5587, 0.4413],
        [0.5570, 0.4430],
        [0.5613, 0.4387],
[2K        [0.5583, 0.4417]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 98/203 [2m0:04:09 â€¢ 0:05:08[0m [2;4m0.34it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 98/203 [2m0:04:09 â€¢ 0:05:08[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.5638, 0.4362]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 98/203 [2m0:04:09 â€¢ 0:05:08[0m [2;4m0.34it/s[0m  
[2KStep 0 - TTA Loss: 1.0272â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 98/203 [2m0:04:09 â€¢ 0:05:08[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.5711, 0.4289]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 98/203 [2m0:04:09 â€¢ 0:05:08[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.5782, 0.4218]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 98/203 [2m0:04:09 â€¢ 0:05:08[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.5845, 0.4155]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 98/203 [2m0:04:09 â€¢ 0:05:08[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.5900, 0.4100]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 98/203 [2m0:04:09 â€¢ 0:05:08[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.5944, 0.4056]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 98/203 [2m0:04:09 â€¢ 0:05:08[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.5978, 0.4022]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 98/203 [2m0:04:09 â€¢ 0:05:08[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6002, 0.3998]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 98/203 [2m0:04:09 â€¢ 0:05:08[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6016, 0.3984]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 98/203 [2m0:04:09 â€¢ 0:05:08[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6023, 0.3977]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 98/203 [2m0:04:09 â€¢ 0:05:08[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.5584, 0.4416]], device='cuda:0')8;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 98/203 [2m0:04:09 â€¢ 0:05:08[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6626, 0.3374],[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 98/203 [2m0:04:09 â€¢ 0:05:08[0m [2;4m0.34it/s[0m  
        [0.5933, 0.4067],
        [0.6025, 0.3975],
        [0.6877, 0.3123],
[2K        [0.6570, 0.3430]], device='cuda:0')[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 98/203 [2m0:04:09 â€¢ 0:05:08[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.5620, 0.4380],[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 99/203 [2m0:04:09 â€¢ 0:04:41[0m [2;4m0.37it/s[0m  
        [0.5587, 0.4413],
        [0.5571, 0.4429],
        [0.5613, 0.4387],
[2K        [0.5584, 0.4416]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 99/203 [2m0:04:09 â€¢ 0:04:41[0m [2;4m0.37it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 99/203 [2m0:04:09 â€¢ 0:04:41[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5741, 0.4259]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 99/203 [2m0:04:09 â€¢ 0:04:41[0m [2;4m0.37it/s[0m  
[2KStep 0 - TTA Loss: 0.7206â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 99/203 [2m0:04:09 â€¢ 0:04:41[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5741, 0.4259]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 99/203 [2m0:04:09 â€¢ 0:04:41[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5744, 0.4256]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 99/203 [2m0:04:09 â€¢ 0:04:41[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5758, 0.4242]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 99/203 [2m0:04:09 â€¢ 0:04:41[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5795, 0.4205]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 99/203 [2m0:04:09 â€¢ 0:04:41[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5861, 0.4139]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 99/203 [2m0:04:09 â€¢ 0:04:41[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5967, 0.4033]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 99/203 [2m0:04:09 â€¢ 0:04:41[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.6115, 0.3885]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 99/203 [2m0:04:09 â€¢ 0:04:41[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.6309, 0.3691]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 99/203 [2m0:04:09 â€¢ 0:04:41[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.6539, 0.3461]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 99/203 [2m0:04:09 â€¢ 0:04:41[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5617, 0.4383]], device='cuda:0')0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 99/203 [2m0:04:09 â€¢ 0:04:41[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.7015, 0.2985],[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 99/203 [2m0:04:09 â€¢ 0:04:41[0m [2;4m0.37it/s[0m  
        [0.6476, 0.3524],
        [0.6796, 0.3204],
        [0.7257, 0.2743],
[2K        [0.7094, 0.2906]], device='cuda:0')[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 99/203 [2m0:04:09 â€¢ 0:04:41[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5621, 0.4379],[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 100/203 [2m0:04:12 â€¢ 0:03:25[0m [2;4m0.50it/s[0m  
        [0.5589, 0.4411],
        [0.5573, 0.4427],
        [0.5614, 0.4386],
[2K        [0.5585, 0.4415]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 100/203 [2m0:04:12 â€¢ 0:03:25[0m [2;4m0.50it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 100/203 [2m0:04:12 â€¢ 0:03:25[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.5678, 0.4322]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 100/203 [2m0:04:12 â€¢ 0:03:25[0m [2;4m0.50it/s[0m  
[2KStep 0 - TTA Loss: 0.6230â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 100/203 [2m0:04:12 â€¢ 0:03:25[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.5952, 0.4048]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 100/203 [2m0:04:12 â€¢ 0:03:25[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.6201, 0.3799]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 100/203 [2m0:04:12 â€¢ 0:03:25[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.6416, 0.3584]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 100/203 [2m0:04:12 â€¢ 0:03:25[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.6592, 0.3408]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 100/203 [2m0:04:12 â€¢ 0:03:25[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.6729, 0.3271]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 100/203 [2m0:04:12 â€¢ 0:03:25[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.6830, 0.3170]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 100/203 [2m0:04:12 â€¢ 0:03:25[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.6897, 0.3103]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 100/203 [2m0:04:12 â€¢ 0:03:25[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.6936, 0.3064]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 100/203 [2m0:04:12 â€¢ 0:03:25[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.6954, 0.3046]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 100/203 [2m0:04:12 â€¢ 0:03:25[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.5626, 0.4374]], device='cuda:0')0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 100/203 [2m0:04:12 â€¢ 0:03:25[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.7089, 0.2911],[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 100/203 [2m0:04:12 â€¢ 0:03:25[0m [2;4m0.50it/s[0m  
        [0.6571, 0.3429],
        [0.6960, 0.3040],
        [0.7319, 0.2681],
[2K        [0.7173, 0.2827]], device='cuda:0')[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 100/203 [2m0:04:12 â€¢ 0:03:25[0m [2;4m0.50it/s[0m  
[2KAttention weights: tensor([[0.5623, 0.4377],[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 101/203 [2m0:04:17 â€¢ 0:03:48[0m [2;4m0.45it/s[0m  
        [0.5591, 0.4409],
        [0.5577, 0.4423],
        [0.5615, 0.4385],
[2K        [0.5587, 0.4413]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 101/203 [2m0:04:17 â€¢ 0:03:48[0m [2;4m0.45it/s[0m  
[2KClass label: SoccerPenaltyâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 101/203 [2m0:04:17 â€¢ 0:03:48[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.6725, 0.3275]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 101/203 [2m0:04:17 â€¢ 0:03:48[0m [2;4m0.45it/s[0m  
[2KStep 0 - TTA Loss: 1.1017â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 101/203 [2m0:04:17 â€¢ 0:03:48[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.6725, 0.3275]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 101/203 [2m0:04:17 â€¢ 0:03:48[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.6728, 0.3272]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 101/203 [2m0:04:17 â€¢ 0:03:48[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.6737, 0.3263]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 101/203 [2m0:04:17 â€¢ 0:03:48[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.6757, 0.3243]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 101/203 [2m0:04:17 â€¢ 0:03:48[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.6790, 0.3210]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 101/203 [2m0:04:17 â€¢ 0:03:48[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.6837, 0.3163]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 101/203 [2m0:04:17 â€¢ 0:03:48[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.6894, 0.3106]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 101/203 [2m0:04:17 â€¢ 0:03:48[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.6958, 0.3042]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 101/203 [2m0:04:17 â€¢ 0:03:48[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.7024, 0.2976]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 101/203 [2m0:04:17 â€¢ 0:03:48[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.5631, 0.4369]], device='cuda:0')0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 101/203 [2m0:04:17 â€¢ 0:03:48[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.6742, 0.3258],[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 101/203 [2m0:04:17 â€¢ 0:03:48[0m [2;4m0.45it/s[0m  
        [0.6038, 0.3962],
        [0.6206, 0.3794],
        [0.7087, 0.2913],
[2K        [0.6661, 0.3339]], device='cuda:0')[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 101/203 [2m0:04:17 â€¢ 0:03:48[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.5623, 0.4377],[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 102/203 [2m0:04:20 â€¢ 0:03:55[0m [2;4m0.43it/s[0m   
        [0.5591, 0.4409],
        [0.5577, 0.4423],
        [0.5615, 0.4385],
[2K        [0.5587, 0.4413]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 102/203 [2m0:04:20 â€¢ 0:03:55[0m [2;4m0.43it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 102/203 [2m0:04:20 â€¢ 0:03:55[0m [2;4m0.43it/s[0m  
[2KAttention weights: tensor([[0.6271, 0.3729]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 102/203 [2m0:04:20 â€¢ 0:03:55[0m [2;4m0.43it/s[0m  
[2KStep 0 - TTA Loss: 1.1982â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 102/203 [2m0:04:20 â€¢ 0:03:55[0m [2;4m0.43it/s[0m  
[2KAttention weights: tensor([[0.6331, 0.3669]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 102/203 [2m0:04:20 â€¢ 0:03:55[0m [2;4m0.43it/s[0m  
[2KAttention weights: tensor([[0.6387, 0.3613]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 102/203 [2m0:04:20 â€¢ 0:03:55[0m [2;4m0.43it/s[0m  
[2KAttention weights: tensor([[0.6443, 0.3557]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 102/203 [2m0:04:20 â€¢ 0:03:55[0m [2;4m0.43it/s[0m  
[2KAttention weights: tensor([[0.6493, 0.3507]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 102/203 [2m0:04:20 â€¢ 0:03:55[0m [2;4m0.43it/s[0m  
[2KAttention weights: tensor([[0.6533, 0.3467]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 102/203 [2m0:04:20 â€¢ 0:03:55[0m [2;4m0.43it/s[0m  
[2KAttention weights: tensor([[0.6562, 0.3438]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 102/203 [2m0:04:20 â€¢ 0:03:55[0m [2;4m0.43it/s[0m  
[2KAttention weights: tensor([[0.6584, 0.3416]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 102/203 [2m0:04:20 â€¢ 0:03:55[0m [2;4m0.43it/s[0m  
[2KAttention weights: tensor([[0.6596, 0.3404]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 102/203 [2m0:04:20 â€¢ 0:03:55[0m [2;4m0.43it/s[0m  
[2KAttention weights: tensor([[0.6602, 0.3398]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 102/203 [2m0:04:20 â€¢ 0:03:55[0m [2;4m0.43it/s[0m  
[2KAttention weights: tensor([[0.5603, 0.4397]], device='cuda:0')38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 102/203 [2m0:04:20 â€¢ 0:03:55[0m [2;4m0.43it/s[0m  
[2KAttention weights: tensor([[0.6664, 0.3336],[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 102/203 [2m0:04:20 â€¢ 0:03:55[0m [2;4m0.43it/s[0m  
        [0.5970, 0.4030],
        [0.6068, 0.3932],
        [0.6984, 0.3016],
[2K        [0.6604, 0.3396]], device='cuda:0')m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 102/203 [2m0:04:20 â€¢ 0:03:55[0m [2;4m0.43it/s[0m  
[2KAttention weights: tensor([[0.5623, 0.4377],[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 103/203 [2m0:04:22 â€¢ 0:03:47[0m [2;4m0.44it/s[0m  
        [0.5591, 0.4409],
        [0.5578, 0.4422],
        [0.5615, 0.4385],
[2K        [0.5588, 0.4412]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 103/203 [2m0:04:22 â€¢ 0:03:47[0m [2;4m0.44it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 103/203 [2m0:04:22 â€¢ 0:03:47[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.6279, 0.3721]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 103/203 [2m0:04:22 â€¢ 0:03:47[0m [2;4m0.44it/s[0m  
[2KStep 0 - TTA Loss: 0.7550â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 103/203 [2m0:04:22 â€¢ 0:03:47[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.6279, 0.3721]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 103/203 [2m0:04:22 â€¢ 0:03:47[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.6281, 0.3719]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 103/203 [2m0:04:22 â€¢ 0:03:47[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.6290, 0.3710]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 103/203 [2m0:04:22 â€¢ 0:03:47[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.6311, 0.3689]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 103/203 [2m0:04:22 â€¢ 0:03:47[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.6347, 0.3653]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 103/203 [2m0:04:22 â€¢ 0:03:47[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.6403, 0.3597]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 103/203 [2m0:04:22 â€¢ 0:03:47[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.6478, 0.3522]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 103/203 [2m0:04:22 â€¢ 0:03:47[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.6574, 0.3426]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 103/203 [2m0:04:22 â€¢ 0:03:47[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.6685, 0.3315]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 103/203 [2m0:04:22 â€¢ 0:03:47[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.5612, 0.4388]], device='cuda:0')38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 103/203 [2m0:04:22 â€¢ 0:03:47[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.6772, 0.3228],[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 103/203 [2m0:04:22 â€¢ 0:03:47[0m [2;4m0.44it/s[0m  
        [0.6092, 0.3908],
        [0.6124, 0.3876],
        [0.6949, 0.3051],
[2K        [0.6808, 0.3192]], device='cuda:0')m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 103/203 [2m0:04:22 â€¢ 0:03:47[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.5623, 0.4377],[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 104/203 [2m0:04:22 â€¢ 0:03:32[0m [2;4m0.47it/s[0m  
        [0.5591, 0.4409],
        [0.5578, 0.4422],
        [0.5615, 0.4385],
[2K        [0.5588, 0.4412]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 104/203 [2m0:04:22 â€¢ 0:03:32[0m [2;4m0.47it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 104/203 [2m0:04:22 â€¢ 0:03:32[0m [2;4m0.47it/s[0m  
[2KAttention weights: tensor([[0.5678, 0.4322]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 104/203 [2m0:04:22 â€¢ 0:03:32[0m [2;4m0.47it/s[0m  
[2KStep 0 - TTA Loss: 0.6138â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 104/203 [2m0:04:22 â€¢ 0:03:32[0m [2;4m0.47it/s[0m  
[2KAttention weights: tensor([[0.5784, 0.4216]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 104/203 [2m0:04:22 â€¢ 0:03:32[0m [2;4m0.47it/s[0m  
[2KAttention weights: tensor([[0.5891, 0.4109]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 104/203 [2m0:04:22 â€¢ 0:03:32[0m [2;4m0.47it/s[0m  
[2KAttention weights: tensor([[0.5988, 0.4012]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 104/203 [2m0:04:22 â€¢ 0:03:32[0m [2;4m0.47it/s[0m  
[2KAttention weights: tensor([[0.6073, 0.3927]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 104/203 [2m0:04:22 â€¢ 0:03:32[0m [2;4m0.47it/s[0m  
[2KAttention weights: tensor([[0.6145, 0.3855]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 104/203 [2m0:04:22 â€¢ 0:03:32[0m [2;4m0.47it/s[0m  
[2KAttention weights: tensor([[0.6201, 0.3799]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 104/203 [2m0:04:22 â€¢ 0:03:32[0m [2;4m0.47it/s[0m  
[2KAttention weights: tensor([[0.6240, 0.3760]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 104/203 [2m0:04:22 â€¢ 0:03:32[0m [2;4m0.47it/s[0m  
[2KAttention weights: tensor([[0.6264, 0.3736]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 104/203 [2m0:04:22 â€¢ 0:03:32[0m [2;4m0.47it/s[0m  
[2KAttention weights: tensor([[0.6276, 0.3724]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 104/203 [2m0:04:22 â€¢ 0:03:32[0m [2;4m0.47it/s[0m  
[2KAttention weights: tensor([[0.5602, 0.4398]], device='cuda:0')38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 104/203 [2m0:04:22 â€¢ 0:03:32[0m [2;4m0.47it/s[0m  
[2KAttention weights: tensor([[0.6877, 0.3123],[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 104/203 [2m0:04:22 â€¢ 0:03:32[0m [2;4m0.47it/s[0m  
        [0.6214, 0.3786],
        [0.6280, 0.3720],
        [0.7051, 0.2949],
[2K        [0.6919, 0.3081]], device='cuda:0')m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 104/203 [2m0:04:22 â€¢ 0:03:32[0m [2;4m0.47it/s[0m  
[2KAttention weights: tensor([[0.5623, 0.4377],[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 105/203 [2m0:04:28 â€¢ 0:04:37[0m [2;4m0.35it/s[0m  
        [0.5592, 0.4408],
        [0.5579, 0.4421],
        [0.5615, 0.4385],
[2K        [0.5589, 0.4411]], device='cuda:0', grad_fn=<SoftmaxBackward0>);237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 105/203 [2m0:04:28 â€¢ 0:04:37[0m [2;4m0.35it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 105/203 [2m0:04:28 â€¢ 0:04:37[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5691, 0.4309]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 105/203 [2m0:04:28 â€¢ 0:04:37[0m [2;4m0.35it/s[0m  
[2KStep 0 - TTA Loss: 0.8365â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 105/203 [2m0:04:28 â€¢ 0:04:37[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5691, 0.4309]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 105/203 [2m0:04:28 â€¢ 0:04:37[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5695, 0.4305]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 105/203 [2m0:04:28 â€¢ 0:04:37[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5710, 0.4290]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 105/203 [2m0:04:28 â€¢ 0:04:37[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5743, 0.4257]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 105/203 [2m0:04:28 â€¢ 0:04:37[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5801, 0.4199]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 105/203 [2m0:04:28 â€¢ 0:04:37[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5889, 0.4111]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 105/203 [2m0:04:28 â€¢ 0:04:37[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6006, 0.3994]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 105/203 [2m0:04:28 â€¢ 0:04:37[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6145, 0.3855]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 105/203 [2m0:04:28 â€¢ 0:04:37[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6308, 0.3692]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 105/203 [2m0:04:28 â€¢ 0:04:37[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5611, 0.4389]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 105/203 [2m0:04:28 â€¢ 0:04:37[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6860, 0.3140],[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 105/203 [2m0:04:28 â€¢ 0:04:37[0m [2;4m0.35it/s[0m  
        [0.6269, 0.3731],
        [0.6490, 0.3510],
        [0.7096, 0.2904],
[2K        [0.6889, 0.3111]], device='cuda:0')m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 105/203 [2m0:04:28 â€¢ 0:04:37[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5624, 0.4376],[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 106/203 [2m0:04:33 â€¢ 0:04:40[0m [2;4m0.35it/s[0m  
        [0.5592, 0.4408],
        [0.5580, 0.4420],
        [0.5616, 0.4384],
[2K        [0.5589, 0.4411]], device='cuda:0', grad_fn=<SoftmaxBackward0>);237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 106/203 [2m0:04:33 â€¢ 0:04:40[0m [2;4m0.35it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 106/203 [2m0:04:33 â€¢ 0:04:40[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6289, 0.3711]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 106/203 [2m0:04:33 â€¢ 0:04:40[0m [2;4m0.35it/s[0m  
[2KStep 0 - TTA Loss: 0.8363â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 106/203 [2m0:04:33 â€¢ 0:04:40[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6412, 0.3588]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 106/203 [2m0:04:33 â€¢ 0:04:40[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6513, 0.3487]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 106/203 [2m0:04:33 â€¢ 0:04:40[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6587, 0.3413]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 106/203 [2m0:04:33 â€¢ 0:04:40[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6638, 0.3362]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 106/203 [2m0:04:33 â€¢ 0:04:40[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6670, 0.3330]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 106/203 [2m0:04:33 â€¢ 0:04:40[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6688, 0.3312]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 106/203 [2m0:04:33 â€¢ 0:04:40[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6696, 0.3304]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 106/203 [2m0:04:33 â€¢ 0:04:40[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6698, 0.3302]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 106/203 [2m0:04:33 â€¢ 0:04:40[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6698, 0.3302]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 106/203 [2m0:04:33 â€¢ 0:04:40[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5609, 0.4391]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 106/203 [2m0:04:33 â€¢ 0:04:40[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6717, 0.3283],[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 106/203 [2m0:04:33 â€¢ 0:04:40[0m [2;4m0.35it/s[0m  
        [0.6060, 0.3940],
        [0.6317, 0.3683],
        [0.6999, 0.3001],
[2K        [0.6699, 0.3301]], device='cuda:0')m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 106/203 [2m0:04:33 â€¢ 0:04:40[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5625, 0.4375],â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 107/203 [2m0:04:36 â€¢ 0:04:46[0m [2;4m0.34it/s[0m   
        [0.5593, 0.4407],
        [0.5581, 0.4419],
        [0.5616, 0.4384],
[2K        [0.5590, 0.4410]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 107/203 [2m0:04:36 â€¢ 0:04:46[0m [2;4m0.34it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 107/203 [2m0:04:36 â€¢ 0:04:46[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6342, 0.3658]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 107/203 [2m0:04:36 â€¢ 0:04:46[0m [2;4m0.34it/s[0m  
[2KStep 0 - TTA Loss: 0.6535â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 107/203 [2m0:04:36 â€¢ 0:04:46[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6342, 0.3658]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 107/203 [2m0:04:36 â€¢ 0:04:46[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6344, 0.3656]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 107/203 [2m0:04:36 â€¢ 0:04:46[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6352, 0.3648]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 107/203 [2m0:04:36 â€¢ 0:04:46[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6376, 0.3624]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 107/203 [2m0:04:36 â€¢ 0:04:46[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6424, 0.3576]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 107/203 [2m0:04:36 â€¢ 0:04:46[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6505, 0.3495]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 107/203 [2m0:04:36 â€¢ 0:04:46[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6624, 0.3376]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 107/203 [2m0:04:36 â€¢ 0:04:46[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6783, 0.3217]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 107/203 [2m0:04:36 â€¢ 0:04:46[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6974, 0.3026]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 107/203 [2m0:04:36 â€¢ 0:04:46[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.5633, 0.4367]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 107/203 [2m0:04:36 â€¢ 0:04:46[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.7058, 0.2942],m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 107/203 [2m0:04:36 â€¢ 0:04:46[0m [2;4m0.34it/s[0m  
        [0.6438, 0.3562],
        [0.6487, 0.3513],
        [0.7169, 0.2831],
[2K        [0.7192, 0.2808]], device='cuda:0')0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 107/203 [2m0:04:36 â€¢ 0:04:46[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.5626, 0.4374],â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 108/203 [2m0:04:41 â€¢ 0:05:46[0m [2;4m0.27it/s[0m  
        [0.5595, 0.4405],
        [0.5582, 0.4418],
        [0.5617, 0.4383],
[2K        [0.5592, 0.4408]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 108/203 [2m0:04:41 â€¢ 0:05:46[0m [2;4m0.27it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 108/203 [2m0:04:41 â€¢ 0:05:46[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6257, 0.3743]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 108/203 [2m0:04:41 â€¢ 0:05:46[0m [2;4m0.27it/s[0m  
[2KStep 0 - TTA Loss: 1.2085â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 108/203 [2m0:04:41 â€¢ 0:05:46[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6482, 0.3518]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 108/203 [2m0:04:41 â€¢ 0:05:46[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6678, 0.3322]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 108/203 [2m0:04:41 â€¢ 0:05:46[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6840, 0.3160]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 108/203 [2m0:04:41 â€¢ 0:05:46[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6967, 0.3033]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 108/203 [2m0:04:41 â€¢ 0:05:46[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.7061, 0.2939]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 108/203 [2m0:04:41 â€¢ 0:05:46[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.7125, 0.2875]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 108/203 [2m0:04:41 â€¢ 0:05:46[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.7166, 0.2834]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 108/203 [2m0:04:41 â€¢ 0:05:46[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.7188, 0.2812]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 108/203 [2m0:04:41 â€¢ 0:05:46[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.7197, 0.2803]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 108/203 [2m0:04:41 â€¢ 0:05:46[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.5636, 0.4364]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 108/203 [2m0:04:41 â€¢ 0:05:46[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.7065, 0.2935],m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 108/203 [2m0:04:41 â€¢ 0:05:46[0m [2;4m0.27it/s[0m  
        [0.6444, 0.3556],
        [0.6449, 0.3551],
        [0.7142, 0.2858],
[2K        [0.7200, 0.2800]], device='cuda:0')0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 108/203 [2m0:04:41 â€¢ 0:05:46[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.5628, 0.4372],â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 109/203 [2m0:04:46 â€¢ 0:05:44[0m [2;4m0.27it/s[0m  
        [0.5597, 0.4403],
        [0.5584, 0.4416],
        [0.5619, 0.4381],
[2K        [0.5596, 0.4404]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 109/203 [2m0:04:46 â€¢ 0:05:44[0m [2;4m0.27it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 109/203 [2m0:04:46 â€¢ 0:05:44[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6322, 0.3678]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 109/203 [2m0:04:46 â€¢ 0:05:44[0m [2;4m0.27it/s[0m  
[2KStep 0 - TTA Loss: 0.8539â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 109/203 [2m0:04:46 â€¢ 0:05:44[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6322, 0.3678]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 109/203 [2m0:04:46 â€¢ 0:05:44[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6324, 0.3676]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 109/203 [2m0:04:46 â€¢ 0:05:44[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6332, 0.3668]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 109/203 [2m0:04:46 â€¢ 0:05:44[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6349, 0.3651]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 109/203 [2m0:04:46 â€¢ 0:05:44[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6379, 0.3621]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 109/203 [2m0:04:46 â€¢ 0:05:44[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6420, 0.3580]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 109/203 [2m0:04:46 â€¢ 0:05:44[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6473, 0.3527]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 109/203 [2m0:04:46 â€¢ 0:05:44[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6537, 0.3463]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 109/203 [2m0:04:46 â€¢ 0:05:44[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6612, 0.3388]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 109/203 [2m0:04:46 â€¢ 0:05:44[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.5616, 0.4384]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 109/203 [2m0:04:46 â€¢ 0:05:44[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6709, 0.3291],m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 109/203 [2m0:04:46 â€¢ 0:05:44[0m [2;4m0.27it/s[0m  
        [0.5984, 0.4016],
        [0.6031, 0.3969],
        [0.6907, 0.3093],
[2K        [0.6689, 0.3311]], device='cuda:0')0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 109/203 [2m0:04:46 â€¢ 0:05:44[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.5629, 0.4371],â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 110/203 [2m0:04:50 â€¢ 0:05:47[0m [2;4m0.27it/s[0m  
        [0.5598, 0.4402],
        [0.5585, 0.4415],
        [0.5619, 0.4381],
[2K        [0.5597, 0.4403]], device='cuda:0', grad_fn=<SoftmaxBackward0>)5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 110/203 [2m0:04:50 â€¢ 0:05:47[0m [2;4m0.27it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 110/203 [2m0:04:50 â€¢ 0:05:47[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.5677, 0.4323]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 110/203 [2m0:04:50 â€¢ 0:05:47[0m [2;4m0.27it/s[0m  
[2KStep 0 - TTA Loss: 0.7598â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 110/203 [2m0:04:50 â€¢ 0:05:47[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.5752, 0.4248]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 110/203 [2m0:04:50 â€¢ 0:05:47[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.5835, 0.4165]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 110/203 [2m0:04:50 â€¢ 0:05:47[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.5920, 0.4080]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 110/203 [2m0:04:50 â€¢ 0:05:47[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6000, 0.4000]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 110/203 [2m0:04:50 â€¢ 0:05:47[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6073, 0.3927]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 110/203 [2m0:04:50 â€¢ 0:05:47[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6130, 0.3870]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 110/203 [2m0:04:50 â€¢ 0:05:47[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6173, 0.3827]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 110/203 [2m0:04:50 â€¢ 0:05:47[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6200, 0.3800]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 110/203 [2m0:04:50 â€¢ 0:05:47[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6213, 0.3787]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 110/203 [2m0:04:50 â€¢ 0:05:47[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.5607, 0.4393]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 110/203 [2m0:04:50 â€¢ 0:05:47[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6772, 0.3228],m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 110/203 [2m0:04:50 â€¢ 0:05:47[0m [2;4m0.27it/s[0m  
        [0.6113, 0.3887],
        [0.6217, 0.3783],
        [0.6993, 0.3007],
[2K        [0.6805, 0.3195]], device='cuda:0')0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 110/203 [2m0:04:50 â€¢ 0:05:47[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.5630, 0.4370],â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 111/203 [2m0:04:52 â€¢ 0:06:30[0m [2;4m0.24it/s[0m  
        [0.5600, 0.4400],
        [0.5587, 0.4413],
        [0.5620, 0.4380],
[2K        [0.5599, 0.4401]], device='cuda:0', grad_fn=<SoftmaxBackward0>)5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 111/203 [2m0:04:52 â€¢ 0:06:30[0m [2;4m0.24it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 111/203 [2m0:04:52 â€¢ 0:06:30[0m [2;4m0.24it/s[0m  
[2KAttention weights: tensor([[0.6328, 0.3672]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 111/203 [2m0:04:52 â€¢ 0:06:30[0m [2;4m0.24it/s[0m  
[2KStep 0 - TTA Loss: 0.4770â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 111/203 [2m0:04:52 â€¢ 0:06:30[0m [2;4m0.24it/s[0m  
[2KAttention weights: tensor([[0.6328, 0.3672]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 111/203 [2m0:04:52 â€¢ 0:06:30[0m [2;4m0.24it/s[0m  
[2KAttention weights: tensor([[0.6331, 0.3669]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 111/203 [2m0:04:52 â€¢ 0:06:30[0m [2;4m0.24it/s[0m  
[2KAttention weights: tensor([[0.6343, 0.3657]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 111/203 [2m0:04:52 â€¢ 0:06:30[0m [2;4m0.24it/s[0m  
[2KAttention weights: tensor([[0.6371, 0.3629]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 111/203 [2m0:04:52 â€¢ 0:06:30[0m [2;4m0.24it/s[0m  
[2KAttention weights: tensor([[0.6416, 0.3584]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 111/203 [2m0:04:52 â€¢ 0:06:30[0m [2;4m0.24it/s[0m  
[2KAttention weights: tensor([[0.6483, 0.3517]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 111/203 [2m0:04:52 â€¢ 0:06:30[0m [2;4m0.24it/s[0m  
[2KAttention weights: tensor([[0.6573, 0.3427]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 111/203 [2m0:04:52 â€¢ 0:06:30[0m [2;4m0.24it/s[0m  
[2KAttention weights: tensor([[0.6681, 0.3319]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 111/203 [2m0:04:52 â€¢ 0:06:30[0m [2;4m0.24it/s[0m  
[2KAttention weights: tensor([[0.6802, 0.3198]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 111/203 [2m0:04:52 â€¢ 0:06:30[0m [2;4m0.24it/s[0m  
[2KAttention weights: tensor([[0.5629, 0.4371]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 111/203 [2m0:04:52 â€¢ 0:06:30[0m [2;4m0.24it/s[0m  
[2KAttention weights: tensor([[0.6881, 0.3119],m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 111/203 [2m0:04:52 â€¢ 0:06:30[0m [2;4m0.24it/s[0m  
        [0.6236, 0.3764],
        [0.6334, 0.3666],
        [0.7071, 0.2929],
[2K        [0.6931, 0.3069]], device='cuda:0')0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 111/203 [2m0:04:52 â€¢ 0:06:30[0m [2;4m0.24it/s[0m  
[2KAttention weights: tensor([[0.5631, 0.4369],â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 112/203 [2m0:04:57 â€¢ 0:06:11[0m [2;4m0.25it/s[0m   
        [0.5601, 0.4399],
        [0.5588, 0.4412],
        [0.5621, 0.4379],
[2K        [0.5601, 0.4399]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 112/203 [2m0:04:57 â€¢ 0:06:11[0m [2;4m0.25it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 112/203 [2m0:04:57 â€¢ 0:06:11[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6327, 0.3673]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 112/203 [2m0:04:57 â€¢ 0:06:11[0m [2;4m0.25it/s[0m  
[2KStep 0 - TTA Loss: 1.0958â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 112/203 [2m0:04:57 â€¢ 0:06:11[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6458, 0.3542]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 112/203 [2m0:04:57 â€¢ 0:06:11[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6574, 0.3426]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 112/203 [2m0:04:57 â€¢ 0:06:11[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6673, 0.3327]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 112/203 [2m0:04:57 â€¢ 0:06:11[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6756, 0.3244]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 112/203 [2m0:04:57 â€¢ 0:06:11[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6820, 0.3180]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 112/203 [2m0:04:57 â€¢ 0:06:11[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6866, 0.3134]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 112/203 [2m0:04:57 â€¢ 0:06:11[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6894, 0.3106]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 112/203 [2m0:04:57 â€¢ 0:06:11[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6910, 0.3090]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 112/203 [2m0:04:57 â€¢ 0:06:11[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6917, 0.3083]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 112/203 [2m0:04:57 â€¢ 0:06:11[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.5630, 0.4370]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 112/203 [2m0:04:57 â€¢ 0:06:11[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6859, 0.3141],0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 112/203 [2m0:04:57 â€¢ 0:06:11[0m [2;4m0.25it/s[0m  
        [0.6218, 0.3782],
        [0.6273, 0.3727],
        [0.7018, 0.2982],
[2K        [0.6920, 0.3080]], device='cuda:0')[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 112/203 [2m0:04:57 â€¢ 0:06:11[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.5632, 0.4368],â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 113/203 [2m0:05:00 â€¢ 0:05:43[0m [2;4m0.26it/s[0m  
        [0.5603, 0.4397],
        [0.5590, 0.4410],
        [0.5622, 0.4378],
[2K        [0.5603, 0.4397]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 113/203 [2m0:05:00 â€¢ 0:05:43[0m [2;4m0.26it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 113/203 [2m0:05:00 â€¢ 0:05:43[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.6306, 0.3694]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 113/203 [2m0:05:00 â€¢ 0:05:43[0m [2;4m0.26it/s[0m  
[2KStep 0 - TTA Loss: 0.6145â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 113/203 [2m0:05:00 â€¢ 0:05:43[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.6306, 0.3694]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 113/203 [2m0:05:00 â€¢ 0:05:43[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.6308, 0.3692]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 113/203 [2m0:05:00 â€¢ 0:05:43[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.6318, 0.3682]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 113/203 [2m0:05:00 â€¢ 0:05:43[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.6341, 0.3659]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 113/203 [2m0:05:00 â€¢ 0:05:43[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.6382, 0.3618]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 113/203 [2m0:05:00 â€¢ 0:05:43[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.6445, 0.3555]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 113/203 [2m0:05:00 â€¢ 0:05:43[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.6530, 0.3470]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 113/203 [2m0:05:00 â€¢ 0:05:43[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.6641, 0.3359]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 113/203 [2m0:05:00 â€¢ 0:05:43[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.6769, 0.3231]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 113/203 [2m0:05:00 â€¢ 0:05:43[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.5632, 0.4368]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 113/203 [2m0:05:00 â€¢ 0:05:43[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.6863, 0.3137],0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 113/203 [2m0:05:00 â€¢ 0:05:43[0m [2;4m0.26it/s[0m  
        [0.6187, 0.3813],
        [0.6195, 0.3805],
        [0.6984, 0.3016],
[2K        [0.6914, 0.3086]], device='cuda:0')[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 113/203 [2m0:05:00 â€¢ 0:05:43[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.5633, 0.4367],â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 114/203 [2m0:05:04 â€¢ 0:05:51[0m [2;4m0.25it/s[0m  
        [0.5604, 0.4396],
        [0.5590, 0.4410],
        [0.5623, 0.4377],
[2K        [0.5604, 0.4396]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 114/203 [2m0:05:04 â€¢ 0:05:51[0m [2;4m0.25it/s[0m  
[2KClass label: HammerThrowâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 114/203 [2m0:05:04 â€¢ 0:05:51[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.5636, 0.4364]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 114/203 [2m0:05:04 â€¢ 0:05:51[0m [2;4m0.25it/s[0m  
[2KStep 0 - TTA Loss: 0.5420â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 114/203 [2m0:05:04 â€¢ 0:05:51[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.5795, 0.4205]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 114/203 [2m0:05:04 â€¢ 0:05:51[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.5974, 0.4026]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 114/203 [2m0:05:04 â€¢ 0:05:51[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6159, 0.3841]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 114/203 [2m0:05:04 â€¢ 0:05:51[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6336, 0.3664]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 114/203 [2m0:05:04 â€¢ 0:05:51[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6493, 0.3507]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 114/203 [2m0:05:04 â€¢ 0:05:51[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6619, 0.3381]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 114/203 [2m0:05:04 â€¢ 0:05:51[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6712, 0.3288]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 114/203 [2m0:05:04 â€¢ 0:05:51[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6771, 0.3229]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 114/203 [2m0:05:04 â€¢ 0:05:51[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6800, 0.3200]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 114/203 [2m0:05:04 â€¢ 0:05:51[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.5651, 0.4349]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 114/203 [2m0:05:04 â€¢ 0:05:51[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.7263, 0.2737],0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 114/203 [2m0:05:04 â€¢ 0:05:51[0m [2;4m0.25it/s[0m  
        [0.6809, 0.3191],
        [0.6550, 0.3450],
        [0.7296, 0.2704],
[2K        [0.7250, 0.2750]], device='cuda:0')[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 114/203 [2m0:05:04 â€¢ 0:05:51[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.5634, 0.4366],â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 115/203 [2m0:05:23 â€¢ 0:12:36[0m [2;4m0.12it/s[0m  
        [0.5606, 0.4394],
        [0.5592, 0.4408],
        [0.5624, 0.4376],
[2K        [0.5607, 0.4393]], device='cuda:0', grad_fn=<SoftmaxBackward0>);5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 115/203 [2m0:05:23 â€¢ 0:12:36[0m [2;4m0.12it/s[0m  
[2KClass label: HammerThrowâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 115/203 [2m0:05:23 â€¢ 0:12:36[0m [2;4m0.12it/s[0m  
[2KAttention weights: tensor([[0.5627, 0.4373]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 115/203 [2m0:05:23 â€¢ 0:12:36[0m [2;4m0.12it/s[0m  
[2KStep 0 - TTA Loss: 0.7905â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 115/203 [2m0:05:23 â€¢ 0:12:36[0m [2;4m0.12it/s[0m  
[2KAttention weights: tensor([[0.5627, 0.4373]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 115/203 [2m0:05:23 â€¢ 0:12:36[0m [2;4m0.12it/s[0m  
[2KAttention weights: tensor([[0.5634, 0.4366]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 115/203 [2m0:05:23 â€¢ 0:12:36[0m [2;4m0.12it/s[0m  
[2KAttention weights: tensor([[0.5662, 0.4338]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 115/203 [2m0:05:23 â€¢ 0:12:36[0m [2;4m0.12it/s[0m  
[2KAttention weights: tensor([[0.5716, 0.4284]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 115/203 [2m0:05:23 â€¢ 0:12:36[0m [2;4m0.12it/s[0m  
[2KAttention weights: tensor([[0.5802, 0.4198]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 115/203 [2m0:05:23 â€¢ 0:12:36[0m [2;4m0.12it/s[0m  
[2KAttention weights: tensor([[0.5917, 0.4083]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 115/203 [2m0:05:23 â€¢ 0:12:36[0m [2;4m0.12it/s[0m  
[2KAttention weights: tensor([[0.6060, 0.3940]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 115/203 [2m0:05:23 â€¢ 0:12:36[0m [2;4m0.12it/s[0m  
[2KAttention weights: tensor([[0.6222, 0.3778]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 115/203 [2m0:05:23 â€¢ 0:12:36[0m [2;4m0.12it/s[0m  
[2KAttention weights: tensor([[0.6397, 0.3603]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 115/203 [2m0:05:23 â€¢ 0:12:36[0m [2;4m0.12it/s[0m  
[2KAttention weights: tensor([[0.5643, 0.4357]], device='cuda:0')â•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 115/203 [2m0:05:23 â€¢ 0:12:36[0m [2;4m0.12it/s[0m  
[2KAttention weights: tensor([[0.7065, 0.2935],0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 115/203 [2m0:05:23 â€¢ 0:12:36[0m [2;4m0.12it/s[0m  
        [0.6571, 0.3429],
        [0.6282, 0.3718],
        [0.7130, 0.2870],
[2K        [0.6918, 0.3082]], device='cuda:0')[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 115/203 [2m0:05:23 â€¢ 0:12:36[0m [2;4m0.12it/s[0m  
[2KAttention weights: tensor([[0.5635, 0.4365],â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 116/203 [2m0:05:24 â€¢ 0:09:59[0m [2;4m0.15it/s[0m  
        [0.5608, 0.4392],
        [0.5593, 0.4407],
        [0.5625, 0.4375],
[2K        [0.5608, 0.4392]], device='cuda:0', grad_fn=<SoftmaxBackward0>);5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 116/203 [2m0:05:24 â€¢ 0:09:59[0m [2;4m0.15it/s[0m  
[2KClass label: HammerThrowâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 116/203 [2m0:05:24 â€¢ 0:09:59[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.5643, 0.4357]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 116/203 [2m0:05:24 â€¢ 0:09:59[0m [2;4m0.15it/s[0m  
[2KStep 0 - TTA Loss: 0.5874â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 116/203 [2m0:05:24 â€¢ 0:09:59[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.5837, 0.4163]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 116/203 [2m0:05:24 â€¢ 0:09:59[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.6026, 0.3974]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 116/203 [2m0:05:24 â€¢ 0:09:59[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.6204, 0.3796]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 116/203 [2m0:05:24 â€¢ 0:09:59[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.6358, 0.3642]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 116/203 [2m0:05:24 â€¢ 0:09:59[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.6487, 0.3513]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 116/203 [2m0:05:24 â€¢ 0:09:59[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.6588, 0.3412]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 116/203 [2m0:05:24 â€¢ 0:09:59[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.6659, 0.3341]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 116/203 [2m0:05:24 â€¢ 0:09:59[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.6702, 0.3298]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 116/203 [2m0:05:24 â€¢ 0:09:59[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.6722, 0.3278]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 116/203 [2m0:05:24 â€¢ 0:09:59[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.5652, 0.4348]], device='cuda:0')â•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 116/203 [2m0:05:24 â€¢ 0:09:59[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.7150, 0.2850],0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 116/203 [2m0:05:24 â€¢ 0:09:59[0m [2;4m0.15it/s[0m  
        [0.6728, 0.3272],
        [0.6412, 0.3588],
        [0.7230, 0.2770],
[2K        [0.7013, 0.2987]], device='cuda:0')[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 116/203 [2m0:05:24 â€¢ 0:09:59[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.5637, 0.4363],â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 117/203 [2m0:05:27 â€¢ 0:08:31[0m [2;4m0.17it/s[0m   
        [0.5611, 0.4389],
        [0.5595, 0.4405],
        [0.5626, 0.4374],
[2K        [0.5610, 0.4390]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 117/203 [2m0:05:27 â€¢ 0:08:31[0m [2;4m0.17it/s[0m  
[2KClass label: HammerThrowâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 117/203 [2m0:05:27 â€¢ 0:08:31[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.5656, 0.4344]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 117/203 [2m0:05:27 â€¢ 0:08:31[0m [2;4m0.17it/s[0m  
[2KStep 0 - TTA Loss: 0.6880â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 117/203 [2m0:05:27 â€¢ 0:08:31[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.5656, 0.4344]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 117/203 [2m0:05:27 â€¢ 0:08:31[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.5661, 0.4339]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 117/203 [2m0:05:27 â€¢ 0:08:31[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.5684, 0.4316]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 117/203 [2m0:05:27 â€¢ 0:08:31[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.5733, 0.4267]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 117/203 [2m0:05:27 â€¢ 0:08:31[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.5812, 0.4188]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 117/203 [2m0:05:27 â€¢ 0:08:31[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.5926, 0.4074]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 117/203 [2m0:05:27 â€¢ 0:08:31[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.6078, 0.3922]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 117/203 [2m0:05:27 â€¢ 0:08:31[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.6261, 0.3739]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 117/203 [2m0:05:27 â€¢ 0:08:31[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.6464, 0.3536]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 117/203 [2m0:05:27 â€¢ 0:08:31[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.5651, 0.4349]], device='cuda:0')m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 117/203 [2m0:05:27 â€¢ 0:08:31[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.7136, 0.2864],[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 117/203 [2m0:05:27 â€¢ 0:08:31[0m [2;4m0.17it/s[0m  
        [0.6678, 0.3322],
        [0.6355, 0.3645],
        [0.7246, 0.2754],
[2K        [0.6965, 0.3035]], device='cuda:0')[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 117/203 [2m0:05:27 â€¢ 0:08:31[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.5638, 0.4362],â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 118/203 [2m0:05:29 â€¢ 0:08:24[0m [2;4m0.17it/s[0m  
        [0.5612, 0.4388],
        [0.5596, 0.4404],
        [0.5627, 0.4373],
[2K        [0.5611, 0.4389]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 118/203 [2m0:05:29 â€¢ 0:08:24[0m [2;4m0.17it/s[0m  
[2KClass label: SoccerPenaltyâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 118/203 [2m0:05:29 â€¢ 0:08:24[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.6704, 0.3296]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 118/203 [2m0:05:29 â€¢ 0:08:24[0m [2;4m0.17it/s[0m  
[2KStep 0 - TTA Loss: 1.1002â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 118/203 [2m0:05:29 â€¢ 0:08:24[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.6867, 0.3133]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 118/203 [2m0:05:29 â€¢ 0:08:24[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.7055, 0.2945]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 118/203 [2m0:05:29 â€¢ 0:08:24[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.7251, 0.2749]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 118/203 [2m0:05:29 â€¢ 0:08:24[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.7436, 0.2564]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 118/203 [2m0:05:29 â€¢ 0:08:24[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.7598, 0.2402]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 118/203 [2m0:05:29 â€¢ 0:08:24[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.7725, 0.2275]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 118/203 [2m0:05:29 â€¢ 0:08:24[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.7815, 0.2185]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 118/203 [2m0:05:29 â€¢ 0:08:24[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.7870, 0.2130]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 118/203 [2m0:05:29 â€¢ 0:08:24[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.7895, 0.2105]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 118/203 [2m0:05:29 â€¢ 0:08:24[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.5684, 0.4316]], device='cuda:0')m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 118/203 [2m0:05:29 â€¢ 0:08:24[0m [2;4m0.17it/s[0m  
Testing [38;2;98;6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 118/203 [2m0:05:29 â€¢ 0:08:24[0m [2;4m0.17it/s[0m
