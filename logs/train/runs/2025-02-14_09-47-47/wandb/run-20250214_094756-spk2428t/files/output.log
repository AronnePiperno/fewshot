[[36m2025-02-14 09:47:57,700[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
You are using a CUDA device ('NVIDIA GeForce RTX 4070 SUPER') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
No of videos in train is 214
Loading train Video Information ...
No of class 10
No of videos in validation is 203
Loading validation Video Information ...
No of class 10
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”“
â”ƒ[1;35m [0m[1;35m   [0m[1;35m [0mâ”ƒ[1;35m [0m[1;35mName                                                   [0m[1;35m [0mâ”ƒ[1;35m [0m[1;35mType                           [0m[1;35m [0mâ”ƒ[1;35m [0m[1;35mParams[0m[1;35m [0mâ”ƒ[1;35m [0m[1;35mMode [0m[1;35m [0mâ”ƒ
â”¡â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”©
â”‚[2m [0m[2m0  [0m[2m [0mâ”‚ net                                                     â”‚ T3ALNet                         â”‚  639 M â”‚ train â”‚
â”‚[2m [0m[2m1  [0m[2m [0mâ”‚ net.model                                               â”‚ CoCa                            â”‚  638 M â”‚ train â”‚
â”‚[2m [0m[2m2  [0m[2m [0mâ”‚ net.model.text                                          â”‚ TextTransformer                 â”‚  123 M â”‚ train â”‚
â”‚[2m [0m[2m3  [0m[2m [0mâ”‚ net.model.text.token_embedding                          â”‚ Embedding                       â”‚ 37.9 M â”‚ train â”‚
â”‚[2m [0m[2m4  [0m[2m [0mâ”‚ net.model.text.transformer                              â”‚ Transformer                     â”‚ 85.1 M â”‚ train â”‚
â”‚[2m [0m[2m5  [0m[2m [0mâ”‚ net.model.text.transformer.resblocks                    â”‚ ModuleList                      â”‚ 85.1 M â”‚ train â”‚
â”‚[2m [0m[2m6  [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m7  [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m8  [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m9  [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m10 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m11 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m12 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m13 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m14 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m15 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m16 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m17 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m18 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m19 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m20 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m21 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m22 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m23 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m24 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m25 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m26 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m27 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m28 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m29 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m30 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m31 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m32 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m33 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m34 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m35 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m36 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m37 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m38 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m39 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m40 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m41 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m42 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m43 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m44 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m45 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m46 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m47 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m48 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m49 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m50 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m51 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m52 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m53 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m54 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m55 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m56 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m57 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m58 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m59 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m60 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m61 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m62 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m63 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m64 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m65 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m66 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m67 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m68 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m69 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m70 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m71 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m72 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m73 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m74 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m75 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m76 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m77 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m78 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m79 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m80 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m81 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m82 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m83 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m84 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m85 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m86 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m87 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m88 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m89 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m90 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m91 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m92 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m93 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m94 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m95 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m96 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m97 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m98 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m99 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m100[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m101[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m102[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m103[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m104[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m105[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m106[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m107[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m108[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m109[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m110[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m111[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m112[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m113[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m114[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m115[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m116[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10                 â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m117[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.ln_1            â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m118[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.attn            â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m119[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.attn.out_proj   â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m120[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.ls_1            â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m121[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.ln_2            â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m122[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.mlp             â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m123[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.mlp.c_fc        â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m124[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.mlp.gelu        â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m125[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.mlp.c_proj      â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m126[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.ls_2            â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m127[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11                 â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m128[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.ln_1            â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m129[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.attn            â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m130[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.attn.out_proj   â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m131[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.ls_1            â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m132[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.ln_2            â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m133[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.mlp             â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m134[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.mlp.c_fc        â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m135[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.mlp.gelu        â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m136[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.mlp.c_proj      â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m137[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.ls_2            â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m138[0m[2m [0mâ”‚ net.model.text.ln_final                                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m139[0m[2m [0mâ”‚ net.model.visual                                        â”‚ VisionTransformer               â”‚  306 M â”‚ train â”‚
â”‚[2m [0m[2m140[0m[2m [0mâ”‚ net.model.visual.conv1                                  â”‚ Conv2d                          â”‚  602 K â”‚ train â”‚
â”‚[2m [0m[2m141[0m[2m [0mâ”‚ net.model.visual.patch_dropout                          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m142[0m[2m [0mâ”‚ net.model.visual.ln_pre                                 â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m143[0m[2m [0mâ”‚ net.model.visual.transformer                            â”‚ Transformer                     â”‚  302 M â”‚ train â”‚
â”‚[2m [0m[2m144[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks                  â”‚ ModuleList                      â”‚  302 M â”‚ train â”‚
â”‚[2m [0m[2m145[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m146[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m147[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m148[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m149[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m150[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m151[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m152[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m153[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m154[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m155[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m156[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m157[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m158[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m159[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m160[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m161[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m162[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m163[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m164[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m165[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m166[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m167[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m168[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m169[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m170[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m171[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m172[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m173[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m174[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m175[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m176[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m177[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m178[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m179[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m180[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m181[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m182[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m183[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m184[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m185[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m186[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m187[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m188[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m189[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m190[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m191[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m192[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m193[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m194[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m195[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m196[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m197[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m198[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m199[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m200[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m201[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m202[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m203[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m204[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m205[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m206[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m207[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m208[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m209[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m210[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m211[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m212[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m213[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m214[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m215[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m216[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m217[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m218[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m219[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m220[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m221[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m222[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m223[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m224[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m225[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m226[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m227[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m228[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m229[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m230[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m231[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m232[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m233[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m234[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m235[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m236[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m237[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m238[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m239[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m240[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m241[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m242[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m243[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m244[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m245[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m246[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m247[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m248[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m249[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m250[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m251[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m252[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m253[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m254[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m255[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m256[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m257[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m258[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m259[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m260[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m261[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m262[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m263[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m264[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m265[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m266[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m267[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m268[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m269[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m270[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m271[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m272[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m273[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m274[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m275[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m276[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m277[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m278[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m279[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m280[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m281[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m282[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m283[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m284[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m285[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m286[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m287[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m288[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m289[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m290[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m291[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m292[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m293[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m294[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m295[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m296[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m297[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m298[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m299[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m300[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m301[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m302[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m303[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m304[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m305[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m306[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m307[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m308[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m309[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m310[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m311[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m312[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m313[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m314[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m315[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m316[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m317[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m318[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m319[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m320[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m321[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m322[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m323[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m324[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m325[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m326[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m327[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m328[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m329[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m330[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m331[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m332[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m333[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m334[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m335[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m336[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m337[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m338[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m339[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m340[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m341[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m342[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m343[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m344[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m345[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m346[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m347[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m348[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m349[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m350[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m351[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m352[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m353[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m354[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m355[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m356[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m357[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m358[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m359[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m360[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m361[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m362[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m363[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m364[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m365[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m366[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m367[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m368[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m369[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m370[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m371[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m372[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m373[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m374[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m375[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m376[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m377[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m378[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m379[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m380[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m381[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m382[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m383[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m384[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m385[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m386[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m387[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m388[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m389[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m390[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m391[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m392[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m393[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m394[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m395[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m396[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m397[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m398[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m399[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m400[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m401[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m402[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m403[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m404[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m405[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m406[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m407[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m408[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m409[0m[2m [0mâ”‚ net.model.visual.attn_pool                              â”‚ AttentionalPooler               â”‚  3.0 M â”‚ train â”‚
â”‚[2m [0m[2m410[0m[2m [0mâ”‚ net.model.visual.attn_pool.attn                         â”‚ MultiheadAttention              â”‚  2.8 M â”‚ train â”‚
â”‚[2m [0m[2m411[0m[2m [0mâ”‚ net.model.visual.attn_pool.attn.out_proj                â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m412[0m[2m [0mâ”‚ net.model.visual.attn_pool.ln_q                         â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m413[0m[2m [0mâ”‚ net.model.visual.attn_pool.ln_k                         â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m414[0m[2m [0mâ”‚ net.model.visual.ln_post                                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m415[0m[2m [0mâ”‚ net.model.text_decoder                                  â”‚ MultimodalTransformer           â”‚  208 M â”‚ train â”‚
â”‚[2m [0m[2m416[0m[2m [0mâ”‚ net.model.text_decoder.resblocks                        â”‚ ModuleList                      â”‚ 85.1 M â”‚ train â”‚
â”‚[2m [0m[2m417[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m418[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m419[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m420[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m421[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m422[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m423[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m424[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m425[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m426[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m427[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m428[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m429[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m430[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m431[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m432[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m433[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m434[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m435[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m436[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m437[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m438[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m439[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m440[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m441[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m442[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m443[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m444[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m445[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m446[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m447[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m448[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m449[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m450[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m451[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m452[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m453[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m454[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m455[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m456[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m457[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m458[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m459[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m460[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m461[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m462[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m463[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m464[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m465[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m466[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m467[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m468[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m469[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m470[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m471[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m472[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m473[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m474[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m475[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m476[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m477[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m478[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m479[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m480[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m481[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m482[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m483[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m484[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m485[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m486[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m487[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m488[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m489[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m490[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m491[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m492[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m493[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m494[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m495[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m496[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m497[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m498[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m499[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m500[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m501[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m502[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m503[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m504[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m505[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m506[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m507[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m508[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m509[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m510[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m511[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m512[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m513[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m514[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m515[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m516[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m517[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m518[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m519[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m520[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m521[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m522[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m523[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m524[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m525[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m526[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m527[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m528[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m529[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m530[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m531[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m532[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m533[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m534[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m535[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m536[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m537[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m538[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m539[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m540[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m541[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m542[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m543[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m544[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m545[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m546[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m547[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m548[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m549[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn                       â”‚ ModuleList                      â”‚ 85.1 M â”‚ train â”‚
â”‚[2m [0m[2m550[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m551[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m552[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m553[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m554[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m555[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m556[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m557[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m558[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m559[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m560[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m561[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m562[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m563[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m564[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m565[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m566[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m567[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m568[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m569[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m570[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m571[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m572[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m573[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m574[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m575[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m576[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m577[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m578[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m579[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m580[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m581[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m582[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m583[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m584[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m585[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m586[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m587[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m588[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m589[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m590[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m591[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m592[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m593[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m594[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m595[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m596[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m597[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m598[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m599[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m600[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m601[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m602[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m603[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m604[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m605[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m606[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m607[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m608[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m609[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m610[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m611[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m612[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m613[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m614[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m615[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m616[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m617[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m618[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m619[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m620[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m621[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m622[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m623[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m624[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m625[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m626[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m627[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m628[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m629[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m630[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m631[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m632[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m633[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m634[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m635[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m636[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m637[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m638[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m639[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m640[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m641[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m642[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m643[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m644[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m645[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m646[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m647[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m648[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m649[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m650[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m651[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m652[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m653[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m654[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m655[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m656[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m657[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m658[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m659[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m660[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m661[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m662[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m663[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m664[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m665[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m666[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m667[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m668[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m669[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m670[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10                    â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m671[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.ln_1               â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m672[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.attn               â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m673[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.attn.out_proj      â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m674[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.ls_1               â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m675[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.ln_1_kv            â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m676[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.ln_2               â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m677[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.mlp                â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m678[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.mlp.c_fc           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m679[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.mlp.gelu           â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m680[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.mlp.c_proj         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m681[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.ls_2               â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m682[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11                    â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m683[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.ln_1               â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m684[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.attn               â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m685[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.attn.out_proj      â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m686[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.ls_1               â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m687[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.ln_1_kv            â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m688[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.ln_2               â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m689[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.mlp                â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m690[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.mlp.c_fc           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m691[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.mlp.gelu           â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m692[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.mlp.c_proj         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m693[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.ls_2               â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m694[0m[2m [0mâ”‚ net.model.text_decoder.ln_final                         â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m695[0m[2m [0mâ”‚ net.tta_loss                                            â”‚ ByolLoss                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m696[0m[2m [0mâ”‚ net.video_proj                                          â”‚ VideoProjector                  â”‚  591 K â”‚ train â”‚
â”‚[2m [0m[2m697[0m[2m [0mâ”‚ net.video_proj.transform                                â”‚ Sequential                      â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m698[0m[2m [0mâ”‚ net.video_proj.transform.0                              â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m699[0m[2m [0mâ”‚ net.video_proj.transform.1                              â”‚ Dropout                         â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m700[0m[2m [0mâ”‚ net.fusion                                              â”‚ Fusion                          â”‚  6.2 K â”‚ train â”‚
â”‚[2m [0m[2m701[0m[2m [0mâ”‚ net.fusion.attn                                         â”‚ Sequential                      â”‚  6.2 K â”‚ train â”‚
â”‚[2m [0m[2m702[0m[2m [0mâ”‚ net.fusion.attn.0                                       â”‚ Linear                          â”‚  6.1 K â”‚ train â”‚
â”‚[2m [0m[2m703[0m[2m [0mâ”‚ net.fusion.attn.1                                       â”‚ Linear                          â”‚     10 â”‚ train â”‚
â”‚[2m [0m[2m704[0m[2m [0mâ”‚ net.fusion.attn.2                                       â”‚ Softmax                         â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m705[0m[2m [0mâ”‚ binary_acc                                              â”‚ BinaryAccuracy                  â”‚      0 â”‚ train â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜
[1mTrainable params[0m: 1.2 M
[1mNon-trainable params[0m: 637 M
[1mTotal params[0m: 639 M
[1mTotal estimated model params size (MB)[0m: 2.6 K
[1mModules in train mode[0m: 706
[1mModules in eval mode[0m: 0
/home/def/miniforge3/envs/fewshot/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.
`Trainer.fit` stopped: `max_epochs=0` reached.
[[36m2025-02-14 09:47:59,435[0m][[34m__main__[0m][[32mINFO[0m] - Starting testing![0m
[[36m2025-02-14 09:47:59,436[0m][[34m__main__[0m][[33mWARNING[0m] - Best ckpt not found! Using current weights for testing...[0m
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/def/miniforge3/envs/fewshot/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.
[2KStart testing...
[2KAttention weights: tensor([[0.5592, 0.4408],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 0/203 [2m0:00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
        [0.5533, 0.4467],
        [0.5549, 0.4451],
        [0.5579, 0.4421],
        [0.5551, 0.4449],
        [0.5537, 0.4463],
        [0.5531, 0.4469],
        [0.5589, 0.4411],
        [0.5579, 0.4421],
[2K        [0.5552, 0.4448]], device='cuda:0', grad_fn=<SoftmaxBackward0>):00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2K/home/def/fewshot/src/models/components/tt_method.py:317: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or
`x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3683.)
  dot_product = (x @ y.T)
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 0/203 [2m0:00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.6016, 0.3984]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KStep 0 - TTA Loss: 0.6512â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 0/203 [2m0:00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.6288, 0.3712]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.6030, 0.3970]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5452, 0.4548]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5070, 0.4930]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5305, 0.4695]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5615, 0.4385]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5880, 0.4120]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.6045, 0.3955]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.6127, 0.3873]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5533, 0.4467]], device='cuda:0')203 [2m0:00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.6485, 0.3515],â”â”â”â”â”â”â”â”â”â”â”[0m 0/203 [2m0:00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
        [0.5694, 0.4306],
        [0.5773, 0.4227],
        [0.6571, 0.3429],
        [0.5706, 0.4294],
        [0.6149, 0.3851],
        [0.5756, 0.4244],
        [0.6766, 0.3234],
        [0.6516, 0.3484],
[2K        [0.6365, 0.3635]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”[0m 0/203 [2m0:00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5592, 0.4408],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 1/203 [2m0:00:02 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
        [0.5533, 0.4467],
        [0.5549, 0.4451],
        [0.5579, 0.4421],
        [0.5551, 0.4449],
        [0.5537, 0.4463],
        [0.5531, 0.4469],
        [0.5589, 0.4411],
        [0.5579, 0.4421],
[2K        [0.5552, 0.4448]], device='cuda:0', grad_fn=<SoftmaxBackward0>):00:02 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KClass label: HammerThrowâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 1/203 [2m0:00:02 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5559, 0.4441]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KStep 0 - TTA Loss: 0.9381â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 1/203 [2m0:00:02 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5559, 0.4441]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5570, 0.4430]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5606, 0.4394]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5666, 0.4334]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5753, 0.4247]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5852, 0.4148]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5934, 0.4066]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5945, 0.4055]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5878, 0.4122]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5552, 0.4448]], device='cuda:0')203 [2m0:00:02 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.6542, 0.3458],â”â”â”â”â”â”â”â”â”â”â”[0m 1/203 [2m0:00:02 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
        [0.5882, 0.4118],
        [0.5974, 0.4026],
        [0.6768, 0.3232],
        [0.5708, 0.4292],
        [0.6721, 0.3279],
        [0.6171, 0.3829],
        [0.7083, 0.2917],
        [0.6780, 0.3220],
[2K        [0.6584, 0.3416]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”[0m 1/203 [2m0:00:02 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5591, 0.4409],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 2/203 [2m0:00:04 â€¢ 0:05:39[0m [2;4m0.59it/s[0m  
        [0.5532, 0.4468],
        [0.5548, 0.4452],
        [0.5578, 0.4422],
        [0.5550, 0.4450],
        [0.5537, 0.4463],
        [0.5531, 0.4469],
        [0.5589, 0.4411],
        [0.5579, 0.4421],
[2K        [0.5551, 0.4449]], device='cuda:0', grad_fn=<SoftmaxBackward0>):00:04 â€¢ 0:05:39[0m [2;4m0.59it/s[0m  
[2KClass label: TennisSwingâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 2/203 [2m0:00:04 â€¢ 0:05:39[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[0.6418, 0.3582]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.59it/s[0m  
[2KStep 0 - TTA Loss: 0.6100â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 2/203 [2m0:00:04 â€¢ 0:05:39[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[0.6322, 0.3678]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[0.6151, 0.3849]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[0.5956, 0.4044]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[0.5743, 0.4257]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[0.5540, 0.4460]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[0.5393, 0.4607]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[0.5305, 0.4695]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[0.5280, 0.4720]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[0.5274, 0.4726]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[0.5540, 0.4460]], device='cuda:0')203 [2m0:00:04 â€¢ 0:05:39[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[0.5361, 0.4639],â”â”â”â”â”â”â”â”â”â”â”[0m 2/203 [2m0:00:04 â€¢ 0:05:39[0m [2;4m0.59it/s[0m  
        [0.4894, 0.5106],
        [0.5100, 0.4900],
        [0.5690, 0.4310],
        [0.4454, 0.5546],
        [0.5454, 0.4546],
        [0.5024, 0.4976],
        [0.6031, 0.3969],
        [0.5273, 0.4727],
[2K        [0.5358, 0.4642]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”[0m 2/203 [2m0:00:04 â€¢ 0:05:39[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[0.5590, 0.4410],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 3/203 [2m0:00:05 â€¢ 0:04:36[0m [2;4m0.73it/s[0m  
        [0.5531, 0.4469],
        [0.5547, 0.4453],
        [0.5577, 0.4423],
        [0.5548, 0.4452],
        [0.5535, 0.4465],
        [0.5529, 0.4471],
        [0.5588, 0.4412],
        [0.5577, 0.4423],
[2K        [0.5550, 0.4450]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”[0m 3/203 [2m0:00:05 â€¢ 0:04:36[0m [2;4m0.73it/s[0m  
[2KClass label: HighJump[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 3/203 [2m0:00:05 â€¢ 0:04:36[0m [2;4m0.73it/s[0m  
[2KAttention weights: tensor([[0.6003, 0.3997]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:36[0m [2;4m0.73it/s[0m  
[2KStep 0 - TTA Loss: 0.6004[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 3/203 [2m0:00:05 â€¢ 0:04:36[0m [2;4m0.73it/s[0m  
[2KAttention weights: tensor([[0.6003, 0.3997]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:36[0m [2;4m0.73it/s[0m  
[2KAttention weights: tensor([[0.6009, 0.3991]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:36[0m [2;4m0.73it/s[0m  
[2KAttention weights: tensor([[0.6043, 0.3957]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:36[0m [2;4m0.73it/s[0m  
[2KAttention weights: tensor([[0.6136, 0.3864]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:36[0m [2;4m0.73it/s[0m  
[2KAttention weights: tensor([[0.6308, 0.3692]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:36[0m [2;4m0.73it/s[0m  
[2KAttention weights: tensor([[0.6573, 0.3427]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:36[0m [2;4m0.73it/s[0m  
[2KAttention weights: tensor([[0.6889, 0.3111]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:36[0m [2;4m0.73it/s[0m  
[2KAttention weights: tensor([[0.7200, 0.2800]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:36[0m [2;4m0.73it/s[0m  
[2KAttention weights: tensor([[0.7419, 0.2581]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:04:36[0m [2;4m0.73it/s[0m  
[2KAttention weights: tensor([[0.5573, 0.4427]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 3/203 [2m0:00:05 â€¢ 0:04:36[0m [2;4m0.73it/s[0m  
[2KAttention weights: tensor([[0.7326, 0.2674],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 3/203 [2m0:00:05 â€¢ 0:04:36[0m [2;4m0.73it/s[0m  
        [0.6405, 0.3595],
        [0.6749, 0.3251],
        [0.7291, 0.2709],
        [0.6803, 0.3197],
        [0.7596, 0.2404],
        [0.7025, 0.2975],
        [0.7748, 0.2252],
        [0.7380, 0.2620],
[2K        [0.7353, 0.2647]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 3/203 [2m0:00:05 â€¢ 0:04:36[0m [2;4m0.73it/s[0m  
[2KAttention weights: tensor([[0.5590, 0.4410],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 4/203 [2m0:00:13 â€¢ 0:12:05[0m [2;4m0.27it/s[0m  
        [0.5531, 0.4469],
        [0.5547, 0.4453],
        [0.5577, 0.4423],
        [0.5548, 0.4452],
        [0.5535, 0.4465],
        [0.5529, 0.4471],
        [0.5588, 0.4412],
        [0.5577, 0.4423],
[2K        [0.5550, 0.4450]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”[0m 4/203 [2m0:00:13 â€¢ 0:12:05[0m [2;4m0.27it/s[0m  
[2KClass label: GolfSwing[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 4/203 [2m0:00:13 â€¢ 0:12:05[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6539, 0.3461]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:13 â€¢ 0:12:05[0m [2;4m0.27it/s[0m  
[2KStep 0 - TTA Loss: 0.8426[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 4/203 [2m0:00:13 â€¢ 0:12:05[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6685, 0.3315]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:13 â€¢ 0:12:05[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6888, 0.3112]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:13 â€¢ 0:12:05[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.7109, 0.2891]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:13 â€¢ 0:12:05[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.7306, 0.2694]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:13 â€¢ 0:12:05[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.7455, 0.2545]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:13 â€¢ 0:12:05[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.7540, 0.2460]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:13 â€¢ 0:12:05[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.7565, 0.2435]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:13 â€¢ 0:12:05[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.7564, 0.2436]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:13 â€¢ 0:12:05[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.7559, 0.2441]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:13 â€¢ 0:12:05[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.5609, 0.4391]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 4/203 [2m0:00:13 â€¢ 0:12:05[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.7259, 0.2741],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 4/203 [2m0:00:13 â€¢ 0:12:05[0m [2;4m0.27it/s[0m  
        [0.6346, 0.3654],
        [0.6473, 0.3527],
        [0.7557, 0.2443],
        [0.6566, 0.3434],
        [0.7160, 0.2840],
        [0.6686, 0.3314],
        [0.7491, 0.2509],
        [0.7314, 0.2686],
[2K        [0.7242, 0.2758]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 4/203 [2m0:00:13 â€¢ 0:12:05[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.5590, 0.4410],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 5/203 [2m0:00:14 â€¢ 0:09:31[0m [2;4m0.35it/s[0m  
        [0.5531, 0.4469],
        [0.5547, 0.4453],
        [0.5577, 0.4423],
        [0.5547, 0.4453],
        [0.5535, 0.4465],
        [0.5529, 0.4471],
        [0.5587, 0.4413],
        [0.5577, 0.4423],
[2K        [0.5549, 0.4451]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”[0m 5/203 [2m0:00:14 â€¢ 0:09:31[0m [2;4m0.35it/s[0m  
[2KClass label: HammerThrowm[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 5/203 [2m0:00:14 â€¢ 0:09:31[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5618, 0.4382]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:14 â€¢ 0:09:31[0m [2;4m0.35it/s[0m  
[2KStep 0 - TTA Loss: 0.5975[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 5/203 [2m0:00:14 â€¢ 0:09:31[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5618, 0.4382]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:14 â€¢ 0:09:31[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5617, 0.4383]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:14 â€¢ 0:09:31[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5613, 0.4387]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:14 â€¢ 0:09:31[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5603, 0.4397]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:14 â€¢ 0:09:31[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5585, 0.4415]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:14 â€¢ 0:09:31[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5524, 0.4476]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:14 â€¢ 0:09:31[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5390, 0.4610]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:14 â€¢ 0:09:31[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5151, 0.4849]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:14 â€¢ 0:09:31[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.4809, 0.5191]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:14 â€¢ 0:09:31[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5523, 0.4477]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 5/203 [2m0:00:14 â€¢ 0:09:31[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5449, 0.4551],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 5/203 [2m0:00:14 â€¢ 0:09:31[0m [2;4m0.35it/s[0m  
        [0.5045, 0.4955],
        [0.5021, 0.4979],
        [0.5586, 0.4414],
        [0.4462, 0.5538],
        [0.5279, 0.4721],
        [0.4921, 0.5079],
        [0.6011, 0.3989],
        [0.5671, 0.4329],
[2K        [0.5375, 0.4625]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 5/203 [2m0:00:14 â€¢ 0:09:31[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5590, 0.4410],m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:16 â€¢ 0:08:46[0m [2;4m0.38it/s[0m  
        [0.5531, 0.4469],
        [0.5547, 0.4453],
        [0.5576, 0.4424],
        [0.5548, 0.4452],
        [0.5535, 0.4465],
        [0.5529, 0.4471],
        [0.5587, 0.4413],
        [0.5577, 0.4423],
[2K        [0.5550, 0.4450]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:16 â€¢ 0:08:46[0m [2;4m0.38it/s[0m  
[2KClass label: HammerThrowm[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:16 â€¢ 0:08:46[0m [2;4m0.38it/s[0m  
[2KAttention weights: tensor([[0.5510, 0.4490]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:16 â€¢ 0:08:46[0m [2;4m0.38it/s[0m  
[2KStep 0 - TTA Loss: 0.6238[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:16 â€¢ 0:08:46[0m [2;4m0.38it/s[0m  
[2KAttention weights: tensor([[0.5175, 0.4825]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:16 â€¢ 0:08:46[0m [2;4m0.38it/s[0m  
[2KAttention weights: tensor([[0.4910, 0.5090]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:16 â€¢ 0:08:46[0m [2;4m0.38it/s[0m  
[2KAttention weights: tensor([[0.4699, 0.5301]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:16 â€¢ 0:08:46[0m [2;4m0.38it/s[0m  
[2KAttention weights: tensor([[0.4510, 0.5490]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:16 â€¢ 0:08:46[0m [2;4m0.38it/s[0m  
[2KAttention weights: tensor([[0.4343, 0.5657]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:16 â€¢ 0:08:46[0m [2;4m0.38it/s[0m  
[2KAttention weights: tensor([[0.4208, 0.5792]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:16 â€¢ 0:08:46[0m [2;4m0.38it/s[0m  
[2KAttention weights: tensor([[0.4133, 0.5867]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:16 â€¢ 0:08:46[0m [2;4m0.38it/s[0m  
[2KAttention weights: tensor([[0.4092, 0.5908]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:16 â€¢ 0:08:46[0m [2;4m0.38it/s[0m  
[2KAttention weights: tensor([[0.4080, 0.5920]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:16 â€¢ 0:08:46[0m [2;4m0.38it/s[0m  
[2KAttention weights: tensor([[0.5516, 0.4484]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:16 â€¢ 0:08:46[0m [2;4m0.38it/s[0m  
[2KAttention weights: tensor([[0.5165, 0.4835],8;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:16 â€¢ 0:08:46[0m [2;4m0.38it/s[0m  
        [0.4875, 0.5125],
        [0.4773, 0.5227],
        [0.5492, 0.4508],
        [0.4077, 0.5923],
        [0.5024, 0.4976],
        [0.4672, 0.5328],
        [0.5769, 0.4231],
        [0.5479, 0.4521],
[2K        [0.5131, 0.4869]], device='cuda:0')38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:16 â€¢ 0:08:46[0m [2;4m0.38it/s[0m  
[2KAttention weights: tensor([[0.5590, 0.4410],m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:24 â€¢ 0:11:50[0m [2;4m0.28it/s[0m  
        [0.5531, 0.4469],
        [0.5547, 0.4453],
        [0.5576, 0.4424],
        [0.5548, 0.4452],
        [0.5535, 0.4465],
        [0.5529, 0.4471],
        [0.5587, 0.4413],
        [0.5577, 0.4423],
[2K        [0.5550, 0.4450]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:24 â€¢ 0:11:50[0m [2;4m0.28it/s[0m  
[2KClass label: Billiards[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:24 â€¢ 0:11:50[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.5589, 0.4411]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:24 â€¢ 0:11:50[0m [2;4m0.28it/s[0m  
[2KStep 0 - TTA Loss: 0.4740[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:24 â€¢ 0:11:50[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.5589, 0.4411]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:24 â€¢ 0:11:50[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.5584, 0.4416]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:24 â€¢ 0:11:50[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.5560, 0.4440]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:24 â€¢ 0:11:50[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.5482, 0.4518]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:24 â€¢ 0:11:50[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.5315, 0.4685]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:24 â€¢ 0:11:50[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.5019, 0.4981]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:24 â€¢ 0:11:50[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.4579, 0.5421]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:24 â€¢ 0:11:50[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.4108, 0.5892]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:24 â€¢ 0:11:50[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.3846, 0.6154]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:24 â€¢ 0:11:50[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.5494, 0.4506]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:24 â€¢ 0:11:50[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.5632, 0.4368],8;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:24 â€¢ 0:11:50[0m [2;4m0.28it/s[0m  
        [0.3879, 0.6121],
        [0.5014, 0.4986],
        [0.5758, 0.4242],
        [0.4860, 0.5140],
        [0.5309, 0.4691],
        [0.4947, 0.5053],
        [0.5873, 0.4127],
        [0.5587, 0.4413],
[2K        [0.5584, 0.4416]], device='cuda:0')38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:24 â€¢ 0:11:50[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.5589, 0.4411],mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:26 â€¢ 0:10:59[0m [2;4m0.30it/s[0m  
        [0.5530, 0.4470],
        [0.5547, 0.4453],
        [0.5576, 0.4424],
        [0.5548, 0.4452],
        [0.5535, 0.4465],
        [0.5529, 0.4471],
        [0.5587, 0.4413],
        [0.5576, 0.4424],
[2K        [0.5550, 0.4450]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:26 â€¢ 0:10:59[0m [2;4m0.30it/s[0m  
[2KClass label: BaseballPitch[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:26 â€¢ 0:10:59[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.6381, 0.3619]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:26 â€¢ 0:10:59[0m [2;4m0.30it/s[0m  
[2KStep 0 - TTA Loss: 0.5708[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:26 â€¢ 0:10:59[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.6529, 0.3471]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:26 â€¢ 0:10:59[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.6726, 0.3274]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:26 â€¢ 0:10:59[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.6917, 0.3083]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:26 â€¢ 0:10:59[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.7098, 0.2902]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:26 â€¢ 0:10:59[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.7188, 0.2812]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:26 â€¢ 0:10:59[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.7163, 0.2837]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:26 â€¢ 0:10:59[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.7113, 0.2887]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:26 â€¢ 0:10:59[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.7047, 0.2953]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:26 â€¢ 0:10:59[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.7002, 0.2998]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:26 â€¢ 0:10:59[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.5596, 0.4404]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:26 â€¢ 0:10:59[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.6989, 0.3011],0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:26 â€¢ 0:10:59[0m [2;4m0.30it/s[0m  
        [0.5904, 0.4096],
        [0.5836, 0.4164],
        [0.6756, 0.3244],
        [0.5864, 0.4136],
        [0.6234, 0.3766],
        [0.5845, 0.4155],
        [0.7002, 0.2998],
        [0.6681, 0.3319],
[2K        [0.6485, 0.3515]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:26 â€¢ 0:10:59[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.5589, 0.4411],mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 9/203 [2m0:00:27 â€¢ 0:10:12[0m [2;4m0.32it/s[0m  
        [0.5530, 0.4470],
        [0.5547, 0.4453],
        [0.5576, 0.4424],
        [0.5548, 0.4452],
        [0.5535, 0.4465],
        [0.5529, 0.4471],
        [0.5587, 0.4413],
        [0.5576, 0.4424],
[2K        [0.5550, 0.4450]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 9/203 [2m0:00:27 â€¢ 0:10:12[0m [2;4m0.32it/s[0m  
[2KClass label: BaseballPitch[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 9/203 [2m0:00:27 â€¢ 0:10:12[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.6397, 0.3603]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:27 â€¢ 0:10:12[0m [2;4m0.32it/s[0m  
[2KStep 0 - TTA Loss: 0.5435[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 9/203 [2m0:00:27 â€¢ 0:10:12[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.6397, 0.3603]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:27 â€¢ 0:10:12[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.6388, 0.3612]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:27 â€¢ 0:10:12[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.6364, 0.3636]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:27 â€¢ 0:10:12[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.6318, 0.3682]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:27 â€¢ 0:10:12[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.6248, 0.3752]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:27 â€¢ 0:10:12[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.6146, 0.3854]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:27 â€¢ 0:10:12[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.5985, 0.4015]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:27 â€¢ 0:10:12[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.5689, 0.4311]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:27 â€¢ 0:10:12[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.5334, 0.4666]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:27 â€¢ 0:10:12[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.5546, 0.4454]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 9/203 [2m0:00:27 â€¢ 0:10:12[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.5008, 0.4992],0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 9/203 [2m0:00:27 â€¢ 0:10:12[0m [2;4m0.32it/s[0m  
        [0.5089, 0.4911],
        [0.5321, 0.4679],
        [0.5925, 0.4075],
        [0.4793, 0.5207],
        [0.5402, 0.4598],
        [0.5081, 0.4919],
        [0.5946, 0.4054],
        [0.5781, 0.4219],
[2K        [0.5617, 0.4383]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 9/203 [2m0:00:27 â€¢ 0:10:12[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.5588, 0.4412],mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 10/203 [2m0:00:28 â€¢ 0:09:19[0m [2;4m0.35it/s[0m  
        [0.5530, 0.4470],
        [0.5546, 0.4454],
        [0.5575, 0.4425],
        [0.5547, 0.4453],
        [0.5535, 0.4465],
        [0.5528, 0.4472],
        [0.5586, 0.4414],
        [0.5576, 0.4424],
[2K        [0.5549, 0.4451]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 10/203 [2m0:00:28 â€¢ 0:09:19[0m [2;4m0.35it/s[0m  
[2KClass label: TennisSwingm[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 10/203 [2m0:00:28 â€¢ 0:09:19[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6398, 0.3602]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:28 â€¢ 0:09:19[0m [2;4m0.35it/s[0m  
[2KStep 0 - TTA Loss: 0.5217[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 10/203 [2m0:00:28 â€¢ 0:09:19[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6256, 0.3744]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:28 â€¢ 0:09:19[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5938, 0.4062]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:28 â€¢ 0:09:19[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5549, 0.4451]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:28 â€¢ 0:09:19[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5164, 0.4836]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:28 â€¢ 0:09:19[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.4839, 0.5161]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:28 â€¢ 0:09:19[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.4612, 0.5388]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:28 â€¢ 0:09:19[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.4509, 0.5491]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:28 â€¢ 0:09:19[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.4466, 0.5534]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:28 â€¢ 0:09:19[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.4451, 0.5549]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:28 â€¢ 0:09:19[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5517, 0.4483]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 10/203 [2m0:00:28 â€¢ 0:09:19[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.4388, 0.5612],0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 10/203 [2m0:00:28 â€¢ 0:09:19[0m [2;4m0.35it/s[0m  
        [0.4464, 0.5536],
        [0.4853, 0.5147],
        [0.5101, 0.4899],
        [0.4188, 0.5812],
        [0.4714, 0.5286],
        [0.4489, 0.5511],
        [0.5305, 0.4695],
        [0.4448, 0.5552],
[2K        [0.4827, 0.5173]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 10/203 [2m0:00:28 â€¢ 0:09:19[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5587, 0.4413],0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 11/203 [2m0:00:29 â€¢ 0:08:38[0m [2;4m0.37it/s[0m   
        [0.5529, 0.4471],
        [0.5546, 0.4454],
        [0.5574, 0.4426],
        [0.5546, 0.4454],
        [0.5534, 0.4466],
        [0.5528, 0.4472],
        [0.5586, 0.4414],
        [0.5575, 0.4425],
[2K        [0.5548, 0.4452]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 11/203 [2m0:00:29 â€¢ 0:08:38[0m [2;4m0.37it/s[0m  
[2KClass label: HammerThrow0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 11/203 [2m0:00:29 â€¢ 0:08:38[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5562, 0.4438]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:29 â€¢ 0:08:38[0m [2;4m0.37it/s[0m  
[2KStep 0 - TTA Loss: 0.6141m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 11/203 [2m0:00:29 â€¢ 0:08:38[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5562, 0.4438]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:29 â€¢ 0:08:38[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5563, 0.4437]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:29 â€¢ 0:08:38[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5566, 0.4434]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:29 â€¢ 0:08:38[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5580, 0.4420]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:29 â€¢ 0:08:38[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5602, 0.4398]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:29 â€¢ 0:08:38[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5612, 0.4388]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:29 â€¢ 0:08:38[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5598, 0.4402]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:29 â€¢ 0:08:38[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5523, 0.4477]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:29 â€¢ 0:08:38[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5357, 0.4643]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:29 â€¢ 0:08:38[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5536, 0.4464]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 11/203 [2m0:00:29 â€¢ 0:08:38[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5846, 0.4154],38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 11/203 [2m0:00:29 â€¢ 0:08:38[0m [2;4m0.37it/s[0m  
        [0.5339, 0.4661],
        [0.5462, 0.4538],
        [0.6217, 0.3783],
        [0.5115, 0.4885],
        [0.5676, 0.4324],
        [0.5317, 0.4683],
        [0.6333, 0.3667],
        [0.6040, 0.3960],
[2K        [0.5895, 0.4105]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 11/203 [2m0:00:29 â€¢ 0:08:38[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5586, 0.4414],0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 12/203 [2m0:00:30 â€¢ 0:08:10[0m [2;4m0.39it/s[0m  
        [0.5529, 0.4471],
        [0.5545, 0.4455],
        [0.5574, 0.4426],
        [0.5546, 0.4454],
        [0.5534, 0.4466],
        [0.5528, 0.4472],
        [0.5586, 0.4414],
        [0.5575, 0.4425],
[2K        [0.5548, 0.4452]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 12/203 [2m0:00:30 â€¢ 0:08:10[0m [2;4m0.39it/s[0m  
[2KClass label: CleanAndJerkm[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 12/203 [2m0:00:30 â€¢ 0:08:10[0m [2;4m0.39it/s[0m  
[2KAttention weights: tensor([[0.5756, 0.4244]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:30 â€¢ 0:08:10[0m [2;4m0.39it/s[0m  
[2KStep 0 - TTA Loss: 0.6213m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 12/203 [2m0:00:30 â€¢ 0:08:10[0m [2;4m0.39it/s[0m  
[2KAttention weights: tensor([[0.5771, 0.4229]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:30 â€¢ 0:08:10[0m [2;4m0.39it/s[0m  
[2KAttention weights: tensor([[0.5849, 0.4151]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:30 â€¢ 0:08:10[0m [2;4m0.39it/s[0m  
[2KAttention weights: tensor([[0.5911, 0.4089]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:30 â€¢ 0:08:10[0m [2;4m0.39it/s[0m  
[2KAttention weights: tensor([[0.5881, 0.4119]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:30 â€¢ 0:08:10[0m [2;4m0.39it/s[0m  
[2KAttention weights: tensor([[0.5838, 0.4162]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:30 â€¢ 0:08:10[0m [2;4m0.39it/s[0m  
[2KAttention weights: tensor([[0.5851, 0.4149]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:30 â€¢ 0:08:10[0m [2;4m0.39it/s[0m  
[2KAttention weights: tensor([[0.5922, 0.4078]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:30 â€¢ 0:08:10[0m [2;4m0.39it/s[0m  
[2KAttention weights: tensor([[0.6011, 0.3989]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:30 â€¢ 0:08:10[0m [2;4m0.39it/s[0m  
[2KAttention weights: tensor([[0.6068, 0.3932]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:30 â€¢ 0:08:10[0m [2;4m0.39it/s[0m  
[2KAttention weights: tensor([[0.5551, 0.4449]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 12/203 [2m0:00:30 â€¢ 0:08:10[0m [2;4m0.39it/s[0m  
[2KAttention weights: tensor([[0.5815, 0.4185],38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 12/203 [2m0:00:30 â€¢ 0:08:10[0m [2;4m0.39it/s[0m  
        [0.5470, 0.4530],
        [0.6094, 0.3906],
        [0.6233, 0.3767],
        [0.5074, 0.4926],
        [0.5754, 0.4246],
        [0.5384, 0.4616],
        [0.6337, 0.3663],
        [0.6099, 0.3901],
[2K        [0.5890, 0.4110]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 12/203 [2m0:00:30 â€¢ 0:08:10[0m [2;4m0.39it/s[0m  
[2KAttention weights: tensor([[0.5586, 0.4414],4mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 13/203 [2m0:00:31 â€¢ 0:07:39[0m [2;4m0.41it/s[0m  
        [0.5529, 0.4471],
        [0.5545, 0.4455],
        [0.5574, 0.4426],
        [0.5546, 0.4454],
        [0.5534, 0.4466],
        [0.5527, 0.4473],
        [0.5585, 0.4415],
        [0.5574, 0.4426],
[2K        [0.5548, 0.4452]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 13/203 [2m0:00:31 â€¢ 0:07:39[0m [2;4m0.41it/s[0m  
[2KClass label: BaseballPitch[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 13/203 [2m0:00:31 â€¢ 0:07:39[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.6383, 0.3617]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:31 â€¢ 0:07:39[0m [2;4m0.41it/s[0m  
[2KStep 0 - TTA Loss: 0.6230m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 13/203 [2m0:00:31 â€¢ 0:07:39[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.6383, 0.3617]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:31 â€¢ 0:07:39[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.6405, 0.3595]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:31 â€¢ 0:07:39[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.6519, 0.3481]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:31 â€¢ 0:07:39[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.6796, 0.3204]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:31 â€¢ 0:07:39[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.7282, 0.2718]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:31 â€¢ 0:07:39[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.7902, 0.2098]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:31 â€¢ 0:07:39[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.8493, 0.1507]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:31 â€¢ 0:07:39[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.8988, 0.1012]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:31 â€¢ 0:07:39[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.9470, 0.0530]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:31 â€¢ 0:07:39[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.5748, 0.4252]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 13/203 [2m0:00:31 â€¢ 0:07:39[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.9871, 0.0129],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 13/203 [2m0:00:31 â€¢ 0:07:39[0m [2;4m0.41it/s[0m  
        [0.8624, 0.1376],
        [0.8992, 0.1008],
        [0.9281, 0.0719],
        [0.9344, 0.0656],
        [0.9153, 0.0847],
        [0.8860, 0.1140],
        [0.9500, 0.0500],
        [0.9256, 0.0744],
[2K        [0.9257, 0.0743]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 13/203 [2m0:00:31 â€¢ 0:07:39[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.5588, 0.4412],4mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:33 â€¢ 0:07:31[0m [2;4m0.42it/s[0m  
        [0.5530, 0.4470],
        [0.5547, 0.4453],
        [0.5575, 0.4425],
        [0.5547, 0.4453],
        [0.5535, 0.4465],
        [0.5528, 0.4472],
        [0.5586, 0.4414],
        [0.5575, 0.4425],
[2K        [0.5549, 0.4451]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:33 â€¢ 0:07:31[0m [2;4m0.42it/s[0m  
[2KClass label: GolfSwing[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:33 â€¢ 0:07:31[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.6488, 0.3512]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:33 â€¢ 0:07:31[0m [2;4m0.42it/s[0m  
[2KStep 0 - TTA Loss: 0.8477m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:33 â€¢ 0:07:31[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.7765, 0.2235]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:33 â€¢ 0:07:31[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.8541, 0.1459]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:33 â€¢ 0:07:31[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.8972, 0.1028]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:33 â€¢ 0:07:31[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.9197, 0.0803]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:33 â€¢ 0:07:31[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.9283, 0.0717]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:33 â€¢ 0:07:31[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.9285, 0.0715]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:33 â€¢ 0:07:31[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.9250, 0.0750]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:33 â€¢ 0:07:31[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.9202, 0.0798]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:33 â€¢ 0:07:31[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.9168, 0.0832]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:33 â€¢ 0:07:31[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.5702, 0.4298]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:33 â€¢ 0:07:31[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.9971, 0.0029],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:33 â€¢ 0:07:31[0m [2;4m0.42it/s[0m  
        [0.8872, 0.1128],
        [0.8937, 0.1063],
        [0.9157, 0.0843],
        [0.9637, 0.0363],
        [0.9394, 0.0606],
        [0.9123, 0.0877],
        [0.9745, 0.0255],
        [0.9431, 0.0569],
[2K        [0.9435, 0.0565]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:33 â€¢ 0:07:31[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.5590, 0.4410],4mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 15/203 [2m0:00:33 â€¢ 0:07:07[0m [2;4m0.44it/s[0m  
        [0.5531, 0.4469],
        [0.5548, 0.4452],
        [0.5577, 0.4423],
        [0.5549, 0.4451],
        [0.5536, 0.4464],
        [0.5530, 0.4470],
        [0.5588, 0.4412],
        [0.5577, 0.4423],
[2K        [0.5551, 0.4449]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 15/203 [2m0:00:33 â€¢ 0:07:07[0m [2;4m0.44it/s[0m  
[2KClass label: HighJumpâ”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 15/203 [2m0:00:33 â€¢ 0:07:07[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.6014, 0.3986]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:33 â€¢ 0:07:07[0m [2;4m0.44it/s[0m  
[2KStep 0 - TTA Loss: 1.0997m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 15/203 [2m0:00:33 â€¢ 0:07:07[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.6014, 0.3986]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:33 â€¢ 0:07:07[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.6010, 0.3990]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:33 â€¢ 0:07:07[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.5998, 0.4002]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:33 â€¢ 0:07:07[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.5984, 0.4016]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:33 â€¢ 0:07:07[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.5963, 0.4037]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:33 â€¢ 0:07:07[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.5914, 0.4086]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:33 â€¢ 0:07:07[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.5831, 0.4169]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:33 â€¢ 0:07:07[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.5725, 0.4275]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:33 â€¢ 0:07:07[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.5574, 0.4426]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:33 â€¢ 0:07:07[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.5517, 0.4483]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 15/203 [2m0:00:33 â€¢ 0:07:07[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.6820, 0.3180],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 15/203 [2m0:00:33 â€¢ 0:07:07[0m [2;4m0.44it/s[0m  
        [0.4826, 0.5174],
        [0.5333, 0.4667],
        [0.3471, 0.6529],
        [0.5314, 0.4686],
        [0.5414, 0.4586],
        [0.4955, 0.5045],
        [0.6487, 0.3513],
        [0.5148, 0.4852],
[2K        [0.5076, 0.4924]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 15/203 [2m0:00:33 â€¢ 0:07:07[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.5590, 0.4410],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 16/203 [2m0:00:34 â€¢ 0:06:59[0m [2;4m0.45it/s[0m   
        [0.5531, 0.4469],
        [0.5548, 0.4452],
        [0.5577, 0.4423],
        [0.5549, 0.4451],
        [0.5536, 0.4464],
        [0.5530, 0.4470],
        [0.5588, 0.4412],
        [0.5577, 0.4423],
[2K        [0.5551, 0.4449]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 16/203 [2m0:00:34 â€¢ 0:06:59[0m [2;4m0.45it/s[0m  
[2KClass label: HighJumpâ”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 16/203 [2m0:00:34 â€¢ 0:06:59[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.5967, 0.4033]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:34 â€¢ 0:06:59[0m [2;4m0.45it/s[0m  
[2KStep 0 - TTA Loss: 0.77340m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 16/203 [2m0:00:34 â€¢ 0:06:59[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.5827, 0.4173]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:34 â€¢ 0:06:59[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.5673, 0.4327]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:34 â€¢ 0:06:59[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.5501, 0.4499]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:34 â€¢ 0:06:59[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.5342, 0.4658]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:34 â€¢ 0:06:59[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.5189, 0.4811]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:34 â€¢ 0:06:59[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.5035, 0.4965]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:34 â€¢ 0:06:59[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.4902, 0.5098]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:34 â€¢ 0:06:59[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.4807, 0.5193]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:34 â€¢ 0:06:59[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.4755, 0.5245]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:34 â€¢ 0:06:59[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.5501, 0.4499]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 16/203 [2m0:00:34 â€¢ 0:06:59[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.6200, 0.3800],[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 16/203 [2m0:00:34 â€¢ 0:06:59[0m [2;4m0.45it/s[0m  
        [0.4675, 0.5325],
        [0.4925, 0.5075],
        [0.4021, 0.5979],
        [0.4759, 0.5241],
        [0.4739, 0.5261],
        [0.4479, 0.5521],
        [0.5960, 0.4040],
        [0.5009, 0.4991],
[2K        [0.4842, 0.5158]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 16/203 [2m0:00:34 â€¢ 0:06:59[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.5590, 0.4410],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 17/203 [2m0:00:35 â€¢ 0:05:20[0m [2;4m0.58it/s[0m  
        [0.5531, 0.4469],
        [0.5548, 0.4452],
        [0.5576, 0.4424],
        [0.5548, 0.4452],
        [0.5535, 0.4465],
        [0.5529, 0.4471],
        [0.5588, 0.4412],
        [0.5576, 0.4424],
[2K        [0.5550, 0.4450]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 17/203 [2m0:00:35 â€¢ 0:05:20[0m [2;4m0.58it/s[0m  
[2KClass label: GolfSwingâ”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 17/203 [2m0:00:35 â€¢ 0:05:20[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.6465, 0.3535]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:35 â€¢ 0:05:20[0m [2;4m0.58it/s[0m  
[2KStep 0 - TTA Loss: 0.61040m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 17/203 [2m0:00:35 â€¢ 0:05:20[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.6465, 0.3535]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:35 â€¢ 0:05:20[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.6455, 0.3545]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:35 â€¢ 0:05:20[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.6416, 0.3584]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:35 â€¢ 0:05:20[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.6329, 0.3671]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:35 â€¢ 0:05:20[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.6182, 0.3818]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:35 â€¢ 0:05:20[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5957, 0.4043]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:35 â€¢ 0:05:20[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5645, 0.4355]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:35 â€¢ 0:05:20[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5231, 0.4769]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:35 â€¢ 0:05:20[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.4746, 0.5254]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:35 â€¢ 0:05:20[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5507, 0.4493]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 17/203 [2m0:00:35 â€¢ 0:05:20[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5163, 0.4837],[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 17/203 [2m0:00:35 â€¢ 0:05:20[0m [2;4m0.58it/s[0m  
        [0.4398, 0.5602],
        [0.4532, 0.5468],
        [0.4257, 0.5743],
        [0.4143, 0.5857],
        [0.4243, 0.5757],
        [0.4074, 0.5926],
        [0.5347, 0.4653],
        [0.4802, 0.5198],
[2K        [0.4570, 0.5430]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 17/203 [2m0:00:35 â€¢ 0:05:20[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5590, 0.4410],24mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 18/203 [2m0:00:37 â€¢ 0:05:14[0m [2;4m0.59it/s[0m  
        [0.5530, 0.4470],
        [0.5547, 0.4453],
        [0.5575, 0.4425],
        [0.5547, 0.4453],
        [0.5534, 0.4466],
        [0.5528, 0.4472],
        [0.5587, 0.4413],
        [0.5576, 0.4424],
[2K        [0.5549, 0.4451]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 18/203 [2m0:00:37 â€¢ 0:05:14[0m [2;4m0.59it/s[0m  
[2KClass label: ThrowDiscus[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 18/203 [2m0:00:37 â€¢ 0:05:14[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[0.6274, 0.3726]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:37 â€¢ 0:05:14[0m [2;4m0.59it/s[0m  
[2KStep 0 - TTA Loss: 0.48650m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 18/203 [2m0:00:37 â€¢ 0:05:14[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[0.5767, 0.4233]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:37 â€¢ 0:05:14[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[0.5106, 0.4894]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:37 â€¢ 0:05:14[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[0.4411, 0.5589]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:37 â€¢ 0:05:14[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[0.3798, 0.6202]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:37 â€¢ 0:05:14[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[0.3333, 0.6667]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:37 â€¢ 0:05:14[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[0.3033, 0.6967]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:37 â€¢ 0:05:14[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[0.2862, 0.7138]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:37 â€¢ 0:05:14[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[0.2779, 0.7221]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:37 â€¢ 0:05:14[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[0.2745, 0.7255]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:37 â€¢ 0:05:14[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[0.5445, 0.4555]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 18/203 [2m0:00:37 â€¢ 0:05:14[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[0.3909, 0.6091],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 18/203 [2m0:00:37 â€¢ 0:05:14[0m [2;4m0.59it/s[0m  
        [0.3673, 0.6327],
        [0.3675, 0.6325],
        [0.3024, 0.6976],
        [0.2846, 0.7154],
        [0.3130, 0.6870],
        [0.2835, 0.7165],
        [0.4573, 0.5427],
        [0.3421, 0.6579],
[2K        [0.2735, 0.7265]], device='cuda:0')â•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 18/203 [2m0:00:37 â€¢ 0:05:14[0m [2;4m0.59it/s[0m  
[2KAttention weights: tensor([[0.5589, 0.4411],24mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 19/203 [2m0:00:38 â€¢ 0:05:06[0m [2;4m0.60it/s[0m  
        [0.5529, 0.4471],
        [0.5546, 0.4454],
        [0.5574, 0.4426],
        [0.5546, 0.4454],
        [0.5532, 0.4468],
        [0.5527, 0.4473],
        [0.5586, 0.4414],
        [0.5575, 0.4425],
[2K        [0.5548, 0.4452]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 19/203 [2m0:00:38 â€¢ 0:05:06[0m [2;4m0.60it/s[0m  
[2KClass label: SoccerPenaltym[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 19/203 [2m0:00:38 â€¢ 0:05:06[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.6695, 0.3305]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:38 â€¢ 0:05:06[0m [2;4m0.60it/s[0m  
[2KStep 0 - TTA Loss: 0.52950m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 19/203 [2m0:00:38 â€¢ 0:05:06[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.6695, 0.3305]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:38 â€¢ 0:05:06[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.6695, 0.3305]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:38 â€¢ 0:05:06[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.6699, 0.3301]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:38 â€¢ 0:05:06[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.6715, 0.3285]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:38 â€¢ 0:05:06[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.6736, 0.3264]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:38 â€¢ 0:05:06[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.6732, 0.3268]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:38 â€¢ 0:05:06[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.6691, 0.3309]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:38 â€¢ 0:05:06[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.6621, 0.3379]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:38 â€¢ 0:05:06[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.6645, 0.3355]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:38 â€¢ 0:05:06[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.5573, 0.4427]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 19/203 [2m0:00:38 â€¢ 0:05:06[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.6248, 0.3752],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 19/203 [2m0:00:38 â€¢ 0:05:06[0m [2;4m0.60it/s[0m  
        [0.5485, 0.4515],
        [0.5469, 0.4531],
        [0.6062, 0.3938],
        [0.5340, 0.4660],
        [0.5784, 0.4216],
        [0.5327, 0.4673],
        [0.6896, 0.3104],
        [0.6159, 0.3841],
[2K        [0.5804, 0.4196]], device='cuda:0')â•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 19/203 [2m0:00:38 â€¢ 0:05:06[0m [2;4m0.60it/s[0m  
[2KAttention weights: tensor([[0.5588, 0.4412],24mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 20/203 [2m0:00:39 â€¢ 0:04:50[0m [2;4m0.63it/s[0m  
        [0.5529, 0.4471],
        [0.5546, 0.4454],
        [0.5574, 0.4426],
        [0.5546, 0.4454],
        [0.5532, 0.4468],
        [0.5526, 0.4474],
        [0.5585, 0.4415],
        [0.5574, 0.4426],
[2K        [0.5547, 0.4453]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 20/203 [2m0:00:39 â€¢ 0:04:50[0m [2;4m0.63it/s[0m  
[2KClass label: HighJumpâ”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 20/203 [2m0:00:39 â€¢ 0:04:50[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.5993, 0.4007]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:39 â€¢ 0:04:50[0m [2;4m0.63it/s[0m  
[2KStep 0 - TTA Loss: 0.64430m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 20/203 [2m0:00:39 â€¢ 0:04:50[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6215, 0.3785]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:39 â€¢ 0:04:50[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6523, 0.3477]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:39 â€¢ 0:04:50[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6846, 0.3154]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:39 â€¢ 0:04:50[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.7122, 0.2878]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:39 â€¢ 0:04:50[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.7311, 0.2689]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:39 â€¢ 0:04:50[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.7411, 0.2589]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:39 â€¢ 0:04:50[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.7458, 0.2542]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:39 â€¢ 0:04:50[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.7470, 0.2530]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:39 â€¢ 0:04:50[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.7472, 0.2528]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:39 â€¢ 0:04:50[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.5569, 0.4431]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 20/203 [2m0:00:39 â€¢ 0:04:50[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.7404, 0.2596],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 20/203 [2m0:00:39 â€¢ 0:04:50[0m [2;4m0.63it/s[0m  
        [0.6495, 0.3505],
        [0.6599, 0.3401],
        [0.7116, 0.2884],
        [0.6776, 0.3224],
        [0.7472, 0.2528],
        [0.6811, 0.3189],
        [0.8227, 0.1773],
        [0.7353, 0.2647],
[2K        [0.7092, 0.2908]], device='cuda:0')â•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 20/203 [2m0:00:39 â€¢ 0:04:50[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.5588, 0.4412],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 21/203 [2m0:00:44 â€¢ 0:05:41[0m [2;4m0.53it/s[0m   
        [0.5529, 0.4471],
        [0.5546, 0.4454],
        [0.5574, 0.4426],
        [0.5546, 0.4454],
        [0.5532, 0.4468],
        [0.5527, 0.4473],
        [0.5585, 0.4415],
        [0.5574, 0.4426],
[2K        [0.5548, 0.4452]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 21/203 [2m0:00:44 â€¢ 0:05:41[0m [2;4m0.53it/s[0m  
[2KClass label: ThrowDiscus[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 21/203 [2m0:00:44 â€¢ 0:05:41[0m [2;4m0.53it/s[0m  
[2KAttention weights: tensor([[0.6249, 0.3751]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:44 â€¢ 0:05:41[0m [2;4m0.53it/s[0m  
[2KStep 0 - TTA Loss: 0.8025[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 21/203 [2m0:00:44 â€¢ 0:05:41[0m [2;4m0.53it/s[0m  
[2KAttention weights: tensor([[0.6249, 0.3751]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:44 â€¢ 0:05:41[0m [2;4m0.53it/s[0m  
[2KAttention weights: tensor([[0.6245, 0.3755]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:44 â€¢ 0:05:41[0m [2;4m0.53it/s[0m  
[2KAttention weights: tensor([[0.6227, 0.3773]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:44 â€¢ 0:05:41[0m [2;4m0.53it/s[0m  
[2KAttention weights: tensor([[0.6200, 0.3800]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:44 â€¢ 0:05:41[0m [2;4m0.53it/s[0m  
[2KAttention weights: tensor([[0.6152, 0.3848]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:44 â€¢ 0:05:41[0m [2;4m0.53it/s[0m  
[2KAttention weights: tensor([[0.6087, 0.3913]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:44 â€¢ 0:05:41[0m [2;4m0.53it/s[0m  
[2KAttention weights: tensor([[0.5890, 0.4110]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:44 â€¢ 0:05:41[0m [2;4m0.53it/s[0m  
[2KAttention weights: tensor([[0.5649, 0.4351]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:44 â€¢ 0:05:41[0m [2;4m0.53it/s[0m  
[2KAttention weights: tensor([[0.5374, 0.4626]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:44 â€¢ 0:05:41[0m [2;4m0.53it/s[0m  
[2KAttention weights: tensor([[0.5514, 0.4486]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 21/203 [2m0:00:44 â€¢ 0:05:41[0m [2;4m0.53it/s[0m  
[2KAttention weights: tensor([[0.5712, 0.4288],[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 21/203 [2m0:00:44 â€¢ 0:05:41[0m [2;4m0.53it/s[0m  
        [0.5129, 0.4871],
        [0.5091, 0.4909],
        [0.5706, 0.4294],
        [0.4706, 0.5294],
        [0.5167, 0.4833],
        [0.4751, 0.5249],
        [0.6291, 0.3709],
        [0.5629, 0.4371],
[2K        [0.5141, 0.4859]], device='cuda:0')m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 21/203 [2m0:00:44 â€¢ 0:05:41[0m [2;4m0.53it/s[0m  
[2KAttention weights: tensor([[0.5588, 0.4412],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 22/203 [2m0:00:45 â€¢ 0:05:36[0m [2;4m0.54it/s[0m  
        [0.5529, 0.4471],
        [0.5545, 0.4455],
        [0.5574, 0.4426],
        [0.5545, 0.4455],
        [0.5532, 0.4468],
        [0.5527, 0.4473],
        [0.5585, 0.4415],
        [0.5574, 0.4426],
[2K        [0.5547, 0.4453]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 22/203 [2m0:00:45 â€¢ 0:05:36[0m [2;4m0.54it/s[0m  
[2KClass label: CleanAndJerk[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 22/203 [2m0:00:45 â€¢ 0:05:36[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.5700, 0.4300]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:45 â€¢ 0:05:36[0m [2;4m0.54it/s[0m  
[2KStep 0 - TTA Loss: 0.7560[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 22/203 [2m0:00:45 â€¢ 0:05:36[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.5617, 0.4383]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:45 â€¢ 0:05:36[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.5550, 0.4450]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:45 â€¢ 0:05:36[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.5500, 0.4500]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:45 â€¢ 0:05:36[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.5442, 0.4558]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:45 â€¢ 0:05:36[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.5358, 0.4642]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:45 â€¢ 0:05:36[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.5269, 0.4731]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:45 â€¢ 0:05:36[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.5167, 0.4833]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:45 â€¢ 0:05:36[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.5083, 0.4917]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:45 â€¢ 0:05:36[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.5034, 0.4966]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:45 â€¢ 0:05:36[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.5523, 0.4477]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 22/203 [2m0:00:45 â€¢ 0:05:36[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.5773, 0.4227],[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 22/203 [2m0:00:45 â€¢ 0:05:36[0m [2;4m0.54it/s[0m  
        [0.5143, 0.4857],
        [0.5018, 0.4982],
        [0.5801, 0.4199],
        [0.4786, 0.5214],
        [0.5237, 0.4763],
        [0.4823, 0.5177],
        [0.6280, 0.3720],
        [0.5712, 0.4288],
[2K        [0.5297, 0.4703]], device='cuda:0')m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 22/203 [2m0:00:45 â€¢ 0:05:36[0m [2;4m0.54it/s[0m  
[2KAttention weights: tensor([[0.5588, 0.4412],224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 23/203 [2m0:00:46 â€¢ 0:04:07[0m [2;4m0.73it/s[0m  
        [0.5529, 0.4471],
        [0.5545, 0.4455],
        [0.5573, 0.4427],
        [0.5545, 0.4455],
        [0.5532, 0.4468],
        [0.5526, 0.4474],
        [0.5585, 0.4415],
        [0.5574, 0.4426],
[2K        [0.5546, 0.4454]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 23/203 [2m0:00:46 â€¢ 0:04:07[0m [2;4m0.73it/s[0m  
[2KClass label: TennisSwing[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 23/203 [2m0:00:46 â€¢ 0:04:07[0m [2;4m0.73it/s[0m  
[2KAttention weights: tensor([[0.6390, 0.3610]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:46 â€¢ 0:04:07[0m [2;4m0.73it/s[0m  
[2KStep 0 - TTA Loss: 0.7822[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 23/203 [2m0:00:46 â€¢ 0:04:07[0m [2;4m0.73it/s[0m  
[2KAttention weights: tensor([[0.6390, 0.3610]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:46 â€¢ 0:04:07[0m [2;4m0.73it/s[0m  
[2KAttention weights: tensor([[0.6378, 0.3622]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:46 â€¢ 0:04:07[0m [2;4m0.73it/s[0m  
[2KAttention weights: tensor([[0.6322, 0.3678]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:46 â€¢ 0:04:07[0m [2;4m0.73it/s[0m  
[2KAttention weights: tensor([[0.6180, 0.3820]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:46 â€¢ 0:04:07[0m [2;4m0.73it/s[0m  
[2KAttention weights: tensor([[0.5906, 0.4094]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:46 â€¢ 0:04:07[0m [2;4m0.73it/s[0m  
[2KAttention weights: tensor([[0.5472, 0.4528]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:46 â€¢ 0:04:07[0m [2;4m0.73it/s[0m  
[2KAttention weights: tensor([[0.4824, 0.5176]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:46 â€¢ 0:04:07[0m [2;4m0.73it/s[0m  
[2KAttention weights: tensor([[0.4064, 0.5936]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:46 â€¢ 0:04:07[0m [2;4m0.73it/s[0m  
[2KAttention weights: tensor([[0.3429, 0.6571]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:46 â€¢ 0:04:07[0m [2;4m0.73it/s[0m  
[2KAttention weights: tensor([[0.5491, 0.4509]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 23/203 [2m0:00:46 â€¢ 0:04:07[0m [2;4m0.73it/s[0m  
[2KAttention weights: tensor([[0.4351, 0.5649],â•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 23/203 [2m0:00:46 â€¢ 0:04:07[0m [2;4m0.73it/s[0m  
        [0.3591, 0.6409],
        [0.3381, 0.6619],
        [0.4240, 0.5760],
        [0.3396, 0.6604],
        [0.3713, 0.6287],
        [0.3571, 0.6429],
        [0.4791, 0.5209],
        [0.3128, 0.6872],
[2K        [0.3893, 0.6107]], device='cuda:0')mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 23/203 [2m0:00:46 â€¢ 0:04:07[0m [2;4m0.73it/s[0m  
[2KAttention weights: tensor([[0.5587, 0.4413],224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 24/203 [2m0:00:47 â€¢ 0:04:05[0m [2;4m0.73it/s[0m  
        [0.5528, 0.4472],
        [0.5543, 0.4457],
        [0.5572, 0.4428],
        [0.5544, 0.4456],
        [0.5531, 0.4469],
        [0.5525, 0.4475],
        [0.5584, 0.4416],
        [0.5572, 0.4428],
[2K        [0.5545, 0.4455]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 24/203 [2m0:00:47 â€¢ 0:04:05[0m [2;4m0.73it/s[0m  
[2KClass label: Billiardsâ”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 24/203 [2m0:00:47 â€¢ 0:04:05[0m [2;4m0.73it/s[0m  
[2KAttention weights: tensor([[0.5570, 0.4430]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:47 â€¢ 0:04:05[0m [2;4m0.73it/s[0m  
[2KStep 0 - TTA Loss: 0.9392[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 24/203 [2m0:00:47 â€¢ 0:04:05[0m [2;4m0.73it/s[0m  
[2KAttention weights: tensor([[0.5367, 0.4633]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:47 â€¢ 0:04:05[0m [2;4m0.73it/s[0m  
[2KAttention weights: tensor([[0.5171, 0.4829]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:47 â€¢ 0:04:05[0m [2;4m0.73it/s[0m  
[2KAttention weights: tensor([[0.5001, 0.4999]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:47 â€¢ 0:04:05[0m [2;4m0.73it/s[0m  
[2KAttention weights: tensor([[0.4864, 0.5136]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:47 â€¢ 0:04:05[0m [2;4m0.73it/s[0m  
[2KAttention weights: tensor([[0.4743, 0.5257]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:47 â€¢ 0:04:05[0m [2;4m0.73it/s[0m  
[2KAttention weights: tensor([[0.4658, 0.5342]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:47 â€¢ 0:04:05[0m [2;4m0.73it/s[0m  
[2KAttention weights: tensor([[0.4602, 0.5398]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:47 â€¢ 0:04:05[0m [2;4m0.73it/s[0m  
[2KAttention weights: tensor([[0.4570, 0.5430]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:47 â€¢ 0:04:05[0m [2;4m0.73it/s[0m  
[2KAttention weights: tensor([[0.4557, 0.5443]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:47 â€¢ 0:04:05[0m [2;4m0.73it/s[0m  
[2KAttention weights: tensor([[0.5496, 0.4504]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 24/203 [2m0:00:47 â€¢ 0:04:05[0m [2;4m0.73it/s[0m  
[2KAttention weights: tensor([[0.5521, 0.4479],â•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 24/203 [2m0:00:47 â€¢ 0:04:05[0m [2;4m0.73it/s[0m  
        [0.4553, 0.5447],
        [0.4431, 0.5569],
        [0.5509, 0.4491],
        [0.4540, 0.5460],
        [0.4948, 0.5052],
        [0.4651, 0.5349],
        [0.5842, 0.4158],
        [0.5031, 0.4969],
[2K        [0.5199, 0.4801]], device='cuda:0')mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 24/203 [2m0:00:47 â€¢ 0:04:05[0m [2;4m0.73it/s[0m  
[2KAttention weights: tensor([[0.5586, 0.4414],224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 25/203 [2m0:00:49 â€¢ 0:04:11[0m [2;4m0.71it/s[0m  
        [0.5527, 0.4473],
        [0.5543, 0.4457],
        [0.5572, 0.4428],
        [0.5543, 0.4457],
        [0.5530, 0.4470],
        [0.5524, 0.4476],
        [0.5583, 0.4417],
        [0.5571, 0.4429],
[2K        [0.5544, 0.4456]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 25/203 [2m0:00:49 â€¢ 0:04:11[0m [2;4m0.71it/s[0m  
[2KClass label: GolfSwingâ”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 25/203 [2m0:00:49 â€¢ 0:04:11[0m [2;4m0.71it/s[0m  
[2KAttention weights: tensor([[0.6436, 0.3564]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:49 â€¢ 0:04:11[0m [2;4m0.71it/s[0m  
[2KStep 0 - TTA Loss: 0.6455[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 25/203 [2m0:00:49 â€¢ 0:04:11[0m [2;4m0.71it/s[0m  
[2KAttention weights: tensor([[0.6436, 0.3564]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:49 â€¢ 0:04:11[0m [2;4m0.71it/s[0m  
[2KAttention weights: tensor([[0.6436, 0.3564]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:49 â€¢ 0:04:11[0m [2;4m0.71it/s[0m  
[2KAttention weights: tensor([[0.6440, 0.3560]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:49 â€¢ 0:04:11[0m [2;4m0.71it/s[0m  
[2KAttention weights: tensor([[0.6457, 0.3543]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:49 â€¢ 0:04:11[0m [2;4m0.71it/s[0m  
[2KAttention weights: tensor([[0.6488, 0.3512]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:49 â€¢ 0:04:11[0m [2;4m0.71it/s[0m  
[2KAttention weights: tensor([[0.6520, 0.3480]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:49 â€¢ 0:04:11[0m [2;4m0.71it/s[0m  
[2KAttention weights: tensor([[0.6516, 0.3484]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:49 â€¢ 0:04:11[0m [2;4m0.71it/s[0m  
[2KAttention weights: tensor([[0.6401, 0.3599]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:49 â€¢ 0:04:11[0m [2;4m0.71it/s[0m  
[2KAttention weights: tensor([[0.6101, 0.3899]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:49 â€¢ 0:04:11[0m [2;4m0.71it/s[0m  
[2KAttention weights: tensor([[0.5540, 0.4460]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 25/203 [2m0:00:49 â€¢ 0:04:11[0m [2;4m0.71it/s[0m  
[2KAttention weights: tensor([[0.5813, 0.4187],â•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 25/203 [2m0:00:49 â€¢ 0:04:11[0m [2;4m0.71it/s[0m  
        [0.4972, 0.5028],
        [0.5190, 0.4810],
        [0.5601, 0.4399],
        [0.5036, 0.4964],
        [0.5487, 0.4513],
        [0.5144, 0.4856],
        [0.6206, 0.3794],
        [0.5680, 0.4320],
[2K        [0.5630, 0.4370]], device='cuda:0')mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 25/203 [2m0:00:49 â€¢ 0:04:11[0m [2;4m0.71it/s[0m  
[2KAttention weights: tensor([[0.5586, 0.4414],â•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 26/203 [2m0:00:50 â€¢ 0:04:07[0m [2;4m0.72it/s[0m   
        [0.5527, 0.4473],
        [0.5543, 0.4457],
        [0.5572, 0.4428],
        [0.5543, 0.4457],
        [0.5530, 0.4470],
        [0.5524, 0.4476],
        [0.5583, 0.4417],
        [0.5571, 0.4429],
[2K        [0.5544, 0.4456]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 26/203 [2m0:00:50 â€¢ 0:04:07[0m [2;4m0.72it/s[0m  
[2KClass label: HighJumpâ”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 26/203 [2m0:00:50 â€¢ 0:04:07[0m [2;4m0.72it/s[0m  
[2KAttention weights: tensor([[0.5985, 0.4015]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:50 â€¢ 0:04:07[0m [2;4m0.72it/s[0m  
[2KStep 0 - TTA Loss: 0.7653[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 26/203 [2m0:00:50 â€¢ 0:04:07[0m [2;4m0.72it/s[0m  
[2KAttention weights: tensor([[0.5883, 0.4117]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:50 â€¢ 0:04:07[0m [2;4m0.72it/s[0m  
[2KAttention weights: tensor([[0.5857, 0.4143]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:50 â€¢ 0:04:07[0m [2;4m0.72it/s[0m  
[2KAttention weights: tensor([[0.5820, 0.4180]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:50 â€¢ 0:04:07[0m [2;4m0.72it/s[0m  
[2KAttention weights: tensor([[0.5789, 0.4211]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:50 â€¢ 0:04:07[0m [2;4m0.72it/s[0m  
[2KAttention weights: tensor([[0.5864, 0.4136]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:50 â€¢ 0:04:07[0m [2;4m0.72it/s[0m  
[2KAttention weights: tensor([[0.6016, 0.3984]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:50 â€¢ 0:04:07[0m [2;4m0.72it/s[0m  
[2KAttention weights: tensor([[0.6174, 0.3826]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:50 â€¢ 0:04:07[0m [2;4m0.72it/s[0m  
[2KAttention weights: tensor([[0.6289, 0.3711]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:50 â€¢ 0:04:07[0m [2;4m0.72it/s[0m  
[2KAttention weights: tensor([[0.6348, 0.3652]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:50 â€¢ 0:04:07[0m [2;4m0.72it/s[0m  
[2KAttention weights: tensor([[0.5539, 0.4461]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 26/203 [2m0:00:50 â€¢ 0:04:07[0m [2;4m0.72it/s[0m  
[2KAttention weights: tensor([[0.6068, 0.3932],m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 26/203 [2m0:00:50 â€¢ 0:04:07[0m [2;4m0.72it/s[0m  
        [0.5201, 0.4799],
        [0.5756, 0.4244],
        [0.5185, 0.4815],
        [0.5610, 0.4390],
        [0.6364, 0.3636],
        [0.5800, 0.4200],
        [0.6718, 0.3282],
        [0.5967, 0.4033],
[2K        [0.5971, 0.4029]], device='cuda:0')0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 26/203 [2m0:00:50 â€¢ 0:04:07[0m [2;4m0.72it/s[0m  
[2KAttention weights: tensor([[0.5586, 0.4414],â•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 27/203 [2m0:00:56 â€¢ 0:04:39[0m [2;4m0.63it/s[0m  
        [0.5527, 0.4473],
        [0.5543, 0.4457],
        [0.5572, 0.4428],
        [0.5543, 0.4457],
        [0.5530, 0.4470],
        [0.5525, 0.4475],
        [0.5584, 0.4416],
        [0.5571, 0.4429],
[2K        [0.5544, 0.4456]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 27/203 [2m0:00:56 â€¢ 0:04:39[0m [2;4m0.63it/s[0m  
[2KClass label: GolfSwingâ”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 27/203 [2m0:00:56 â€¢ 0:04:39[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6500, 0.3500]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:56 â€¢ 0:04:39[0m [2;4m0.63it/s[0m  
[2KStep 0 - TTA Loss: 0.6675[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 27/203 [2m0:00:56 â€¢ 0:04:39[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6500, 0.3500]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:56 â€¢ 0:04:39[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6503, 0.3497]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:56 â€¢ 0:04:39[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6508, 0.3492]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:56 â€¢ 0:04:39[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6503, 0.3497]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:56 â€¢ 0:04:39[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6449, 0.3551]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:56 â€¢ 0:04:39[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6300, 0.3700]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:56 â€¢ 0:04:39[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.6046, 0.3954]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:56 â€¢ 0:04:39[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.5658, 0.4342]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:56 â€¢ 0:04:39[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.5137, 0.4863]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:56 â€¢ 0:04:39[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.5518, 0.4482]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 27/203 [2m0:00:56 â€¢ 0:04:39[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.5897, 0.4103],m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 27/203 [2m0:00:56 â€¢ 0:04:39[0m [2;4m0.63it/s[0m  
        [0.5093, 0.4907],
        [0.5742, 0.4258],
        [0.4563, 0.5437],
        [0.5504, 0.4496],
        [0.6353, 0.3647],
        [0.5770, 0.4230],
        [0.6674, 0.3326],
        [0.5745, 0.4255],
[2K        [0.5736, 0.4264]], device='cuda:0')0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 27/203 [2m0:00:56 â€¢ 0:04:39[0m [2;4m0.63it/s[0m  
[2KAttention weights: tensor([[0.5586, 0.4414],;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 28/203 [2m0:00:58 â€¢ 0:04:47[0m [2;4m0.61it/s[0m  
        [0.5527, 0.4473],
        [0.5543, 0.4457],
        [0.5572, 0.4428],
        [0.5543, 0.4457],
        [0.5531, 0.4469],
        [0.5525, 0.4475],
        [0.5584, 0.4416],
        [0.5571, 0.4429],
[2K        [0.5545, 0.4455]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 28/203 [2m0:00:58 â€¢ 0:04:47[0m [2;4m0.61it/s[0m  
[2KClass label: CleanAndJerk[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 28/203 [2m0:00:58 â€¢ 0:04:47[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.5683, 0.4317]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:58 â€¢ 0:04:47[0m [2;4m0.61it/s[0m  
[2KStep 0 - TTA Loss: 0.8285[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 28/203 [2m0:00:58 â€¢ 0:04:47[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.5786, 0.4214]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:58 â€¢ 0:04:47[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.5989, 0.4011]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:58 â€¢ 0:04:47[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.6191, 0.3809]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:58 â€¢ 0:04:47[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.6331, 0.3669]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:58 â€¢ 0:04:47[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.6368, 0.3632]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:58 â€¢ 0:04:47[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.6390, 0.3610]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:58 â€¢ 0:04:47[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.6402, 0.3598]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:58 â€¢ 0:04:47[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.6440, 0.3560]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:58 â€¢ 0:04:47[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.6474, 0.3526]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:58 â€¢ 0:04:47[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.5559, 0.4441]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 28/203 [2m0:00:58 â€¢ 0:04:47[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.5868, 0.4132],mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 28/203 [2m0:00:58 â€¢ 0:04:47[0m [2;4m0.61it/s[0m  
        [0.5245, 0.4755],
        [0.6488, 0.3512],
        [0.4601, 0.5399],
        [0.5602, 0.4398],
        [0.6283, 0.3717],
        [0.5746, 0.4254],
        [0.6569, 0.3431],
        [0.5703, 0.4297],
[2K        [0.5757, 0.4243]], device='cuda:0')4mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 28/203 [2m0:00:58 â€¢ 0:04:47[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.5586, 0.4414],;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 29/203 [2m0:00:59 â€¢ 0:04:44[0m [2;4m0.61it/s[0m  
        [0.5527, 0.4473],
        [0.5543, 0.4457],
        [0.5571, 0.4429],
        [0.5543, 0.4457],
        [0.5531, 0.4469],
        [0.5525, 0.4475],
        [0.5584, 0.4416],
        [0.5571, 0.4429],
[2K        [0.5545, 0.4455]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 29/203 [2m0:00:59 â€¢ 0:04:44[0m [2;4m0.61it/s[0m  
[2KClass label: Billiardsâ”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 29/203 [2m0:00:59 â€¢ 0:04:44[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.5574, 0.4426]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:59 â€¢ 0:04:44[0m [2;4m0.61it/s[0m  
[2KStep 0 - TTA Loss: 0.5627[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 29/203 [2m0:00:59 â€¢ 0:04:44[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.5574, 0.4426]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:59 â€¢ 0:04:44[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.5579, 0.4421]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:59 â€¢ 0:04:44[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.5599, 0.4401]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:59 â€¢ 0:04:44[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.5640, 0.4360]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:59 â€¢ 0:04:44[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.5698, 0.4302]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:59 â€¢ 0:04:44[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.5752, 0.4248]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:59 â€¢ 0:04:44[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.5773, 0.4227]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:59 â€¢ 0:04:44[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.5866, 0.4134]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:59 â€¢ 0:04:44[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.6258, 0.3742]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:59 â€¢ 0:04:44[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.5557, 0.4443]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 29/203 [2m0:00:59 â€¢ 0:04:44[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.6943, 0.3057],mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 29/203 [2m0:00:59 â€¢ 0:04:44[0m [2;4m0.61it/s[0m  
        [0.7063, 0.2937],
        [0.7210, 0.2790],
        [0.6832, 0.3168],
        [0.6435, 0.3565],
        [0.6840, 0.3160],
        [0.6451, 0.3549],
        [0.7330, 0.2670],
        [0.7003, 0.2997],
[2K        [0.6814, 0.3186]], device='cuda:0')4mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 29/203 [2m0:00:59 â€¢ 0:04:44[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.5586, 0.4414],;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 30/203 [2m0:01:00 â€¢ 0:04:45[0m [2;4m0.61it/s[0m  
        [0.5527, 0.4473],
        [0.5543, 0.4457],
        [0.5571, 0.4429],
        [0.5544, 0.4456],
        [0.5531, 0.4469],
        [0.5525, 0.4475],
        [0.5584, 0.4416],
        [0.5571, 0.4429],
[2K        [0.5545, 0.4455]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 30/203 [2m0:01:00 â€¢ 0:04:45[0m [2;4m0.61it/s[0m  
[2KClass label: ThrowDiscusâ”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 30/203 [2m0:01:00 â€¢ 0:04:45[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.6239, 0.3761]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:01:00 â€¢ 0:04:45[0m [2;4m0.61it/s[0m  
[2KStep 0 - TTA Loss: 0.8021[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 30/203 [2m0:01:00 â€¢ 0:04:45[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.6415, 0.3585]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:01:00 â€¢ 0:04:45[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.6471, 0.3529]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:01:00 â€¢ 0:04:45[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.6371, 0.3629]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:01:00 â€¢ 0:04:45[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.6192, 0.3808]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:01:00 â€¢ 0:04:45[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.5969, 0.4031]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:01:00 â€¢ 0:04:45[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.5761, 0.4239]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:01:00 â€¢ 0:04:45[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.5610, 0.4390]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:01:00 â€¢ 0:04:45[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.5513, 0.4487]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:01:00 â€¢ 0:04:45[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.5466, 0.4534]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:01:00 â€¢ 0:04:45[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.5519, 0.4481]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 30/203 [2m0:01:00 â€¢ 0:04:45[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.6309, 0.3691],mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 30/203 [2m0:01:00 â€¢ 0:04:45[0m [2;4m0.61it/s[0m  
        [0.7902, 0.2098],
        [0.6505, 0.3495],
        [0.6209, 0.3791],
        [0.5381, 0.4619],
        [0.5996, 0.4004],
        [0.5492, 0.4508],
        [0.7118, 0.2882],
        [0.6425, 0.3575],
[2K        [0.5454, 0.4546]], device='cuda:0')4mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 30/203 [2m0:01:00 â€¢ 0:04:45[0m [2;4m0.61it/s[0m  
[2KAttention weights: tensor([[0.5586, 0.4414],mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 31/203 [2m0:01:02 â€¢ 0:04:59[0m [2;4m0.58it/s[0m   
        [0.5528, 0.4472],
        [0.5543, 0.4457],
        [0.5571, 0.4429],
        [0.5543, 0.4457],
        [0.5531, 0.4469],
        [0.5525, 0.4475],
        [0.5584, 0.4416],
        [0.5571, 0.4429],
[2K        [0.5544, 0.4456]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 31/203 [2m0:01:02 â€¢ 0:04:59[0m [2;4m0.58it/s[0m  
[2KClass label: CleanAndJerkâ”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 31/203 [2m0:01:02 â€¢ 0:04:59[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5676, 0.4324]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:01:02 â€¢ 0:04:59[0m [2;4m0.58it/s[0m  
[2KStep 0 - TTA Loss: 0.5946â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 31/203 [2m0:01:02 â€¢ 0:04:59[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5676, 0.4324]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:01:02 â€¢ 0:04:59[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5676, 0.4324]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:01:02 â€¢ 0:04:59[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5679, 0.4321]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:01:02 â€¢ 0:04:59[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5692, 0.4308]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:01:02 â€¢ 0:04:59[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5723, 0.4277]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:01:02 â€¢ 0:04:59[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5768, 0.4232]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:01:02 â€¢ 0:04:59[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5782, 0.4218]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:01:02 â€¢ 0:04:59[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5695, 0.4305]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:01:02 â€¢ 0:04:59[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5511, 0.4489]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:01:02 â€¢ 0:04:59[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5532, 0.4468]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 31/203 [2m0:01:02 â€¢ 0:04:59[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5811, 0.4189],0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 31/203 [2m0:01:02 â€¢ 0:04:59[0m [2;4m0.58it/s[0m  
        [0.5739, 0.4261],
        [0.5320, 0.4680],
        [0.5850, 0.4150],
        [0.4823, 0.5177],
        [0.5369, 0.4631],
        [0.4931, 0.5069],
        [0.6381, 0.3619],
        [0.5802, 0.4198],
[2K        [0.5242, 0.4758]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 31/203 [2m0:01:02 â€¢ 0:04:59[0m [2;4m0.58it/s[0m  
[2KAttention weights: tensor([[0.5586, 0.4414],mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 32/203 [2m0:01:04 â€¢ 0:05:29[0m [2;4m0.52it/s[0m  
        [0.5528, 0.4472],
        [0.5543, 0.4457],
        [0.5571, 0.4429],
        [0.5543, 0.4457],
        [0.5531, 0.4469],
        [0.5525, 0.4475],
        [0.5584, 0.4416],
        [0.5571, 0.4429],
[2K        [0.5544, 0.4456]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 32/203 [2m0:01:04 â€¢ 0:05:29[0m [2;4m0.52it/s[0m  
[2KClass label: TennisSwingâ”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 32/203 [2m0:01:04 â€¢ 0:05:29[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.6418, 0.3582]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:01:04 â€¢ 0:05:29[0m [2;4m0.52it/s[0m  
[2KStep 0 - TTA Loss: 0.7210â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 32/203 [2m0:01:04 â€¢ 0:05:29[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.6089, 0.3911]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:01:04 â€¢ 0:05:29[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.5570, 0.4430]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:01:04 â€¢ 0:05:29[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.4894, 0.5106]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:01:04 â€¢ 0:05:29[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.4158, 0.5842]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:01:04 â€¢ 0:05:29[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.3505, 0.6495]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:01:04 â€¢ 0:05:29[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.3055, 0.6945]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:01:04 â€¢ 0:05:29[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.2817, 0.7183]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:01:04 â€¢ 0:05:29[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.2724, 0.7276]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:01:04 â€¢ 0:05:29[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.2702, 0.7298]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:01:04 â€¢ 0:05:29[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.5466, 0.4534]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 32/203 [2m0:01:04 â€¢ 0:05:29[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.4037, 0.5963],0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 32/203 [2m0:01:04 â€¢ 0:05:29[0m [2;4m0.52it/s[0m  
        [0.3900, 0.6100],
        [0.3771, 0.6229],
        [0.3889, 0.6111],
        [0.3147, 0.6853],
        [0.3467, 0.6533],
        [0.3295, 0.6705],
        [0.4575, 0.5425],
        [0.2700, 0.7300],
[2K        [0.3384, 0.6616]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 32/203 [2m0:01:04 â€¢ 0:05:29[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.5585, 0.4415],6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 33/203 [2m0:01:10 â€¢ 0:06:10[0m [2;4m0.46it/s[0m  
        [0.5527, 0.4473],
        [0.5542, 0.4458],
        [0.5570, 0.4430],
        [0.5542, 0.4458],
        [0.5530, 0.4470],
        [0.5524, 0.4476],
        [0.5583, 0.4417],
        [0.5570, 0.4430],
[2K        [0.5543, 0.4457]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 33/203 [2m0:01:10 â€¢ 0:06:10[0m [2;4m0.46it/s[0m  
[2KClass label: CleanAndJerkâ”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 33/203 [2m0:01:10 â€¢ 0:06:10[0m [2;4m0.46it/s[0m  
[2KAttention weights: tensor([[0.5706, 0.4294]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:01:10 â€¢ 0:06:10[0m [2;4m0.46it/s[0m  
[2KStep 0 - TTA Loss: 0.4985â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 33/203 [2m0:01:10 â€¢ 0:06:10[0m [2;4m0.46it/s[0m  
[2KAttention weights: tensor([[0.5706, 0.4294]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:01:10 â€¢ 0:06:10[0m [2;4m0.46it/s[0m  
[2KAttention weights: tensor([[0.5712, 0.4288]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:01:10 â€¢ 0:06:10[0m [2;4m0.46it/s[0m  
[2KAttention weights: tensor([[0.5749, 0.4251]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:01:10 â€¢ 0:06:10[0m [2;4m0.46it/s[0m  
[2KAttention weights: tensor([[0.5847, 0.4153]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:01:10 â€¢ 0:06:10[0m [2;4m0.46it/s[0m  
[2KAttention weights: tensor([[0.6026, 0.3974]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:01:10 â€¢ 0:06:10[0m [2;4m0.46it/s[0m  
[2KAttention weights: tensor([[0.6269, 0.3731]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:01:10 â€¢ 0:06:10[0m [2;4m0.46it/s[0m  
[2KAttention weights: tensor([[0.6527, 0.3473]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:01:10 â€¢ 0:06:10[0m [2;4m0.46it/s[0m  
[2KAttention weights: tensor([[0.6746, 0.3254]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:01:10 â€¢ 0:06:10[0m [2;4m0.46it/s[0m  
[2KAttention weights: tensor([[0.6836, 0.3164]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:01:10 â€¢ 0:06:10[0m [2;4m0.46it/s[0m  
[2KAttention weights: tensor([[0.5566, 0.4434]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 33/203 [2m0:01:10 â€¢ 0:06:10[0m [2;4m0.46it/s[0m  
[2KAttention weights: tensor([[0.6717, 0.3283],4mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 33/203 [2m0:01:10 â€¢ 0:06:10[0m [2;4m0.46it/s[0m  
        [0.6066, 0.3934],
        [0.6988, 0.3012],
        [0.6777, 0.3223],
        [0.6193, 0.3807],
        [0.6540, 0.3460],
        [0.6154, 0.3846],
        [0.6994, 0.3006],
        [0.6755, 0.3245],
[2K        [0.6622, 0.3378]], device='cuda:0')24mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 33/203 [2m0:01:10 â€¢ 0:06:10[0m [2;4m0.46it/s[0m  
[2KAttention weights: tensor([[0.5585, 0.4415],6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 34/203 [2m0:01:13 â€¢ 0:06:17[0m [2;4m0.45it/s[0m  
        [0.5527, 0.4473],
        [0.5542, 0.4458],
        [0.5570, 0.4430],
        [0.5542, 0.4458],
        [0.5530, 0.4470],
        [0.5524, 0.4476],
        [0.5583, 0.4417],
        [0.5569, 0.4431],
[2K        [0.5542, 0.4458]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 34/203 [2m0:01:13 â€¢ 0:06:17[0m [2;4m0.45it/s[0m  
[2KClass label: HammerThrowâ”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 34/203 [2m0:01:13 â€¢ 0:06:17[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.5623, 0.4377]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:01:13 â€¢ 0:06:17[0m [2;4m0.45it/s[0m  
[2KStep 0 - TTA Loss: 0.5471â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 34/203 [2m0:01:13 â€¢ 0:06:17[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.5671, 0.4329]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:01:13 â€¢ 0:06:17[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.5682, 0.4318]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:01:13 â€¢ 0:06:17[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.5626, 0.4374]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:01:13 â€¢ 0:06:17[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.5497, 0.4503]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:01:13 â€¢ 0:06:17[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.5344, 0.4656]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:01:13 â€¢ 0:06:17[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.5224, 0.4776]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:01:13 â€¢ 0:06:17[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.5161, 0.4839]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:01:13 â€¢ 0:06:17[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.5143, 0.4857]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:01:13 â€¢ 0:06:17[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.5144, 0.4856]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:01:13 â€¢ 0:06:17[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.5530, 0.4470]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 34/203 [2m0:01:13 â€¢ 0:06:17[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.5993, 0.4007],4mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 34/203 [2m0:01:13 â€¢ 0:06:17[0m [2;4m0.45it/s[0m  
        [0.5545, 0.4455],
        [0.5935, 0.4065],
        [0.6323, 0.3677],
        [0.5147, 0.4853],
        [0.5786, 0.4214],
        [0.5417, 0.4583],
        [0.6429, 0.3571],
        [0.6220, 0.3780],
[2K        [0.5970, 0.4030]], device='cuda:0')24mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 34/203 [2m0:01:13 â€¢ 0:06:17[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.5585, 0.4415],6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 35/203 [2m0:01:14 â€¢ 0:06:18[0m [2;4m0.45it/s[0m  
        [0.5527, 0.4473],
        [0.5541, 0.4459],
        [0.5570, 0.4430],
        [0.5541, 0.4459],
        [0.5530, 0.4470],
        [0.5524, 0.4476],
        [0.5582, 0.4418],
        [0.5569, 0.4431],
[2K        [0.5542, 0.4458]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 35/203 [2m0:01:14 â€¢ 0:06:18[0m [2;4m0.45it/s[0m  
[2KClass label: GolfSwingâ”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 35/203 [2m0:01:14 â€¢ 0:06:18[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.6505, 0.3495]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:01:14 â€¢ 0:06:18[0m [2;4m0.45it/s[0m  
[2KStep 0 - TTA Loss: 1.0044â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 35/203 [2m0:01:14 â€¢ 0:06:18[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.6505, 0.3495]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:01:14 â€¢ 0:06:18[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.6512, 0.3488]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:01:14 â€¢ 0:06:18[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.6545, 0.3455]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:01:14 â€¢ 0:06:18[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.6648, 0.3352]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:01:14 â€¢ 0:06:18[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.6839, 0.3161]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:01:14 â€¢ 0:06:18[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.7116, 0.2884]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:01:14 â€¢ 0:06:18[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.7294, 0.2706]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:01:14 â€¢ 0:06:18[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.7269, 0.2731]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:01:14 â€¢ 0:06:18[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.6888, 0.3112]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:01:14 â€¢ 0:06:18[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.5555, 0.4445]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 35/203 [2m0:01:14 â€¢ 0:06:18[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.6418, 0.3582],4mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 35/203 [2m0:01:14 â€¢ 0:06:18[0m [2;4m0.45it/s[0m  
        [0.5641, 0.4359],
        [0.5905, 0.4095],
        [0.6463, 0.3537],
        [0.5714, 0.4286],
        [0.6086, 0.3914],
        [0.5705, 0.4295],
        [0.6690, 0.3310],
        [0.6437, 0.3563],
[2K        [0.6275, 0.3725]], device='cuda:0')24mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 35/203 [2m0:01:14 â€¢ 0:06:18[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.5585, 0.4415],7mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 36/203 [2m0:01:15 â€¢ 0:05:57[0m [2;4m0.47it/s[0m   
        [0.5526, 0.4474],
        [0.5541, 0.4459],
        [0.5569, 0.4431],
        [0.5541, 0.4459],
        [0.5529, 0.4471],
        [0.5523, 0.4477],
        [0.5582, 0.4418],
        [0.5569, 0.4431],
[2K        [0.5542, 0.4458]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 36/203 [2m0:01:15 â€¢ 0:05:57[0m [2;4m0.47it/s[0m  
[2KClass label: TennisSwingâ”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 36/203 [2m0:01:15 â€¢ 0:05:57[0m [2;4m0.47it/s[0m  
[2KAttention weights: tensor([[0.6421, 0.3579]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:01:15 â€¢ 0:05:57[0m [2;4m0.47it/s[0m  
[2KStep 0 - TTA Loss: 0.7514â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 36/203 [2m0:01:15 â€¢ 0:05:57[0m [2;4m0.47it/s[0m  
[2KAttention weights: tensor([[0.6081, 0.3919]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:01:15 â€¢ 0:05:57[0m [2;4m0.47it/s[0m  
[2KAttention weights: tensor([[0.5770, 0.4230]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:01:15 â€¢ 0:05:57[0m [2;4m0.47it/s[0m  
[2KAttention weights: tensor([[0.5497, 0.4503]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:01:15 â€¢ 0:05:57[0m [2;4m0.47it/s[0m  
[2KAttention weights: tensor([[0.5341, 0.4659]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:01:15 â€¢ 0:05:57[0m [2;4m0.47it/s[0m  
[2KAttention weights: tensor([[0.5152, 0.4848]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:01:15 â€¢ 0:05:57[0m [2;4m0.47it/s[0m  
[2KAttention weights: tensor([[0.4925, 0.5075]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:01:15 â€¢ 0:05:57[0m [2;4m0.47it/s[0m  
[2KAttention weights: tensor([[0.4721, 0.5279]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:01:15 â€¢ 0:05:57[0m [2;4m0.47it/s[0m  
[2KAttention weights: tensor([[0.4612, 0.5388]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:01:15 â€¢ 0:05:57[0m [2;4m0.47it/s[0m  
[2KAttention weights: tensor([[0.4554, 0.5446]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:01:15 â€¢ 0:05:57[0m [2;4m0.47it/s[0m  
[2KAttention weights: tensor([[0.5513, 0.4487]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 36/203 [2m0:01:15 â€¢ 0:05:57[0m [2;4m0.47it/s[0m  
[2KAttention weights: tensor([[0.5000, 0.5000],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 36/203 [2m0:01:15 â€¢ 0:05:57[0m [2;4m0.47it/s[0m  
        [0.4396, 0.5604],
        [0.4915, 0.5085],
        [0.4154, 0.5846],
        [0.4349, 0.5651],
        [0.4771, 0.5229],
        [0.4462, 0.5538],
        [0.5543, 0.4457],
        [0.4536, 0.5464],
[2K        [0.4695, 0.5305]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 36/203 [2m0:01:15 â€¢ 0:05:57[0m [2;4m0.47it/s[0m  
[2KAttention weights: tensor([[0.5584, 0.4416],7mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 37/203 [2m0:01:16 â€¢ 0:06:11[0m [2;4m0.45it/s[0m  
        [0.5526, 0.4474],
        [0.5541, 0.4459],
        [0.5568, 0.4432],
        [0.5541, 0.4459],
        [0.5529, 0.4471],
        [0.5523, 0.4477],
        [0.5581, 0.4419],
        [0.5568, 0.4432],
[2K        [0.5541, 0.4459]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 37/203 [2m0:01:16 â€¢ 0:06:11[0m [2;4m0.45it/s[0m  
[2KClass label: SoccerPenaltyâ”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 37/203 [2m0:01:16 â€¢ 0:06:11[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.6666, 0.3334]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:01:16 â€¢ 0:06:11[0m [2;4m0.45it/s[0m  
[2KStep 0 - TTA Loss: 1.0282â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 37/203 [2m0:01:16 â€¢ 0:06:11[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.6666, 0.3334]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:01:16 â€¢ 0:06:11[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.6672, 0.3328]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:01:16 â€¢ 0:06:11[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.6721, 0.3279]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:01:16 â€¢ 0:06:11[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.6868, 0.3132]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:01:16 â€¢ 0:06:11[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.7162, 0.2838]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:01:16 â€¢ 0:06:11[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.7574, 0.2426]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:01:16 â€¢ 0:06:11[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.8039, 0.1961]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:01:16 â€¢ 0:06:11[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.8394, 0.1606]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:01:16 â€¢ 0:06:11[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.8346, 0.1654]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:01:16 â€¢ 0:06:11[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.5628, 0.4372]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 37/203 [2m0:01:16 â€¢ 0:06:11[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.7555, 0.2445],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 37/203 [2m0:01:16 â€¢ 0:06:11[0m [2;4m0.45it/s[0m  
        [0.6415, 0.3585],
        [0.6268, 0.3732],
        [0.6716, 0.3284],
        [0.6607, 0.3393],
        [0.7115, 0.2885],
        [0.6378, 0.3622],
        [0.8901, 0.1099],
        [0.6630, 0.3370],
[2K        [0.6633, 0.3367]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 37/203 [2m0:01:16 â€¢ 0:06:11[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.5584, 0.4416],7mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 38/203 [2m0:01:18 â€¢ 0:06:06[0m [2;4m0.45it/s[0m  
        [0.5526, 0.4474],
        [0.5541, 0.4459],
        [0.5569, 0.4431],
        [0.5541, 0.4459],
        [0.5529, 0.4471],
        [0.5523, 0.4477],
        [0.5582, 0.4418],
        [0.5568, 0.4432],
[2K        [0.5542, 0.4458]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 38/203 [2m0:01:18 â€¢ 0:06:06[0m [2;4m0.45it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 38/203 [2m0:01:18 â€¢ 0:06:06[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.6271, 0.3729]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:01:18 â€¢ 0:06:06[0m [2;4m0.45it/s[0m  
[2KStep 0 - TTA Loss: 0.9132â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 38/203 [2m0:01:18 â€¢ 0:06:06[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.6579, 0.3421]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:01:18 â€¢ 0:06:06[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.6925, 0.3075]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:01:18 â€¢ 0:06:06[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.7232, 0.2768]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:01:18 â€¢ 0:06:06[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.7430, 0.2570]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:01:18 â€¢ 0:06:06[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.7532, 0.2468]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:01:18 â€¢ 0:06:06[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.7599, 0.2401]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:01:18 â€¢ 0:06:06[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.7690, 0.2310]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:01:18 â€¢ 0:06:06[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.7785, 0.2215]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:01:18 â€¢ 0:06:06[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.7848, 0.2152]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:01:18 â€¢ 0:06:06[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.5588, 0.4412]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 38/203 [2m0:01:18 â€¢ 0:06:06[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.8454, 0.1546],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 38/203 [2m0:01:18 â€¢ 0:06:06[0m [2;4m0.45it/s[0m  
        [0.7282, 0.2718],
        [0.7052, 0.2948],
        [0.7803, 0.2197],
        [0.7758, 0.2242],
        [0.8126, 0.1874],
        [0.7478, 0.2522],
        [0.9388, 0.0612],
        [0.7855, 0.2145],
[2K        [0.7870, 0.2130]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 38/203 [2m0:01:18 â€¢ 0:06:06[0m [2;4m0.45it/s[0m  
[2KAttention weights: tensor([[0.5585, 0.4415],;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 39/203 [2m0:01:24 â€¢ 0:06:22[0m [2;4m0.43it/s[0m  
        [0.5527, 0.4473],
        [0.5541, 0.4459],
        [0.5569, 0.4431],
        [0.5542, 0.4458],
        [0.5530, 0.4470],
        [0.5524, 0.4476],
        [0.5584, 0.4416],
        [0.5569, 0.4431],
[2K        [0.5542, 0.4458]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 39/203 [2m0:01:24 â€¢ 0:06:22[0m [2;4m0.43it/s[0m  
[2KClass label: GolfSwingâ”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 39/203 [2m0:01:24 â€¢ 0:06:22[0m [2;4m0.43it/s[0m  
[2KAttention weights: tensor([[0.6479, 0.3521]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:24 â€¢ 0:06:22[0m [2;4m0.43it/s[0m  
[2KStep 0 - TTA Loss: 0.6963â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 39/203 [2m0:01:24 â€¢ 0:06:22[0m [2;4m0.43it/s[0m  
[2KAttention weights: tensor([[0.6479, 0.3521]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:24 â€¢ 0:06:22[0m [2;4m0.43it/s[0m  
[2KAttention weights: tensor([[0.6493, 0.3507]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:24 â€¢ 0:06:22[0m [2;4m0.43it/s[0m  
[2KAttention weights: tensor([[0.6538, 0.3462]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:24 â€¢ 0:06:22[0m [2;4m0.43it/s[0m  
[2KAttention weights: tensor([[0.6598, 0.3402]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:24 â€¢ 0:06:22[0m [2;4m0.43it/s[0m  
[2KAttention weights: tensor([[0.6671, 0.3329]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:24 â€¢ 0:06:22[0m [2;4m0.43it/s[0m  
[2KAttention weights: tensor([[0.6737, 0.3263]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:24 â€¢ 0:06:22[0m [2;4m0.43it/s[0m  
[2KAttention weights: tensor([[0.6774, 0.3226]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:24 â€¢ 0:06:22[0m [2;4m0.43it/s[0m  
[2KAttention weights: tensor([[0.6732, 0.3268]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:24 â€¢ 0:06:22[0m [2;4m0.43it/s[0m  
[2KAttention weights: tensor([[0.6553, 0.3447]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 39/203 [2m0:01:24 â€¢ 0:06:22[0m [2;4m0.43it/s[0m  
[2KAttention weights: tensor([[0.5557, 0.4443]], device='cuda:0')mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 39/203 [2m0:01:24 â€¢ 0:06:22[0m [2;4m0.43it/s[0m  
[2KAttention weights: tensor([[0.7158, 0.2842],24mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 39/203 [2m0:01:24 â€¢ 0:06:22[0m [2;4m0.43it/s[0m  
        [0.5950, 0.4050],
        [0.6367, 0.3633],
        [0.6267, 0.3733],
        [0.6767, 0.3233],
        [0.7077, 0.2923],
        [0.6736, 0.3264],
        [0.7628, 0.2372],
        [0.6991, 0.3009],
[2K        [0.7389, 0.2611]], device='cuda:0')224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 39/203 [2m0:01:24 â€¢ 0:06:22[0m [2;4m0.43it/s[0m  
[2KAttention weights: tensor([[0.5585, 0.4415],;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 40/203 [2m0:01:26 â€¢ 0:06:27[0m [2;4m0.42it/s[0m  
        [0.5527, 0.4473],
        [0.5542, 0.4458],
        [0.5569, 0.4431],
        [0.5542, 0.4458],
        [0.5530, 0.4470],
        [0.5524, 0.4476],
        [0.5584, 0.4416],
        [0.5568, 0.4432],
[2K        [0.5543, 0.4457]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 40/203 [2m0:01:26 â€¢ 0:06:27[0m [2;4m0.42it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 40/203 [2m0:01:26 â€¢ 0:06:27[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.5647, 0.4353]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:26 â€¢ 0:06:27[0m [2;4m0.42it/s[0m  
[2KStep 0 - TTA Loss: 0.6770â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 40/203 [2m0:01:26 â€¢ 0:06:27[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.5674, 0.4326]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:26 â€¢ 0:06:27[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.5567, 0.4433]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:26 â€¢ 0:06:27[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.5369, 0.4631]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:26 â€¢ 0:06:27[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.5073, 0.4927]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:26 â€¢ 0:06:27[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.4755, 0.5245]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:26 â€¢ 0:06:27[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.4499, 0.5501]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:26 â€¢ 0:06:27[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.4329, 0.5671]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:26 â€¢ 0:06:27[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.4243, 0.5757]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:26 â€¢ 0:06:27[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.4211, 0.5789]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 40/203 [2m0:01:26 â€¢ 0:06:27[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.5483, 0.4517]], device='cuda:0')mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 40/203 [2m0:01:26 â€¢ 0:06:27[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.5513, 0.4487],24mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 40/203 [2m0:01:26 â€¢ 0:06:27[0m [2;4m0.42it/s[0m  
        [0.4572, 0.5428],
        [0.4795, 0.5205],
        [0.4465, 0.5535],
        [0.4653, 0.5347],
        [0.4869, 0.5131],
        [0.4203, 0.5797],
        [0.6131, 0.3869],
        [0.5256, 0.4744],
[2K        [0.5218, 0.4782]], device='cuda:0')224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 40/203 [2m0:01:26 â€¢ 0:06:27[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.5584, 0.4416],37mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 41/203 [2m0:01:33 â€¢ 0:08:36[0m [2;4m0.31it/s[0m   
        [0.5526, 0.4474],
        [0.5541, 0.4459],
        [0.5568, 0.4432],
        [0.5541, 0.4459],
        [0.5529, 0.4471],
        [0.5522, 0.4478],
        [0.5583, 0.4417],
        [0.5568, 0.4432],
[2K        [0.5542, 0.4458]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 41/203 [2m0:01:33 â€¢ 0:08:36[0m [2;4m0.31it/s[0m  
[2KClass label: HammerThrowâ”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 41/203 [2m0:01:33 â€¢ 0:08:36[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5561, 0.4439]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:01:33 â€¢ 0:08:36[0m [2;4m0.31it/s[0m  
[2KStep 0 - TTA Loss: 0.4906â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 41/203 [2m0:01:33 â€¢ 0:08:36[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5561, 0.4439]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:01:33 â€¢ 0:08:36[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5560, 0.4440]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:01:33 â€¢ 0:08:36[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5567, 0.4433]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:01:33 â€¢ 0:08:36[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5585, 0.4415]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:01:33 â€¢ 0:08:36[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5612, 0.4388]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:01:33 â€¢ 0:08:36[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5647, 0.4353]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:01:33 â€¢ 0:08:36[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5666, 0.4334]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:01:33 â€¢ 0:08:36[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5648, 0.4352]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:01:33 â€¢ 0:08:36[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5578, 0.4422]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 41/203 [2m0:01:33 â€¢ 0:08:36[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5539, 0.4461]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 41/203 [2m0:01:33 â€¢ 0:08:36[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.6253, 0.3747],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 41/203 [2m0:01:33 â€¢ 0:08:36[0m [2;4m0.31it/s[0m  
        [0.5355, 0.4645],
        [0.5574, 0.4426],
        [0.6123, 0.3877],
        [0.5519, 0.4481],
        [0.5758, 0.4242],
        [0.5252, 0.4748],
        [0.6570, 0.3430],
        [0.6178, 0.3822],
[2K        [0.6046, 0.3954]], device='cuda:0')â•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 41/203 [2m0:01:33 â€¢ 0:08:36[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5584, 0.4416],37mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 42/203 [2m0:01:36 â€¢ 0:07:51[0m [2;4m0.34it/s[0m  
        [0.5526, 0.4474],
        [0.5541, 0.4459],
        [0.5568, 0.4432],
        [0.5540, 0.4460],
        [0.5528, 0.4472],
        [0.5522, 0.4478],
        [0.5583, 0.4417],
        [0.5567, 0.4433],
[2K        [0.5542, 0.4458]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 42/203 [2m0:01:36 â€¢ 0:07:51[0m [2;4m0.34it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 42/203 [2m0:01:36 â€¢ 0:07:51[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6203, 0.3797]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:01:36 â€¢ 0:07:51[0m [2;4m0.34it/s[0m  
[2KStep 0 - TTA Loss: 0.9123â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 42/203 [2m0:01:36 â€¢ 0:07:51[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6188, 0.3812]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:01:36 â€¢ 0:07:51[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6219, 0.3781]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:01:36 â€¢ 0:07:51[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6267, 0.3733]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:01:36 â€¢ 0:07:51[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6322, 0.3678]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:01:36 â€¢ 0:07:51[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6370, 0.3630]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:01:36 â€¢ 0:07:51[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6410, 0.3590]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:01:36 â€¢ 0:07:51[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6425, 0.3575]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:01:36 â€¢ 0:07:51[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6432, 0.3568]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:01:36 â€¢ 0:07:51[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6435, 0.3565]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 42/203 [2m0:01:36 â€¢ 0:07:51[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.5546, 0.4454]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 42/203 [2m0:01:36 â€¢ 0:07:51[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6447, 0.3553],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 42/203 [2m0:01:36 â€¢ 0:07:51[0m [2;4m0.34it/s[0m  
        [0.5577, 0.4423],
        [0.5707, 0.4293],
        [0.6486, 0.3514],
        [0.5707, 0.4293],
        [0.6060, 0.3940],
        [0.5622, 0.4378],
        [0.6724, 0.3276],
        [0.6481, 0.3519],
[2K        [0.6436, 0.3564]], device='cuda:0')â•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 42/203 [2m0:01:36 â€¢ 0:07:51[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.5584, 0.4416],37mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 43/203 [2m0:01:37 â€¢ 0:07:16[0m [2;4m0.37it/s[0m  
        [0.5526, 0.4474],
        [0.5541, 0.4459],
        [0.5568, 0.4432],
        [0.5540, 0.4460],
        [0.5528, 0.4472],
        [0.5521, 0.4479],
        [0.5583, 0.4417],
        [0.5567, 0.4433],
[2K        [0.5542, 0.4458]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 43/203 [2m0:01:37 â€¢ 0:07:16[0m [2;4m0.37it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 43/203 [2m0:01:37 â€¢ 0:07:16[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.6216, 0.3784]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:01:37 â€¢ 0:07:16[0m [2;4m0.37it/s[0m  
[2KStep 0 - TTA Loss: 0.7684â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 43/203 [2m0:01:37 â€¢ 0:07:16[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.6216, 0.3784]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:01:37 â€¢ 0:07:16[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.6219, 0.3781]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:01:37 â€¢ 0:07:16[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.6232, 0.3768]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:01:37 â€¢ 0:07:16[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.6268, 0.3732]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:01:37 â€¢ 0:07:16[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.6331, 0.3669]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:01:37 â€¢ 0:07:16[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.6416, 0.3584]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:01:37 â€¢ 0:07:16[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.6485, 0.3515]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:01:37 â€¢ 0:07:16[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.6482, 0.3518]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:01:37 â€¢ 0:07:16[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.6374, 0.3626]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 43/203 [2m0:01:37 â€¢ 0:07:16[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5536, 0.4464]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 43/203 [2m0:01:37 â€¢ 0:07:16[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.6371, 0.3629],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 43/203 [2m0:01:37 â€¢ 0:07:16[0m [2;4m0.37it/s[0m  
        [0.5527, 0.4473],
        [0.5691, 0.4309],
        [0.6432, 0.3568],
        [0.5595, 0.4405],
        [0.5995, 0.4005],
        [0.5606, 0.4394],
        [0.6632, 0.3368],
        [0.6365, 0.3635],
[2K        [0.6241, 0.3759]], device='cuda:0')â•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 43/203 [2m0:01:37 â€¢ 0:07:16[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5584, 0.4416],8;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 44/203 [2m0:01:40 â€¢ 0:07:20[0m [2;4m0.36it/s[0m  
        [0.5526, 0.4474],
        [0.5541, 0.4459],
        [0.5568, 0.4432],
        [0.5541, 0.4459],
        [0.5529, 0.4471],
        [0.5522, 0.4478],
        [0.5583, 0.4417],
        [0.5568, 0.4432],
[2K        [0.5543, 0.4457]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 44/203 [2m0:01:40 â€¢ 0:07:20[0m [2;4m0.36it/s[0m  
[2KClass label: HighJumpâ”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 44/203 [2m0:01:40 â€¢ 0:07:20[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.5968, 0.4032]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:01:40 â€¢ 0:07:20[0m [2;4m0.36it/s[0m  
[2KStep 0 - TTA Loss: 0.4959â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 44/203 [2m0:01:40 â€¢ 0:07:20[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.5979, 0.4021]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:01:40 â€¢ 0:07:20[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.6042, 0.3958]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:01:40 â€¢ 0:07:20[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.6097, 0.3903]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:01:40 â€¢ 0:07:20[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.6042, 0.3958]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:01:40 â€¢ 0:07:20[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.5944, 0.4056]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:01:40 â€¢ 0:07:20[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.5913, 0.4087]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:01:40 â€¢ 0:07:20[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.5977, 0.4023]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:01:40 â€¢ 0:07:20[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.6076, 0.3924]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:01:40 â€¢ 0:07:20[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.6150, 0.3850]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 44/203 [2m0:01:40 â€¢ 0:07:20[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.5529, 0.4471]], device='cuda:0')7mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 44/203 [2m0:01:40 â€¢ 0:07:20[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.6344, 0.3656],224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 44/203 [2m0:01:40 â€¢ 0:07:20[0m [2;4m0.36it/s[0m  
        [0.5521, 0.4479],
        [0.5785, 0.4215],
        [0.6349, 0.3651],
        [0.5613, 0.4387],
        [0.6176, 0.3824],
        [0.5665, 0.4335],
        [0.6763, 0.3237],
        [0.6340, 0.3660],
[2K        [0.6102, 0.3898]], device='cuda:0');224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 44/203 [2m0:01:40 â€¢ 0:07:20[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.5584, 0.4416],8;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 45/203 [2m0:01:51 â€¢ 0:11:43[0m [2;4m0.22it/s[0m  
        [0.5526, 0.4474],
        [0.5542, 0.4458],
        [0.5568, 0.4432],
        [0.5541, 0.4459],
        [0.5529, 0.4471],
        [0.5522, 0.4478],
        [0.5583, 0.4417],
        [0.5568, 0.4432],
[2K        [0.5544, 0.4456]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 45/203 [2m0:01:51 â€¢ 0:11:43[0m [2;4m0.22it/s[0m  
[2KClass label: GolfSwingâ”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 45/203 [2m0:01:51 â€¢ 0:11:43[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.6469, 0.3531]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:01:51 â€¢ 0:11:43[0m [2;4m0.22it/s[0m  
[2KStep 0 - TTA Loss: 0.7667â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 45/203 [2m0:01:51 â€¢ 0:11:43[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.6469, 0.3531]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:01:51 â€¢ 0:11:43[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.6482, 0.3518]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:01:51 â€¢ 0:11:43[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.6531, 0.3469]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:01:51 â€¢ 0:11:43[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.6636, 0.3364]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:01:51 â€¢ 0:11:43[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.6796, 0.3204]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:01:51 â€¢ 0:11:43[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.6981, 0.3019]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:01:51 â€¢ 0:11:43[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.7137, 0.2863]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:01:51 â€¢ 0:11:43[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.7220, 0.2780]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:01:51 â€¢ 0:11:43[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.7157, 0.2843]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 45/203 [2m0:01:51 â€¢ 0:11:43[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.5566, 0.4434]], device='cuda:0')7mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 45/203 [2m0:01:51 â€¢ 0:11:43[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.7253, 0.2747],224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 45/203 [2m0:01:51 â€¢ 0:11:43[0m [2;4m0.22it/s[0m  
        [0.6309, 0.3691],
        [0.6787, 0.3213],
        [0.6960, 0.3040],
        [0.6913, 0.3087],
        [0.7683, 0.2317],
        [0.7072, 0.2928],
        [0.7716, 0.2284],
        [0.7307, 0.2693],
[2K        [0.7285, 0.2715]], device='cuda:0');224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 45/203 [2m0:01:51 â€¢ 0:11:43[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.5585, 0.4415],237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 46/203 [2m0:01:54 â€¢ 0:12:09[0m [2;4m0.22it/s[0m   
        [0.5526, 0.4474],
        [0.5542, 0.4458],
        [0.5568, 0.4432],
        [0.5542, 0.4458],
        [0.5530, 0.4470],
        [0.5523, 0.4477],
        [0.5584, 0.4416],
        [0.5568, 0.4432],
[2K        [0.5544, 0.4456]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 46/203 [2m0:01:54 â€¢ 0:12:09[0m [2;4m0.22it/s[0m  
[2KClass label: GolfSwingâ”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 46/203 [2m0:01:54 â€¢ 0:12:09[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.6453, 0.3547]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 46/203 [2m0:01:54 â€¢ 0:12:09[0m [2;4m0.22it/s[0m  
[2KStep 0 - TTA Loss: 0.7379â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 46/203 [2m0:01:54 â€¢ 0:12:09[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.6354, 0.3646]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 46/203 [2m0:01:54 â€¢ 0:12:09[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.6285, 0.3715]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 46/203 [2m0:01:54 â€¢ 0:12:09[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.6133, 0.3867]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 46/203 [2m0:01:54 â€¢ 0:12:09[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.5968, 0.4032]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 46/203 [2m0:01:54 â€¢ 0:12:09[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.5759, 0.4241]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 46/203 [2m0:01:54 â€¢ 0:12:09[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.5520, 0.4480]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 46/203 [2m0:01:54 â€¢ 0:12:09[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.5296, 0.4704]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 46/203 [2m0:01:54 â€¢ 0:12:09[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.5143, 0.4857]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 46/203 [2m0:01:54 â€¢ 0:12:09[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.5057, 0.4943]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 46/203 [2m0:01:54 â€¢ 0:12:09[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.5525, 0.4475]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 46/203 [2m0:01:54 â€¢ 0:12:09[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.6073, 0.3927],â•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 46/203 [2m0:01:54 â€¢ 0:12:09[0m [2;4m0.22it/s[0m  
        [0.5229, 0.4771],
        [0.5840, 0.4160],
        [0.5032, 0.4968],
        [0.5647, 0.4353],
        [0.6455, 0.3545],
        [0.5870, 0.4130],
        [0.6754, 0.3246],
        [0.5988, 0.4012],
[2K        [0.5953, 0.4047]], device='cuda:0')mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 46/203 [2m0:01:54 â€¢ 0:12:09[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.5585, 0.4415],237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 47/203 [2m0:01:56 â€¢ 0:10:08[0m [2;4m0.26it/s[0m  
        [0.5526, 0.4474],
        [0.5542, 0.4458],
        [0.5568, 0.4432],
        [0.5542, 0.4458],
        [0.5530, 0.4470],
        [0.5523, 0.4477],
        [0.5584, 0.4416],
        [0.5568, 0.4432],
[2K        [0.5544, 0.4456]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 47/203 [2m0:01:56 â€¢ 0:10:08[0m [2;4m0.26it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 47/203 [2m0:01:56 â€¢ 0:10:08[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.5594, 0.4406]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 47/203 [2m0:01:56 â€¢ 0:10:08[0m [2;4m0.26it/s[0m  
[2KStep 0 - TTA Loss: 0.7648â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 47/203 [2m0:01:56 â€¢ 0:10:08[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.5594, 0.4406]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 47/203 [2m0:01:56 â€¢ 0:10:08[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.5596, 0.4404]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 47/203 [2m0:01:56 â€¢ 0:10:08[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.5620, 0.4380]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 47/203 [2m0:01:56 â€¢ 0:10:08[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.5703, 0.4297]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 47/203 [2m0:01:56 â€¢ 0:10:08[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.5888, 0.4112]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 47/203 [2m0:01:56 â€¢ 0:10:08[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.6190, 0.3810]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 47/203 [2m0:01:56 â€¢ 0:10:08[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.6571, 0.3429]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 47/203 [2m0:01:56 â€¢ 0:10:08[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.6976, 0.3024]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 47/203 [2m0:01:56 â€¢ 0:10:08[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.7304, 0.2696]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 47/203 [2m0:01:56 â€¢ 0:10:08[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.5572, 0.4428]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 47/203 [2m0:01:56 â€¢ 0:10:08[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.7002, 0.2998],â•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 47/203 [2m0:01:56 â€¢ 0:10:08[0m [2;4m0.26it/s[0m  
        [0.6106, 0.3894],
        [0.6902, 0.3098],
        [0.5945, 0.4055],
        [0.6937, 0.3063],
        [0.7633, 0.2367],
        [0.7675, 0.2325],
        [0.7457, 0.2543],
        [0.6943, 0.3057],
[2K        [0.7292, 0.2708]], device='cuda:0')mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 47/203 [2m0:01:56 â€¢ 0:10:08[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.5585, 0.4415],237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 48/203 [2m0:02:06 â€¢ 0:12:44[0m [2;4m0.20it/s[0m  
        [0.5526, 0.4474],
        [0.5542, 0.4458],
        [0.5568, 0.4432],
        [0.5542, 0.4458],
        [0.5530, 0.4470],
        [0.5524, 0.4476],
        [0.5584, 0.4416],
        [0.5568, 0.4432],
[2K        [0.5544, 0.4456]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 48/203 [2m0:02:06 â€¢ 0:12:44[0m [2;4m0.20it/s[0m  
[2KClass label: TennisSwingâ”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 48/203 [2m0:02:06 â€¢ 0:12:44[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.6436, 0.3564]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 48/203 [2m0:02:06 â€¢ 0:12:44[0m [2;4m0.20it/s[0m  
[2KStep 0 - TTA Loss: 0.7541â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 48/203 [2m0:02:06 â€¢ 0:12:44[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.6576, 0.3424]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 48/203 [2m0:02:06 â€¢ 0:12:44[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.6674, 0.3326]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 48/203 [2m0:02:06 â€¢ 0:12:44[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.6747, 0.3253]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 48/203 [2m0:02:06 â€¢ 0:12:44[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.6763, 0.3237]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 48/203 [2m0:02:06 â€¢ 0:12:44[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.6726, 0.3274]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 48/203 [2m0:02:06 â€¢ 0:12:44[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.6643, 0.3357]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 48/203 [2m0:02:06 â€¢ 0:12:44[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.6547, 0.3453]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 48/203 [2m0:02:06 â€¢ 0:12:44[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.6458, 0.3542]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 48/203 [2m0:02:06 â€¢ 0:12:44[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.6405, 0.3595]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 48/203 [2m0:02:06 â€¢ 0:12:44[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.5562, 0.4438]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 48/203 [2m0:02:06 â€¢ 0:12:44[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.6652, 0.3348],â•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 48/203 [2m0:02:06 â€¢ 0:12:44[0m [2;4m0.20it/s[0m  
        [0.5791, 0.4209],
        [0.6547, 0.3453],
        [0.5971, 0.4029],
        [0.6479, 0.3521],
        [0.7124, 0.2876],
        [0.7136, 0.2864],
        [0.7089, 0.2911],
        [0.6389, 0.3611],
[2K        [0.6898, 0.3102]], device='cuda:0')mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 48/203 [2m0:02:06 â€¢ 0:12:44[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.5585, 0.4415],98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 49/203 [2m0:02:07 â€¢ 0:12:46[0m [2;4m0.20it/s[0m  
        [0.5526, 0.4474],
        [0.5542, 0.4458],
        [0.5568, 0.4432],
        [0.5542, 0.4458],
        [0.5531, 0.4469],
        [0.5524, 0.4476],
        [0.5584, 0.4416],
        [0.5568, 0.4432],
[2K        [0.5544, 0.4456]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 49/203 [2m0:02:07 â€¢ 0:12:46[0m [2;4m0.20it/s[0m  
[2KClass label: CleanAndJerkâ”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 49/203 [2m0:02:07 â€¢ 0:12:46[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.5706, 0.4294]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 49/203 [2m0:02:07 â€¢ 0:12:46[0m [2;4m0.20it/s[0m  
[2KStep 0 - TTA Loss: 0.5721â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 49/203 [2m0:02:07 â€¢ 0:12:46[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.5706, 0.4294]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 49/203 [2m0:02:07 â€¢ 0:12:46[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.5707, 0.4293]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 49/203 [2m0:02:07 â€¢ 0:12:46[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.5718, 0.4282]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 49/203 [2m0:02:07 â€¢ 0:12:46[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.5756, 0.4244]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 49/203 [2m0:02:07 â€¢ 0:12:46[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.5832, 0.4168]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 49/203 [2m0:02:07 â€¢ 0:12:46[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.5960, 0.4040]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 49/203 [2m0:02:07 â€¢ 0:12:46[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.6110, 0.3890]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 49/203 [2m0:02:07 â€¢ 0:12:46[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.6208, 0.3792]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 49/203 [2m0:02:07 â€¢ 0:12:46[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.6231, 0.3769]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 49/203 [2m0:02:07 â€¢ 0:12:46[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.5551, 0.4449]], device='cuda:0')37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 49/203 [2m0:02:07 â€¢ 0:12:46[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.5891, 0.4109],;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 49/203 [2m0:02:07 â€¢ 0:12:46[0m [2;4m0.20it/s[0m  
        [0.5164, 0.4836],
        [0.6243, 0.3757],
        [0.5766, 0.4234],
        [0.5412, 0.4588],
        [0.5809, 0.4191],
        [0.5580, 0.4420],
        [0.6270, 0.3730],
        [0.5223, 0.4777],
[2K        [0.5822, 0.4178]], device='cuda:0')6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 49/203 [2m0:02:07 â€¢ 0:12:46[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.5585, 0.4415],98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 50/203 [2m0:02:10 â€¢ 0:10:08[0m [2;4m0.25it/s[0m  
        [0.5526, 0.4474],
        [0.5542, 0.4458],
        [0.5567, 0.4433],
        [0.5542, 0.4458],
        [0.5531, 0.4469],
        [0.5524, 0.4476],
        [0.5584, 0.4416],
        [0.5568, 0.4432],
[2K        [0.5545, 0.4455]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 50/203 [2m0:02:10 â€¢ 0:10:08[0m [2;4m0.25it/s[0m  
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 50/203 [2m0:02:10 â€¢ 0:10:08[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6051, 0.3949]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 50/203 [2m0:02:10 â€¢ 0:10:08[0m [2;4m0.25it/s[0m  
[2KStep 0 - TTA Loss: 0.6098â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 50/203 [2m0:02:10 â€¢ 0:10:08[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.5906, 0.4094]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 50/203 [2m0:02:10 â€¢ 0:10:08[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.5675, 0.4325]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 50/203 [2m0:02:10 â€¢ 0:10:08[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.5362, 0.4638]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 50/203 [2m0:02:10 â€¢ 0:10:08[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.5070, 0.4930]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 50/203 [2m0:02:10 â€¢ 0:10:08[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.4934, 0.5066]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 50/203 [2m0:02:10 â€¢ 0:10:08[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.4981, 0.5019]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 50/203 [2m0:02:10 â€¢ 0:10:08[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.5110, 0.4890]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 50/203 [2m0:02:10 â€¢ 0:10:08[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.5239, 0.4761]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 50/203 [2m0:02:10 â€¢ 0:10:08[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.5317, 0.4683]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 50/203 [2m0:02:10 â€¢ 0:10:08[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.5509, 0.4491]], device='cuda:0')37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 50/203 [2m0:02:10 â€¢ 0:10:08[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.5731, 0.4269],;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 50/203 [2m0:02:10 â€¢ 0:10:08[0m [2;4m0.25it/s[0m  
        [0.5032, 0.4968],
        [0.5492, 0.4508],
        [0.5748, 0.4252],
        [0.5007, 0.4993],
        [0.5340, 0.4660],
        [0.5140, 0.4860],
        [0.6033, 0.3967],
        [0.5277, 0.4723],
[2K        [0.5592, 0.4408]], device='cuda:0')6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 50/203 [2m0:02:10 â€¢ 0:10:08[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.5584, 0.4416],;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 51/203 [2m0:02:14 â€¢ 0:10:04[0m [2;4m0.25it/s[0m   
        [0.5526, 0.4474],
        [0.5542, 0.4458],
        [0.5567, 0.4433],
        [0.5542, 0.4458],
        [0.5530, 0.4470],
        [0.5524, 0.4476],
        [0.5583, 0.4417],
        [0.5567, 0.4433],
[2K        [0.5544, 0.4456]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 51/203 [2m0:02:14 â€¢ 0:10:04[0m [2;4m0.25it/s[0m  
[2KClass label: BaseballPitchâ”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 51/203 [2m0:02:14 â€¢ 0:10:04[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6379, 0.3621]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 51/203 [2m0:02:14 â€¢ 0:10:04[0m [2;4m0.25it/s[0m  
[2KStep 0 - TTA Loss: 0.9027â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 51/203 [2m0:02:14 â€¢ 0:10:04[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6379, 0.3621]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 51/203 [2m0:02:14 â€¢ 0:10:04[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6390, 0.3610]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 51/203 [2m0:02:14 â€¢ 0:10:04[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6428, 0.3572]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 51/203 [2m0:02:14 â€¢ 0:10:04[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6504, 0.3496]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 51/203 [2m0:02:14 â€¢ 0:10:04[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6625, 0.3375]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 51/203 [2m0:02:14 â€¢ 0:10:04[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6776, 0.3224]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 51/203 [2m0:02:14 â€¢ 0:10:04[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6922, 0.3078]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 51/203 [2m0:02:14 â€¢ 0:10:04[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6977, 0.3023]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 51/203 [2m0:02:14 â€¢ 0:10:04[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6764, 0.3236]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 51/203 [2m0:02:14 â€¢ 0:10:04[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.5569, 0.4431]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 51/203 [2m0:02:14 â€¢ 0:10:04[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6236, 0.3764],mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 51/203 [2m0:02:14 â€¢ 0:10:04[0m [2;4m0.25it/s[0m  
        [0.5975, 0.4025],
        [0.6548, 0.3452],
        [0.6789, 0.3211],
        [0.6324, 0.3676],
        [0.7257, 0.2743],
        [0.6689, 0.3311],
        [0.7218, 0.2782],
        [0.6873, 0.3127],
[2K        [0.6921, 0.3079]], device='cuda:0')7mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 51/203 [2m0:02:14 â€¢ 0:10:04[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.5584, 0.4416],;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 52/203 [2m0:02:15 â€¢ 0:08:55[0m [2;4m0.28it/s[0m  
        [0.5526, 0.4474],
        [0.5542, 0.4458],
        [0.5567, 0.4433],
        [0.5542, 0.4458],
        [0.5530, 0.4470],
        [0.5524, 0.4476],
        [0.5583, 0.4417],
        [0.5567, 0.4433],
[2K        [0.5544, 0.4456]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 52/203 [2m0:02:15 â€¢ 0:08:55[0m [2;4m0.28it/s[0m  
[2KClass label: CleanAndJerkâ”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 52/203 [2m0:02:15 â€¢ 0:08:55[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.5685, 0.4315]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 52/203 [2m0:02:15 â€¢ 0:08:55[0m [2;4m0.28it/s[0m  
[2KStep 0 - TTA Loss: 0.6315â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 52/203 [2m0:02:15 â€¢ 0:08:55[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.5855, 0.4145]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 52/203 [2m0:02:15 â€¢ 0:08:55[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6168, 0.3832]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 52/203 [2m0:02:15 â€¢ 0:08:55[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6500, 0.3500]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 52/203 [2m0:02:15 â€¢ 0:08:55[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6752, 0.3248]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 52/203 [2m0:02:15 â€¢ 0:08:55[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6887, 0.3113]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 52/203 [2m0:02:15 â€¢ 0:08:55[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6935, 0.3065]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 52/203 [2m0:02:15 â€¢ 0:08:55[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6930, 0.3070]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 52/203 [2m0:02:15 â€¢ 0:08:55[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6909, 0.3091]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 52/203 [2m0:02:15 â€¢ 0:08:55[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6892, 0.3108]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 52/203 [2m0:02:15 â€¢ 0:08:55[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.5570, 0.4430]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 52/203 [2m0:02:15 â€¢ 0:08:55[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.4467, 0.5533],mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 52/203 [2m0:02:15 â€¢ 0:08:55[0m [2;4m0.28it/s[0m  
        [0.5467, 0.4533],
        [0.6886, 0.3114],
        [0.6144, 0.3856],
        [0.5415, 0.4585],
        [0.6505, 0.3495],
        [0.6040, 0.3960],
        [0.6236, 0.3764],
        [0.6152, 0.3848],
[2K        [0.6192, 0.3808]], device='cuda:0')7mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 52/203 [2m0:02:15 â€¢ 0:08:55[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.5583, 0.4417],;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 53/203 [2m0:02:16 â€¢ 0:08:01[0m [2;4m0.31it/s[0m  
        [0.5526, 0.4474],
        [0.5542, 0.4458],
        [0.5566, 0.4434],
        [0.5542, 0.4458],
        [0.5530, 0.4470],
        [0.5524, 0.4476],
        [0.5583, 0.4417],
        [0.5567, 0.4433],
[2K        [0.5544, 0.4456]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 53/203 [2m0:02:16 â€¢ 0:08:01[0m [2;4m0.31it/s[0m  
[2KClass label: Billiardsâ”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 53/203 [2m0:02:16 â€¢ 0:08:01[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5564, 0.4436]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 53/203 [2m0:02:16 â€¢ 0:08:01[0m [2;4m0.31it/s[0m  
[2KStep 0 - TTA Loss: 0.8818â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 53/203 [2m0:02:16 â€¢ 0:08:01[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5564, 0.4436]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 53/203 [2m0:02:16 â€¢ 0:08:01[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5562, 0.4438]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 53/203 [2m0:02:16 â€¢ 0:08:01[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5554, 0.4446]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 53/203 [2m0:02:16 â€¢ 0:08:01[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5540, 0.4460]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 53/203 [2m0:02:16 â€¢ 0:08:01[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5505, 0.4495]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 53/203 [2m0:02:16 â€¢ 0:08:01[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5447, 0.4553]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 53/203 [2m0:02:16 â€¢ 0:08:01[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5344, 0.4656]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 53/203 [2m0:02:16 â€¢ 0:08:01[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5150, 0.4850]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 53/203 [2m0:02:16 â€¢ 0:08:01[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.4851, 0.5149]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 53/203 [2m0:02:16 â€¢ 0:08:01[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5503, 0.4497]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 53/203 [2m0:02:16 â€¢ 0:08:01[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5550, 0.4450],mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 53/203 [2m0:02:16 â€¢ 0:08:01[0m [2;4m0.31it/s[0m  
        [0.4515, 0.5485],
        [0.4871, 0.5129],
        [0.5957, 0.4043],
        [0.4930, 0.5070],
        [0.5515, 0.4485],
        [0.5148, 0.4852],
        [0.6125, 0.3875],
        [0.5825, 0.4175],
[2K        [0.5739, 0.4261]], device='cuda:0')7mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 53/203 [2m0:02:16 â€¢ 0:08:01[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5582, 0.4418],;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 54/203 [2m0:02:19 â€¢ 0:07:50[0m [2;4m0.32it/s[0m  
        [0.5525, 0.4475],
        [0.5542, 0.4458],
        [0.5566, 0.4434],
        [0.5541, 0.4459],
        [0.5530, 0.4470],
        [0.5523, 0.4477],
        [0.5583, 0.4417],
        [0.5566, 0.4434],
[2K        [0.5543, 0.4457]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 54/203 [2m0:02:19 â€¢ 0:07:50[0m [2;4m0.32it/s[0m  
[2KClass label: SoccerPenaltyâ”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 54/203 [2m0:02:19 â€¢ 0:07:50[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.6660, 0.3340]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 54/203 [2m0:02:19 â€¢ 0:07:50[0m [2;4m0.32it/s[0m  
[2KStep 0 - TTA Loss: 0.4510â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 54/203 [2m0:02:19 â€¢ 0:07:50[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.6796, 0.3204]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 54/203 [2m0:02:19 â€¢ 0:07:50[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.7112, 0.2888]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 54/203 [2m0:02:19 â€¢ 0:07:50[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.7452, 0.2548]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 54/203 [2m0:02:19 â€¢ 0:07:50[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.7671, 0.2329]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 54/203 [2m0:02:19 â€¢ 0:07:50[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.7736, 0.2264]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 54/203 [2m0:02:19 â€¢ 0:07:50[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.7699, 0.2301]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 54/203 [2m0:02:19 â€¢ 0:07:50[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.7644, 0.2356]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 54/203 [2m0:02:19 â€¢ 0:07:50[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.7605, 0.2395]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 54/203 [2m0:02:19 â€¢ 0:07:50[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.7588, 0.2412]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 54/203 [2m0:02:19 â€¢ 0:07:50[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.5594, 0.4406]], device='cuda:0')237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 54/203 [2m0:02:19 â€¢ 0:07:50[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.6510, 0.3490],6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 54/203 [2m0:02:19 â€¢ 0:07:50[0m [2;4m0.32it/s[0m  
        [0.4929, 0.5071],
        [0.5419, 0.4581],
        [0.6471, 0.3529],
        [0.5751, 0.4249],
        [0.6340, 0.3660],
        [0.5759, 0.4241],
        [0.7586, 0.2414],
        [0.6418, 0.3582],
[2K        [0.6296, 0.3704]], device='cuda:0');6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 54/203 [2m0:02:19 â€¢ 0:07:50[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.5582, 0.4418],;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 55/203 [2m0:02:23 â€¢ 0:07:56[0m [2;4m0.31it/s[0m  
        [0.5524, 0.4476],
        [0.5541, 0.4459],
        [0.5566, 0.4434],
        [0.5541, 0.4459],
        [0.5530, 0.4470],
        [0.5523, 0.4477],
        [0.5582, 0.4418],
        [0.5566, 0.4434],
[2K        [0.5543, 0.4457]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 55/203 [2m0:02:23 â€¢ 0:07:56[0m [2;4m0.31it/s[0m  
[2KClass label: HammerThrowâ”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 55/203 [2m0:02:23 â€¢ 0:07:56[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5560, 0.4440]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 55/203 [2m0:02:23 â€¢ 0:07:56[0m [2;4m0.31it/s[0m  
[2KStep 0 - TTA Loss: 0.6568â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 55/203 [2m0:02:23 â€¢ 0:07:56[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5560, 0.4440]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 55/203 [2m0:02:23 â€¢ 0:07:56[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5563, 0.4437]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 55/203 [2m0:02:23 â€¢ 0:07:56[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5589, 0.4411]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 55/203 [2m0:02:23 â€¢ 0:07:56[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5666, 0.4334]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 55/203 [2m0:02:23 â€¢ 0:07:56[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5818, 0.4182]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 55/203 [2m0:02:23 â€¢ 0:07:56[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.6046, 0.3954]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 55/203 [2m0:02:23 â€¢ 0:07:56[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.6307, 0.3693]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 55/203 [2m0:02:23 â€¢ 0:07:56[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.6596, 0.3404]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 55/203 [2m0:02:23 â€¢ 0:07:56[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.6892, 0.3108]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 55/203 [2m0:02:23 â€¢ 0:07:56[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5588, 0.4412]], device='cuda:0')237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 55/203 [2m0:02:23 â€¢ 0:07:56[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.7674, 0.2326],6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 55/203 [2m0:02:23 â€¢ 0:07:56[0m [2;4m0.31it/s[0m  
        [0.6182, 0.3818],
        [0.6923, 0.3077],
        [0.7391, 0.2609],
        [0.7619, 0.2381],
        [0.7371, 0.2629],
        [0.6979, 0.3021],
        [0.7550, 0.2450],
        [0.7395, 0.2605],
[2K        [0.7595, 0.2405]], device='cuda:0');6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 55/203 [2m0:02:23 â€¢ 0:07:56[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5582, 0.4418],5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 56/203 [2m0:02:25 â€¢ 0:07:41[0m [2;4m0.32it/s[0m   
        [0.5525, 0.4475],
        [0.5542, 0.4458],
        [0.5566, 0.4434],
        [0.5542, 0.4458],
        [0.5530, 0.4470],
        [0.5524, 0.4476],
        [0.5582, 0.4418],
        [0.5566, 0.4434],
[2K        [0.5544, 0.4456]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 56/203 [2m0:02:25 â€¢ 0:07:41[0m [2;4m0.32it/s[0m  
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 56/203 [2m0:02:25 â€¢ 0:07:41[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.6015, 0.3985]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 56/203 [2m0:02:25 â€¢ 0:07:41[0m [2;4m0.32it/s[0m  
[2KStep 0 - TTA Loss: 1.3103â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 56/203 [2m0:02:25 â€¢ 0:07:41[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.6568, 0.3432]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 56/203 [2m0:02:25 â€¢ 0:07:41[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.7041, 0.2959]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 56/203 [2m0:02:25 â€¢ 0:07:41[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.7384, 0.2616]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 56/203 [2m0:02:25 â€¢ 0:07:41[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.7619, 0.2381]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 56/203 [2m0:02:25 â€¢ 0:07:41[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.7778, 0.2222]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 56/203 [2m0:02:25 â€¢ 0:07:41[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.7856, 0.2144]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 56/203 [2m0:02:25 â€¢ 0:07:41[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.7853, 0.2147]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 56/203 [2m0:02:25 â€¢ 0:07:41[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.7843, 0.2157]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 56/203 [2m0:02:25 â€¢ 0:07:41[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.7829, 0.2171]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 56/203 [2m0:02:25 â€¢ 0:07:41[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.5579, 0.4421]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 56/203 [2m0:02:25 â€¢ 0:07:41[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.5583, 0.4417],5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 57/203 [2m0:02:25 â€¢ 0:07:01[0m [2;4m0.35it/s[0m  
        [0.5525, 0.4475],
        [0.5543, 0.4457],
        [0.5567, 0.4433],
        [0.5544, 0.4456],
        [0.5531, 0.4469],
        [0.5525, 0.4475],
        [0.5583, 0.4417],
        [0.5567, 0.4433],
[2K        [0.5545, 0.4455]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 57/203 [2m0:02:25 â€¢ 0:07:01[0m [2;4m0.35it/s[0m  
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 57/203 [2m0:02:25 â€¢ 0:07:01[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5983, 0.4017]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 57/203 [2m0:02:25 â€¢ 0:07:01[0m [2;4m0.35it/s[0m  
[2KStep 0 - TTA Loss: 0.6033â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 57/203 [2m0:02:25 â€¢ 0:07:01[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5983, 0.4017]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 57/203 [2m0:02:25 â€¢ 0:07:01[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5978, 0.4022]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 57/203 [2m0:02:25 â€¢ 0:07:01[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5963, 0.4037]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 57/203 [2m0:02:25 â€¢ 0:07:01[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5932, 0.4068]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 57/203 [2m0:02:25 â€¢ 0:07:01[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5870, 0.4130]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 57/203 [2m0:02:25 â€¢ 0:07:01[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5776, 0.4224]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 57/203 [2m0:02:25 â€¢ 0:07:01[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5625, 0.4375]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 57/203 [2m0:02:25 â€¢ 0:07:01[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5418, 0.4582]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 57/203 [2m0:02:25 â€¢ 0:07:01[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5235, 0.4765]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 57/203 [2m0:02:25 â€¢ 0:07:01[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5506, 0.4494]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 57/203 [2m0:02:25 â€¢ 0:07:01[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6103, 0.3897],7mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 57/203 [2m0:02:25 â€¢ 0:07:01[0m [2;4m0.35it/s[0m  
        [0.5200, 0.4800],
        [0.5315, 0.4685],
        [0.6201, 0.3799],
        [0.5234, 0.4766],
        [0.5179, 0.4821],
        [0.4998, 0.5002],
        [0.6123, 0.3877],
        [0.5978, 0.4022],
[2K        [0.5834, 0.4166]], device='cuda:0')37mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 57/203 [2m0:02:25 â€¢ 0:07:01[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5583, 0.4417],5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 58/203 [2m0:02:29 â€¢ 0:05:45[0m [2;4m0.42it/s[0m  
        [0.5525, 0.4475],
        [0.5543, 0.4457],
        [0.5566, 0.4434],
        [0.5543, 0.4457],
        [0.5531, 0.4469],
        [0.5524, 0.4476],
        [0.5582, 0.4418],
        [0.5567, 0.4433],
[2K        [0.5545, 0.4455]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 58/203 [2m0:02:29 â€¢ 0:05:45[0m [2;4m0.42it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 58/203 [2m0:02:29 â€¢ 0:05:45[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.6259, 0.3741]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 58/203 [2m0:02:29 â€¢ 0:05:45[0m [2;4m0.42it/s[0m  
[2KStep 0 - TTA Loss: 0.9975â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 58/203 [2m0:02:29 â€¢ 0:05:45[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.6487, 0.3513]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 58/203 [2m0:02:29 â€¢ 0:05:45[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.6872, 0.3128]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 58/203 [2m0:02:29 â€¢ 0:05:45[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.7279, 0.2721]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 58/203 [2m0:02:29 â€¢ 0:05:45[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.7600, 0.2400]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 58/203 [2m0:02:29 â€¢ 0:05:45[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.7817, 0.2183]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 58/203 [2m0:02:29 â€¢ 0:05:45[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.7960, 0.2040]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 58/203 [2m0:02:29 â€¢ 0:05:45[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.8056, 0.1944]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 58/203 [2m0:02:29 â€¢ 0:05:45[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.8118, 0.1882]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 58/203 [2m0:02:29 â€¢ 0:05:45[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.8150, 0.1850]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 58/203 [2m0:02:29 â€¢ 0:05:45[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.5595, 0.4405]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 58/203 [2m0:02:29 â€¢ 0:05:45[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.7792, 0.2208],7mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 58/203 [2m0:02:29 â€¢ 0:05:45[0m [2;4m0.42it/s[0m  
        [0.6506, 0.3494],
        [0.6840, 0.3160],
        [0.7893, 0.2107],
        [0.7411, 0.2589],
        [0.7412, 0.2588],
        [0.7227, 0.2773],
        [0.7581, 0.2419],
        [0.7814, 0.2186],
[2K        [0.8162, 0.1838]], device='cuda:0')37mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 58/203 [2m0:02:29 â€¢ 0:05:45[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.5583, 0.4417],2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 59/203 [2m0:02:32 â€¢ 0:05:38[0m [2;4m0.43it/s[0m  
        [0.5525, 0.4475],
        [0.5543, 0.4457],
        [0.5567, 0.4433],
        [0.5544, 0.4456],
        [0.5531, 0.4469],
        [0.5524, 0.4476],
        [0.5583, 0.4417],
        [0.5567, 0.4433],
[2K        [0.5546, 0.4454]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 59/203 [2m0:02:32 â€¢ 0:05:38[0m [2;4m0.43it/s[0m  
[2KClass label: Billiardsâ”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 59/203 [2m0:02:32 â€¢ 0:05:38[0m [2;4m0.43it/s[0m  
[2KAttention weights: tensor([[0.5567, 0.4433]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 59/203 [2m0:02:32 â€¢ 0:05:38[0m [2;4m0.43it/s[0m  
[2KStep 0 - TTA Loss: 0.7995â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 59/203 [2m0:02:32 â€¢ 0:05:38[0m [2;4m0.43it/s[0m  
[2KAttention weights: tensor([[0.5567, 0.4433]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 59/203 [2m0:02:32 â€¢ 0:05:38[0m [2;4m0.43it/s[0m  
[2KAttention weights: tensor([[0.5573, 0.4427]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 59/203 [2m0:02:32 â€¢ 0:05:38[0m [2;4m0.43it/s[0m  
[2KAttention weights: tensor([[0.5593, 0.4407]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 59/203 [2m0:02:32 â€¢ 0:05:38[0m [2;4m0.43it/s[0m  
[2KAttention weights: tensor([[0.5634, 0.4366]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 59/203 [2m0:02:32 â€¢ 0:05:38[0m [2;4m0.43it/s[0m  
[2KAttention weights: tensor([[0.5694, 0.4306]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 59/203 [2m0:02:32 â€¢ 0:05:38[0m [2;4m0.43it/s[0m  
[2KAttention weights: tensor([[0.5766, 0.4234]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 59/203 [2m0:02:32 â€¢ 0:05:38[0m [2;4m0.43it/s[0m  
[2KAttention weights: tensor([[0.5823, 0.4177]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 59/203 [2m0:02:32 â€¢ 0:05:38[0m [2;4m0.43it/s[0m  
[2KAttention weights: tensor([[0.5837, 0.4163]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 59/203 [2m0:02:32 â€¢ 0:05:38[0m [2;4m0.43it/s[0m  
[2KAttention weights: tensor([[0.5814, 0.4186]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 59/203 [2m0:02:32 â€¢ 0:05:38[0m [2;4m0.43it/s[0m  
[2KAttention weights: tensor([[0.5527, 0.4473]], device='cuda:0');237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 59/203 [2m0:02:32 â€¢ 0:05:38[0m [2;4m0.43it/s[0m  
[2KAttention weights: tensor([[0.7029, 0.2971],;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 59/203 [2m0:02:32 â€¢ 0:05:38[0m [2;4m0.43it/s[0m  
        [0.5746, 0.4254],
        [0.6221, 0.3779],
        [0.7145, 0.2855],
        [0.6474, 0.3526],
        [0.6748, 0.3252],
        [0.6458, 0.3542],
        [0.7077, 0.2923],
        [0.7075, 0.2925],
[2K        [0.7265, 0.2735]], device='cuda:0')8;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 59/203 [2m0:02:32 â€¢ 0:05:38[0m [2;4m0.43it/s[0m  
[2KAttention weights: tensor([[0.5583, 0.4417],2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 60/203 [2m0:02:34 â€¢ 0:05:40[0m [2;4m0.42it/s[0m  
        [0.5525, 0.4475],
        [0.5543, 0.4457],
        [0.5567, 0.4433],
        [0.5544, 0.4456],
        [0.5531, 0.4469],
        [0.5524, 0.4476],
        [0.5583, 0.4417],
        [0.5567, 0.4433],
[2K        [0.5546, 0.4454]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 60/203 [2m0:02:34 â€¢ 0:05:40[0m [2;4m0.42it/s[0m  
[2KClass label: CleanAndJerkâ”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 60/203 [2m0:02:34 â€¢ 0:05:40[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.5675, 0.4325]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 60/203 [2m0:02:34 â€¢ 0:05:40[0m [2;4m0.42it/s[0m  
[2KStep 0 - TTA Loss: 0.4986â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 60/203 [2m0:02:34 â€¢ 0:05:40[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.5992, 0.4008]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 60/203 [2m0:02:34 â€¢ 0:05:40[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.6448, 0.3552]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 60/203 [2m0:02:34 â€¢ 0:05:40[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.6930, 0.3070]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 60/203 [2m0:02:34 â€¢ 0:05:40[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.7341, 0.2659]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 60/203 [2m0:02:34 â€¢ 0:05:40[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.7619, 0.2381]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 60/203 [2m0:02:34 â€¢ 0:05:40[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.7780, 0.2220]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 60/203 [2m0:02:34 â€¢ 0:05:40[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.7870, 0.2130]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 60/203 [2m0:02:34 â€¢ 0:05:40[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.7919, 0.2081]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 60/203 [2m0:02:34 â€¢ 0:05:40[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.7939, 0.2061]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 60/203 [2m0:02:34 â€¢ 0:05:40[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.5604, 0.4396]], device='cuda:0');237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 60/203 [2m0:02:34 â€¢ 0:05:40[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.7328, 0.2672],;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 60/203 [2m0:02:34 â€¢ 0:05:40[0m [2;4m0.42it/s[0m  
        [0.6097, 0.3903],
        [0.7945, 0.2055],
        [0.7406, 0.2594],
        [0.7150, 0.2850],
        [0.7324, 0.2676],
        [0.7001, 0.2999],
        [0.7388, 0.2612],
        [0.7340, 0.2660],
[2K        [0.7568, 0.2432]], device='cuda:0')8;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 60/203 [2m0:02:34 â€¢ 0:05:40[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.5584, 0.4416],;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 61/203 [2m0:02:40 â€¢ 0:06:21[0m [2;4m0.37it/s[0m   
        [0.5525, 0.4475],
        [0.5544, 0.4456],
        [0.5567, 0.4433],
        [0.5545, 0.4455],
        [0.5531, 0.4469],
        [0.5525, 0.4475],
        [0.5583, 0.4417],
        [0.5567, 0.4433],
[2K        [0.5547, 0.4453]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 61/203 [2m0:02:40 â€¢ 0:06:21[0m [2;4m0.37it/s[0m  
[2KClass label: Billiardsâ”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 61/203 [2m0:02:40 â€¢ 0:06:21[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5547, 0.4453]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 61/203 [2m0:02:40 â€¢ 0:06:21[0m [2;4m0.37it/s[0m  
[2KStep 0 - TTA Loss: 0.8032â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 61/203 [2m0:02:40 â€¢ 0:06:21[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5547, 0.4453]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 61/203 [2m0:02:40 â€¢ 0:06:21[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5539, 0.4461]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 61/203 [2m0:02:40 â€¢ 0:06:21[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5496, 0.4504]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 61/203 [2m0:02:40 â€¢ 0:06:21[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5376, 0.4624]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 61/203 [2m0:02:40 â€¢ 0:06:21[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5126, 0.4874]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 61/203 [2m0:02:40 â€¢ 0:06:21[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.4699, 0.5301]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 61/203 [2m0:02:40 â€¢ 0:06:21[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.4073, 0.5927]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 61/203 [2m0:02:40 â€¢ 0:06:21[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.3303, 0.6697]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 61/203 [2m0:02:40 â€¢ 0:06:21[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.2526, 0.7474]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 61/203 [2m0:02:40 â€¢ 0:06:21[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5444, 0.4556]], device='cuda:0')mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 61/203 [2m0:02:40 â€¢ 0:06:21[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5170, 0.4830],37mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 61/203 [2m0:02:40 â€¢ 0:06:21[0m [2;4m0.37it/s[0m  
        [0.1875, 0.8125],
        [0.4962, 0.5038],
        [0.5312, 0.4688],
        [0.4674, 0.5326],
        [0.4978, 0.5022],
        [0.4642, 0.5358],
        [0.5236, 0.4764],
        [0.4983, 0.5017],
[2K        [0.5395, 0.4605]], device='cuda:0')237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 61/203 [2m0:02:40 â€¢ 0:06:21[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5583, 0.4417],;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 62/203 [2m0:02:43 â€¢ 0:06:06[0m [2;4m0.39it/s[0m  
        [0.5524, 0.4476],
        [0.5544, 0.4456],
        [0.5567, 0.4433],
        [0.5545, 0.4455],
        [0.5531, 0.4469],
        [0.5525, 0.4475],
        [0.5583, 0.4417],
        [0.5567, 0.4433],
[2K        [0.5547, 0.4453]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 62/203 [2m0:02:43 â€¢ 0:06:06[0m [2;4m0.39it/s[0m  
[2KClass label: Billiardsâ”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 62/203 [2m0:02:43 â€¢ 0:06:06[0m [2;4m0.39it/s[0m  
[2KAttention weights: tensor([[0.5508, 0.4492]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 62/203 [2m0:02:43 â€¢ 0:06:06[0m [2;4m0.39it/s[0m  
[2KStep 0 - TTA Loss: 0.4173â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 62/203 [2m0:02:43 â€¢ 0:06:06[0m [2;4m0.39it/s[0m  
[2KAttention weights: tensor([[0.4590, 0.5410]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 62/203 [2m0:02:43 â€¢ 0:06:06[0m [2;4m0.39it/s[0m  
[2KAttention weights: tensor([[0.3731, 0.6269]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 62/203 [2m0:02:43 â€¢ 0:06:06[0m [2;4m0.39it/s[0m  
[2KAttention weights: tensor([[0.3020, 0.6980]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 62/203 [2m0:02:43 â€¢ 0:06:06[0m [2;4m0.39it/s[0m  
[2KAttention weights: tensor([[0.2508, 0.7492]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 62/203 [2m0:02:43 â€¢ 0:06:06[0m [2;4m0.39it/s[0m  
[2KAttention weights: tensor([[0.2178, 0.7822]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 62/203 [2m0:02:43 â€¢ 0:06:06[0m [2;4m0.39it/s[0m  
[2KAttention weights: tensor([[0.1980, 0.8020]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 62/203 [2m0:02:43 â€¢ 0:06:06[0m [2;4m0.39it/s[0m  
[2KAttention weights: tensor([[0.1873, 0.8127]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 62/203 [2m0:02:43 â€¢ 0:06:06[0m [2;4m0.39it/s[0m  
[2KAttention weights: tensor([[0.1823, 0.8177]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 62/203 [2m0:02:43 â€¢ 0:06:06[0m [2;4m0.39it/s[0m  
[2KAttention weights: tensor([[0.1807, 0.8193]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 62/203 [2m0:02:43 â€¢ 0:06:06[0m [2;4m0.39it/s[0m  
[2KAttention weights: tensor([[0.5440, 0.4560]], device='cuda:0')mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 62/203 [2m0:02:43 â€¢ 0:06:06[0m [2;4m0.39it/s[0m  
[2KAttention weights: tensor([[0.5053, 0.4947],37mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 62/203 [2m0:02:43 â€¢ 0:06:06[0m [2;4m0.39it/s[0m  
        [0.1803, 0.8197],
        [0.4698, 0.5302],
        [0.5130, 0.4870],
        [0.4514, 0.5486],
        [0.4808, 0.5192],
        [0.4496, 0.5504],
        [0.5125, 0.4875],
        [0.4837, 0.5163],
[2K        [0.5221, 0.4779]], device='cuda:0')237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 62/203 [2m0:02:43 â€¢ 0:06:06[0m [2;4m0.39it/s[0m  
[2KAttention weights: tensor([[0.5583, 0.4417],;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 63/203 [2m0:02:48 â€¢ 0:07:40[0m [2;4m0.30it/s[0m  
        [0.5523, 0.4477],
        [0.5544, 0.4456],
        [0.5566, 0.4434],
        [0.5544, 0.4456],
        [0.5531, 0.4469],
        [0.5524, 0.4476],
        [0.5582, 0.4418],
        [0.5567, 0.4433],
[2K        [0.5547, 0.4453]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 63/203 [2m0:02:48 â€¢ 0:07:40[0m [2;4m0.30it/s[0m  
[2KClass label: CleanAndJerkâ”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 63/203 [2m0:02:48 â€¢ 0:07:40[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.5768, 0.4232]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 63/203 [2m0:02:48 â€¢ 0:07:40[0m [2;4m0.30it/s[0m  
[2KStep 0 - TTA Loss: 0.4313â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 63/203 [2m0:02:48 â€¢ 0:07:40[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.5768, 0.4232]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 63/203 [2m0:02:48 â€¢ 0:07:40[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.5771, 0.4229]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 63/203 [2m0:02:48 â€¢ 0:07:40[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.5802, 0.4198]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 63/203 [2m0:02:48 â€¢ 0:07:40[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.5878, 0.4122]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 63/203 [2m0:02:48 â€¢ 0:07:40[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.6048, 0.3952]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 63/203 [2m0:02:48 â€¢ 0:07:40[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.6265, 0.3735]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 63/203 [2m0:02:48 â€¢ 0:07:40[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.6493, 0.3507]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 63/203 [2m0:02:48 â€¢ 0:07:40[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.6724, 0.3276]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 63/203 [2m0:02:48 â€¢ 0:07:40[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.7003, 0.2997]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 63/203 [2m0:02:48 â€¢ 0:07:40[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.5584, 0.4416]], device='cuda:0')mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 63/203 [2m0:02:48 â€¢ 0:07:40[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.6847, 0.3153],37mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 63/203 [2m0:02:48 â€¢ 0:07:40[0m [2;4m0.30it/s[0m  
        [0.5824, 0.4176],
        [0.7478, 0.2522],
        [0.6929, 0.3071],
        [0.6512, 0.3488],
        [0.6833, 0.3167],
        [0.6436, 0.3564],
        [0.7082, 0.2918],
        [0.6869, 0.3131],
[2K        [0.6902, 0.3098]], device='cuda:0')237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 63/203 [2m0:02:48 â€¢ 0:07:40[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.5583, 0.4417],;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 64/203 [2m0:02:53 â€¢ 0:08:20[0m [2;4m0.28it/s[0m  
        [0.5523, 0.4477],
        [0.5544, 0.4456],
        [0.5566, 0.4434],
        [0.5544, 0.4456],
        [0.5531, 0.4469],
        [0.5524, 0.4476],
        [0.5582, 0.4418],
        [0.5567, 0.4433],
[2K        [0.5547, 0.4453]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 64/203 [2m0:02:53 â€¢ 0:08:20[0m [2;4m0.28it/s[0m  
[2KClass label: BaseballPitchâ”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 64/203 [2m0:02:53 â€¢ 0:08:20[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6374, 0.3626]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 64/203 [2m0:02:53 â€¢ 0:08:20[0m [2;4m0.28it/s[0m  
[2KStep 0 - TTA Loss: 0.4640â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 64/203 [2m0:02:53 â€¢ 0:08:20[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6739, 0.3261]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 64/203 [2m0:02:53 â€¢ 0:08:20[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.7213, 0.2787]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 64/203 [2m0:02:53 â€¢ 0:08:20[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.7632, 0.2368]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 64/203 [2m0:02:53 â€¢ 0:08:20[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.7940, 0.2060]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 64/203 [2m0:02:53 â€¢ 0:08:20[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.8106, 0.1894]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 64/203 [2m0:02:53 â€¢ 0:08:20[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.8185, 0.1815]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 64/203 [2m0:02:53 â€¢ 0:08:20[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.8201, 0.1799]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 64/203 [2m0:02:53 â€¢ 0:08:20[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.8184, 0.1816]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 64/203 [2m0:02:53 â€¢ 0:08:20[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.8160, 0.1840]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 64/203 [2m0:02:53 â€¢ 0:08:20[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.5628, 0.4372]], device='cuda:0')5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 64/203 [2m0:02:53 â€¢ 0:08:20[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.8153, 0.1847],8;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 64/203 [2m0:02:53 â€¢ 0:08:20[0m [2;4m0.28it/s[0m  
        [0.6586, 0.3414],
        [0.8003, 0.1997],
        [0.7574, 0.2426],
        [0.7427, 0.2573],
        [0.7491, 0.2509],
        [0.7088, 0.2912],
        [0.7849, 0.2151],
        [0.7547, 0.2453],
[2K        [0.7590, 0.2410]], device='cuda:0')98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 64/203 [2m0:02:53 â€¢ 0:08:20[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.5583, 0.4417],;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 65/203 [2m0:02:57 â€¢ 0:09:02[0m [2;4m0.25it/s[0m  
        [0.5523, 0.4477],
        [0.5545, 0.4455],
        [0.5566, 0.4434],
        [0.5545, 0.4455],
        [0.5531, 0.4469],
        [0.5524, 0.4476],
        [0.5583, 0.4417],
        [0.5567, 0.4433],
[2K        [0.5547, 0.4453]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 65/203 [2m0:02:57 â€¢ 0:09:02[0m [2;4m0.25it/s[0m  
[2KClass label: BaseballPitchâ”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 65/203 [2m0:02:57 â€¢ 0:09:02[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6391, 0.3609]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 65/203 [2m0:02:57 â€¢ 0:09:02[0m [2;4m0.25it/s[0m  
[2KStep 0 - TTA Loss: 0.8118â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 65/203 [2m0:02:57 â€¢ 0:09:02[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6391, 0.3609]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 65/203 [2m0:02:57 â€¢ 0:09:02[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6387, 0.3613]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 65/203 [2m0:02:57 â€¢ 0:09:02[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6378, 0.3622]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 65/203 [2m0:02:57 â€¢ 0:09:02[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6384, 0.3616]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 65/203 [2m0:02:57 â€¢ 0:09:02[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6430, 0.3570]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 65/203 [2m0:02:57 â€¢ 0:09:02[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6539, 0.3461]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 65/203 [2m0:02:57 â€¢ 0:09:02[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6673, 0.3327]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 65/203 [2m0:02:57 â€¢ 0:09:02[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6786, 0.3214]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 65/203 [2m0:02:57 â€¢ 0:09:02[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6842, 0.3158]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 65/203 [2m0:02:57 â€¢ 0:09:02[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.5584, 0.4416]], device='cuda:0')5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 65/203 [2m0:02:57 â€¢ 0:09:02[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6791, 0.3209],8;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 65/203 [2m0:02:57 â€¢ 0:09:02[0m [2;4m0.25it/s[0m  
        [0.5810, 0.4190],
        [0.6260, 0.3740],
        [0.6760, 0.3240],
        [0.6006, 0.3994],
        [0.6344, 0.3656],
        [0.5951, 0.4049],
        [0.6945, 0.3055],
        [0.6675, 0.3325],
[2K        [0.6547, 0.3453]], device='cuda:0')98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 65/203 [2m0:02:57 â€¢ 0:09:02[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.5583, 0.4417],8;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 66/203 [2m0:02:58 â€¢ 0:08:03[0m [2;4m0.28it/s[0m   
        [0.5523, 0.4477],
        [0.5545, 0.4455],
        [0.5567, 0.4433],
        [0.5545, 0.4455],
        [0.5531, 0.4469],
        [0.5524, 0.4476],
        [0.5583, 0.4417],
        [0.5567, 0.4433],
[2K        [0.5547, 0.4453]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 66/203 [2m0:02:58 â€¢ 0:08:03[0m [2;4m0.28it/s[0m  
[2KClass label: SoccerPenaltyâ”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 66/203 [2m0:02:58 â€¢ 0:08:03[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6668, 0.3332]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 66/203 [2m0:02:58 â€¢ 0:08:03[0m [2;4m0.28it/s[0m  
[2KStep 0 - TTA Loss: 0.8352â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 66/203 [2m0:02:58 â€¢ 0:08:03[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6692, 0.3308]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 66/203 [2m0:02:58 â€¢ 0:08:03[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6759, 0.3241]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 66/203 [2m0:02:58 â€¢ 0:08:03[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6825, 0.3175]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 66/203 [2m0:02:58 â€¢ 0:08:03[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6898, 0.3102]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 66/203 [2m0:02:58 â€¢ 0:08:03[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6924, 0.3076]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 66/203 [2m0:02:58 â€¢ 0:08:03[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.7070, 0.2930]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 66/203 [2m0:02:58 â€¢ 0:08:03[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.7123, 0.2877]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 66/203 [2m0:02:58 â€¢ 0:08:03[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.7127, 0.2873]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 66/203 [2m0:02:58 â€¢ 0:08:03[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.7108, 0.2892]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 66/203 [2m0:02:58 â€¢ 0:08:03[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.5592, 0.4408]], device='cuda:0')7mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 66/203 [2m0:02:58 â€¢ 0:08:03[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6459, 0.3541],237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 66/203 [2m0:02:58 â€¢ 0:08:03[0m [2;4m0.28it/s[0m  
        [0.5764, 0.4236],
        [0.6050, 0.3950],
        [0.6607, 0.3393],
        [0.5790, 0.4210],
        [0.6266, 0.3734],
        [0.5849, 0.4151],
        [0.7100, 0.2900],
        [0.6556, 0.3444],
[2K        [0.6391, 0.3609]], device='cuda:0');237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 66/203 [2m0:02:58 â€¢ 0:08:03[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.5584, 0.4416],8;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 67/203 [2m0:02:59 â€¢ 0:07:20[0m [2;4m0.31it/s[0m  
        [0.5523, 0.4477],
        [0.5545, 0.4455],
        [0.5567, 0.4433],
        [0.5545, 0.4455],
        [0.5531, 0.4469],
        [0.5525, 0.4475],
        [0.5583, 0.4417],
        [0.5567, 0.4433],
[2K        [0.5547, 0.4453]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 67/203 [2m0:02:59 â€¢ 0:07:20[0m [2;4m0.31it/s[0m  
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 67/203 [2m0:02:59 â€¢ 0:07:20[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5986, 0.4014]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 67/203 [2m0:02:59 â€¢ 0:07:20[0m [2;4m0.31it/s[0m  
[2KStep 0 - TTA Loss: 0.6518â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 67/203 [2m0:02:59 â€¢ 0:07:20[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5986, 0.4014]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 67/203 [2m0:02:59 â€¢ 0:07:20[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5990, 0.4010]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 67/203 [2m0:02:59 â€¢ 0:07:20[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.6023, 0.3977]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 67/203 [2m0:02:59 â€¢ 0:07:20[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.6124, 0.3876]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 67/203 [2m0:02:59 â€¢ 0:07:20[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.6346, 0.3654]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 67/203 [2m0:02:59 â€¢ 0:07:20[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.6693, 0.3307]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 67/203 [2m0:02:59 â€¢ 0:07:20[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.7100, 0.2900]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 67/203 [2m0:02:59 â€¢ 0:07:20[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.7486, 0.2514]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 67/203 [2m0:02:59 â€¢ 0:07:20[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.7838, 0.2162]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 67/203 [2m0:02:59 â€¢ 0:07:20[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5589, 0.4411]], device='cuda:0')7mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 67/203 [2m0:02:59 â€¢ 0:07:20[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.7645, 0.2355],237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 67/203 [2m0:02:59 â€¢ 0:07:20[0m [2;4m0.31it/s[0m  
        [0.6671, 0.3329],
        [0.7317, 0.2683],
        [0.7630, 0.2370],
        [0.7503, 0.2497],
        [0.8265, 0.1735],
        [0.7699, 0.2301],
        [0.7847, 0.2153],
        [0.7842, 0.2158],
[2K        [0.7893, 0.2107]], device='cuda:0');237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 67/203 [2m0:02:59 â€¢ 0:07:20[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5584, 0.4416],8;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 68/203 [2m0:03:16 â€¢ 0:12:26[0m [2;4m0.18it/s[0m  
        [0.5524, 0.4476],
        [0.5545, 0.4455],
        [0.5567, 0.4433],
        [0.5545, 0.4455],
        [0.5532, 0.4468],
        [0.5525, 0.4475],
        [0.5583, 0.4417],
        [0.5567, 0.4433],
[2K        [0.5547, 0.4453]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 68/203 [2m0:03:16 â€¢ 0:12:26[0m [2;4m0.18it/s[0m  
[2KClass label: TennisSwingâ”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 68/203 [2m0:03:16 â€¢ 0:12:26[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.6410, 0.3590]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 68/203 [2m0:03:16 â€¢ 0:12:26[0m [2;4m0.18it/s[0m  
[2KStep 0 - TTA Loss: 0.7032â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 68/203 [2m0:03:16 â€¢ 0:12:26[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.6839, 0.3161]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 68/203 [2m0:03:16 â€¢ 0:12:26[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.7298, 0.2702]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 68/203 [2m0:03:16 â€¢ 0:12:26[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.7714, 0.2286]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 68/203 [2m0:03:16 â€¢ 0:12:26[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.8013, 0.1987]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 68/203 [2m0:03:16 â€¢ 0:12:26[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.8158, 0.1842]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 68/203 [2m0:03:16 â€¢ 0:12:26[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.8185, 0.1815]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 68/203 [2m0:03:16 â€¢ 0:12:26[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.8129, 0.1871]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 68/203 [2m0:03:16 â€¢ 0:12:26[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.8071, 0.1929]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 68/203 [2m0:03:16 â€¢ 0:12:26[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.8035, 0.1965]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 68/203 [2m0:03:16 â€¢ 0:12:26[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.5612, 0.4388]], device='cuda:0')7mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 68/203 [2m0:03:16 â€¢ 0:12:26[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.7766, 0.2234],237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 68/203 [2m0:03:16 â€¢ 0:12:26[0m [2;4m0.18it/s[0m  
        [0.6806, 0.3194],
        [0.7349, 0.2651],
        [0.7748, 0.2252],
        [0.7564, 0.2436],
        [0.8260, 0.1740],
        [0.7708, 0.2292],
        [0.8006, 0.1994],
        [0.8024, 0.1976],
[2K        [0.7967, 0.2033]], device='cuda:0');237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 68/203 [2m0:03:16 â€¢ 0:12:26[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.5584, 0.4416],8;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 69/203 [2m0:03:17 â€¢ 0:10:44[0m [2;4m0.21it/s[0m  
        [0.5524, 0.4476],
        [0.5545, 0.4455],
        [0.5567, 0.4433],
        [0.5545, 0.4455],
        [0.5532, 0.4468],
        [0.5525, 0.4475],
        [0.5583, 0.4417],
        [0.5568, 0.4432],
[2K        [0.5548, 0.4452]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 69/203 [2m0:03:17 â€¢ 0:10:44[0m [2;4m0.21it/s[0m  
[2KClass label: Billiardsâ”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 69/203 [2m0:03:17 â€¢ 0:10:44[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.5558, 0.4442]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 69/203 [2m0:03:17 â€¢ 0:10:44[0m [2;4m0.21it/s[0m  
[2KStep 0 - TTA Loss: 1.0610â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 69/203 [2m0:03:17 â€¢ 0:10:44[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.5558, 0.4442]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 69/203 [2m0:03:17 â€¢ 0:10:44[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.5556, 0.4444]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 69/203 [2m0:03:17 â€¢ 0:10:44[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.5562, 0.4438]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 69/203 [2m0:03:17 â€¢ 0:10:44[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.5598, 0.4402]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 69/203 [2m0:03:17 â€¢ 0:10:44[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.5686, 0.4314]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 69/203 [2m0:03:17 â€¢ 0:10:44[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.5862, 0.4138]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 69/203 [2m0:03:17 â€¢ 0:10:44[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.6094, 0.3906]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 69/203 [2m0:03:17 â€¢ 0:10:44[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.6397, 0.3603]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 69/203 [2m0:03:17 â€¢ 0:10:44[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.7011, 0.2989]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 69/203 [2m0:03:17 â€¢ 0:10:44[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.5578, 0.4422]], device='cuda:0');5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 69/203 [2m0:03:17 â€¢ 0:10:44[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.5584, 0.4416],8;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 70/203 [2m0:03:20 â€¢ 0:09:42[0m [2;4m0.23it/s[0m  
        [0.5524, 0.4476],
        [0.5545, 0.4455],
        [0.5567, 0.4433],
        [0.5545, 0.4455],
        [0.5532, 0.4468],
        [0.5525, 0.4475],
        [0.5583, 0.4417],
        [0.5567, 0.4433],
[2K        [0.5548, 0.4452]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 70/203 [2m0:03:20 â€¢ 0:09:42[0m [2;4m0.23it/s[0m  
[2KClass label: HammerThrowâ”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 70/203 [2m0:03:20 â€¢ 0:09:42[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.5616, 0.4384]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 70/203 [2m0:03:20 â€¢ 0:09:42[0m [2;4m0.23it/s[0m  
[2KStep 0 - TTA Loss: 0.5476â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 70/203 [2m0:03:20 â€¢ 0:09:42[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.6038, 0.3962]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 70/203 [2m0:03:20 â€¢ 0:09:42[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.6614, 0.3386]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 70/203 [2m0:03:20 â€¢ 0:09:42[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.7224, 0.2776]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 70/203 [2m0:03:20 â€¢ 0:09:42[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.7746, 0.2254]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 70/203 [2m0:03:20 â€¢ 0:09:42[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.8102, 0.1898]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 70/203 [2m0:03:20 â€¢ 0:09:42[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.8300, 0.1700]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 70/203 [2m0:03:20 â€¢ 0:09:42[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.8392, 0.1608]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 70/203 [2m0:03:20 â€¢ 0:09:42[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.8421, 0.1579]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 70/203 [2m0:03:20 â€¢ 0:09:42[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.8433, 0.1567]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 70/203 [2m0:03:20 â€¢ 0:09:42[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.5637, 0.4363]], device='cuda:0');5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 70/203 [2m0:03:20 â€¢ 0:09:42[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.8565, 0.1435],98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 70/203 [2m0:03:20 â€¢ 0:09:42[0m [2;4m0.23it/s[0m  
        [0.9072, 0.0928],
        [0.8023, 0.1977],
        [0.8234, 0.1766],
        [0.8435, 0.1565],
        [0.8326, 0.1674],
        [0.7994, 0.2006],
        [0.8647, 0.1353],
        [0.8162, 0.1838],
[2K        [0.8297, 0.1703]], device='cuda:0');98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 70/203 [2m0:03:20 â€¢ 0:09:42[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.5585, 0.4415],8;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 71/203 [2m0:03:23 â€¢ 0:09:24[0m [2;4m0.23it/s[0m  
        [0.5526, 0.4474],
        [0.5546, 0.4454],
        [0.5568, 0.4432],
        [0.5547, 0.4453],
        [0.5533, 0.4467],
        [0.5527, 0.4473],
        [0.5584, 0.4416],
        [0.5568, 0.4432],
[2K        [0.5549, 0.4451]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 71/203 [2m0:03:23 â€¢ 0:09:24[0m [2;4m0.23it/s[0m  
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 71/203 [2m0:03:23 â€¢ 0:09:24[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.6027, 0.3973]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 71/203 [2m0:03:23 â€¢ 0:09:24[0m [2;4m0.23it/s[0m  
[2KStep 0 - TTA Loss: 0.6249â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 71/203 [2m0:03:23 â€¢ 0:09:24[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.6027, 0.3973]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 71/203 [2m0:03:23 â€¢ 0:09:24[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.6035, 0.3965]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 71/203 [2m0:03:23 â€¢ 0:09:24[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.6067, 0.3933]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 71/203 [2m0:03:23 â€¢ 0:09:24[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.6162, 0.3838]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 71/203 [2m0:03:23 â€¢ 0:09:24[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.6313, 0.3687]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 71/203 [2m0:03:23 â€¢ 0:09:24[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.6527, 0.3473]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 71/203 [2m0:03:23 â€¢ 0:09:24[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.6801, 0.3199]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 71/203 [2m0:03:23 â€¢ 0:09:24[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.7046, 0.2954]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 71/203 [2m0:03:23 â€¢ 0:09:24[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.7241, 0.2759]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 71/203 [2m0:03:23 â€¢ 0:09:24[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.5566, 0.4434]], device='cuda:0');5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 71/203 [2m0:03:23 â€¢ 0:09:24[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.7303, 0.2697],98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 71/203 [2m0:03:23 â€¢ 0:09:24[0m [2;4m0.23it/s[0m  
        [0.6924, 0.3076],
        [0.6736, 0.3264],
        [0.7322, 0.2678],
        [0.6784, 0.3216],
        [0.7471, 0.2529],
        [0.6938, 0.3062],
        [0.7689, 0.2311],
        [0.7382, 0.2618],
[2K        [0.7304, 0.2696]], device='cuda:0');98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 71/203 [2m0:03:23 â€¢ 0:09:24[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.5585, 0.4415],38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 72/203 [2m0:03:28 â€¢ 0:12:53[0m [2;4m0.17it/s[0m   
        [0.5526, 0.4474],
        [0.5547, 0.4453],
        [0.5568, 0.4432],
        [0.5547, 0.4453],
        [0.5534, 0.4466],
        [0.5527, 0.4473],
        [0.5585, 0.4415],
        [0.5568, 0.4432],
[2K        [0.5549, 0.4451]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 72/203 [2m0:03:28 â€¢ 0:12:53[0m [2;4m0.17it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 72/203 [2m0:03:28 â€¢ 0:12:53[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.6246, 0.3754]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 72/203 [2m0:03:28 â€¢ 0:12:53[0m [2;4m0.17it/s[0m  
[2KStep 0 - TTA Loss: 0.6037â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 72/203 [2m0:03:28 â€¢ 0:12:53[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.6728, 0.3272]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 72/203 [2m0:03:28 â€¢ 0:12:53[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.7410, 0.2590]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 72/203 [2m0:03:28 â€¢ 0:12:53[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.8146, 0.1854]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 72/203 [2m0:03:28 â€¢ 0:12:53[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.8760, 0.1240]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 72/203 [2m0:03:28 â€¢ 0:12:53[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.9168, 0.0832]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 72/203 [2m0:03:28 â€¢ 0:12:53[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.9398, 0.0602]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 72/203 [2m0:03:28 â€¢ 0:12:53[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.9513, 0.0487]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 72/203 [2m0:03:28 â€¢ 0:12:53[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.9565, 0.0435]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 72/203 [2m0:03:28 â€¢ 0:12:53[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.9584, 0.0416]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 72/203 [2m0:03:28 â€¢ 0:12:53[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.5699, 0.4301]], device='cuda:0')37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 72/203 [2m0:03:28 â€¢ 0:12:53[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.9197, 0.0803],;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 72/203 [2m0:03:28 â€¢ 0:12:53[0m [2;4m0.17it/s[0m  
        [0.8295, 0.1705],
        [0.8534, 0.1466],
        [0.9261, 0.0739],
        [0.9195, 0.0805],
        [0.9283, 0.0717],
        [0.9176, 0.0824],
        [0.8999, 0.1001],
        [0.9294, 0.0706],
[2K        [0.9589, 0.0411]], device='cuda:0')5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 72/203 [2m0:03:28 â€¢ 0:12:53[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.5586, 0.4414],38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 73/203 [2m0:03:36 â€¢ 0:08:35[0m [2;4m0.25it/s[0m  
        [0.5527, 0.4473],
        [0.5548, 0.4452],
        [0.5569, 0.4431],
        [0.5549, 0.4451],
        [0.5535, 0.4465],
        [0.5528, 0.4472],
        [0.5586, 0.4414],
        [0.5570, 0.4430],
[2K        [0.5551, 0.4449]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 73/203 [2m0:03:36 â€¢ 0:08:35[0m [2;4m0.25it/s[0m  
[2KClass label: GolfSwingâ”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 73/203 [2m0:03:36 â€¢ 0:08:35[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6480, 0.3520]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 73/203 [2m0:03:36 â€¢ 0:08:35[0m [2;4m0.25it/s[0m  
[2KStep 0 - TTA Loss: 0.7321â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 73/203 [2m0:03:36 â€¢ 0:08:35[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6480, 0.3520]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 73/203 [2m0:03:36 â€¢ 0:08:35[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6495, 0.3505]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 73/203 [2m0:03:36 â€¢ 0:08:35[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6541, 0.3459]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 73/203 [2m0:03:36 â€¢ 0:08:35[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6624, 0.3376]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 73/203 [2m0:03:36 â€¢ 0:08:35[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6755, 0.3245]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 73/203 [2m0:03:36 â€¢ 0:08:35[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6924, 0.3076]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 73/203 [2m0:03:36 â€¢ 0:08:35[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.7060, 0.2940]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 73/203 [2m0:03:36 â€¢ 0:08:35[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.7091, 0.2909]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 73/203 [2m0:03:36 â€¢ 0:08:35[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6930, 0.3070]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 73/203 [2m0:03:36 â€¢ 0:08:35[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.5565, 0.4435]], device='cuda:0')37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 73/203 [2m0:03:36 â€¢ 0:08:35[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.7037, 0.2963],;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 73/203 [2m0:03:36 â€¢ 0:08:35[0m [2;4m0.25it/s[0m  
        [0.6011, 0.3989],
        [0.6389, 0.3611],
        [0.6618, 0.3382],
        [0.6650, 0.3350],
        [0.6962, 0.3038],
        [0.6697, 0.3303],
        [0.7164, 0.2836],
        [0.7059, 0.2941],
[2K        [0.7400, 0.2600]], device='cuda:0')5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 73/203 [2m0:03:36 â€¢ 0:08:35[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.5587, 0.4413],38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 74/203 [2m0:03:37 â€¢ 0:07:38[0m [2;4m0.28it/s[0m  
        [0.5528, 0.4472],
        [0.5549, 0.4451],
        [0.5569, 0.4431],
        [0.5549, 0.4451],
        [0.5536, 0.4464],
        [0.5529, 0.4471],
        [0.5586, 0.4414],
        [0.5570, 0.4430],
[2K        [0.5552, 0.4448]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 74/203 [2m0:03:37 â€¢ 0:07:38[0m [2;4m0.28it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 74/203 [2m0:03:37 â€¢ 0:07:38[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6289, 0.3711]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 74/203 [2m0:03:37 â€¢ 0:07:38[0m [2;4m0.28it/s[0m  
[2KStep 0 - TTA Loss: 0.7721â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 74/203 [2m0:03:37 â€¢ 0:07:38[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6354, 0.3646]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 74/203 [2m0:03:37 â€¢ 0:07:38[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6477, 0.3523]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 74/203 [2m0:03:37 â€¢ 0:07:38[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6605, 0.3395]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 74/203 [2m0:03:37 â€¢ 0:07:38[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6726, 0.3274]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 74/203 [2m0:03:37 â€¢ 0:07:38[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6807, 0.3193]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 74/203 [2m0:03:37 â€¢ 0:07:38[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6828, 0.3172]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 74/203 [2m0:03:37 â€¢ 0:07:38[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6787, 0.3213]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 74/203 [2m0:03:37 â€¢ 0:07:38[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6741, 0.3259]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 74/203 [2m0:03:37 â€¢ 0:07:38[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6723, 0.3277]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 74/203 [2m0:03:37 â€¢ 0:07:38[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.5566, 0.4434]], device='cuda:0')8;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 74/203 [2m0:03:37 â€¢ 0:07:38[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6406, 0.3594],;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 74/203 [2m0:03:37 â€¢ 0:07:38[0m [2;4m0.28it/s[0m  
        [0.5494, 0.4506],
        [0.5977, 0.4023],
        [0.5492, 0.4508],
        [0.6036, 0.3964],
        [0.6440, 0.3560],
        [0.6133, 0.3867],
        [0.6727, 0.3273],
        [0.6377, 0.3623],
[2K        [0.6719, 0.3281]], device='cuda:0')2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 74/203 [2m0:03:37 â€¢ 0:07:38[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.5587, 0.4413],38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 75/203 [2m0:03:39 â€¢ 0:06:59[0m [2;4m0.31it/s[0m  
        [0.5528, 0.4472],
        [0.5549, 0.4451],
        [0.5570, 0.4430],
        [0.5550, 0.4450],
        [0.5536, 0.4464],
        [0.5529, 0.4471],
        [0.5586, 0.4414],
        [0.5570, 0.4430],
[2K        [0.5553, 0.4447]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 75/203 [2m0:03:39 â€¢ 0:06:59[0m [2;4m0.31it/s[0m  
[2KClass label: BaseballPitchâ”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 75/203 [2m0:03:39 â€¢ 0:06:59[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.6360, 0.3640]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 75/203 [2m0:03:39 â€¢ 0:06:59[0m [2;4m0.31it/s[0m  
[2KStep 0 - TTA Loss: 0.8722â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 75/203 [2m0:03:39 â€¢ 0:06:59[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.6360, 0.3640]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 75/203 [2m0:03:39 â€¢ 0:06:59[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.6375, 0.3625]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 75/203 [2m0:03:39 â€¢ 0:06:59[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.6462, 0.3538]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 75/203 [2m0:03:39 â€¢ 0:06:59[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.6689, 0.3311]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 75/203 [2m0:03:39 â€¢ 0:06:59[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.7104, 0.2896]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 75/203 [2m0:03:39 â€¢ 0:06:59[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.7673, 0.2327]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 75/203 [2m0:03:39 â€¢ 0:06:59[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.8231, 0.1769]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 75/203 [2m0:03:39 â€¢ 0:06:59[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.8652, 0.1348]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 75/203 [2m0:03:39 â€¢ 0:06:59[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.9150, 0.0850]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 75/203 [2m0:03:39 â€¢ 0:06:59[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5703, 0.4297]], device='cuda:0')8;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 75/203 [2m0:03:39 â€¢ 0:06:59[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.9807, 0.0193],;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 75/203 [2m0:03:39 â€¢ 0:06:59[0m [2;4m0.31it/s[0m  
        [0.7907, 0.2093],
        [0.7568, 0.2432],
        [0.8728, 0.1272],
        [0.8949, 0.1051],
        [0.8572, 0.1428],
        [0.8144, 0.1856],
        [0.9247, 0.0753],
        [0.8795, 0.1205],
[2K        [0.8742, 0.1258]], device='cuda:0')2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 75/203 [2m0:03:39 â€¢ 0:06:59[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5589, 0.4411],38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 76/203 [2m0:03:40 â€¢ 0:06:26[0m [2;4m0.33it/s[0m  
        [0.5529, 0.4471],
        [0.5550, 0.4450],
        [0.5571, 0.4429],
        [0.5551, 0.4449],
        [0.5537, 0.4463],
        [0.5530, 0.4470],
        [0.5587, 0.4413],
        [0.5571, 0.4429],
[2K        [0.5554, 0.4446]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 76/203 [2m0:03:40 â€¢ 0:06:26[0m [2;4m0.33it/s[0m  
[2KClass label: Billiardsâ”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 76/203 [2m0:03:40 â€¢ 0:06:26[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.5558, 0.4442]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 76/203 [2m0:03:40 â€¢ 0:06:26[0m [2;4m0.33it/s[0m  
[2KStep 0 - TTA Loss: 0.5354â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 76/203 [2m0:03:40 â€¢ 0:06:26[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.6720, 0.3280]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 76/203 [2m0:03:40 â€¢ 0:06:26[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.7597, 0.2403]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 76/203 [2m0:03:40 â€¢ 0:06:26[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.8100, 0.1900]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 76/203 [2m0:03:40 â€¢ 0:06:26[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.8326, 0.1674]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 76/203 [2m0:03:40 â€¢ 0:06:26[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.8390, 0.1610]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 76/203 [2m0:03:40 â€¢ 0:06:26[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.8385, 0.1615]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 76/203 [2m0:03:40 â€¢ 0:06:26[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.8376, 0.1624]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 76/203 [2m0:03:40 â€¢ 0:06:26[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.8383, 0.1617]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 76/203 [2m0:03:40 â€¢ 0:06:26[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.8390, 0.1610]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 76/203 [2m0:03:40 â€¢ 0:06:26[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.5612, 0.4388]], device='cuda:0')8;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 76/203 [2m0:03:40 â€¢ 0:06:26[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.9984, 0.0016],;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 76/203 [2m0:03:40 â€¢ 0:06:26[0m [2;4m0.33it/s[0m  
        [0.8394, 0.1606],
        [0.8460, 0.1540],
        [0.9513, 0.0487],
        [0.9724, 0.0276],
        [0.9428, 0.0572],
        [0.9147, 0.0853],
        [0.9775, 0.0225],
        [0.9528, 0.0472],
[2K        [0.9553, 0.0447]], device='cuda:0')2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 76/203 [2m0:03:40 â€¢ 0:06:26[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.5592, 0.4408],[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 77/203 [2m0:03:44 â€¢ 0:06:27[0m [2;4m0.33it/s[0m   
        [0.5530, 0.4470],
        [0.5551, 0.4449],
        [0.5572, 0.4428],
        [0.5553, 0.4447],
        [0.5538, 0.4462],
        [0.5532, 0.4468],
        [0.5589, 0.4411],
        [0.5573, 0.4427],
[2K        [0.5556, 0.4444]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 77/203 [2m0:03:44 â€¢ 0:06:27[0m [2;4m0.33it/s[0m  
[2KClass label: SoccerPenaltyâ”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 77/203 [2m0:03:44 â€¢ 0:06:27[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.6715, 0.3285]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 77/203 [2m0:03:44 â€¢ 0:06:27[0m [2;4m0.33it/s[0m  
[2KStep 0 - TTA Loss: 0.6210â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 77/203 [2m0:03:44 â€¢ 0:06:27[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.6715, 0.3285]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 77/203 [2m0:03:44 â€¢ 0:06:27[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.6739, 0.3261]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 77/203 [2m0:03:44 â€¢ 0:06:27[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.6841, 0.3159]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 77/203 [2m0:03:44 â€¢ 0:06:27[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.7074, 0.2926]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 77/203 [2m0:03:44 â€¢ 0:06:27[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.7457, 0.2543]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 77/203 [2m0:03:44 â€¢ 0:06:27[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.7937, 0.2063]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 77/203 [2m0:03:44 â€¢ 0:06:27[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.8347, 0.1653]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 77/203 [2m0:03:44 â€¢ 0:06:27[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.8673, 0.1327]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 77/203 [2m0:03:44 â€¢ 0:06:27[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.8960, 0.1040]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 77/203 [2m0:03:44 â€¢ 0:06:27[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.5669, 0.4331]], device='cuda:0')237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 77/203 [2m0:03:44 â€¢ 0:06:27[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.9239, 0.0761],5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 77/203 [2m0:03:44 â€¢ 0:06:27[0m [2;4m0.33it/s[0m  
        [0.7344, 0.2656],
        [0.7241, 0.2759],
        [0.8283, 0.1717],
        [0.8287, 0.1713],
        [0.8386, 0.1614],
        [0.7758, 0.2242],
        [0.9449, 0.0551],
        [0.8429, 0.1571],
[2K        [0.8214, 0.1786]], device='cuda:0');5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 77/203 [2m0:03:44 â€¢ 0:06:27[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.5592, 0.4408],[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 78/203 [2m0:03:45 â€¢ 0:06:06[0m [2;4m0.34it/s[0m  
        [0.5530, 0.4470],
        [0.5551, 0.4449],
        [0.5572, 0.4428],
        [0.5553, 0.4447],
        [0.5538, 0.4462],
        [0.5532, 0.4468],
        [0.5588, 0.4412],
        [0.5573, 0.4427],
[2K        [0.5556, 0.4444]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 78/203 [2m0:03:45 â€¢ 0:06:06[0m [2;4m0.34it/s[0m  
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 78/203 [2m0:03:45 â€¢ 0:06:06[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6007, 0.3993]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 78/203 [2m0:03:45 â€¢ 0:06:06[0m [2;4m0.34it/s[0m  
[2KStep 0 - TTA Loss: 0.6461â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 78/203 [2m0:03:45 â€¢ 0:06:06[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6883, 0.3117]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 78/203 [2m0:03:45 â€¢ 0:06:06[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.7709, 0.2291]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 78/203 [2m0:03:45 â€¢ 0:06:06[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.8376, 0.1624]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 78/203 [2m0:03:45 â€¢ 0:06:06[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.8797, 0.1203]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 78/203 [2m0:03:45 â€¢ 0:06:06[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.9033, 0.0967]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 78/203 [2m0:03:45 â€¢ 0:06:06[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.9144, 0.0856]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 78/203 [2m0:03:45 â€¢ 0:06:06[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.9202, 0.0798]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 78/203 [2m0:03:45 â€¢ 0:06:06[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.9227, 0.0773]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 78/203 [2m0:03:45 â€¢ 0:06:06[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.9236, 0.0764]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 78/203 [2m0:03:45 â€¢ 0:06:06[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.5660, 0.4340]], device='cuda:0')237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 78/203 [2m0:03:45 â€¢ 0:06:06[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.9478, 0.0522],5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 78/203 [2m0:03:45 â€¢ 0:06:06[0m [2;4m0.34it/s[0m  
        [0.8210, 0.1790],
        [0.8129, 0.1871],
        [0.8791, 0.1209],
        [0.9000, 0.1000],
        [0.9240, 0.0760],
        [0.8692, 0.1308],
        [0.9793, 0.0207],
        [0.9036, 0.0964],
[2K        [0.8864, 0.1136]], device='cuda:0');5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 78/203 [2m0:03:45 â€¢ 0:06:06[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.5592, 0.4408],[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 79/203 [2m0:03:53 â€¢ 0:07:43[0m [2;4m0.27it/s[0m  
        [0.5529, 0.4471],
        [0.5551, 0.4449],
        [0.5572, 0.4428],
        [0.5552, 0.4448],
        [0.5538, 0.4462],
        [0.5531, 0.4469],
        [0.5588, 0.4412],
        [0.5573, 0.4427],
[2K        [0.5556, 0.4444]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 79/203 [2m0:03:53 â€¢ 0:07:43[0m [2;4m0.27it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 79/203 [2m0:03:53 â€¢ 0:07:43[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6270, 0.3730]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 79/203 [2m0:03:53 â€¢ 0:07:43[0m [2;4m0.27it/s[0m  
[2KStep 0 - TTA Loss: 0.5509â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 79/203 [2m0:03:53 â€¢ 0:07:43[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6270, 0.3730]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 79/203 [2m0:03:53 â€¢ 0:07:43[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6271, 0.3729]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 79/203 [2m0:03:53 â€¢ 0:07:43[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6270, 0.3730]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 79/203 [2m0:03:53 â€¢ 0:07:43[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6253, 0.3747]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 79/203 [2m0:03:53 â€¢ 0:07:43[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6199, 0.3801]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 79/203 [2m0:03:53 â€¢ 0:07:43[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6082, 0.3918]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 79/203 [2m0:03:53 â€¢ 0:07:43[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.5887, 0.4113]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 79/203 [2m0:03:53 â€¢ 0:07:43[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.5676, 0.4324]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 79/203 [2m0:03:53 â€¢ 0:07:43[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.5596, 0.4404]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 79/203 [2m0:03:53 â€¢ 0:07:43[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.5537, 0.4463]], device='cuda:0')38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 79/203 [2m0:03:53 â€¢ 0:07:43[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6612, 0.3388],2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 79/203 [2m0:03:53 â€¢ 0:07:43[0m [2;4m0.27it/s[0m  
        [0.5647, 0.4353],
        [0.5533, 0.4467],
        [0.6293, 0.3707],
        [0.5464, 0.4536],
        [0.5986, 0.4014],
        [0.5414, 0.4586],
        [0.7342, 0.2658],
        [0.6285, 0.3715],
[2K        [0.5788, 0.4212]], device='cuda:0');2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 79/203 [2m0:03:53 â€¢ 0:07:43[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.5592, 0.4408],[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 80/203 [2m0:03:55 â€¢ 0:07:02[0m [2;4m0.29it/s[0m  
        [0.5529, 0.4471],
        [0.5550, 0.4450],
        [0.5572, 0.4428],
        [0.5551, 0.4449],
        [0.5537, 0.4463],
        [0.5531, 0.4469],
        [0.5588, 0.4412],
        [0.5572, 0.4428],
[2K        [0.5555, 0.4445]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 80/203 [2m0:03:55 â€¢ 0:07:02[0m [2;4m0.29it/s[0m  
[2KClass label: BaseballPitchâ”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 80/203 [2m0:03:55 â€¢ 0:07:02[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.6406, 0.3594]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 80/203 [2m0:03:55 â€¢ 0:07:02[0m [2;4m0.29it/s[0m  
[2KStep 0 - TTA Loss: 1.1180â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 80/203 [2m0:03:55 â€¢ 0:07:02[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.6896, 0.3104]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 80/203 [2m0:03:55 â€¢ 0:07:02[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.7331, 0.2669]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 80/203 [2m0:03:55 â€¢ 0:07:02[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.7849, 0.2151]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 80/203 [2m0:03:55 â€¢ 0:07:02[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.8256, 0.1744]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 80/203 [2m0:03:55 â€¢ 0:07:02[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.8541, 0.1459]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 80/203 [2m0:03:55 â€¢ 0:07:02[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.8723, 0.1277]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 80/203 [2m0:03:55 â€¢ 0:07:02[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.8802, 0.1198]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 80/203 [2m0:03:55 â€¢ 0:07:02[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.8817, 0.1183]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 80/203 [2m0:03:55 â€¢ 0:07:02[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.8810, 0.1190]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 80/203 [2m0:03:55 â€¢ 0:07:02[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.5674, 0.4326]], device='cuda:0')38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 80/203 [2m0:03:55 â€¢ 0:07:02[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.5591, 0.4409],[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 81/203 [2m0:03:56 â€¢ 0:06:24[0m [2;4m0.32it/s[0m  
        [0.5529, 0.4471],
        [0.5550, 0.4450],
        [0.5571, 0.4429],
        [0.5551, 0.4449],
        [0.5537, 0.4463],
        [0.5530, 0.4470],
        [0.5587, 0.4413],
        [0.5572, 0.4428],
[2K        [0.5554, 0.4446]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 81/203 [2m0:03:56 â€¢ 0:06:24[0m [2;4m0.32it/s[0m  
[2KClass label: TennisSwingâ”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 81/203 [2m0:03:56 â€¢ 0:06:24[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.6443, 0.3557]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 81/203 [2m0:03:56 â€¢ 0:06:24[0m [2;4m0.32it/s[0m  
[2KStep 0 - TTA Loss: 0.8178â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 81/203 [2m0:03:56 â€¢ 0:06:24[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.6443, 0.3557]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 81/203 [2m0:03:56 â€¢ 0:06:24[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.6436, 0.3564]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 81/203 [2m0:03:56 â€¢ 0:06:24[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.6409, 0.3591]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 81/203 [2m0:03:56 â€¢ 0:06:24[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.6354, 0.3646]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 81/203 [2m0:03:56 â€¢ 0:06:24[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.6262, 0.3738]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 81/203 [2m0:03:56 â€¢ 0:06:24[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.6129, 0.3871]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 81/203 [2m0:03:56 â€¢ 0:06:24[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.5946, 0.4054]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 81/203 [2m0:03:56 â€¢ 0:06:24[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.5728, 0.4272]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 81/203 [2m0:03:56 â€¢ 0:06:24[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.5464, 0.4536]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 81/203 [2m0:03:56 â€¢ 0:06:24[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.5528, 0.4472]], device='cuda:0')38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 81/203 [2m0:03:56 â€¢ 0:06:24[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.4562, 0.5438],2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 81/203 [2m0:03:56 â€¢ 0:06:24[0m [2;4m0.32it/s[0m  
        [0.4808, 0.5192],
        [0.5123, 0.4877],
        [0.5532, 0.4468],
        [0.4451, 0.5549],
        [0.5103, 0.4897],
        [0.4816, 0.5184],
        [0.5682, 0.4318],
        [0.5170, 0.4830],
[2K        [0.5277, 0.4723]], device='cuda:0');2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 81/203 [2m0:03:56 â€¢ 0:06:24[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.5590, 0.4410],[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 82/203 [2m0:03:57 â€¢ 0:05:53[0m [2;4m0.34it/s[0m   
        [0.5528, 0.4472],
        [0.5549, 0.4451],
        [0.5571, 0.4429],
        [0.5550, 0.4450],
        [0.5536, 0.4464],
        [0.5530, 0.4470],
        [0.5587, 0.4413],
        [0.5571, 0.4429],
[2K        [0.5554, 0.4446]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 82/203 [2m0:03:57 â€¢ 0:05:53[0m [2;4m0.34it/s[0m  
[2KClass label: SoccerPenaltyâ”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 82/203 [2m0:03:57 â€¢ 0:05:53[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6688, 0.3312]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 82/203 [2m0:03:57 â€¢ 0:05:53[0m [2;4m0.34it/s[0m  
[2KStep 0 - TTA Loss: 0.9140â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 82/203 [2m0:03:57 â€¢ 0:05:53[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6546, 0.3454]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 82/203 [2m0:03:57 â€¢ 0:05:53[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6471, 0.3529]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 82/203 [2m0:03:57 â€¢ 0:05:53[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6447, 0.3553]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 82/203 [2m0:03:57 â€¢ 0:05:53[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6453, 0.3547]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 82/203 [2m0:03:57 â€¢ 0:05:53[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6473, 0.3527]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 82/203 [2m0:03:57 â€¢ 0:05:53[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6483, 0.3517]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 82/203 [2m0:03:57 â€¢ 0:05:53[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6482, 0.3518]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 82/203 [2m0:03:57 â€¢ 0:05:53[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6481, 0.3519]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 82/203 [2m0:03:57 â€¢ 0:05:53[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6480, 0.3520]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 82/203 [2m0:03:57 â€¢ 0:05:53[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.5577, 0.4423]], device='cuda:0');237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 82/203 [2m0:03:57 â€¢ 0:05:53[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.5397, 0.4603],;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 82/203 [2m0:03:57 â€¢ 0:05:53[0m [2;4m0.34it/s[0m  
        [0.5150, 0.4850],
        [0.5404, 0.4596],
        [0.5886, 0.4114],
        [0.4969, 0.5031],
        [0.5543, 0.4457],
        [0.5166, 0.4834],
        [0.6480, 0.3520],
        [0.5539, 0.4461],
[2K        [0.5632, 0.4368]], device='cuda:0')8;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 82/203 [2m0:03:57 â€¢ 0:05:53[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.5589, 0.4411],[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 83/203 [2m0:03:58 â€¢ 0:04:32[0m [2;4m0.44it/s[0m  
        [0.5528, 0.4472],
        [0.5549, 0.4451],
        [0.5570, 0.4430],
        [0.5550, 0.4450],
        [0.5536, 0.4464],
        [0.5529, 0.4471],
        [0.5586, 0.4414],
        [0.5571, 0.4429],
[2K        [0.5553, 0.4447]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 83/203 [2m0:03:58 â€¢ 0:04:32[0m [2;4m0.44it/s[0m  
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 83/203 [2m0:03:58 â€¢ 0:04:32[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.5981, 0.4019]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 83/203 [2m0:03:58 â€¢ 0:04:32[0m [2;4m0.44it/s[0m  
[2KStep 0 - TTA Loss: 0.6586â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 83/203 [2m0:03:58 â€¢ 0:04:32[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.5981, 0.4019]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 83/203 [2m0:03:58 â€¢ 0:04:32[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.5987, 0.4013]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 83/203 [2m0:03:58 â€¢ 0:04:32[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.6016, 0.3984]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 83/203 [2m0:03:58 â€¢ 0:04:32[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.6108, 0.3892]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 83/203 [2m0:03:58 â€¢ 0:04:32[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.6261, 0.3739]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 83/203 [2m0:03:58 â€¢ 0:04:32[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.6503, 0.3497]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 83/203 [2m0:03:58 â€¢ 0:04:32[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.6864, 0.3136]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 83/203 [2m0:03:58 â€¢ 0:04:32[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.7227, 0.2773]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 83/203 [2m0:03:58 â€¢ 0:04:32[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.7579, 0.2421]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 83/203 [2m0:03:58 â€¢ 0:04:32[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.5581, 0.4419]], device='cuda:0');237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 83/203 [2m0:03:58 â€¢ 0:04:32[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.7487, 0.2513],;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 83/203 [2m0:03:58 â€¢ 0:04:32[0m [2;4m0.44it/s[0m  
        [0.6582, 0.3418],
        [0.7043, 0.2957],
        [0.7441, 0.2559],
        [0.7208, 0.2792],
        [0.7944, 0.2056],
        [0.7346, 0.2654],
        [0.7989, 0.2011],
        [0.7584, 0.2416],
[2K        [0.7607, 0.2393]], device='cuda:0')8;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 83/203 [2m0:03:58 â€¢ 0:04:32[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.5589, 0.4411],[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 84/203 [2m0:04:01 â€¢ 0:04:28[0m [2;4m0.44it/s[0m  
        [0.5528, 0.4472],
        [0.5549, 0.4451],
        [0.5570, 0.4430],
        [0.5550, 0.4450],
        [0.5536, 0.4464],
        [0.5529, 0.4471],
        [0.5586, 0.4414],
        [0.5571, 0.4429],
[2K        [0.5553, 0.4447]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 84/203 [2m0:04:01 â€¢ 0:04:28[0m [2;4m0.44it/s[0m  
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 84/203 [2m0:04:01 â€¢ 0:04:28[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.5995, 0.4005]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 84/203 [2m0:04:01 â€¢ 0:04:28[0m [2;4m0.44it/s[0m  
[2KStep 0 - TTA Loss: 0.6484â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 84/203 [2m0:04:01 â€¢ 0:04:28[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.6561, 0.3439]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 84/203 [2m0:04:01 â€¢ 0:04:28[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.7116, 0.2884]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 84/203 [2m0:04:01 â€¢ 0:04:28[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.7565, 0.2435]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 84/203 [2m0:04:01 â€¢ 0:04:28[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.7904, 0.2096]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 84/203 [2m0:04:01 â€¢ 0:04:28[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.8068, 0.1932]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 84/203 [2m0:04:01 â€¢ 0:04:28[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.8141, 0.1859]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 84/203 [2m0:04:01 â€¢ 0:04:28[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.8170, 0.1830]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 84/203 [2m0:04:01 â€¢ 0:04:28[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.8181, 0.1819]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 84/203 [2m0:04:01 â€¢ 0:04:28[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.8186, 0.1814]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 84/203 [2m0:04:01 â€¢ 0:04:28[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.5587, 0.4413]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 84/203 [2m0:04:01 â€¢ 0:04:28[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.7689, 0.2311],;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 84/203 [2m0:04:01 â€¢ 0:04:28[0m [2;4m0.44it/s[0m  
        [0.6745, 0.3255],
        [0.7232, 0.2768],
        [0.7626, 0.2374],
        [0.7471, 0.2529],
        [0.8188, 0.1812],
        [0.7600, 0.2400],
        [0.8151, 0.1849],
        [0.7797, 0.2203],
[2K        [0.7816, 0.2184]], device='cuda:0')8;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 84/203 [2m0:04:01 â€¢ 0:04:28[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.5589, 0.4411],[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 85/203 [2m0:04:03 â€¢ 0:04:26[0m [2;4m0.44it/s[0m  
        [0.5528, 0.4472],
        [0.5549, 0.4451],
        [0.5570, 0.4430],
        [0.5550, 0.4450],
        [0.5535, 0.4465],
        [0.5529, 0.4471],
        [0.5586, 0.4414],
        [0.5571, 0.4429],
[2K        [0.5553, 0.4447]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 85/203 [2m0:04:03 â€¢ 0:04:26[0m [2;4m0.44it/s[0m  
[2KClass label: TennisSwingâ”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 85/203 [2m0:04:03 â€¢ 0:04:26[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.6400, 0.3600]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 85/203 [2m0:04:03 â€¢ 0:04:26[0m [2;4m0.44it/s[0m  
[2KStep 0 - TTA Loss: 0.4855â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 85/203 [2m0:04:03 â€¢ 0:04:26[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.6400, 0.3600]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 85/203 [2m0:04:03 â€¢ 0:04:26[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.6408, 0.3592]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 85/203 [2m0:04:03 â€¢ 0:04:26[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.6451, 0.3549]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 85/203 [2m0:04:03 â€¢ 0:04:26[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.6546, 0.3454]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 85/203 [2m0:04:03 â€¢ 0:04:26[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.6738, 0.3262]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 85/203 [2m0:04:03 â€¢ 0:04:26[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.7014, 0.2986]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 85/203 [2m0:04:03 â€¢ 0:04:26[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.7327, 0.2673]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 85/203 [2m0:04:03 â€¢ 0:04:26[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.7590, 0.2410]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 85/203 [2m0:04:03 â€¢ 0:04:26[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.7782, 0.2218]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 85/203 [2m0:04:03 â€¢ 0:04:26[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.5599, 0.4401]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 85/203 [2m0:04:03 â€¢ 0:04:26[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.7296, 0.2704],;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 85/203 [2m0:04:03 â€¢ 0:04:26[0m [2;4m0.44it/s[0m  
        [0.6427, 0.3573],
        [0.6377, 0.3623],
        [0.7414, 0.2586],
        [0.6547, 0.3453],
        [0.6999, 0.3001],
        [0.6541, 0.3459],
        [0.7445, 0.2555],
        [0.7827, 0.2173],
[2K        [0.7237, 0.2763]], device='cuda:0')8;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 85/203 [2m0:04:03 â€¢ 0:04:26[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.5589, 0.4411],[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 86/203 [2m0:04:06 â€¢ 0:04:40[0m [2;4m0.42it/s[0m  
        [0.5528, 0.4472],
        [0.5549, 0.4451],
        [0.5571, 0.4429],
        [0.5550, 0.4450],
        [0.5536, 0.4464],
        [0.5529, 0.4471],
        [0.5586, 0.4414],
        [0.5571, 0.4429],
[2K        [0.5553, 0.4447]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 86/203 [2m0:04:06 â€¢ 0:04:40[0m [2;4m0.42it/s[0m  
[2KClass label: HammerThrowâ”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 86/203 [2m0:04:06 â€¢ 0:04:40[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.5580, 0.4420]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 86/203 [2m0:04:06 â€¢ 0:04:40[0m [2;4m0.42it/s[0m  
[2KStep 0 - TTA Loss: 0.5889â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 86/203 [2m0:04:06 â€¢ 0:04:40[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.5928, 0.4072]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 86/203 [2m0:04:06 â€¢ 0:04:40[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.6500, 0.3500]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 86/203 [2m0:04:06 â€¢ 0:04:40[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.7112, 0.2888]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 86/203 [2m0:04:06 â€¢ 0:04:40[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.7571, 0.2429]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 86/203 [2m0:04:06 â€¢ 0:04:40[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.7885, 0.2115]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 86/203 [2m0:04:06 â€¢ 0:04:40[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.8166, 0.1834]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 86/203 [2m0:04:06 â€¢ 0:04:40[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.8432, 0.1568]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 86/203 [2m0:04:06 â€¢ 0:04:40[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.8632, 0.1368]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 86/203 [2m0:04:06 â€¢ 0:04:40[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.8741, 0.1259]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 86/203 [2m0:04:06 â€¢ 0:04:40[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.5656, 0.4344]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 86/203 [2m0:04:06 â€¢ 0:04:40[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.8654, 0.1346],;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 86/203 [2m0:04:06 â€¢ 0:04:40[0m [2;4m0.42it/s[0m  
        [0.7152, 0.2848],
        [0.7885, 0.2115],
        [0.8162, 0.1838],
        [0.8774, 0.1226],
        [0.8384, 0.1616],
        [0.8003, 0.1997],
        [0.8501, 0.1499],
        [0.8292, 0.1708],
[2K        [0.8493, 0.1507]], device='cuda:0')8;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 86/203 [2m0:04:06 â€¢ 0:04:40[0m [2;4m0.42it/s[0m  
[2KAttention weights: tensor([[0.5590, 0.4410],m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 87/203 [2m0:04:08 â€¢ 0:04:46[0m [2;4m0.41it/s[0m   
        [0.5529, 0.4471],
        [0.5550, 0.4450],
        [0.5571, 0.4429],
        [0.5551, 0.4449],
        [0.5536, 0.4464],
        [0.5530, 0.4470],
        [0.5587, 0.4413],
        [0.5572, 0.4428],
[2K        [0.5554, 0.4446]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 87/203 [2m0:04:08 â€¢ 0:04:46[0m [2;4m0.41it/s[0m  
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 87/203 [2m0:04:08 â€¢ 0:04:46[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.6028, 0.3972]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 87/203 [2m0:04:08 â€¢ 0:04:46[0m [2;4m0.41it/s[0m  
[2KStep 0 - TTA Loss: 0.8270â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 87/203 [2m0:04:08 â€¢ 0:04:46[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.6028, 0.3972]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 87/203 [2m0:04:08 â€¢ 0:04:46[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.6078, 0.3922]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 87/203 [2m0:04:08 â€¢ 0:04:46[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.6272, 0.3728]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 87/203 [2m0:04:08 â€¢ 0:04:46[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.6688, 0.3312]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 87/203 [2m0:04:08 â€¢ 0:04:46[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.7355, 0.2645]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 87/203 [2m0:04:08 â€¢ 0:04:46[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.8165, 0.1835]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 87/203 [2m0:04:08 â€¢ 0:04:46[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.8917, 0.1083]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 87/203 [2m0:04:08 â€¢ 0:04:46[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.9452, 0.0548]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 87/203 [2m0:04:08 â€¢ 0:04:46[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.9724, 0.0276]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 87/203 [2m0:04:08 â€¢ 0:04:46[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.5724, 0.4276]], device='cuda:0')5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 87/203 [2m0:04:08 â€¢ 0:04:46[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.9751, 0.0249],8;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 87/203 [2m0:04:08 â€¢ 0:04:46[0m [2;4m0.41it/s[0m  
        [0.8962, 0.1038],
        [0.9572, 0.0428],
        [0.9488, 0.0512],
        [0.9871, 0.0129],
        [0.9866, 0.0134],
        [0.9736, 0.0264],
        [0.9750, 0.0250],
        [0.9650, 0.0350],
[2K        [0.9763, 0.0237]], device='cuda:0')38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 87/203 [2m0:04:08 â€¢ 0:04:46[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.5591, 0.4409],m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 88/203 [2m0:04:14 â€¢ 0:05:36[0m [2;4m0.34it/s[0m  
        [0.5529, 0.4471],
        [0.5551, 0.4449],
        [0.5572, 0.4428],
        [0.5552, 0.4448],
        [0.5537, 0.4463],
        [0.5531, 0.4469],
        [0.5588, 0.4412],
        [0.5573, 0.4427],
[2K        [0.5555, 0.4445]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 88/203 [2m0:04:14 â€¢ 0:05:36[0m [2;4m0.34it/s[0m  
[2KClass label: CleanAndJerkâ”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 88/203 [2m0:04:14 â€¢ 0:05:36[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.5701, 0.4299]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 88/203 [2m0:04:14 â€¢ 0:05:36[0m [2;4m0.34it/s[0m  
[2KStep 0 - TTA Loss: 0.5573â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 88/203 [2m0:04:14 â€¢ 0:05:36[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.7011, 0.2989]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 88/203 [2m0:04:14 â€¢ 0:05:36[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.8092, 0.1908]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 88/203 [2m0:04:14 â€¢ 0:05:36[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.8805, 0.1195]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 88/203 [2m0:04:14 â€¢ 0:05:36[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.9190, 0.0810]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 88/203 [2m0:04:14 â€¢ 0:05:36[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.9392, 0.0608]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 88/203 [2m0:04:14 â€¢ 0:05:36[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.9527, 0.0473]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 88/203 [2m0:04:14 â€¢ 0:05:36[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.9624, 0.0376]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 88/203 [2m0:04:14 â€¢ 0:05:36[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.9698, 0.0302]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 88/203 [2m0:04:14 â€¢ 0:05:36[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.9732, 0.0268]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 88/203 [2m0:04:14 â€¢ 0:05:36[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.5731, 0.4269]], device='cuda:0')5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 88/203 [2m0:04:14 â€¢ 0:05:36[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.9585, 0.0415],8;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 88/203 [2m0:04:14 â€¢ 0:05:36[0m [2;4m0.34it/s[0m  
        [0.8864, 0.1136],
        [0.9742, 0.0258],
        [0.9336, 0.0664],
        [0.9781, 0.0219],
        [0.9795, 0.0205],
        [0.9634, 0.0366],
        [0.9618, 0.0382],
        [0.9513, 0.0487],
[2K        [0.9647, 0.0353]], device='cuda:0')38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 88/203 [2m0:04:14 â€¢ 0:05:36[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.5592, 0.4408],m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 89/203 [2m0:04:21 â€¢ 0:05:24[0m [2;4m0.35it/s[0m  
        [0.5530, 0.4470],
        [0.5552, 0.4448],
        [0.5573, 0.4427],
        [0.5554, 0.4446],
        [0.5539, 0.4461],
        [0.5532, 0.4468],
        [0.5589, 0.4411],
        [0.5574, 0.4426],
[2K        [0.5556, 0.4444]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 89/203 [2m0:04:21 â€¢ 0:05:24[0m [2;4m0.35it/s[0m  
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 89/203 [2m0:04:21 â€¢ 0:05:24[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5999, 0.4001]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 89/203 [2m0:04:21 â€¢ 0:05:24[0m [2;4m0.35it/s[0m  
[2KStep 0 - TTA Loss: 0.6867â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 89/203 [2m0:04:21 â€¢ 0:05:24[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5999, 0.4001]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 89/203 [2m0:04:21 â€¢ 0:05:24[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6047, 0.3953]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 89/203 [2m0:04:21 â€¢ 0:05:24[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6224, 0.3776]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 89/203 [2m0:04:21 â€¢ 0:05:24[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6579, 0.3421]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 89/203 [2m0:04:21 â€¢ 0:05:24[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.7126, 0.2874]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 89/203 [2m0:04:21 â€¢ 0:05:24[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.7826, 0.2174]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 89/203 [2m0:04:21 â€¢ 0:05:24[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.8555, 0.1445]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 89/203 [2m0:04:21 â€¢ 0:05:24[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.9228, 0.0772]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 89/203 [2m0:04:21 â€¢ 0:05:24[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.9687, 0.0313]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 89/203 [2m0:04:21 â€¢ 0:05:24[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5772, 0.4228]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 89/203 [2m0:04:21 â€¢ 0:05:24[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.9596, 0.0404],8;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 89/203 [2m0:04:21 â€¢ 0:05:24[0m [2;4m0.35it/s[0m  
        [0.9211, 0.0789],
        [0.9931, 0.0069],
        [0.9470, 0.0530],
        [0.9809, 0.0191],
        [0.9891, 0.0109],
        [0.9778, 0.0222],
        [0.9711, 0.0289],
        [0.9637, 0.0363],
[2K        [0.9730, 0.0270]], device='cuda:0')38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 89/203 [2m0:04:21 â€¢ 0:05:24[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5592, 0.4408],m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 90/203 [2m0:04:30 â€¢ 0:09:12[0m [2;4m0.21it/s[0m  
        [0.5531, 0.4469],
        [0.5553, 0.4447],
        [0.5573, 0.4427],
        [0.5554, 0.4446],
        [0.5539, 0.4461],
        [0.5533, 0.4467],
        [0.5589, 0.4411],
        [0.5574, 0.4426],
[2K        [0.5557, 0.4443]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 90/203 [2m0:04:30 â€¢ 0:09:12[0m [2;4m0.21it/s[0m  
[2KClass label: HammerThrowâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 90/203 [2m0:04:30 â€¢ 0:09:12[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.5566, 0.4434]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 90/203 [2m0:04:30 â€¢ 0:09:12[0m [2;4m0.21it/s[0m  
[2KStep 0 - TTA Loss: 0.7362â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 90/203 [2m0:04:30 â€¢ 0:09:12[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.7482, 0.2518]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 90/203 [2m0:04:30 â€¢ 0:09:12[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.8691, 0.1309]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 90/203 [2m0:04:30 â€¢ 0:09:12[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.9325, 0.0675]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 90/203 [2m0:04:30 â€¢ 0:09:12[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.9626, 0.0374]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 90/203 [2m0:04:30 â€¢ 0:09:12[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.9765, 0.0235]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 90/203 [2m0:04:30 â€¢ 0:09:12[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.9824, 0.0176]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 90/203 [2m0:04:30 â€¢ 0:09:12[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.9846, 0.0154]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 90/203 [2m0:04:30 â€¢ 0:09:12[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.9851, 0.0149]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 90/203 [2m0:04:30 â€¢ 0:09:12[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.9849, 0.0151]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 90/203 [2m0:04:30 â€¢ 0:09:12[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.5797, 0.4203]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 90/203 [2m0:04:30 â€¢ 0:09:12[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.9690, 0.0310],8;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 90/203 [2m0:04:30 â€¢ 0:09:12[0m [2;4m0.21it/s[0m  
        [0.9241, 0.0759],
        [0.9889, 0.0111],
        [0.9542, 0.0458],
        [0.9849, 0.0151],
        [0.9921, 0.0079],
        [0.9824, 0.0176],
        [0.9785, 0.0215],
        [0.9708, 0.0292],
[2K        [0.9786, 0.0214]], device='cuda:0')38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 90/203 [2m0:04:30 â€¢ 0:09:12[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.5593, 0.4407],m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 91/203 [2m0:04:32 â€¢ 0:09:00[0m [2;4m0.21it/s[0m  
        [0.5531, 0.4469],
        [0.5554, 0.4446],
        [0.5574, 0.4426],
        [0.5555, 0.4445],
        [0.5540, 0.4460],
        [0.5533, 0.4467],
        [0.5589, 0.4411],
        [0.5575, 0.4425],
[2K        [0.5557, 0.4443]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 91/203 [2m0:04:32 â€¢ 0:09:00[0m [2;4m0.21it/s[0m  
[2KClass label: SoccerPenaltyâ”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 91/203 [2m0:04:32 â€¢ 0:09:00[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.6679, 0.3321]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 91/203 [2m0:04:32 â€¢ 0:09:00[0m [2;4m0.21it/s[0m  
[2KStep 0 - TTA Loss: 1.1498â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 91/203 [2m0:04:32 â€¢ 0:09:00[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.6679, 0.3321]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 91/203 [2m0:04:32 â€¢ 0:09:00[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.6674, 0.3326]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 91/203 [2m0:04:32 â€¢ 0:09:00[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.6656, 0.3344]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 91/203 [2m0:04:32 â€¢ 0:09:00[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.6624, 0.3376]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 91/203 [2m0:04:32 â€¢ 0:09:00[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.6570, 0.3430]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 91/203 [2m0:04:32 â€¢ 0:09:00[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.6481, 0.3519]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 91/203 [2m0:04:32 â€¢ 0:09:00[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.6373, 0.3627]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 91/203 [2m0:04:32 â€¢ 0:09:00[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.6217, 0.3783]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 91/203 [2m0:04:32 â€¢ 0:09:00[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.6006, 0.3994]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 91/203 [2m0:04:32 â€¢ 0:09:00[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.5553, 0.4447]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 91/203 [2m0:04:32 â€¢ 0:09:00[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.5057, 0.4943],8;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 91/203 [2m0:04:32 â€¢ 0:09:00[0m [2;4m0.21it/s[0m  
        [0.5186, 0.4814],
        [0.5540, 0.4460],
        [0.5866, 0.4134],
        [0.3855, 0.6145],
        [0.5504, 0.4496],
        [0.5073, 0.4927],
        [0.5692, 0.4308],
        [0.5764, 0.4236],
[2K        [0.5355, 0.4645]], device='cuda:0')38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 91/203 [2m0:04:32 â€¢ 0:09:00[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.5592, 0.4408],0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 92/203 [2m0:04:32 â€¢ 0:07:50[0m [2;4m0.24it/s[0m   
        [0.5531, 0.4469],
        [0.5553, 0.4447],
        [0.5573, 0.4427],
        [0.5554, 0.4446],
        [0.5540, 0.4460],
        [0.5533, 0.4467],
        [0.5589, 0.4411],
        [0.5574, 0.4426],
[2K        [0.5557, 0.4443]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 92/203 [2m0:04:32 â€¢ 0:07:50[0m [2;4m0.24it/s[0m  
[2KClass label: BaseballPitchâ”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 92/203 [2m0:04:32 â€¢ 0:07:50[0m [2;4m0.24it/s[0m  
[2KAttention weights: tensor([[0.6423, 0.3577]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 92/203 [2m0:04:32 â€¢ 0:07:50[0m [2;4m0.24it/s[0m  
[2KStep 0 - TTA Loss: 1.0044â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 92/203 [2m0:04:32 â€¢ 0:07:50[0m [2;4m0.24it/s[0m  
[2KAttention weights: tensor([[0.6133, 0.3867]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 92/203 [2m0:04:32 â€¢ 0:07:50[0m [2;4m0.24it/s[0m  
[2KAttention weights: tensor([[0.5861, 0.4139]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 92/203 [2m0:04:32 â€¢ 0:07:50[0m [2;4m0.24it/s[0m  
[2KAttention weights: tensor([[0.5619, 0.4381]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 92/203 [2m0:04:32 â€¢ 0:07:50[0m [2;4m0.24it/s[0m  
[2KAttention weights: tensor([[0.5425, 0.4575]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 92/203 [2m0:04:32 â€¢ 0:07:50[0m [2;4m0.24it/s[0m  
[2KAttention weights: tensor([[0.5263, 0.4737]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 92/203 [2m0:04:32 â€¢ 0:07:50[0m [2;4m0.24it/s[0m  
[2KAttention weights: tensor([[0.5169, 0.4831]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 92/203 [2m0:04:32 â€¢ 0:07:50[0m [2;4m0.24it/s[0m  
[2KAttention weights: tensor([[0.5122, 0.4878]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 92/203 [2m0:04:32 â€¢ 0:07:50[0m [2;4m0.24it/s[0m  
[2KAttention weights: tensor([[0.5086, 0.4914]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 92/203 [2m0:04:32 â€¢ 0:07:50[0m [2;4m0.24it/s[0m  
[2KAttention weights: tensor([[0.5067, 0.4933]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 92/203 [2m0:04:32 â€¢ 0:07:50[0m [2;4m0.24it/s[0m  
[2KAttention weights: tensor([[0.5547, 0.4453]], device='cuda:0');5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 92/203 [2m0:04:32 â€¢ 0:07:50[0m [2;4m0.24it/s[0m  
[2KAttention weights: tensor([[0.5062, 0.4938],38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 92/203 [2m0:04:32 â€¢ 0:07:50[0m [2;4m0.24it/s[0m  
        [0.5038, 0.4962],
        [0.5396, 0.4604],
        [0.5821, 0.4179],
        [0.4102, 0.5898],
        [0.5313, 0.4687],
        [0.4971, 0.5029],
        [0.5386, 0.4614],
        [0.5680, 0.4320],
[2K        [0.5393, 0.4607]], device='cuda:0')[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 92/203 [2m0:04:32 â€¢ 0:07:50[0m [2;4m0.24it/s[0m  
[2KAttention weights: tensor([[0.5592, 0.4408],0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 93/203 [2m0:04:33 â€¢ 0:07:07[0m [2;4m0.26it/s[0m  
        [0.5531, 0.4469],
        [0.5553, 0.4447],
        [0.5573, 0.4427],
        [0.5553, 0.4447],
        [0.5539, 0.4461],
        [0.5533, 0.4467],
        [0.5589, 0.4411],
        [0.5574, 0.4426],
[2K        [0.5556, 0.4444]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 93/203 [2m0:04:33 â€¢ 0:07:07[0m [2;4m0.26it/s[0m  
[2KClass label: SoccerPenaltyâ”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 93/203 [2m0:04:33 â€¢ 0:07:07[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.6717, 0.3283]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 93/203 [2m0:04:33 â€¢ 0:07:07[0m [2;4m0.26it/s[0m  
[2KStep 0 - TTA Loss: 0.8307â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 93/203 [2m0:04:33 â€¢ 0:07:07[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.6717, 0.3283]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 93/203 [2m0:04:33 â€¢ 0:07:07[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.6723, 0.3277]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 93/203 [2m0:04:33 â€¢ 0:07:07[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.6763, 0.3237]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 93/203 [2m0:04:33 â€¢ 0:07:07[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.6875, 0.3125]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 93/203 [2m0:04:33 â€¢ 0:07:07[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.7089, 0.2911]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 93/203 [2m0:04:33 â€¢ 0:07:07[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.7440, 0.2560]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 93/203 [2m0:04:33 â€¢ 0:07:07[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.7847, 0.2153]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 93/203 [2m0:04:33 â€¢ 0:07:07[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.8269, 0.1731]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 93/203 [2m0:04:33 â€¢ 0:07:07[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.8575, 0.1425]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 93/203 [2m0:04:33 â€¢ 0:07:07[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.5635, 0.4365]], device='cuda:0');5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 93/203 [2m0:04:33 â€¢ 0:07:07[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.7501, 0.2499],38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 93/203 [2m0:04:33 â€¢ 0:07:07[0m [2;4m0.26it/s[0m  
        [0.6581, 0.3419],
        [0.6444, 0.3556],
        [0.7250, 0.2750],
        [0.6623, 0.3377],
        [0.7223, 0.2777],
        [0.6574, 0.3426],
        [0.8629, 0.1371],
        [0.7341, 0.2659],
[2K        [0.7006, 0.2994]], device='cuda:0')[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 93/203 [2m0:04:33 â€¢ 0:07:07[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.5592, 0.4408],0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 94/203 [2m0:04:35 â€¢ 0:06:30[0m [2;4m0.28it/s[0m  
        [0.5531, 0.4469],
        [0.5553, 0.4447],
        [0.5573, 0.4427],
        [0.5553, 0.4447],
        [0.5539, 0.4461],
        [0.5533, 0.4467],
        [0.5589, 0.4411],
        [0.5574, 0.4426],
[2K        [0.5556, 0.4444]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 94/203 [2m0:04:35 â€¢ 0:06:30[0m [2;4m0.28it/s[0m  
[2KClass label: SoccerPenaltyâ”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 94/203 [2m0:04:35 â€¢ 0:06:30[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6644, 0.3356]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 94/203 [2m0:04:35 â€¢ 0:06:30[0m [2;4m0.28it/s[0m  
[2KStep 0 - TTA Loss: 0.7008â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 94/203 [2m0:04:35 â€¢ 0:06:30[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6969, 0.3031]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 94/203 [2m0:04:35 â€¢ 0:06:30[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.7232, 0.2768]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 94/203 [2m0:04:35 â€¢ 0:06:30[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.7356, 0.2644]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 94/203 [2m0:04:35 â€¢ 0:06:30[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.7215, 0.2785]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 94/203 [2m0:04:35 â€¢ 0:06:30[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6859, 0.3141]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 94/203 [2m0:04:35 â€¢ 0:06:30[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6565, 0.3435]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 94/203 [2m0:04:35 â€¢ 0:06:30[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6595, 0.3405]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 94/203 [2m0:04:35 â€¢ 0:06:30[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6783, 0.3217]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 94/203 [2m0:04:35 â€¢ 0:06:30[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6928, 0.3072]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 94/203 [2m0:04:35 â€¢ 0:06:30[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.5576, 0.4424]], device='cuda:0')m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 94/203 [2m0:04:35 â€¢ 0:06:30[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6459, 0.3541],38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 94/203 [2m0:04:35 â€¢ 0:06:30[0m [2;4m0.28it/s[0m  
        [0.5697, 0.4303],
        [0.5804, 0.4196],
        [0.6565, 0.3435],
        [0.5625, 0.4375],
        [0.6142, 0.3858],
        [0.5723, 0.4277],
        [0.6982, 0.3018],
        [0.6495, 0.3505],
[2K        [0.6301, 0.3699]], device='cuda:0')[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 94/203 [2m0:04:35 â€¢ 0:06:30[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.5592, 0.4408],0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 95/203 [2m0:04:36 â€¢ 0:06:15[0m [2;4m0.29it/s[0m  
        [0.5531, 0.4469],
        [0.5553, 0.4447],
        [0.5573, 0.4427],
        [0.5553, 0.4447],
        [0.5539, 0.4461],
        [0.5533, 0.4467],
        [0.5589, 0.4411],
        [0.5574, 0.4426],
[2K        [0.5556, 0.4444]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 95/203 [2m0:04:36 â€¢ 0:06:15[0m [2;4m0.29it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 95/203 [2m0:04:36 â€¢ 0:06:15[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.6215, 0.3785]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 95/203 [2m0:04:36 â€¢ 0:06:15[0m [2;4m0.29it/s[0m  
[2KStep 0 - TTA Loss: 0.6646â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 95/203 [2m0:04:36 â€¢ 0:06:15[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.6215, 0.3785]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 95/203 [2m0:04:36 â€¢ 0:06:15[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.6235, 0.3765]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 95/203 [2m0:04:36 â€¢ 0:06:15[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.6312, 0.3688]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 95/203 [2m0:04:36 â€¢ 0:06:15[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.6487, 0.3513]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 95/203 [2m0:04:36 â€¢ 0:06:15[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.6760, 0.3240]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 95/203 [2m0:04:36 â€¢ 0:06:15[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.7157, 0.2843]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 95/203 [2m0:04:36 â€¢ 0:06:15[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.7610, 0.2390]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 95/203 [2m0:04:36 â€¢ 0:06:15[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.8144, 0.1856]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 95/203 [2m0:04:36 â€¢ 0:06:15[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.8612, 0.1388]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 95/203 [2m0:04:36 â€¢ 0:06:15[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.5649, 0.4351]], device='cuda:0')m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 95/203 [2m0:04:36 â€¢ 0:06:15[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.9113, 0.0887],38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 95/203 [2m0:04:36 â€¢ 0:06:15[0m [2;4m0.29it/s[0m  
        [0.8079, 0.1921],
        [0.7923, 0.2077],
        [0.8809, 0.1191],
        [0.8751, 0.1249],
        [0.8958, 0.1042],
        [0.8531, 0.1469],
        [0.9636, 0.0364],
        [0.8951, 0.1049],
[2K        [0.8972, 0.1028]], device='cuda:0')[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 95/203 [2m0:04:36 â€¢ 0:06:15[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.5593, 0.4407],0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 96/203 [2m0:04:39 â€¢ 0:05:33[0m [2;4m0.32it/s[0m  
        [0.5531, 0.4469],
        [0.5554, 0.4446],
        [0.5574, 0.4426],
        [0.5554, 0.4446],
        [0.5540, 0.4460],
        [0.5533, 0.4467],
        [0.5590, 0.4410],
        [0.5575, 0.4425],
[2K        [0.5557, 0.4443]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 96/203 [2m0:04:39 â€¢ 0:05:33[0m [2;4m0.32it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 96/203 [2m0:04:39 â€¢ 0:05:33[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.5673, 0.4327]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 96/203 [2m0:04:39 â€¢ 0:05:33[0m [2;4m0.32it/s[0m  
[2KStep 0 - TTA Loss: 0.5913â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 96/203 [2m0:04:39 â€¢ 0:05:33[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.6490, 0.3510]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 96/203 [2m0:04:39 â€¢ 0:05:33[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.7316, 0.2684]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 96/203 [2m0:04:39 â€¢ 0:05:33[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.8024, 0.1976]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 96/203 [2m0:04:39 â€¢ 0:05:33[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.8553, 0.1447]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 96/203 [2m0:04:39 â€¢ 0:05:33[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.8903, 0.1097]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 96/203 [2m0:04:39 â€¢ 0:05:33[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.9104, 0.0896]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 96/203 [2m0:04:39 â€¢ 0:05:33[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.9202, 0.0798]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 96/203 [2m0:04:39 â€¢ 0:05:33[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.9240, 0.0760]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 96/203 [2m0:04:39 â€¢ 0:05:33[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.9250, 0.0750]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 96/203 [2m0:04:39 â€¢ 0:05:33[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.5669, 0.4331]], device='cuda:0')m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 96/203 [2m0:04:39 â€¢ 0:05:33[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.9244, 0.0756],38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 96/203 [2m0:04:39 â€¢ 0:05:33[0m [2;4m0.32it/s[0m  
        [0.8352, 0.1648],
        [0.8531, 0.1469],
        [0.9061, 0.0939],
        [0.9131, 0.0869],
        [0.9335, 0.0665],
        [0.9253, 0.0747],
        [0.9563, 0.0437],
        [0.9189, 0.0811],
[2K        [0.9351, 0.0649]], device='cuda:0')[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 96/203 [2m0:04:39 â€¢ 0:05:33[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.5594, 0.4406],[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 97/203 [2m0:04:41 â€¢ 0:05:14[0m [2;4m0.34it/s[0m   
        [0.5532, 0.4468],
        [0.5555, 0.4445],
        [0.5574, 0.4426],
        [0.5555, 0.4445],
        [0.5541, 0.4459],
        [0.5534, 0.4466],
        [0.5590, 0.4410],
        [0.5575, 0.4425],
[2K        [0.5559, 0.4441]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 97/203 [2m0:04:41 â€¢ 0:05:14[0m [2;4m0.34it/s[0m  
[2KClass label: GolfSwingâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 97/203 [2m0:04:41 â€¢ 0:05:14[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6504, 0.3496]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 97/203 [2m0:04:41 â€¢ 0:05:14[0m [2;4m0.34it/s[0m  
[2KStep 0 - TTA Loss: 0.6448â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 97/203 [2m0:04:41 â€¢ 0:05:14[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6504, 0.3496]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 97/203 [2m0:04:41 â€¢ 0:05:14[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6507, 0.3493]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 97/203 [2m0:04:41 â€¢ 0:05:14[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6510, 0.3490]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 97/203 [2m0:04:41 â€¢ 0:05:14[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6511, 0.3489]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 97/203 [2m0:04:41 â€¢ 0:05:14[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6492, 0.3508]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 97/203 [2m0:04:41 â€¢ 0:05:14[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6448, 0.3552]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 97/203 [2m0:04:41 â€¢ 0:05:14[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6368, 0.3632]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 97/203 [2m0:04:41 â€¢ 0:05:14[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6303, 0.3697]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 97/203 [2m0:04:41 â€¢ 0:05:14[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6346, 0.3654]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 97/203 [2m0:04:41 â€¢ 0:05:14[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.5565, 0.4435]], device='cuda:0')8;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 97/203 [2m0:04:41 â€¢ 0:05:14[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6728, 0.3272],[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 97/203 [2m0:04:41 â€¢ 0:05:14[0m [2;4m0.34it/s[0m  
        [0.5818, 0.4182],
        [0.5886, 0.4114],
        [0.6620, 0.3380],
        [0.5969, 0.4031],
        [0.6340, 0.3660],
        [0.5902, 0.4098],
        [0.7238, 0.2762],
        [0.6696, 0.3304],
[2K        [0.6586, 0.3414]], device='cuda:0')[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 97/203 [2m0:04:41 â€¢ 0:05:14[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.5593, 0.4407],[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 98/203 [2m0:04:43 â€¢ 0:04:59[0m [2;4m0.35it/s[0m  
        [0.5532, 0.4468],
        [0.5554, 0.4446],
        [0.5574, 0.4426],
        [0.5554, 0.4446],
        [0.5540, 0.4460],
        [0.5533, 0.4467],
        [0.5590, 0.4410],
        [0.5575, 0.4425],
[2K        [0.5559, 0.4441]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 98/203 [2m0:04:43 â€¢ 0:04:59[0m [2;4m0.35it/s[0m  
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 98/203 [2m0:04:43 â€¢ 0:04:59[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5978, 0.4022]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 98/203 [2m0:04:43 â€¢ 0:04:59[0m [2;4m0.35it/s[0m  
[2KStep 0 - TTA Loss: 0.9698â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 98/203 [2m0:04:43 â€¢ 0:04:59[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6222, 0.3778]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 98/203 [2m0:04:43 â€¢ 0:04:59[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6462, 0.3538]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 98/203 [2m0:04:43 â€¢ 0:04:59[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6649, 0.3351]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 98/203 [2m0:04:43 â€¢ 0:04:59[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6878, 0.3122]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 98/203 [2m0:04:43 â€¢ 0:04:59[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.7083, 0.2917]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 98/203 [2m0:04:43 â€¢ 0:04:59[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.7282, 0.2718]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 98/203 [2m0:04:43 â€¢ 0:04:59[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.7404, 0.2596]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 98/203 [2m0:04:43 â€¢ 0:04:59[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.7474, 0.2526]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 98/203 [2m0:04:43 â€¢ 0:04:59[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.7509, 0.2491]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 98/203 [2m0:04:43 â€¢ 0:04:59[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5592, 0.4408]], device='cuda:0')8;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 98/203 [2m0:04:43 â€¢ 0:04:59[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.7581, 0.2419],[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 98/203 [2m0:04:43 â€¢ 0:04:59[0m [2;4m0.35it/s[0m  
        [0.6568, 0.3432],
        [0.6746, 0.3254],
        [0.7825, 0.2175],
        [0.7031, 0.2969],
        [0.7519, 0.2481],
        [0.6996, 0.3004],
        [0.7903, 0.2097],
        [0.7636, 0.2364],
[2K        [0.7568, 0.2432]], device='cuda:0')[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 98/203 [2m0:04:43 â€¢ 0:04:59[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5593, 0.4407],[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 99/203 [2m0:04:43 â€¢ 0:04:35[0m [2;4m0.38it/s[0m  
        [0.5531, 0.4469],
        [0.5554, 0.4446],
        [0.5574, 0.4426],
        [0.5554, 0.4446],
        [0.5540, 0.4460],
        [0.5532, 0.4468],
        [0.5590, 0.4410],
        [0.5575, 0.4425],
[2K        [0.5558, 0.4442]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 99/203 [2m0:04:43 â€¢ 0:04:35[0m [2;4m0.38it/s[0m  
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 99/203 [2m0:04:43 â€¢ 0:04:35[0m [2;4m0.38it/s[0m  
[2KAttention weights: tensor([[0.6062, 0.3938]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 99/203 [2m0:04:43 â€¢ 0:04:35[0m [2;4m0.38it/s[0m  
[2KStep 0 - TTA Loss: 0.7523â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 99/203 [2m0:04:43 â€¢ 0:04:35[0m [2;4m0.38it/s[0m  
[2KAttention weights: tensor([[0.6062, 0.3938]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 99/203 [2m0:04:43 â€¢ 0:04:35[0m [2;4m0.38it/s[0m  
[2KAttention weights: tensor([[0.6097, 0.3903]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 99/203 [2m0:04:43 â€¢ 0:04:35[0m [2;4m0.38it/s[0m  
[2KAttention weights: tensor([[0.6269, 0.3731]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 99/203 [2m0:04:43 â€¢ 0:04:35[0m [2;4m0.38it/s[0m  
[2KAttention weights: tensor([[0.6708, 0.3292]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 99/203 [2m0:04:43 â€¢ 0:04:35[0m [2;4m0.38it/s[0m  
[2KAttention weights: tensor([[0.7481, 0.2519]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 99/203 [2m0:04:43 â€¢ 0:04:35[0m [2;4m0.38it/s[0m  
[2KAttention weights: tensor([[0.8475, 0.1525]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 99/203 [2m0:04:43 â€¢ 0:04:35[0m [2;4m0.38it/s[0m  
[2KAttention weights: tensor([[0.9335, 0.0665]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 99/203 [2m0:04:43 â€¢ 0:04:35[0m [2;4m0.38it/s[0m  
[2KAttention weights: tensor([[0.9758, 0.0242]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 99/203 [2m0:04:43 â€¢ 0:04:35[0m [2;4m0.38it/s[0m  
[2KAttention weights: tensor([[0.9905, 0.0095]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 99/203 [2m0:04:43 â€¢ 0:04:35[0m [2;4m0.38it/s[0m  
[2KAttention weights: tensor([[0.5789, 0.4211]], device='cuda:0')0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 99/203 [2m0:04:43 â€¢ 0:04:35[0m [2;4m0.38it/s[0m  
[2KAttention weights: tensor([[0.9775, 0.0225],[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 99/203 [2m0:04:43 â€¢ 0:04:35[0m [2;4m0.38it/s[0m  
        [0.9334, 0.0666],
        [0.9709, 0.0291],
        [0.9660, 0.0340],
        [0.9865, 0.0135],
        [0.9963, 0.0037],
        [0.9892, 0.0108],
        [0.9878, 0.0122],
        [0.9817, 0.0183],
[2K        [0.9855, 0.0145]], device='cuda:0')[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 99/203 [2m0:04:43 â€¢ 0:04:35[0m [2;4m0.38it/s[0m  
[2KAttention weights: tensor([[0.5594, 0.4406],[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 100/203 [2m0:04:46 â€¢ 0:03:54[0m [2;4m0.44it/s[0m  
        [0.5532, 0.4468],
        [0.5555, 0.4445],
        [0.5575, 0.4425],
        [0.5555, 0.4445],
        [0.5541, 0.4459],
        [0.5534, 0.4466],
        [0.5591, 0.4409],
        [0.5576, 0.4424],
[2K        [0.5559, 0.4441]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 100/203 [2m0:04:46 â€¢ 0:03:54[0m [2;4m0.44it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 100/203 [2m0:04:46 â€¢ 0:03:54[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.5639, 0.4361]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 100/203 [2m0:04:46 â€¢ 0:03:54[0m [2;4m0.44it/s[0m  
[2KStep 0 - TTA Loss: 0.6220â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 100/203 [2m0:04:46 â€¢ 0:03:54[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.7362, 0.2638]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 100/203 [2m0:04:46 â€¢ 0:03:54[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.8537, 0.1463]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 100/203 [2m0:04:46 â€¢ 0:03:54[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.9287, 0.0713]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 100/203 [2m0:04:46 â€¢ 0:03:54[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.9663, 0.0337]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 100/203 [2m0:04:46 â€¢ 0:03:54[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.9821, 0.0179]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 100/203 [2m0:04:46 â€¢ 0:03:54[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.9894, 0.0106]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 100/203 [2m0:04:46 â€¢ 0:03:54[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.9897, 0.0103]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 100/203 [2m0:04:46 â€¢ 0:03:54[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.9892, 0.0108]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 100/203 [2m0:04:46 â€¢ 0:03:54[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.9884, 0.0116]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 100/203 [2m0:04:46 â€¢ 0:03:54[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.5771, 0.4229]], device='cuda:0')0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 100/203 [2m0:04:46 â€¢ 0:03:54[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.9702, 0.0298],[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 100/203 [2m0:04:46 â€¢ 0:03:54[0m [2;4m0.44it/s[0m  
        [0.9205, 0.0795],
        [0.9626, 0.0374],
        [0.9578, 0.0422],
        [0.9810, 0.0190],
        [0.9937, 0.0063],
        [0.9880, 0.0120],
        [0.9807, 0.0193],
        [0.9748, 0.0252],
[2K        [0.9819, 0.0181]], device='cuda:0')[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 100/203 [2m0:04:46 â€¢ 0:03:54[0m [2;4m0.44it/s[0m  
[2KAttention weights: tensor([[0.5595, 0.4405],[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 101/203 [2m0:04:50 â€¢ 0:04:07[0m [2;4m0.41it/s[0m  
        [0.5533, 0.4467],
        [0.5556, 0.4444],
        [0.5576, 0.4424],
        [0.5556, 0.4444],
        [0.5543, 0.4457],
        [0.5535, 0.4465],
        [0.5592, 0.4408],
        [0.5577, 0.4423],
[2K        [0.5560, 0.4440]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 101/203 [2m0:04:50 â€¢ 0:04:07[0m [2;4m0.41it/s[0m  
[2KClass label: Billiardsâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 101/203 [2m0:04:50 â€¢ 0:04:07[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.5638, 0.4362]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 101/203 [2m0:04:50 â€¢ 0:04:07[0m [2;4m0.41it/s[0m  
[2KStep 0 - TTA Loss: 0.5618â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 101/203 [2m0:04:50 â€¢ 0:04:07[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.5638, 0.4362]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 101/203 [2m0:04:50 â€¢ 0:04:07[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.5592, 0.4408]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 101/203 [2m0:04:50 â€¢ 0:04:07[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.5403, 0.4597]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 101/203 [2m0:04:50 â€¢ 0:04:07[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.4982, 0.5018]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 101/203 [2m0:04:50 â€¢ 0:04:07[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.4257, 0.5743]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 101/203 [2m0:04:50 â€¢ 0:04:07[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.3255, 0.6745]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 101/203 [2m0:04:50 â€¢ 0:04:07[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.2219, 0.7781]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 101/203 [2m0:04:50 â€¢ 0:04:07[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.1460, 0.8540]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 101/203 [2m0:04:50 â€¢ 0:04:07[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.1015, 0.8985]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 101/203 [2m0:04:50 â€¢ 0:04:07[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.5409, 0.4591]], device='cuda:0')0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 101/203 [2m0:04:50 â€¢ 0:04:07[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.1614, 0.8386],[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 101/203 [2m0:04:50 â€¢ 0:04:07[0m [2;4m0.41it/s[0m  
        [0.0773, 0.9227],
        [0.1162, 0.8838],
        [0.1907, 0.8093],
        [0.0763, 0.9237],
        [0.0701, 0.9299],
        [0.0307, 0.9693],
        [0.1975, 0.8025],
        [0.1444, 0.8556],
[2K        [0.0899, 0.9101]], device='cuda:0')[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 101/203 [2m0:04:50 â€¢ 0:04:07[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.5594, 0.4406],[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 102/203 [2m0:04:53 â€¢ 0:03:17[0m [2;4m0.51it/s[0m   
        [0.5532, 0.4468],
        [0.5555, 0.4445],
        [0.5575, 0.4425],
        [0.5555, 0.4445],
        [0.5542, 0.4458],
        [0.5534, 0.4466],
        [0.5591, 0.4409],
        [0.5576, 0.4424],
[2K        [0.5559, 0.4441]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 102/203 [2m0:04:53 â€¢ 0:03:17[0m [2;4m0.51it/s[0m  
[2KClass label: CleanAndJerkâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 102/203 [2m0:04:53 â€¢ 0:03:17[0m [2;4m0.51it/s[0m  
[2KAttention weights: tensor([[0.5729, 0.4271]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 102/203 [2m0:04:53 â€¢ 0:03:17[0m [2;4m0.51it/s[0m  
[2KStep 0 - TTA Loss: 0.7646â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 102/203 [2m0:04:53 â€¢ 0:03:17[0m [2;4m0.51it/s[0m  
[2KAttention weights: tensor([[0.5141, 0.4859]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 102/203 [2m0:04:53 â€¢ 0:03:17[0m [2;4m0.51it/s[0m  
[2KAttention weights: tensor([[0.4725, 0.5275]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 102/203 [2m0:04:53 â€¢ 0:03:17[0m [2;4m0.51it/s[0m  
[2KAttention weights: tensor([[0.4452, 0.5548]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 102/203 [2m0:04:53 â€¢ 0:03:17[0m [2;4m0.51it/s[0m  
[2KAttention weights: tensor([[0.4302, 0.5698]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 102/203 [2m0:04:53 â€¢ 0:03:17[0m [2;4m0.51it/s[0m  
[2KAttention weights: tensor([[0.4238, 0.5762]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 102/203 [2m0:04:53 â€¢ 0:03:17[0m [2;4m0.51it/s[0m  
[2KAttention weights: tensor([[0.4238, 0.5762]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 102/203 [2m0:04:53 â€¢ 0:03:17[0m [2;4m0.51it/s[0m  
[2KAttention weights: tensor([[0.4252, 0.5748]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 102/203 [2m0:04:53 â€¢ 0:03:17[0m [2;4m0.51it/s[0m  
[2KAttention weights: tensor([[0.4268, 0.5732]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 102/203 [2m0:04:53 â€¢ 0:03:17[0m [2;4m0.51it/s[0m  
[2KAttention weights: tensor([[0.4279, 0.5721]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 102/203 [2m0:04:53 â€¢ 0:03:17[0m [2;4m0.51it/s[0m  
[2KAttention weights: tensor([[0.5512, 0.4488]], device='cuda:0')38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 102/203 [2m0:04:53 â€¢ 0:03:17[0m [2;4m0.51it/s[0m  
[2KAttention weights: tensor([[0.3803, 0.6197],[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 102/203 [2m0:04:53 â€¢ 0:03:17[0m [2;4m0.51it/s[0m  
        [0.3285, 0.6715],
        [0.4282, 0.5718],
        [0.4174, 0.5826],
        [0.2667, 0.7333],
        [0.2611, 0.7389],
        [0.1559, 0.8441],
        [0.4340, 0.5660],
        [0.3729, 0.6271],
[2K        [0.2878, 0.7122]], device='cuda:0')m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 102/203 [2m0:04:53 â€¢ 0:03:17[0m [2;4m0.51it/s[0m  
[2KAttention weights: tensor([[0.5594, 0.4406],[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 103/203 [2m0:04:55 â€¢ 0:03:12[0m [2;4m0.52it/s[0m  
        [0.5531, 0.4469],
        [0.5555, 0.4445],
        [0.5575, 0.4425],
        [0.5555, 0.4445],
        [0.5542, 0.4458],
        [0.5533, 0.4467],
        [0.5591, 0.4409],
        [0.5576, 0.4424],
[2K        [0.5559, 0.4441]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 103/203 [2m0:04:55 â€¢ 0:03:12[0m [2;4m0.52it/s[0m  
[2KClass label: TennisSwingâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 103/203 [2m0:04:55 â€¢ 0:03:12[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.6398, 0.3602]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 103/203 [2m0:04:55 â€¢ 0:03:12[0m [2;4m0.52it/s[0m  
[2KStep 0 - TTA Loss: 0.4541â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 103/203 [2m0:04:55 â€¢ 0:03:12[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.6398, 0.3602]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 103/203 [2m0:04:55 â€¢ 0:03:12[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.6401, 0.3599]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 103/203 [2m0:04:55 â€¢ 0:03:12[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.6427, 0.3573]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 103/203 [2m0:04:55 â€¢ 0:03:12[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.6500, 0.3500]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 103/203 [2m0:04:55 â€¢ 0:03:12[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.6644, 0.3356]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 103/203 [2m0:04:55 â€¢ 0:03:12[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.6862, 0.3138]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 103/203 [2m0:04:55 â€¢ 0:03:12[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.7137, 0.2863]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 103/203 [2m0:04:55 â€¢ 0:03:12[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.7437, 0.2563]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 103/203 [2m0:04:55 â€¢ 0:03:12[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.7710, 0.2290]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 103/203 [2m0:04:55 â€¢ 0:03:12[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.5618, 0.4382]], device='cuda:0')38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 103/203 [2m0:04:55 â€¢ 0:03:12[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.7198, 0.2802],[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 103/203 [2m0:04:55 â€¢ 0:03:12[0m [2;4m0.52it/s[0m  
        [0.6375, 0.3625],
        [0.6715, 0.3285],
        [0.7429, 0.2571],
        [0.6450, 0.3550],
        [0.6778, 0.3222],
        [0.6063, 0.3937],
        [0.7382, 0.2618],
        [0.7957, 0.2043],
[2K        [0.7088, 0.2912]], device='cuda:0')m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 103/203 [2m0:04:55 â€¢ 0:03:12[0m [2;4m0.52it/s[0m  
[2KAttention weights: tensor([[0.5594, 0.4406],[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 104/203 [2m0:04:55 â€¢ 0:03:00[0m [2;4m0.55it/s[0m  
        [0.5531, 0.4469],
        [0.5555, 0.4445],
        [0.5575, 0.4425],
        [0.5555, 0.4445],
        [0.5542, 0.4458],
        [0.5533, 0.4467],
        [0.5591, 0.4409],
        [0.5575, 0.4425],
[2K        [0.5559, 0.4441]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 104/203 [2m0:04:55 â€¢ 0:03:00[0m [2;4m0.55it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 104/203 [2m0:04:55 â€¢ 0:03:00[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.5633, 0.4367]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 104/203 [2m0:04:55 â€¢ 0:03:00[0m [2;4m0.55it/s[0m  
[2KStep 0 - TTA Loss: 0.6142â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 104/203 [2m0:04:55 â€¢ 0:03:00[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.5776, 0.4224]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 104/203 [2m0:04:55 â€¢ 0:03:00[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.6090, 0.3910]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 104/203 [2m0:04:55 â€¢ 0:03:00[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.6412, 0.3588]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 104/203 [2m0:04:55 â€¢ 0:03:00[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.6702, 0.3298]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 104/203 [2m0:04:55 â€¢ 0:03:00[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.6912, 0.3088]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 104/203 [2m0:04:55 â€¢ 0:03:00[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.7045, 0.2955]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 104/203 [2m0:04:55 â€¢ 0:03:00[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.7119, 0.2881]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 104/203 [2m0:04:55 â€¢ 0:03:00[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.7161, 0.2839]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 104/203 [2m0:04:55 â€¢ 0:03:00[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.7177, 0.2823]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 104/203 [2m0:04:55 â€¢ 0:03:00[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.5582, 0.4418]], device='cuda:0')38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 104/203 [2m0:04:55 â€¢ 0:03:00[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.7621, 0.2379],[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 104/203 [2m0:04:55 â€¢ 0:03:00[0m [2;4m0.55it/s[0m  
        [0.6730, 0.3270],
        [0.7151, 0.2849],
        [0.7739, 0.2261],
        [0.7124, 0.2876],
        [0.7526, 0.2474],
        [0.7181, 0.2819],
        [0.7776, 0.2224],
        [0.8126, 0.1874],
[2K        [0.7703, 0.2297]], device='cuda:0')m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 104/203 [2m0:04:55 â€¢ 0:03:00[0m [2;4m0.55it/s[0m  
[2KAttention weights: tensor([[0.5594, 0.4406],[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 105/203 [2m0:05:00 â€¢ 0:03:20[0m [2;4m0.49it/s[0m  
        [0.5531, 0.4469],
        [0.5555, 0.4445],
        [0.5574, 0.4426],
        [0.5555, 0.4445],
        [0.5541, 0.4459],
        [0.5533, 0.4467],
        [0.5591, 0.4409],
        [0.5575, 0.4425],
[2K        [0.5559, 0.4441]], device='cuda:0', grad_fn=<SoftmaxBackward0>);237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 105/203 [2m0:05:00 â€¢ 0:03:20[0m [2;4m0.49it/s[0m  
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 105/203 [2m0:05:00 â€¢ 0:03:20[0m [2;4m0.49it/s[0m  
[2KAttention weights: tensor([[0.6021, 0.3979]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 105/203 [2m0:05:00 â€¢ 0:03:20[0m [2;4m0.49it/s[0m  
[2KStep 0 - TTA Loss: 0.7999â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 105/203 [2m0:05:00 â€¢ 0:03:20[0m [2;4m0.49it/s[0m  
[2KAttention weights: tensor([[0.6021, 0.3979]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 105/203 [2m0:05:00 â€¢ 0:03:20[0m [2;4m0.49it/s[0m  
[2KAttention weights: tensor([[0.6034, 0.3966]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 105/203 [2m0:05:00 â€¢ 0:03:20[0m [2;4m0.49it/s[0m  
[2KAttention weights: tensor([[0.6101, 0.3899]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 105/203 [2m0:05:00 â€¢ 0:03:20[0m [2;4m0.49it/s[0m  
[2KAttention weights: tensor([[0.6280, 0.3720]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 105/203 [2m0:05:00 â€¢ 0:03:20[0m [2;4m0.49it/s[0m  
[2KAttention weights: tensor([[0.6624, 0.3376]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 105/203 [2m0:05:00 â€¢ 0:03:20[0m [2;4m0.49it/s[0m  
[2KAttention weights: tensor([[0.7127, 0.2873]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 105/203 [2m0:05:00 â€¢ 0:03:20[0m [2;4m0.49it/s[0m  
[2KAttention weights: tensor([[0.7716, 0.2284]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 105/203 [2m0:05:00 â€¢ 0:03:20[0m [2;4m0.49it/s[0m  
[2KAttention weights: tensor([[0.8269, 0.1731]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 105/203 [2m0:05:00 â€¢ 0:03:20[0m [2;4m0.49it/s[0m  
[2KAttention weights: tensor([[0.8666, 0.1334]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 105/203 [2m0:05:00 â€¢ 0:03:20[0m [2;4m0.49it/s[0m  
[2KAttention weights: tensor([[0.5630, 0.4370]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 105/203 [2m0:05:00 â€¢ 0:03:20[0m [2;4m0.49it/s[0m  
[2KAttention weights: tensor([[0.8491, 0.1509],[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 105/203 [2m0:05:00 â€¢ 0:03:20[0m [2;4m0.49it/s[0m  
        [0.7492, 0.2508],
        [0.8101, 0.1899],
        [0.8338, 0.1662],
        [0.8450, 0.1550],
        [0.9032, 0.0968],
        [0.8531, 0.1469],
        [0.8812, 0.1188],
        [0.8634, 0.1366],
[2K        [0.8635, 0.1365]], device='cuda:0')m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 105/203 [2m0:05:00 â€¢ 0:03:20[0m [2;4m0.49it/s[0m  
[2KAttention weights: tensor([[0.5594, 0.4406],[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 106/203 [2m0:05:04 â€¢ 0:03:58[0m [2;4m0.41it/s[0m  
        [0.5531, 0.4469],
        [0.5555, 0.4445],
        [0.5574, 0.4426],
        [0.5555, 0.4445],
        [0.5542, 0.4458],
        [0.5533, 0.4467],
        [0.5591, 0.4409],
        [0.5575, 0.4425],
[2K        [0.5559, 0.4441]], device='cuda:0', grad_fn=<SoftmaxBackward0>);237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 106/203 [2m0:05:04 â€¢ 0:03:58[0m [2;4m0.41it/s[0m  
[2KClass label: TennisSwingâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 106/203 [2m0:05:04 â€¢ 0:03:58[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.6408, 0.3592]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 106/203 [2m0:05:04 â€¢ 0:03:58[0m [2;4m0.41it/s[0m  
[2KStep 0 - TTA Loss: 0.7234â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 106/203 [2m0:05:04 â€¢ 0:03:58[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.6580, 0.3420]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 106/203 [2m0:05:04 â€¢ 0:03:58[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.6279, 0.3721]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 106/203 [2m0:05:04 â€¢ 0:03:58[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.5556, 0.4444]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 106/203 [2m0:05:04 â€¢ 0:03:58[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.4614, 0.5386]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 106/203 [2m0:05:04 â€¢ 0:03:58[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.3814, 0.6186]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 106/203 [2m0:05:04 â€¢ 0:03:58[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.3357, 0.6643]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 106/203 [2m0:05:04 â€¢ 0:03:58[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.3173, 0.6827]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 106/203 [2m0:05:04 â€¢ 0:03:58[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.3135, 0.6865]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 106/203 [2m0:05:04 â€¢ 0:03:58[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.3144, 0.6856]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 106/203 [2m0:05:04 â€¢ 0:03:58[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.5487, 0.4513]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 106/203 [2m0:05:04 â€¢ 0:03:58[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.5450, 0.4550],[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 106/203 [2m0:05:04 â€¢ 0:03:58[0m [2;4m0.41it/s[0m  
        [0.4459, 0.5541],
        [0.5856, 0.4144],
        [0.4831, 0.5169],
        [0.5337, 0.4663],
        [0.6309, 0.3691],
        [0.5706, 0.4294],
        [0.6309, 0.3691],
        [0.3152, 0.6848],
[2K        [0.5313, 0.4687]], device='cuda:0')m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 106/203 [2m0:05:04 â€¢ 0:03:58[0m [2;4m0.41it/s[0m  
[2KAttention weights: tensor([[0.5593, 0.4407],â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 107/203 [2m0:05:07 â€¢ 0:03:59[0m [2;4m0.40it/s[0m   
        [0.5531, 0.4469],
        [0.5555, 0.4445],
        [0.5574, 0.4426],
        [0.5554, 0.4446],
        [0.5541, 0.4459],
        [0.5533, 0.4467],
        [0.5590, 0.4410],
        [0.5575, 0.4425],
[2K        [0.5559, 0.4441]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 107/203 [2m0:05:07 â€¢ 0:03:59[0m [2;4m0.40it/s[0m  
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 107/203 [2m0:05:07 â€¢ 0:03:59[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.6069, 0.3931]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 107/203 [2m0:05:07 â€¢ 0:03:59[0m [2;4m0.40it/s[0m  
[2KStep 0 - TTA Loss: 0.5464â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 107/203 [2m0:05:07 â€¢ 0:03:59[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.6069, 0.3931]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 107/203 [2m0:05:07 â€¢ 0:03:59[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.6095, 0.3905]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 107/203 [2m0:05:07 â€¢ 0:03:59[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.6219, 0.3781]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 107/203 [2m0:05:07 â€¢ 0:03:59[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.6516, 0.3484]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 107/203 [2m0:05:07 â€¢ 0:03:59[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.7064, 0.2936]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 107/203 [2m0:05:07 â€¢ 0:03:59[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.7848, 0.2152]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 107/203 [2m0:05:07 â€¢ 0:03:59[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.8713, 0.1287]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 107/203 [2m0:05:07 â€¢ 0:03:59[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.9379, 0.0621]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 107/203 [2m0:05:07 â€¢ 0:03:59[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.9740, 0.0260]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 107/203 [2m0:05:07 â€¢ 0:03:59[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.5742, 0.4258]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 107/203 [2m0:05:07 â€¢ 0:03:59[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.9578, 0.0422],m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 107/203 [2m0:05:07 â€¢ 0:03:59[0m [2;4m0.40it/s[0m  
        [0.8916, 0.1084],
        [0.9442, 0.0558],
        [0.9418, 0.0582],
        [0.9690, 0.0310],
        [0.9888, 0.0112],
        [0.9731, 0.0269],
        [0.9734, 0.0266],
        [0.9651, 0.0349],
[2K        [0.9691, 0.0309]], device='cuda:0')0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 107/203 [2m0:05:07 â€¢ 0:03:59[0m [2;4m0.40it/s[0m  
[2KAttention weights: tensor([[0.5594, 0.4406],â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 108/203 [2m0:05:11 â€¢ 0:04:19[0m [2;4m0.37it/s[0m  
        [0.5531, 0.4469],
        [0.5555, 0.4445],
        [0.5575, 0.4425],
        [0.5555, 0.4445],
        [0.5543, 0.4457],
        [0.5534, 0.4466],
        [0.5591, 0.4409],
        [0.5575, 0.4425],
[2K        [0.5559, 0.4441]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 108/203 [2m0:05:11 â€¢ 0:04:19[0m [2;4m0.37it/s[0m  
[2KClass label: CleanAndJerkâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 108/203 [2m0:05:11 â€¢ 0:04:19[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5689, 0.4311]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 108/203 [2m0:05:11 â€¢ 0:04:19[0m [2;4m0.37it/s[0m  
[2KStep 0 - TTA Loss: 0.6083â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 108/203 [2m0:05:11 â€¢ 0:04:19[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.6936, 0.3064]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 108/203 [2m0:05:11 â€¢ 0:04:19[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.7896, 0.2104]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 108/203 [2m0:05:11 â€¢ 0:04:19[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.8529, 0.1471]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 108/203 [2m0:05:11 â€¢ 0:04:19[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.8838, 0.1162]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 108/203 [2m0:05:11 â€¢ 0:04:19[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.8960, 0.1040]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 108/203 [2m0:05:11 â€¢ 0:04:19[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.8975, 0.1025]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 108/203 [2m0:05:11 â€¢ 0:04:19[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.8914, 0.1086]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 108/203 [2m0:05:11 â€¢ 0:04:19[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.8854, 0.1146]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 108/203 [2m0:05:11 â€¢ 0:04:19[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.8815, 0.1185]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 108/203 [2m0:05:11 â€¢ 0:04:19[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5654, 0.4346]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 108/203 [2m0:05:11 â€¢ 0:04:19[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.9283, 0.0717],m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 108/203 [2m0:05:11 â€¢ 0:04:19[0m [2;4m0.37it/s[0m  
        [0.8398, 0.1602],
        [0.8803, 0.1197],
        [0.9061, 0.0939],
        [0.9364, 0.0636],
        [0.9732, 0.0268],
        [0.9443, 0.0557],
        [0.9517, 0.0483],
        [0.9365, 0.0635],
[2K        [0.9414, 0.0586]], device='cuda:0')0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 108/203 [2m0:05:11 â€¢ 0:04:19[0m [2;4m0.37it/s[0m  
[2KAttention weights: tensor([[0.5595, 0.4405],â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 109/203 [2m0:05:15 â€¢ 0:05:04[0m [2;4m0.31it/s[0m  
        [0.5532, 0.4468],
        [0.5556, 0.4444],
        [0.5575, 0.4425],
        [0.5556, 0.4444],
        [0.5544, 0.4456],
        [0.5534, 0.4466],
        [0.5592, 0.4408],
        [0.5576, 0.4424],
[2K        [0.5560, 0.4440]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 109/203 [2m0:05:15 â€¢ 0:05:04[0m [2;4m0.31it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 109/203 [2m0:05:15 â€¢ 0:05:04[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.6288, 0.3712]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 109/203 [2m0:05:15 â€¢ 0:05:04[0m [2;4m0.31it/s[0m  
[2KStep 0 - TTA Loss: 0.8394â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 109/203 [2m0:05:15 â€¢ 0:05:04[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.6288, 0.3712]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 109/203 [2m0:05:15 â€¢ 0:05:04[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.6287, 0.3713]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 109/203 [2m0:05:15 â€¢ 0:05:04[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.6285, 0.3715]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 109/203 [2m0:05:15 â€¢ 0:05:04[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.6285, 0.3715]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 109/203 [2m0:05:15 â€¢ 0:05:04[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.6304, 0.3696]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 109/203 [2m0:05:15 â€¢ 0:05:04[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.6359, 0.3641]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 109/203 [2m0:05:15 â€¢ 0:05:04[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.6431, 0.3569]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 109/203 [2m0:05:15 â€¢ 0:05:04[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.6441, 0.3559]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 109/203 [2m0:05:15 â€¢ 0:05:04[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.6353, 0.3647]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 109/203 [2m0:05:15 â€¢ 0:05:04[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5554, 0.4446]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 109/203 [2m0:05:15 â€¢ 0:05:04[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.6519, 0.3481],m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 109/203 [2m0:05:15 â€¢ 0:05:04[0m [2;4m0.31it/s[0m  
        [0.5209, 0.4791],
        [0.3249, 0.6751],
        [0.6478, 0.3522],
        [0.5268, 0.4732],
        [0.6145, 0.3855],
        [0.5603, 0.4397],
        [0.6895, 0.3105],
        [0.6536, 0.3464],
[2K        [0.6306, 0.3694]], device='cuda:0')0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 109/203 [2m0:05:15 â€¢ 0:05:04[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5595, 0.4405],â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 110/203 [2m0:05:21 â€¢ 0:05:25[0m [2;4m0.29it/s[0m  
        [0.5532, 0.4468],
        [0.5556, 0.4444],
        [0.5575, 0.4425],
        [0.5556, 0.4444],
        [0.5544, 0.4456],
        [0.5535, 0.4465],
        [0.5592, 0.4408],
        [0.5576, 0.4424],
[2K        [0.5560, 0.4440]], device='cuda:0', grad_fn=<SoftmaxBackward0>)5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 110/203 [2m0:05:21 â€¢ 0:05:25[0m [2;4m0.29it/s[0m  
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 110/203 [2m0:05:21 â€¢ 0:05:25[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.6013, 0.3987]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 110/203 [2m0:05:21 â€¢ 0:05:25[0m [2;4m0.29it/s[0m  
[2KStep 0 - TTA Loss: 0.7134â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 110/203 [2m0:05:21 â€¢ 0:05:25[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.6103, 0.3897]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 110/203 [2m0:05:21 â€¢ 0:05:25[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.6261, 0.3739]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 110/203 [2m0:05:21 â€¢ 0:05:25[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.6480, 0.3520]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 110/203 [2m0:05:21 â€¢ 0:05:25[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.6728, 0.3272]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 110/203 [2m0:05:21 â€¢ 0:05:25[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.6967, 0.3033]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 110/203 [2m0:05:21 â€¢ 0:05:25[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.7149, 0.2851]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 110/203 [2m0:05:21 â€¢ 0:05:25[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.7284, 0.2716]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 110/203 [2m0:05:21 â€¢ 0:05:25[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.7355, 0.2645]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 110/203 [2m0:05:21 â€¢ 0:05:25[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.7384, 0.2616]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 110/203 [2m0:05:21 â€¢ 0:05:25[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.5582, 0.4418]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 110/203 [2m0:05:21 â€¢ 0:05:25[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.7203, 0.2797],m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 110/203 [2m0:05:21 â€¢ 0:05:25[0m [2;4m0.29it/s[0m  
        [0.6003, 0.3997],
        [0.4938, 0.5062],
        [0.7074, 0.2926],
        [0.6445, 0.3555],
        [0.7392, 0.2608],
        [0.6697, 0.3303],
        [0.7669, 0.2331],
        [0.7272, 0.2728],
[2K        [0.7122, 0.2878]], device='cuda:0')0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 110/203 [2m0:05:21 â€¢ 0:05:25[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.5595, 0.4405],â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 111/203 [2m0:05:23 â€¢ 0:05:05[0m [2;4m0.30it/s[0m  
        [0.5532, 0.4468],
        [0.5556, 0.4444],
        [0.5575, 0.4425],
        [0.5556, 0.4444],
        [0.5544, 0.4456],
        [0.5535, 0.4465],
        [0.5592, 0.4408],
        [0.5576, 0.4424],
[2K        [0.5560, 0.4440]], device='cuda:0', grad_fn=<SoftmaxBackward0>)5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 111/203 [2m0:05:23 â€¢ 0:05:05[0m [2;4m0.30it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 111/203 [2m0:05:23 â€¢ 0:05:05[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.6291, 0.3709]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 111/203 [2m0:05:23 â€¢ 0:05:05[0m [2;4m0.30it/s[0m  
[2KStep 0 - TTA Loss: 0.4805â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 111/203 [2m0:05:23 â€¢ 0:05:05[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.6291, 0.3709]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 111/203 [2m0:05:23 â€¢ 0:05:05[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.6298, 0.3702]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 111/203 [2m0:05:23 â€¢ 0:05:05[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.6332, 0.3668]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 111/203 [2m0:05:23 â€¢ 0:05:05[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.6413, 0.3587]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 111/203 [2m0:05:23 â€¢ 0:05:05[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.6532, 0.3468]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 111/203 [2m0:05:23 â€¢ 0:05:05[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.6664, 0.3336]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 111/203 [2m0:05:23 â€¢ 0:05:05[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.6786, 0.3214]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 111/203 [2m0:05:23 â€¢ 0:05:05[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.6961, 0.3039]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 111/203 [2m0:05:23 â€¢ 0:05:05[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.7392, 0.2608]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 111/203 [2m0:05:23 â€¢ 0:05:05[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.5608, 0.4392]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 111/203 [2m0:05:23 â€¢ 0:05:05[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.7857, 0.2143],m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 111/203 [2m0:05:23 â€¢ 0:05:05[0m [2;4m0.30it/s[0m  
        [0.6646, 0.3354],
        [0.6794, 0.3206],
        [0.7913, 0.2087],
        [0.7477, 0.2523],
        [0.7815, 0.2185],
        [0.7475, 0.2525],
        [0.7886, 0.2114],
        [0.7941, 0.2059],
[2K        [0.8175, 0.1825]], device='cuda:0')0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 111/203 [2m0:05:23 â€¢ 0:05:05[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.5595, 0.4405],â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 112/203 [2m0:05:27 â€¢ 0:05:54[0m [2;4m0.26it/s[0m   
        [0.5532, 0.4468],
        [0.5556, 0.4444],
        [0.5575, 0.4425],
        [0.5556, 0.4444],
        [0.5544, 0.4456],
        [0.5535, 0.4465],
        [0.5592, 0.4408],
        [0.5576, 0.4424],
[2K        [0.5560, 0.4440]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 112/203 [2m0:05:27 â€¢ 0:05:54[0m [2;4m0.26it/s[0m  
[2KClass label: GolfSwingâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 112/203 [2m0:05:27 â€¢ 0:05:54[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.6501, 0.3499]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 112/203 [2m0:05:27 â€¢ 0:05:54[0m [2;4m0.26it/s[0m  
[2KStep 0 - TTA Loss: 0.7510â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 112/203 [2m0:05:27 â€¢ 0:05:54[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.7204, 0.2796]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 112/203 [2m0:05:27 â€¢ 0:05:54[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.7767, 0.2233]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 112/203 [2m0:05:27 â€¢ 0:05:54[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.8192, 0.1808]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 112/203 [2m0:05:27 â€¢ 0:05:54[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.8498, 0.1502]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 112/203 [2m0:05:27 â€¢ 0:05:54[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.8687, 0.1313]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 112/203 [2m0:05:27 â€¢ 0:05:54[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.8797, 0.1203]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 112/203 [2m0:05:27 â€¢ 0:05:54[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.8851, 0.1149]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 112/203 [2m0:05:27 â€¢ 0:05:54[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.8869, 0.1131]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 112/203 [2m0:05:27 â€¢ 0:05:54[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.8872, 0.1128]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 112/203 [2m0:05:27 â€¢ 0:05:54[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.5670, 0.4330]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 112/203 [2m0:05:27 â€¢ 0:05:54[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.8715, 0.1285],0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 112/203 [2m0:05:27 â€¢ 0:05:54[0m [2;4m0.26it/s[0m  
        [0.7472, 0.2528],
        [0.7771, 0.2229],
        [0.8873, 0.1127],
        [0.8576, 0.1424],
        [0.8681, 0.1319],
        [0.8498, 0.1502],
        [0.8478, 0.1522],
        [0.8827, 0.1173],
[2K        [0.9170, 0.0830]], device='cuda:0')[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 112/203 [2m0:05:27 â€¢ 0:05:54[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.5595, 0.4405],â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 113/203 [2m0:05:30 â€¢ 0:05:32[0m [2;4m0.27it/s[0m  
        [0.5532, 0.4468],
        [0.5557, 0.4443],
        [0.5576, 0.4424],
        [0.5556, 0.4444],
        [0.5544, 0.4456],
        [0.5535, 0.4465],
        [0.5592, 0.4408],
        [0.5577, 0.4423],
[2K        [0.5561, 0.4439]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 113/203 [2m0:05:30 â€¢ 0:05:32[0m [2;4m0.27it/s[0m  
[2KClass label: TennisSwingâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 113/203 [2m0:05:30 â€¢ 0:05:32[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6401, 0.3599]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 113/203 [2m0:05:30 â€¢ 0:05:32[0m [2;4m0.27it/s[0m  
[2KStep 0 - TTA Loss: 0.4433â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 113/203 [2m0:05:30 â€¢ 0:05:32[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6401, 0.3599]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 113/203 [2m0:05:30 â€¢ 0:05:32[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6403, 0.3597]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 113/203 [2m0:05:30 â€¢ 0:05:32[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6419, 0.3581]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 113/203 [2m0:05:30 â€¢ 0:05:32[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6467, 0.3533]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 113/203 [2m0:05:30 â€¢ 0:05:32[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6564, 0.3436]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 113/203 [2m0:05:30 â€¢ 0:05:32[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6706, 0.3294]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 113/203 [2m0:05:30 â€¢ 0:05:32[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6854, 0.3146]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 113/203 [2m0:05:30 â€¢ 0:05:32[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6985, 0.3015]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 113/203 [2m0:05:30 â€¢ 0:05:32[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.7005, 0.2995]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 113/203 [2m0:05:30 â€¢ 0:05:32[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.5582, 0.4418]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 113/203 [2m0:05:30 â€¢ 0:05:32[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6831, 0.3169],0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 113/203 [2m0:05:30 â€¢ 0:05:32[0m [2;4m0.27it/s[0m  
        [0.5886, 0.4114],
        [0.6031, 0.3969],
        [0.6667, 0.3333],
        [0.6198, 0.3802],
        [0.6567, 0.3433],
        [0.6202, 0.3798],
        [0.7013, 0.2987],
        [0.7006, 0.2994],
[2K        [0.6911, 0.3089]], device='cuda:0')[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 113/203 [2m0:05:30 â€¢ 0:05:32[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.5595, 0.4405],â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 114/203 [2m0:05:34 â€¢ 0:05:30[0m [2;4m0.27it/s[0m  
        [0.5533, 0.4467],
        [0.5557, 0.4443],
        [0.5576, 0.4424],
        [0.5557, 0.4443],
        [0.5545, 0.4455],
        [0.5535, 0.4465],
        [0.5592, 0.4408],
        [0.5577, 0.4423],
[2K        [0.5561, 0.4439]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 114/203 [2m0:05:34 â€¢ 0:05:30[0m [2;4m0.27it/s[0m  
[2KClass label: HammerThrowâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 114/203 [2m0:05:34 â€¢ 0:05:30[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.5589, 0.4411]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 114/203 [2m0:05:34 â€¢ 0:05:30[0m [2;4m0.27it/s[0m  
[2KStep 0 - TTA Loss: 0.5489â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 114/203 [2m0:05:34 â€¢ 0:05:30[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6023, 0.3977]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 114/203 [2m0:05:34 â€¢ 0:05:30[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6776, 0.3224]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 114/203 [2m0:05:34 â€¢ 0:05:30[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.7667, 0.2333]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 114/203 [2m0:05:34 â€¢ 0:05:30[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.8525, 0.1475]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 114/203 [2m0:05:34 â€¢ 0:05:30[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.9146, 0.0854]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 114/203 [2m0:05:34 â€¢ 0:05:30[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.9472, 0.0528]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 114/203 [2m0:05:34 â€¢ 0:05:30[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.9626, 0.0374]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 114/203 [2m0:05:34 â€¢ 0:05:30[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.9692, 0.0308]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 114/203 [2m0:05:34 â€¢ 0:05:30[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.9714, 0.0286]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 114/203 [2m0:05:34 â€¢ 0:05:30[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.5758, 0.4242]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 114/203 [2m0:05:34 â€¢ 0:05:30[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.9520, 0.0480],0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 114/203 [2m0:05:34 â€¢ 0:05:30[0m [2;4m0.27it/s[0m  
        [0.8062, 0.1938],
        [0.9041, 0.0959],
        [0.8907, 0.1093],
        [0.9718, 0.0282],
        [0.9344, 0.0656],
        [0.9114, 0.0886],
        [0.9281, 0.0719],
        [0.9083, 0.0917],
[2K        [0.9415, 0.0585]], device='cuda:0')[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 114/203 [2m0:05:34 â€¢ 0:05:30[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.5596, 0.4404],â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 115/203 [2m0:05:50 â€¢ 0:08:20[0m [2;4m0.18it/s[0m  
        [0.5533, 0.4467],
        [0.5557, 0.4443],
        [0.5576, 0.4424],
        [0.5557, 0.4443],
        [0.5545, 0.4455],
        [0.5536, 0.4464],
        [0.5593, 0.4407],
        [0.5577, 0.4423],
[2K        [0.5562, 0.4438]], device='cuda:0', grad_fn=<SoftmaxBackward0>);5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 115/203 [2m0:05:50 â€¢ 0:08:20[0m [2;4m0.18it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 115/203 [2m0:05:50 â€¢ 0:08:20[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.6241, 0.3759]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 115/203 [2m0:05:50 â€¢ 0:08:20[0m [2;4m0.18it/s[0m  
[2KStep 0 - TTA Loss: 0.8159â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 115/203 [2m0:05:50 â€¢ 0:08:20[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.6241, 0.3759]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 115/203 [2m0:05:50 â€¢ 0:08:20[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.6264, 0.3736]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 115/203 [2m0:05:50 â€¢ 0:08:20[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.6353, 0.3647]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 115/203 [2m0:05:50 â€¢ 0:08:20[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.6548, 0.3452]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 115/203 [2m0:05:50 â€¢ 0:08:20[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.6859, 0.3141]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 115/203 [2m0:05:50 â€¢ 0:08:20[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.7284, 0.2716]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 115/203 [2m0:05:50 â€¢ 0:08:20[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.7790, 0.2210]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 115/203 [2m0:05:50 â€¢ 0:08:20[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.8294, 0.1706]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 115/203 [2m0:05:50 â€¢ 0:08:20[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.8719, 0.1281]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 115/203 [2m0:05:50 â€¢ 0:08:20[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.5679, 0.4321]], device='cuda:0')â•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 115/203 [2m0:05:50 â€¢ 0:08:20[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.8867, 0.1133],0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 115/203 [2m0:05:50 â€¢ 0:08:20[0m [2;4m0.18it/s[0m  
        [0.7398, 0.2602],
        [0.8090, 0.1911],
        [0.8594, 0.1406],
        [0.8950, 0.1050],
        [0.8648, 0.1352],
        [0.8414, 0.1586],
        [0.8578, 0.1422],
        [0.8642, 0.1358],
[2K        [0.9014, 0.0986]], device='cuda:0')[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 115/203 [2m0:05:50 â€¢ 0:08:20[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.5596, 0.4404],â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 116/203 [2m0:05:51 â€¢ 0:07:15[0m [2;4m0.20it/s[0m  
        [0.5533, 0.4467],
        [0.5558, 0.4442],
        [0.5577, 0.4423],
        [0.5558, 0.4442],
        [0.5545, 0.4455],
        [0.5536, 0.4464],
        [0.5593, 0.4407],
        [0.5578, 0.4422],
[2K        [0.5563, 0.4437]], device='cuda:0', grad_fn=<SoftmaxBackward0>);5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 116/203 [2m0:05:51 â€¢ 0:07:15[0m [2;4m0.20it/s[0m  
[2KClass label: HammerThrowâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 116/203 [2m0:05:51 â€¢ 0:07:15[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.5593, 0.4407]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 116/203 [2m0:05:51 â€¢ 0:07:15[0m [2;4m0.20it/s[0m  
[2KStep 0 - TTA Loss: 0.5904â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 116/203 [2m0:05:51 â€¢ 0:07:15[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.6464, 0.3536]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 116/203 [2m0:05:51 â€¢ 0:07:15[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.7287, 0.2713]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 116/203 [2m0:05:51 â€¢ 0:07:15[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.7980, 0.2020]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 116/203 [2m0:05:51 â€¢ 0:07:15[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.8420, 0.1580]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 116/203 [2m0:05:51 â€¢ 0:07:15[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.8639, 0.1361]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 116/203 [2m0:05:51 â€¢ 0:07:15[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.8769, 0.1231]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 116/203 [2m0:05:51 â€¢ 0:07:15[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.8882, 0.1118]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 116/203 [2m0:05:51 â€¢ 0:07:15[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.8975, 0.1025]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 116/203 [2m0:05:51 â€¢ 0:07:15[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.9026, 0.0974]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 116/203 [2m0:05:51 â€¢ 0:07:15[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.5679, 0.4321]], device='cuda:0')â•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 116/203 [2m0:05:51 â€¢ 0:07:15[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.8909, 0.1091],0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 116/203 [2m0:05:51 â€¢ 0:07:15[0m [2;4m0.20it/s[0m  
        [0.7420, 0.2580],
        [0.8176, 0.1824],
        [0.8551, 0.1449],
        [0.9044, 0.0956],
        [0.8693, 0.1307],
        [0.8431, 0.1569],
        [0.8650, 0.1350],
        [0.8621, 0.1379],
[2K        [0.8974, 0.1026]], device='cuda:0')[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 116/203 [2m0:05:51 â€¢ 0:07:15[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.5597, 0.4403],â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 117/203 [2m0:05:53 â€¢ 0:07:10[0m [2;4m0.20it/s[0m   
        [0.5534, 0.4466],
        [0.5558, 0.4442],
        [0.5577, 0.4423],
        [0.5559, 0.4441],
        [0.5546, 0.4454],
        [0.5536, 0.4464],
        [0.5593, 0.4407],
        [0.5578, 0.4422],
[2K        [0.5563, 0.4437]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 117/203 [2m0:05:53 â€¢ 0:07:10[0m [2;4m0.20it/s[0m  
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 117/203 [2m0:05:53 â€¢ 0:07:10[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.6039, 0.3961]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 117/203 [2m0:05:53 â€¢ 0:07:10[0m [2;4m0.20it/s[0m  
[2KStep 0 - TTA Loss: 0.8400â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 117/203 [2m0:05:53 â€¢ 0:07:10[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.6039, 0.3961]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 117/203 [2m0:05:53 â€¢ 0:07:10[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.6073, 0.3927]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 117/203 [2m0:05:53 â€¢ 0:07:10[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.6204, 0.3796]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 117/203 [2m0:05:53 â€¢ 0:07:10[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.6489, 0.3511]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 117/203 [2m0:05:53 â€¢ 0:07:10[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.6970, 0.3030]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 117/203 [2m0:05:53 â€¢ 0:07:10[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.7616, 0.2384]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 117/203 [2m0:05:53 â€¢ 0:07:10[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.8343, 0.1657]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 117/203 [2m0:05:53 â€¢ 0:07:10[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.8986, 0.1014]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 117/203 [2m0:05:53 â€¢ 0:07:10[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.9447, 0.0553]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 117/203 [2m0:05:53 â€¢ 0:07:10[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.5710, 0.4290]], device='cuda:0')m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 117/203 [2m0:05:53 â€¢ 0:07:10[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.9425, 0.0575],[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 117/203 [2m0:05:53 â€¢ 0:07:10[0m [2;4m0.20it/s[0m  
        [0.8343, 0.1657],
        [0.9078, 0.0922],
        [0.9093, 0.0907],
        [0.9579, 0.0421],
        [0.9614, 0.0386],
        [0.9346, 0.0654],
        [0.9445, 0.0555],
        [0.9287, 0.0713],
[2K        [0.9444, 0.0556]], device='cuda:0')[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 117/203 [2m0:05:53 â€¢ 0:07:10[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.5597, 0.4403],â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 118/203 [2m0:05:55 â€¢ 0:06:34[0m [2;4m0.22it/s[0m  
        [0.5534, 0.4466],
        [0.5559, 0.4441],
        [0.5577, 0.4423],
        [0.5559, 0.4441],
        [0.5547, 0.4453],
        [0.5537, 0.4463],
        [0.5594, 0.4406],
        [0.5579, 0.4421],
[2K        [0.5564, 0.4436]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 118/203 [2m0:05:55 â€¢ 0:06:34[0m [2;4m0.22it/s[0m  
[2KClass label: Billiardsâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 118/203 [2m0:05:55 â€¢ 0:06:34[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.5581, 0.4419]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 118/203 [2m0:05:55 â€¢ 0:06:34[0m [2;4m0.22it/s[0m  
[2KStep 0 - TTA Loss: 0.4760â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 118/203 [2m0:05:55 â€¢ 0:06:34[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.6260, 0.3740]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 118/203 [2m0:05:55 â€¢ 0:06:34[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.6983, 0.3017]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 118/203 [2m0:05:55 â€¢ 0:06:34[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.7555, 0.2445]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 118/203 [2m0:05:55 â€¢ 0:06:34[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.7959, 0.2041]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 118/203 [2m0:05:55 â€¢ 0:06:34[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.8268, 0.1732]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 118/203 [2m0:05:55 â€¢ 0:06:34[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.8541, 0.1459]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 118/203 [2m0:05:55 â€¢ 0:06:34[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.8751, 0.1249]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 118/203 [2m0:05:55 â€¢ 0:06:34[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.8891, 0.1109]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 118/203 [2m0:05:55 â€¢ 0:06:34[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.8961, 0.1039]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 118/203 [2m0:05:55 â€¢ 0:06:34[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.5666, 0.4334]], device='cuda:0')m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 118/203 [2m0:05:55 â€¢ 0:06:34[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.9023, 0.0977],[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 118/203 [2m0:05:55 â€¢ 0:06:34[0m [2;4m0.22it/s[0m  
        [0.8981, 0.1019],
        [0.8522, 0.1478],
        [0.8758, 0.1242],
        [0.9011, 0.0989],
        [0.9057, 0.0943],
        [0.8696, 0.1304],
        [0.9084, 0.0916],
        [0.8928, 0.1072],
[2K        [0.8929, 0.1071]], device='cuda:0')[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 118/203 [2m0:05:55 â€¢ 0:06:34[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.5598, 0.4402],â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 119/203 [2m0:06:09 â€¢ 0:06:42[0m [2;4m0.21it/s[0m  
        [0.5535, 0.4465],
        [0.5559, 0.4441],
        [0.5578, 0.4422],
        [0.5560, 0.4440],
        [0.5548, 0.4452],
        [0.5538, 0.4462],
        [0.5594, 0.4406],
        [0.5579, 0.4421],
[2K        [0.5565, 0.4435]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 119/203 [2m0:06:09 â€¢ 0:06:42[0m [2;4m0.21it/s[0m  
[2KClass label: HammerThrowâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 119/203 [2m0:06:09 â€¢ 0:06:42[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.5610, 0.4390]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 119/203 [2m0:06:09 â€¢ 0:06:42[0m [2;4m0.21it/s[0m  
[2KStep 0 - TTA Loss: 0.5825â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 119/203 [2m0:06:09 â€¢ 0:06:42[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.5610, 0.4390]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 119/203 [2m0:06:09 â€¢ 0:06:42[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.5644, 0.4356]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 119/203 [2m0:06:09 â€¢ 0:06:42[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.5796, 0.4204]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 119/203 [2m0:06:09 â€¢ 0:06:42[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.6143, 0.3857]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 119/203 [2m0:06:09 â€¢ 0:06:42[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.6736, 0.3264]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 119/203 [2m0:06:09 â€¢ 0:06:42[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.7573, 0.2427]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 119/203 [2m0:06:09 â€¢ 0:06:42[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.8507, 0.1493]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 119/203 [2m0:06:09 â€¢ 0:06:42[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.9265, 0.0735]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 119/203 [2m0:06:09 â€¢ 0:06:42[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.9694, 0.0306]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 119/203 [2m0:06:09 â€¢ 0:06:42[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.5770, 0.4230]], device='cuda:0')m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 119/203 [2m0:06:09 â€¢ 0:06:42[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.9781, 0.0219],[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 119/203 [2m0:06:09 â€¢ 0:06:42[0m [2;4m0.21it/s[0m  
        [0.9613, 0.0387],
        [0.9504, 0.0496],
        [0.9497, 0.0503],
        [0.9866, 0.0134],
        [0.9685, 0.0315],
        [0.9544, 0.0456],
        [0.9683, 0.0317],
        [0.9600, 0.0400],
[2K        [0.9690, 0.0310]], device='cuda:0')[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 119/203 [2m0:06:09 â€¢ 0:06:42[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.5599, 0.4401],â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 120/203 [2m0:06:15 â€¢ 0:07:08[0m [2;4m0.19it/s[0m  
        [0.5536, 0.4464],
        [0.5560, 0.4440],
        [0.5579, 0.4421],
        [0.5561, 0.4439],
        [0.5548, 0.4452],
        [0.5539, 0.4461],
        [0.5595, 0.4405],
        [0.5580, 0.4420],
[2K        [0.5566, 0.4434]], device='cuda:0', grad_fn=<SoftmaxBackward0>)8;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 120/203 [2m0:06:15 â€¢ 0:07:08[0m [2;4m0.19it/s[0m  
[2KClass label: SoccerPenaltyâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 120/203 [2m0:06:15 â€¢ 0:07:08[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.6645, 0.3355]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 120/203 [2m0:06:15 â€¢ 0:07:08[0m [2;4m0.19it/s[0m  
[2KStep 0 - TTA Loss: 0.9576â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 120/203 [2m0:06:15 â€¢ 0:07:08[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.7854, 0.2146]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 120/203 [2m0:06:15 â€¢ 0:07:08[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.8799, 0.1201]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 120/203 [2m0:06:15 â€¢ 0:07:08[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.9393, 0.0607]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 120/203 [2m0:06:15 â€¢ 0:07:08[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.9645, 0.0355]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 120/203 [2m0:06:15 â€¢ 0:07:08[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.9759, 0.0241]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 120/203 [2m0:06:15 â€¢ 0:07:08[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.9761, 0.0239]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 120/203 [2m0:06:15 â€¢ 0:07:08[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.9741, 0.0259]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 120/203 [2m0:06:15 â€¢ 0:07:08[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.9693, 0.0307]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 120/203 [2m0:06:15 â€¢ 0:07:08[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.9667, 0.0333]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 120/203 [2m0:06:15 â€¢ 0:07:08[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.5735, 0.4265]], device='cuda:0')mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 120/203 [2m0:06:15 â€¢ 0:07:08[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.9683, 0.0317],[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 120/203 [2m0:06:15 â€¢ 0:07:08[0m [2;4m0.19it/s[0m  
        [0.9307, 0.0693],
        [0.9264, 0.0736],
        [0.9308, 0.0692],
        [0.9771, 0.0229],
        [0.9552, 0.0448],
        [0.9345, 0.0655],
        [0.9659, 0.0341],
        [0.9434, 0.0566],
[2K        [0.9536, 0.0464]], device='cuda:0')[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 120/203 [2m0:06:15 â€¢ 0:07:08[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.5600, 0.4400],â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 121/203 [2m0:06:16 â€¢ 0:06:04[0m [2;4m0.23it/s[0m  
        [0.5537, 0.4463],
        [0.5561, 0.4439],
        [0.5579, 0.4421],
        [0.5563, 0.4437],
        [0.5549, 0.4451],
        [0.5540, 0.4460],
        [0.5596, 0.4404],
        [0.5581, 0.4419],
[2K        [0.5567, 0.4433]], device='cuda:0', grad_fn=<SoftmaxBackward0>)8;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 121/203 [2m0:06:16 â€¢ 0:06:04[0m [2;4m0.23it/s[0m  
[2KClass label: SoccerPenaltyâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 121/203 [2m0:06:16 â€¢ 0:06:04[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.6675, 0.3325]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 121/203 [2m0:06:16 â€¢ 0:06:04[0m [2;4m0.23it/s[0m  
[2KStep 0 - TTA Loss: 0.9984â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 121/203 [2m0:06:16 â€¢ 0:06:04[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.6675, 0.3325]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 121/203 [2m0:06:16 â€¢ 0:06:04[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.6648, 0.3352]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 121/203 [2m0:06:16 â€¢ 0:06:04[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.6584, 0.3416]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 121/203 [2m0:06:16 â€¢ 0:06:04[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.6506, 0.3494]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 121/203 [2m0:06:16 â€¢ 0:06:04[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.6489, 0.3511]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 121/203 [2m0:06:16 â€¢ 0:06:04[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.6593, 0.3407]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 121/203 [2m0:06:16 â€¢ 0:06:04[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.6792, 0.3208]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 121/203 [2m0:06:16 â€¢ 0:06:04[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.7011, 0.2989]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 121/203 [2m0:06:16 â€¢ 0:06:04[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.7150, 0.2850]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 121/203 [2m0:06:16 â€¢ 0:06:04[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.5590, 0.4410]], device='cuda:0')mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 121/203 [2m0:06:16 â€¢ 0:06:04[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.7256, 0.2744],[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 121/203 [2m0:06:16 â€¢ 0:06:04[0m [2;4m0.23it/s[0m  
        [0.6453, 0.3547],
        [0.6568, 0.3432],
        [0.7138, 0.2862],
        [0.6850, 0.3150],
        [0.6875, 0.3125],
        [0.6495, 0.3505],
        [0.7215, 0.2785],
        [0.7141, 0.2859],
[2K        [0.7124, 0.2876]], device='cuda:0')[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 121/203 [2m0:06:16 â€¢ 0:06:04[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.5600, 0.4400],â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 122/203 [2m0:06:17 â€¢ 0:05:14[0m [2;4m0.26it/s[0m   
        [0.5537, 0.4463],
        [0.5562, 0.4438],
        [0.5580, 0.4420],
        [0.5563, 0.4437],
        [0.5550, 0.4450],
        [0.5540, 0.4460],
        [0.5596, 0.4404],
        [0.5581, 0.4419],
[2K        [0.5567, 0.4433]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 122/203 [2m0:06:17 â€¢ 0:05:14[0m [2;4m0.26it/s[0m  
[2KClass label: SoccerPenaltyâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 122/203 [2m0:06:17 â€¢ 0:05:14[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.6667, 0.3333]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 122/203 [2m0:06:17 â€¢ 0:05:14[0m [2;4m0.26it/s[0m  
[2KStep 0 - TTA Loss: 0.4034â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 122/203 [2m0:06:17 â€¢ 0:05:14[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.7029, 0.2971]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 122/203 [2m0:06:17 â€¢ 0:05:14[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.7471, 0.2529]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 122/203 [2m0:06:17 â€¢ 0:05:14[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.7840, 0.2160]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 122/203 [2m0:06:17 â€¢ 0:05:14[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.8098, 0.1902]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 122/203 [2m0:06:17 â€¢ 0:05:14[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.8301, 0.1699]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 122/203 [2m0:06:17 â€¢ 0:05:14[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.8485, 0.1515]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 122/203 [2m0:06:17 â€¢ 0:05:14[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.8645, 0.1355]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 122/203 [2m0:06:17 â€¢ 0:05:14[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.8763, 0.1237]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 122/203 [2m0:06:17 â€¢ 0:05:14[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.8825, 0.1175]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 122/203 [2m0:06:17 â€¢ 0:05:14[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.5641, 0.4359]], device='cuda:0')0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 122/203 [2m0:06:17 â€¢ 0:05:14[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.8091, 0.1909],[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 122/203 [2m0:06:17 â€¢ 0:05:14[0m [2;4m0.26it/s[0m  
        [0.7104, 0.2896],
        [0.6936, 0.3064],
        [0.7668, 0.2332],
        [0.7529, 0.2471],
        [0.7721, 0.2279],
        [0.7125, 0.2875],
        [0.8844, 0.1156],
        [0.7793, 0.2207],
[2K        [0.7597, 0.2403]], device='cuda:0')â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 122/203 [2m0:06:17 â€¢ 0:05:14[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.5600, 0.4400],â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 123/203 [2m0:06:19 â€¢ 0:04:51[0m [2;4m0.28it/s[0m  
        [0.5537, 0.4463],
        [0.5562, 0.4438],
        [0.5580, 0.4420],
        [0.5564, 0.4436],
        [0.5550, 0.4450],
        [0.5541, 0.4459],
        [0.5597, 0.4403],
        [0.5582, 0.4418],
[2K        [0.5568, 0.4432]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 123/203 [2m0:06:19 â€¢ 0:04:51[0m [2;4m0.28it/s[0m  
[2KClass label: TennisSwingâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 123/203 [2m0:06:19 â€¢ 0:04:51[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6421, 0.3579]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 123/203 [2m0:06:19 â€¢ 0:04:51[0m [2;4m0.28it/s[0m  
[2KStep 0 - TTA Loss: 0.5485â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 123/203 [2m0:06:19 â€¢ 0:04:51[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6421, 0.3579]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 123/203 [2m0:06:19 â€¢ 0:04:51[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6455, 0.3545]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 123/203 [2m0:06:19 â€¢ 0:04:51[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6617, 0.3383]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 123/203 [2m0:06:19 â€¢ 0:04:51[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.7007, 0.2993]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 123/203 [2m0:06:19 â€¢ 0:04:51[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.7650, 0.2350]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 123/203 [2m0:06:19 â€¢ 0:04:51[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.8530, 0.1470]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 123/203 [2m0:06:19 â€¢ 0:04:51[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.9346, 0.0654]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 123/203 [2m0:06:19 â€¢ 0:04:51[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.9805, 0.0195]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 123/203 [2m0:06:19 â€¢ 0:04:51[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.9947, 0.0053]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 123/203 [2m0:06:19 â€¢ 0:04:51[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.5863, 0.4137]], device='cuda:0')0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 123/203 [2m0:06:19 â€¢ 0:04:51[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.9872, 0.0128],[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 123/203 [2m0:06:19 â€¢ 0:04:51[0m [2;4m0.28it/s[0m  
        [0.9688, 0.0312],
        [0.9309, 0.0691],
        [0.9883, 0.0117],
        [0.9766, 0.0234],
        [0.9835, 0.0165],
        [0.9679, 0.0321],
        [0.9923, 0.0077],
        [0.9985, 0.0015],
[2K        [0.9861, 0.0139]], device='cuda:0')â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 123/203 [2m0:06:19 â€¢ 0:04:51[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.5601, 0.4399],â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 124/203 [2m0:06:23 â€¢ 0:05:38[0m [2;4m0.23it/s[0m  
        [0.5538, 0.4462],
        [0.5563, 0.4437],
        [0.5581, 0.4419],
        [0.5564, 0.4436],
        [0.5551, 0.4449],
        [0.5541, 0.4459],
        [0.5598, 0.4402],
        [0.5583, 0.4417],
[2K        [0.5568, 0.4432]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 124/203 [2m0:06:23 â€¢ 0:05:38[0m [2;4m0.23it/s[0m  
[2KClass label: SoccerPenaltyâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 124/203 [2m0:06:23 â€¢ 0:05:38[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.6661, 0.3339]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 124/203 [2m0:06:23 â€¢ 0:05:38[0m [2;4m0.23it/s[0m  
[2KStep 0 - TTA Loss: 0.8046â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 124/203 [2m0:06:23 â€¢ 0:05:38[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.8300, 0.1700]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 124/203 [2m0:06:23 â€¢ 0:05:38[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.9285, 0.0715]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 124/203 [2m0:06:23 â€¢ 0:05:38[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.9733, 0.0267]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 124/203 [2m0:06:23 â€¢ 0:05:38[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.9877, 0.0123]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 124/203 [2m0:06:23 â€¢ 0:05:38[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.9924, 0.0076]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 124/203 [2m0:06:23 â€¢ 0:05:38[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.9941, 0.0059]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 124/203 [2m0:06:23 â€¢ 0:05:38[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.9944, 0.0056]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 124/203 [2m0:06:23 â€¢ 0:05:38[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.9938, 0.0062]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 124/203 [2m0:06:23 â€¢ 0:05:38[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.9933, 0.0067]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 124/203 [2m0:06:23 â€¢ 0:05:38[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.5810, 0.4190]], device='cuda:0')0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 124/203 [2m0:06:23 â€¢ 0:05:38[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.9851, 0.0149],[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 124/203 [2m0:06:23 â€¢ 0:05:38[0m [2;4m0.23it/s[0m  
        [0.9629, 0.0371],
        [0.9208, 0.0792],
        [0.9843, 0.0157],
        [0.9720, 0.0280],
        [0.9804, 0.0196],
        [0.9613, 0.0387],
        [0.9931, 0.0069],
        [0.9975, 0.0025],
[2K        [0.9815, 0.0185]], device='cuda:0')â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 124/203 [2m0:06:23 â€¢ 0:05:38[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.5601, 0.4399],â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 125/203 [2m0:06:25 â€¢ 0:05:34[0m [2;4m0.23it/s[0m  
        [0.5538, 0.4462],
        [0.5563, 0.4437],
        [0.5581, 0.4419],
        [0.5564, 0.4436],
        [0.5551, 0.4449],
        [0.5541, 0.4459],
        [0.5597, 0.4403],
        [0.5583, 0.4417],
[2K        [0.5568, 0.4432]], device='cuda:0', grad_fn=<SoftmaxBackward0>)38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 125/203 [2m0:06:25 â€¢ 0:05:34[0m [2;4m0.23it/s[0m  
[2KClass label: Billiardsâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 125/203 [2m0:06:25 â€¢ 0:05:34[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.5605, 0.4395]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 125/203 [2m0:06:25 â€¢ 0:05:34[0m [2;4m0.23it/s[0m  
[2KStep 0 - TTA Loss: 0.5870â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 125/203 [2m0:06:25 â€¢ 0:05:34[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.5605, 0.4395]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 125/203 [2m0:06:25 â€¢ 0:05:34[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.5598, 0.4402]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 125/203 [2m0:06:25 â€¢ 0:05:34[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.5591, 0.4409]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 125/203 [2m0:06:25 â€¢ 0:05:34[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.5617, 0.4383]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 125/203 [2m0:06:25 â€¢ 0:05:34[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.5711, 0.4289]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 125/203 [2m0:06:25 â€¢ 0:05:34[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.5898, 0.4102]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 125/203 [2m0:06:25 â€¢ 0:05:34[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.6140, 0.3860]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 125/203 [2m0:06:25 â€¢ 0:05:34[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.6382, 0.3618]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 125/203 [2m0:06:25 â€¢ 0:05:34[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.6615, 0.3385]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 125/203 [2m0:06:25 â€¢ 0:05:34[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.5562, 0.4438]], device='cuda:0')4mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 125/203 [2m0:06:25 â€¢ 0:05:34[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.4806, 0.5194],[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 125/203 [2m0:06:25 â€¢ 0:05:34[0m [2;4m0.23it/s[0m  
        [0.6922, 0.3078],
        [0.5558, 0.4442],
        [0.6520, 0.3480],
        [0.4427, 0.5573],
        [0.4627, 0.5373],
        [0.5011, 0.4989],
        [0.2107, 0.7893],
        [0.6745, 0.3255],
[2K        [0.6075, 0.3925]], device='cuda:0')â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 125/203 [2m0:06:25 â€¢ 0:05:34[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.5600, 0.4400],â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 126/203 [2m0:06:29 â€¢ 0:03:43[0m [2;4m0.35it/s[0m  
        [0.5537, 0.4463],
        [0.5562, 0.4438],
        [0.5580, 0.4420],
        [0.5563, 0.4437],
        [0.5550, 0.4450],
        [0.5540, 0.4460],
        [0.5595, 0.4405],
        [0.5582, 0.4418],
[2K        [0.5568, 0.4432]], device='cuda:0', grad_fn=<SoftmaxBackward0>)38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 126/203 [2m0:06:29 â€¢ 0:03:43[0m [2;4m0.35it/s[0m  
[2KClass label: BaseballPitchâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 126/203 [2m0:06:29 â€¢ 0:03:43[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6410, 0.3590]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 126/203 [2m0:06:29 â€¢ 0:03:43[0m [2;4m0.35it/s[0m  
[2KStep 0 - TTA Loss: 0.5541â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 126/203 [2m0:06:29 â€¢ 0:03:43[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6632, 0.3368]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 126/203 [2m0:06:29 â€¢ 0:03:43[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.7153, 0.2847]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 126/203 [2m0:06:29 â€¢ 0:03:43[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.7811, 0.2189]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 126/203 [2m0:06:29 â€¢ 0:03:43[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.8405, 0.1595]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 126/203 [2m0:06:29 â€¢ 0:03:43[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.8855, 0.1145]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 126/203 [2m0:06:29 â€¢ 0:03:43[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.9125, 0.0875]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 126/203 [2m0:06:29 â€¢ 0:03:43[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.9265, 0.0735]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 126/203 [2m0:06:29 â€¢ 0:03:43[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.9330, 0.0670]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 126/203 [2m0:06:29 â€¢ 0:03:43[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.9369, 0.0631]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 126/203 [2m0:06:29 â€¢ 0:03:43[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5711, 0.4289]], device='cuda:0')4mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 126/203 [2m0:06:29 â€¢ 0:03:43[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.9379, 0.0621],[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 126/203 [2m0:06:29 â€¢ 0:03:43[0m [2;4m0.35it/s[0m  
        [0.8372, 0.1628],
        [0.7262, 0.2738],
        [0.8590, 0.1410],
        [0.8070, 0.1930],
        [0.7627, 0.2373],
        [0.7501, 0.2499],
        [0.6843, 0.3157],
        [0.8672, 0.1328],
[2K        [0.8415, 0.1585]], device='cuda:0')â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 126/203 [2m0:06:29 â€¢ 0:03:43[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5600, 0.4400],â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 127/203 [2m0:06:31 â€¢ 0:03:36[0m [2;4m0.35it/s[0m   
        [0.5537, 0.4463],
        [0.5561, 0.4439],
        [0.5580, 0.4420],
        [0.5563, 0.4437],
        [0.5549, 0.4451],
        [0.5540, 0.4460],
        [0.5594, 0.4406],
        [0.5582, 0.4418],
[2K        [0.5567, 0.4433]], device='cuda:0', grad_fn=<SoftmaxBackward0>);237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 127/203 [2m0:06:31 â€¢ 0:03:36[0m [2;4m0.35it/s[0m  
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 127/203 [2m0:06:31 â€¢ 0:03:36[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6007, 0.3993]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 127/203 [2m0:06:31 â€¢ 0:03:36[0m [2;4m0.35it/s[0m  
[2KStep 0 - TTA Loss: 0.7001â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 127/203 [2m0:06:31 â€¢ 0:03:36[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6007, 0.3993]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 127/203 [2m0:06:31 â€¢ 0:03:36[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6029, 0.3971]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 127/203 [2m0:06:31 â€¢ 0:03:36[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6127, 0.3873]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 127/203 [2m0:06:31 â€¢ 0:03:36[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6355, 0.3645]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 127/203 [2m0:06:31 â€¢ 0:03:36[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6767, 0.3233]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 127/203 [2m0:06:31 â€¢ 0:03:36[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.7364, 0.2636]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 127/203 [2m0:06:31 â€¢ 0:03:36[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.8094, 0.1906]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 127/203 [2m0:06:31 â€¢ 0:03:36[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.8832, 0.1168]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 127/203 [2m0:06:31 â€¢ 0:03:36[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.9405, 0.0595]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 127/203 [2m0:06:31 â€¢ 0:03:36[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5733, 0.4267]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 127/203 [2m0:06:31 â€¢ 0:03:36[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.9732, 0.0268],â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 127/203 [2m0:06:31 â€¢ 0:03:36[0m [2;4m0.35it/s[0m  
        [0.8872, 0.1128],
        [0.9168, 0.0832],
        [0.9349, 0.0651],
        [0.9590, 0.0410],
        [0.9764, 0.0236],
        [0.9538, 0.0462],
        [0.9595, 0.0405],
        [0.9524, 0.0476],
[2K        [0.9569, 0.0431]], device='cuda:0')â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 127/203 [2m0:06:31 â€¢ 0:03:36[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5600, 0.4400],â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 128/203 [2m0:06:36 â€¢ 0:03:47[0m [2;4m0.33it/s[0m  
        [0.5537, 0.4463],
        [0.5561, 0.4439],
        [0.5580, 0.4420],
        [0.5563, 0.4437],
        [0.5550, 0.4450],
        [0.5540, 0.4460],
        [0.5594, 0.4406],
        [0.5582, 0.4418],
[2K        [0.5567, 0.4433]], device='cuda:0', grad_fn=<SoftmaxBackward0>);237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 128/203 [2m0:06:36 â€¢ 0:03:47[0m [2;4m0.33it/s[0m  
[2KClass label: Billiardsâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 128/203 [2m0:06:36 â€¢ 0:03:47[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.5597, 0.4403]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 128/203 [2m0:06:36 â€¢ 0:03:47[0m [2;4m0.33it/s[0m  
[2KStep 0 - TTA Loss: 0.4198â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 128/203 [2m0:06:36 â€¢ 0:03:47[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.6869, 0.3131]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 128/203 [2m0:06:36 â€¢ 0:03:47[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.7917, 0.2083]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 128/203 [2m0:06:36 â€¢ 0:03:47[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.8607, 0.1393]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 128/203 [2m0:06:36 â€¢ 0:03:47[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.8997, 0.1003]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 128/203 [2m0:06:36 â€¢ 0:03:47[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.9233, 0.0767]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 128/203 [2m0:06:36 â€¢ 0:03:47[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.9388, 0.0612]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 128/203 [2m0:06:36 â€¢ 0:03:47[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.9495, 0.0505]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 128/203 [2m0:06:36 â€¢ 0:03:47[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.9570, 0.0430]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 128/203 [2m0:06:36 â€¢ 0:03:47[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.9609, 0.0391]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 128/203 [2m0:06:36 â€¢ 0:03:47[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.5710, 0.4290]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 128/203 [2m0:06:36 â€¢ 0:03:47[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.9799, 0.0201],â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 128/203 [2m0:06:36 â€¢ 0:03:47[0m [2;4m0.33it/s[0m  
        [0.9619, 0.0381],
        [0.9526, 0.0474],
        [0.9589, 0.0411],
        [0.9759, 0.0241],
        [0.9890, 0.0110],
        [0.9758, 0.0242],
        [0.9787, 0.0213],
        [0.9737, 0.0263],
[2K        [0.9743, 0.0257]], device='cuda:0')â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 128/203 [2m0:06:36 â€¢ 0:03:47[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.5600, 0.4400],â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 129/203 [2m0:06:41 â€¢ 0:03:34[0m [2;4m0.35it/s[0m  
        [0.5537, 0.4463],
        [0.5562, 0.4438],
        [0.5580, 0.4420],
        [0.5563, 0.4437],
        [0.5550, 0.4450],
        [0.5541, 0.4459],
        [0.5594, 0.4406],
        [0.5582, 0.4418],
[2K        [0.5568, 0.4432]], device='cuda:0', grad_fn=<SoftmaxBackward0>);237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 129/203 [2m0:06:41 â€¢ 0:03:34[0m [2;4m0.35it/s[0m  
[2KClass label: Billiardsâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 129/203 [2m0:06:41 â€¢ 0:03:34[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5581, 0.4419]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 129/203 [2m0:06:41 â€¢ 0:03:34[0m [2;4m0.35it/s[0m  
[2KStep 0 - TTA Loss: 0.5597â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 129/203 [2m0:06:41 â€¢ 0:03:34[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5581, 0.4419]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 129/203 [2m0:06:41 â€¢ 0:03:34[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5631, 0.4369]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 129/203 [2m0:06:41 â€¢ 0:03:34[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5798, 0.4202]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 129/203 [2m0:06:41 â€¢ 0:03:34[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6105, 0.3895]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 129/203 [2m0:06:41 â€¢ 0:03:34[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6519, 0.3481]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 129/203 [2m0:06:41 â€¢ 0:03:34[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6955, 0.3045]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 129/203 [2m0:06:41 â€¢ 0:03:34[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.7302, 0.2698]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 129/203 [2m0:06:41 â€¢ 0:03:34[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.7847, 0.2153]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 129/203 [2m0:06:41 â€¢ 0:03:34[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.8863, 0.1137]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 129/203 [2m0:06:41 â€¢ 0:03:34[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5703, 0.4297]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 129/203 [2m0:06:41 â€¢ 0:03:34[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.9053, 0.0947],â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 129/203 [2m0:06:41 â€¢ 0:03:34[0m [2;4m0.35it/s[0m  
        [0.9809, 0.0191],
        [0.8579, 0.1421],
        [0.9002, 0.0998],
        [0.8630, 0.1370],
        [0.8980, 0.1020],
        [0.8651, 0.1349],
        [0.9208, 0.0792],
        [0.9157, 0.0843],
[2K        [0.8814, 0.1186]], device='cuda:0')â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 129/203 [2m0:06:41 â€¢ 0:03:34[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5600, 0.4400],â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 130/203 [2m0:06:45 â€¢ 0:03:58[0m [2;4m0.31it/s[0m  
        [0.5538, 0.4462],
        [0.5562, 0.4438],
        [0.5581, 0.4419],
        [0.5564, 0.4436],
        [0.5551, 0.4449],
        [0.5541, 0.4459],
        [0.5594, 0.4406],
        [0.5583, 0.4417],
[2K        [0.5568, 0.4432]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 130/203 [2m0:06:45 â€¢ 0:03:58[0m [2;4m0.31it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 130/203 [2m0:06:45 â€¢ 0:03:58[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5612, 0.4388]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 130/203 [2m0:06:45 â€¢ 0:03:58[0m [2;4m0.31it/s[0m  
[2KStep 0 - TTA Loss: 0.7455â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 130/203 [2m0:06:45 â€¢ 0:03:58[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.7051, 0.2949]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 130/203 [2m0:06:45 â€¢ 0:03:58[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.8054, 0.1946]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 130/203 [2m0:06:45 â€¢ 0:03:58[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.8673, 0.1327]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 130/203 [2m0:06:45 â€¢ 0:03:58[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.9004, 0.0996]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 130/203 [2m0:06:45 â€¢ 0:03:58[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.9173, 0.0827]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 130/203 [2m0:06:45 â€¢ 0:03:58[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.9271, 0.0729]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 130/203 [2m0:06:45 â€¢ 0:03:58[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.9326, 0.0674]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 130/203 [2m0:06:45 â€¢ 0:03:58[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.9356, 0.0644]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 130/203 [2m0:06:45 â€¢ 0:03:58[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.9365, 0.0635]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 130/203 [2m0:06:45 â€¢ 0:03:58[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5700, 0.4300]], device='cuda:0')24mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 130/203 [2m0:06:45 â€¢ 0:03:58[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[9.6480e-01, 3.5201e-02],8;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 130/203 [2m0:06:45 â€¢ 0:03:58[0m [2;4m0.31it/s[0m  
        [9.9948e-01, 5.2277e-04],
        [9.4139e-01, 5.8609e-02],
        [9.6610e-01, 3.3901e-02],
        [9.3681e-01, 6.3189e-02],
        [9.5403e-01, 4.5969e-02],
        [9.3687e-01, 6.3129e-02],
        [9.7494e-01, 2.5057e-02],
        [9.7495e-01, 2.5047e-02],
[2K        [9.4445e-01, 5.5551e-02]], device='cuda:0')38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 130/203 [2m0:06:45 â€¢ 0:03:58[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5601, 0.4399],â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 131/203 [2m0:06:48 â€¢ 0:04:22[0m [2;4m0.28it/s[0m  
        [0.5540, 0.4460],
        [0.5562, 0.4438],
        [0.5581, 0.4419],
        [0.5564, 0.4436],
        [0.5551, 0.4449],
        [0.5541, 0.4459],
        [0.5595, 0.4405],
        [0.5583, 0.4417],
[2K        [0.5568, 0.4432]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 131/203 [2m0:06:48 â€¢ 0:04:22[0m [2;4m0.28it/s[0m  
[2KClass label: CleanAndJerkâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 131/203 [2m0:06:48 â€¢ 0:04:22[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.5734, 0.4266]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 131/203 [2m0:06:48 â€¢ 0:04:22[0m [2;4m0.28it/s[0m  
[2KStep 0 - TTA Loss: 0.4357â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 131/203 [2m0:06:48 â€¢ 0:04:22[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.5734, 0.4266]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 131/203 [2m0:06:48 â€¢ 0:04:22[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.5751, 0.4249]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 131/203 [2m0:06:48 â€¢ 0:04:22[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.5835, 0.4165]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 131/203 [2m0:06:48 â€¢ 0:04:22[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6045, 0.3955]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 131/203 [2m0:06:48 â€¢ 0:04:22[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6415, 0.3585]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 131/203 [2m0:06:48 â€¢ 0:04:22[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6965, 0.3035]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 131/203 [2m0:06:48 â€¢ 0:04:22[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.7628, 0.2372]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 131/203 [2m0:06:48 â€¢ 0:04:22[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.8256, 0.1744]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 131/203 [2m0:06:48 â€¢ 0:04:22[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.8750, 0.1250]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 131/203 [2m0:06:48 â€¢ 0:04:22[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.5662, 0.4338]], device='cuda:0')24mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 131/203 [2m0:06:48 â€¢ 0:04:22[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.8097, 0.1903],â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 131/203 [2m0:06:48 â€¢ 0:04:22[0m [2;4m0.28it/s[0m  
        [0.8889, 0.1111],
        [0.9173, 0.0827],
        [0.8194, 0.1806],
        [0.8004, 0.1996],
        [0.8174, 0.1826],
        [0.7723, 0.2277],
        [0.8347, 0.1653],
        [0.8260, 0.1740],
[2K        [0.8045, 0.1955]], device='cuda:0')â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 131/203 [2m0:06:48 â€¢ 0:04:22[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.5601, 0.4399],â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 132/203 [2m0:06:50 â€¢ 0:04:03[0m [2;4m0.29it/s[0m   
        [0.5540, 0.4460],
        [0.5563, 0.4437],
        [0.5581, 0.4419],
        [0.5564, 0.4436],
        [0.5551, 0.4449],
        [0.5541, 0.4459],
        [0.5595, 0.4405],
        [0.5583, 0.4417],
[2K        [0.5568, 0.4432]], device='cuda:0', grad_fn=<SoftmaxBackward0>)5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 132/203 [2m0:06:50 â€¢ 0:04:03[0m [2;4m0.29it/s[0m  
[2KClass label: Billiardsâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 132/203 [2m0:06:50 â€¢ 0:04:03[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.5621, 0.4379]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 132/203 [2m0:06:50 â€¢ 0:04:03[0m [2;4m0.29it/s[0m  
[2KStep 0 - TTA Loss: 0.5734â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 132/203 [2m0:06:50 â€¢ 0:04:03[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.6648, 0.3352]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 132/203 [2m0:06:50 â€¢ 0:04:03[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.7660, 0.2340]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 132/203 [2m0:06:50 â€¢ 0:04:03[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.8383, 0.1617]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 132/203 [2m0:06:50 â€¢ 0:04:03[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.8814, 0.1186]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 132/203 [2m0:06:50 â€¢ 0:04:03[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.9090, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 132/203 [2m0:06:50 â€¢ 0:04:03[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.9307, 0.0693]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 132/203 [2m0:06:50 â€¢ 0:04:03[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.9471, 0.0529]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 132/203 [2m0:06:50 â€¢ 0:04:03[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.9578, 0.0422]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 132/203 [2m0:06:50 â€¢ 0:04:03[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.9631, 0.0369]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 132/203 [2m0:06:50 â€¢ 0:04:03[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.5723, 0.4277]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 132/203 [2m0:06:50 â€¢ 0:04:03[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.8660, 0.1340],â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 132/203 [2m0:06:50 â€¢ 0:04:03[0m [2;4m0.29it/s[0m  
        [0.9645, 0.0355],
        [0.9344, 0.0656],
        [0.8752, 0.1248],
        [0.8487, 0.1513],
        [0.8651, 0.1349],
        [0.8307, 0.1693],
        [0.8878, 0.1122],
        [0.8838, 0.1162],
[2K        [0.8535, 0.1465]], device='cuda:0')â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 132/203 [2m0:06:50 â€¢ 0:04:03[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.5601, 0.4399],â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 133/203 [2m0:06:55 â€¢ 0:04:23[0m [2;4m0.27it/s[0m  
        [0.5540, 0.4460],
        [0.5563, 0.4437],
        [0.5582, 0.4418],
        [0.5564, 0.4436],
        [0.5551, 0.4449],
        [0.5541, 0.4459],
        [0.5595, 0.4405],
        [0.5583, 0.4417],
[2K        [0.5568, 0.4432]], device='cuda:0', grad_fn=<SoftmaxBackward0>)5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 133/203 [2m0:06:55 â€¢ 0:04:23[0m [2;4m0.27it/s[0m  
[2KClass label: TennisSwingâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 133/203 [2m0:06:55 â€¢ 0:04:23[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6443, 0.3557]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 133/203 [2m0:06:55 â€¢ 0:04:23[0m [2;4m0.27it/s[0m  
[2KStep 0 - TTA Loss: 0.7284â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 133/203 [2m0:06:55 â€¢ 0:04:23[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6443, 0.3557]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 133/203 [2m0:06:55 â€¢ 0:04:23[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6469, 0.3531]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 133/203 [2m0:06:55 â€¢ 0:04:23[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6543, 0.3457]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 133/203 [2m0:06:55 â€¢ 0:04:23[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6653, 0.3347]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 133/203 [2m0:06:55 â€¢ 0:04:23[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6749, 0.3251]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 133/203 [2m0:06:55 â€¢ 0:04:23[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6756, 0.3244]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 133/203 [2m0:06:55 â€¢ 0:04:23[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6552, 0.3448]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 133/203 [2m0:06:55 â€¢ 0:04:23[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6149, 0.3851]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 133/203 [2m0:06:55 â€¢ 0:04:23[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.5914, 0.4086]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 133/203 [2m0:06:55 â€¢ 0:04:23[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.5565, 0.4435]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 133/203 [2m0:06:55 â€¢ 0:04:23[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.7214, 0.2786],â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 133/203 [2m0:06:55 â€¢ 0:04:23[0m [2;4m0.27it/s[0m  
        [0.9480, 0.0520],
        [0.7533, 0.2467],
        [0.7198, 0.2802],
        [0.6457, 0.3543],
        [0.6812, 0.3188],
        [0.6581, 0.3419],
        [0.7798, 0.2202],
        [0.6204, 0.3796],
[2K        [0.6599, 0.3401]], device='cuda:0')â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 133/203 [2m0:06:55 â€¢ 0:04:23[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.5601, 0.4399],â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 134/203 [2m0:07:08 â€¢ 0:06:10[0m [2;4m0.19it/s[0m  
        [0.5541, 0.4459],
        [0.5563, 0.4437],
        [0.5581, 0.4419],
        [0.5564, 0.4436],
        [0.5551, 0.4449],
        [0.5541, 0.4459],
        [0.5595, 0.4405],
        [0.5583, 0.4417],
[2K        [0.5568, 0.4432]], device='cuda:0', grad_fn=<SoftmaxBackward0>)5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 134/203 [2m0:07:08 â€¢ 0:06:10[0m [2;4m0.19it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 134/203 [2m0:07:08 â€¢ 0:06:10[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.6278, 0.3722]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 134/203 [2m0:07:08 â€¢ 0:06:10[0m [2;4m0.19it/s[0m  
[2KStep 0 - TTA Loss: 0.6692â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 134/203 [2m0:07:08 â€¢ 0:06:10[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.6728, 0.3272]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 134/203 [2m0:07:08 â€¢ 0:06:10[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.7328, 0.2672]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 134/203 [2m0:07:08 â€¢ 0:06:10[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.7937, 0.2063]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 134/203 [2m0:07:08 â€¢ 0:06:10[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.8525, 0.1475]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 134/203 [2m0:07:08 â€¢ 0:06:10[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.8976, 0.1024]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 134/203 [2m0:07:08 â€¢ 0:06:10[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.9278, 0.0722]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 134/203 [2m0:07:08 â€¢ 0:06:10[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.9461, 0.0539]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 134/203 [2m0:07:08 â€¢ 0:06:10[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.9570, 0.0430]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 134/203 [2m0:07:08 â€¢ 0:06:10[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.9620, 0.0380]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 134/203 [2m0:07:08 â€¢ 0:06:10[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.5729, 0.4271]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 134/203 [2m0:07:08 â€¢ 0:06:10[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.9377, 0.0623],â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 134/203 [2m0:07:08 â€¢ 0:06:10[0m [2;4m0.19it/s[0m  
        [0.9610, 0.0390],
        [0.9020, 0.0980],
        [0.9454, 0.0546],
        [0.9330, 0.0670],
        [0.9336, 0.0664],
        [0.9278, 0.0722],
        [0.9227, 0.0773],
        [0.9403, 0.0597],
[2K        [0.9634, 0.0366]], device='cuda:0')â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 134/203 [2m0:07:08 â€¢ 0:06:10[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.5601, 0.4399],â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 135/203 [2m0:07:12 â€¢ 0:06:04[0m [2;4m0.19it/s[0m  
        [0.5541, 0.4459],
        [0.5563, 0.4437],
        [0.5582, 0.4418],
        [0.5564, 0.4436],
        [0.5551, 0.4449],
        [0.5541, 0.4459],
        [0.5595, 0.4405],
        [0.5583, 0.4417],
[2K        [0.5569, 0.4431]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 135/203 [2m0:07:12 â€¢ 0:06:04[0m [2;4m0.19it/s[0m  
[2KClass label: CleanAndJerkâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 135/203 [2m0:07:12 â€¢ 0:06:04[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.5705, 0.4295]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 135/203 [2m0:07:12 â€¢ 0:06:04[0m [2;4m0.19it/s[0m  
[2KStep 0 - TTA Loss: 0.4886â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 135/203 [2m0:07:12 â€¢ 0:06:04[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.5705, 0.4295]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 135/203 [2m0:07:12 â€¢ 0:06:04[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.5754, 0.4246]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 135/203 [2m0:07:12 â€¢ 0:06:04[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.5941, 0.4059]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 135/203 [2m0:07:12 â€¢ 0:06:04[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.6330, 0.3670]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 135/203 [2m0:07:12 â€¢ 0:06:04[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.6918, 0.3082]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 135/203 [2m0:07:12 â€¢ 0:06:04[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.7615, 0.2385]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 135/203 [2m0:07:12 â€¢ 0:06:04[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.8266, 0.1734]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 135/203 [2m0:07:12 â€¢ 0:06:04[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.8774, 0.1226]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 135/203 [2m0:07:12 â€¢ 0:06:04[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.9215, 0.0785]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 135/203 [2m0:07:12 â€¢ 0:06:04[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.5704, 0.4296]], device='cuda:0')224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 135/203 [2m0:07:12 â€¢ 0:06:04[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.9598, 0.0402],â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 135/203 [2m0:07:12 â€¢ 0:06:04[0m [2;4m0.19it/s[0m  
        [0.9126, 0.0874],
        [0.9687, 0.0313],
        [0.9656, 0.0344],
        [0.9705, 0.0295],
        [0.9672, 0.0328],
        [0.9638, 0.0362],
        [0.9380, 0.0620],
        [0.9653, 0.0347],
[2K        [0.9848, 0.0152]], device='cuda:0')â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 135/203 [2m0:07:12 â€¢ 0:06:04[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.5601, 0.4399],â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 136/203 [2m0:07:19 â€¢ 0:07:58[0m [2;4m0.14it/s[0m  
        [0.5541, 0.4459],
        [0.5563, 0.4437],
        [0.5582, 0.4418],
        [0.5565, 0.4435],
        [0.5551, 0.4449],
        [0.5541, 0.4459],
        [0.5595, 0.4405],
        [0.5583, 0.4417],
[2K        [0.5569, 0.4431]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 136/203 [2m0:07:19 â€¢ 0:07:58[0m [2;4m0.14it/s[0m  
[2KClass label: GolfSwingâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 136/203 [2m0:07:19 â€¢ 0:07:58[0m [2;4m0.14it/s[0m  
[2KAttention weights: tensor([[0.6511, 0.3489]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 136/203 [2m0:07:19 â€¢ 0:07:58[0m [2;4m0.14it/s[0m  
[2KStep 0 - TTA Loss: 0.4080â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 136/203 [2m0:07:19 â€¢ 0:07:58[0m [2;4m0.14it/s[0m  
[2KAttention weights: tensor([[0.7845, 0.2155]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 136/203 [2m0:07:19 â€¢ 0:07:58[0m [2;4m0.14it/s[0m  
[2KAttention weights: tensor([[0.8885, 0.1115]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 136/203 [2m0:07:19 â€¢ 0:07:58[0m [2;4m0.14it/s[0m  
[2KAttention weights: tensor([[0.9504, 0.0496]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 136/203 [2m0:07:19 â€¢ 0:07:58[0m [2;4m0.14it/s[0m  
[2KAttention weights: tensor([[0.9791, 0.0209]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 136/203 [2m0:07:19 â€¢ 0:07:58[0m [2;4m0.14it/s[0m  
[2KAttention weights: tensor([[0.9904, 0.0096]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 136/203 [2m0:07:19 â€¢ 0:07:58[0m [2;4m0.14it/s[0m  
[2KAttention weights: tensor([[0.9947, 0.0053]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 136/203 [2m0:07:19 â€¢ 0:07:58[0m [2;4m0.14it/s[0m  
[2KAttention weights: tensor([[0.9964, 0.0036]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 136/203 [2m0:07:19 â€¢ 0:07:58[0m [2;4m0.14it/s[0m  
[2KAttention weights: tensor([[0.9970, 0.0030]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 136/203 [2m0:07:19 â€¢ 0:07:58[0m [2;4m0.14it/s[0m  
[2KAttention weights: tensor([[0.9973, 0.0027]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 136/203 [2m0:07:19 â€¢ 0:07:58[0m [2;4m0.14it/s[0m  
[2KAttention weights: tensor([[0.5887, 0.4113]], device='cuda:0')224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 136/203 [2m0:07:19 â€¢ 0:07:58[0m [2;4m0.14it/s[0m  
[2KAttention weights: tensor([[0.9874, 0.0126],â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 136/203 [2m0:07:19 â€¢ 0:07:58[0m [2;4m0.14it/s[0m  
        [0.9686, 0.0314],
        [0.9937, 0.0063],
        [0.9973, 0.0027],
        [0.9902, 0.0098],
        [0.9882, 0.0118],
        [0.9855, 0.0145],
        [0.9760, 0.0240],
        [0.9910, 0.0090],
[2K        [0.9950, 0.0050]], device='cuda:0')â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 136/203 [2m0:07:19 â€¢ 0:07:58[0m [2;4m0.14it/s[0m  
[2KAttention weights: tensor([[0.5602, 0.4398],â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 137/203 [2m0:07:23 â€¢ 0:07:33[0m [2;4m0.15it/s[0m  
        [0.5542, 0.4458],
        [0.5564, 0.4436],
        [0.5582, 0.4418],
        [0.5565, 0.4435],
        [0.5552, 0.4448],
        [0.5542, 0.4458],
        [0.5596, 0.4404],
        [0.5584, 0.4416],
[2K        [0.5570, 0.4430]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 137/203 [2m0:07:23 â€¢ 0:07:33[0m [2;4m0.15it/s[0m  
[2KClass label: SoccerPenaltyâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 137/203 [2m0:07:23 â€¢ 0:07:33[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.6622, 0.3378]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 137/203 [2m0:07:23 â€¢ 0:07:33[0m [2;4m0.15it/s[0m  
[2KStep 0 - TTA Loss: 0.8701â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 137/203 [2m0:07:23 â€¢ 0:07:33[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.6622, 0.3378]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 137/203 [2m0:07:23 â€¢ 0:07:33[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.6640, 0.3360]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 137/203 [2m0:07:23 â€¢ 0:07:33[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.6705, 0.3295]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 137/203 [2m0:07:23 â€¢ 0:07:33[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.6829, 0.3171]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 137/203 [2m0:07:23 â€¢ 0:07:33[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.7012, 0.2988]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 137/203 [2m0:07:23 â€¢ 0:07:33[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.7214, 0.2786]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 137/203 [2m0:07:23 â€¢ 0:07:33[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.7378, 0.2622]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 137/203 [2m0:07:23 â€¢ 0:07:33[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.7386, 0.2614]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 137/203 [2m0:07:23 â€¢ 0:07:33[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.7157, 0.2843]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 137/203 [2m0:07:23 â€¢ 0:07:33[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.5584, 0.4416]], device='cuda:0')224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 137/203 [2m0:07:23 â€¢ 0:07:33[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.7658, 0.2342],â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 137/203 [2m0:07:23 â€¢ 0:07:33[0m [2;4m0.15it/s[0m  
        [0.6892, 0.3108],
        [0.7575, 0.2425],
        [0.8685, 0.1315],
        [0.7230, 0.2770],
        [0.7348, 0.2652],
        [0.7173, 0.2827],
        [0.6859, 0.3141],
        [0.7977, 0.2023],
[2K        [0.8108, 0.1892]], device='cuda:0')â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m 137/203 [2m0:07:23 â€¢ 0:07:33[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.5602, 0.4398],â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 138/203 [2m0:07:25 â€¢ 0:06:24[0m [2;4m0.17it/s[0m   
        [0.5542, 0.4458],
        [0.5564, 0.4436],
        [0.5582, 0.4418],
        [0.5565, 0.4435],
        [0.5552, 0.4448],
        [0.5542, 0.4458],
        [0.5596, 0.4404],
        [0.5584, 0.4416],
[2K        [0.5570, 0.4430]], device='cuda:0', grad_fn=<SoftmaxBackward0>);5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 138/203 [2m0:07:25 â€¢ 0:06:24[0m [2;4m0.17it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 138/203 [2m0:07:25 â€¢ 0:06:24[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.5673, 0.4327]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 138/203 [2m0:07:25 â€¢ 0:06:24[0m [2;4m0.17it/s[0m  
[2KStep 0 - TTA Loss: 0.7386â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 138/203 [2m0:07:25 â€¢ 0:06:24[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.6039, 0.3961]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 138/203 [2m0:07:25 â€¢ 0:06:24[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.6605, 0.3395]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 138/203 [2m0:07:25 â€¢ 0:06:24[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.7178, 0.2822]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 138/203 [2m0:07:25 â€¢ 0:06:24[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.7793, 0.2207]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 138/203 [2m0:07:25 â€¢ 0:06:24[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.8285, 0.1715]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 138/203 [2m0:07:25 â€¢ 0:06:24[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.8592, 0.1408]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 138/203 [2m0:07:25 â€¢ 0:06:24[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.8756, 0.1244]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 138/203 [2m0:07:25 â€¢ 0:06:24[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.8847, 0.1153]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 138/203 [2m0:07:25 â€¢ 0:06:24[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.8871, 0.1129]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 138/203 [2m0:07:25 â€¢ 0:06:24[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.5664, 0.4336]], device='cuda:0')â•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 138/203 [2m0:07:25 â€¢ 0:06:24[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.8255, 0.1745],â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 138/203 [2m0:07:25 â€¢ 0:06:24[0m [2;4m0.17it/s[0m  
        [0.7466, 0.2534],
        [0.8370, 0.1630],
        [0.8871, 0.1129],
        [0.8341, 0.1659],
        [0.8639, 0.1361],
        [0.8876, 0.1124],
        [0.7311, 0.2689],
        [0.8574, 0.1426],
[2K        [0.8946, 0.1054]], device='cuda:0')â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 138/203 [2m0:07:25 â€¢ 0:06:24[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.5602, 0.4398],â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 139/203 [2m0:07:33 â€¢ 0:05:17[0m [2;4m0.20it/s[0m  
        [0.5542, 0.4458],
        [0.5564, 0.4436],
        [0.5582, 0.4418],
        [0.5565, 0.4435],
        [0.5552, 0.4448],
        [0.5542, 0.4458],
        [0.5595, 0.4405],
        [0.5584, 0.4416],
[2K        [0.5570, 0.4430]], device='cuda:0', grad_fn=<SoftmaxBackward0>);5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 139/203 [2m0:07:33 â€¢ 0:05:17[0m [2;4m0.20it/s[0m  
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 139/203 [2m0:07:33 â€¢ 0:05:17[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.6008, 0.3992]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 139/203 [2m0:07:33 â€¢ 0:05:17[0m [2;4m0.20it/s[0m  
[2KStep 0 - TTA Loss: 0.6103â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 139/203 [2m0:07:33 â€¢ 0:05:17[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.6008, 0.3992]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 139/203 [2m0:07:33 â€¢ 0:05:17[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.6022, 0.3978]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 139/203 [2m0:07:33 â€¢ 0:05:17[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.6087, 0.3913]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 139/203 [2m0:07:33 â€¢ 0:05:17[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.6254, 0.3746]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 139/203 [2m0:07:33 â€¢ 0:05:17[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.6566, 0.3434]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 139/203 [2m0:07:33 â€¢ 0:05:17[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.7005, 0.2995]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 139/203 [2m0:07:33 â€¢ 0:05:17[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.7644, 0.2356]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 139/203 [2m0:07:33 â€¢ 0:05:17[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.8448, 0.1552]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 139/203 [2m0:07:33 â€¢ 0:05:17[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.9183, 0.0817]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 139/203 [2m0:07:33 â€¢ 0:05:17[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.5740, 0.4260]], device='cuda:0')â•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 139/203 [2m0:07:33 â€¢ 0:05:17[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.9193, 0.0807],â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 139/203 [2m0:07:33 â€¢ 0:05:17[0m [2;4m0.20it/s[0m  
        [0.8317, 0.1683],
        [0.9027, 0.0973],
        [0.9075, 0.0925],
        [0.9324, 0.0676],
        [0.9687, 0.0313],
        [0.9427, 0.0573],
        [0.9361, 0.0639],
        [0.9282, 0.0718],
[2K        [0.9392, 0.0608]], device='cuda:0')â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 139/203 [2m0:07:33 â€¢ 0:05:17[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.5602, 0.4398],â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 140/203 [2m0:07:36 â€¢ 0:04:54[0m [2;4m0.21it/s[0m  
        [0.5542, 0.4458],
        [0.5564, 0.4436],
        [0.5582, 0.4418],
        [0.5565, 0.4435],
        [0.5552, 0.4448],
        [0.5542, 0.4458],
        [0.5595, 0.4405],
        [0.5584, 0.4416],
[2K        [0.5570, 0.4430]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 140/203 [2m0:07:36 â€¢ 0:04:54[0m [2;4m0.21it/s[0m  
[2KClass label: HammerThrowâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 140/203 [2m0:07:36 â€¢ 0:04:54[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.5609, 0.4391]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 140/203 [2m0:07:36 â€¢ 0:04:54[0m [2;4m0.21it/s[0m  
[2KStep 0 - TTA Loss: 0.5598â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 140/203 [2m0:07:36 â€¢ 0:04:54[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.7332, 0.2668]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 140/203 [2m0:07:36 â€¢ 0:04:54[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.8566, 0.1434]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 140/203 [2m0:07:36 â€¢ 0:04:54[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.9262, 0.0738]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 140/203 [2m0:07:36 â€¢ 0:04:54[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.9637, 0.0363]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 140/203 [2m0:07:36 â€¢ 0:04:54[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.9820, 0.0180]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 140/203 [2m0:07:36 â€¢ 0:04:54[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.9897, 0.0103]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 140/203 [2m0:07:36 â€¢ 0:04:54[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.9930, 0.0070]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 140/203 [2m0:07:36 â€¢ 0:04:54[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.9943, 0.0057]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 140/203 [2m0:07:36 â€¢ 0:04:54[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.9948, 0.0052]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 140/203 [2m0:07:36 â€¢ 0:04:54[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.5854, 0.4146]], device='cuda:0');224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 140/203 [2m0:07:36 â€¢ 0:04:54[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.9884, 0.0116],â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 140/203 [2m0:07:36 â€¢ 0:04:54[0m [2;4m0.21it/s[0m  
        [0.9386, 0.0614],
        [0.9807, 0.0193],
        [0.9747, 0.0253],
        [0.9949, 0.0051],
        [0.9966, 0.0034],
        [0.9914, 0.0086],
        [0.9888, 0.0112],
        [0.9848, 0.0152],
[2K        [0.9909, 0.0091]], device='cuda:0')â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 140/203 [2m0:07:36 â€¢ 0:04:54[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.5602, 0.4398],â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 141/203 [2m0:07:38 â€¢ 0:04:30[0m [2;4m0.23it/s[0m  
        [0.5542, 0.4458],
        [0.5564, 0.4436],
        [0.5582, 0.4418],
        [0.5565, 0.4435],
        [0.5552, 0.4448],
        [0.5542, 0.4458],
        [0.5595, 0.4405],
        [0.5584, 0.4416],
[2K        [0.5570, 0.4430]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 141/203 [2m0:07:38 â€¢ 0:04:30[0m [2;4m0.23it/s[0m  
[2KClass label: TennisSwingâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 141/203 [2m0:07:38 â€¢ 0:04:30[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.6393, 0.3607]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 141/203 [2m0:07:38 â€¢ 0:04:30[0m [2;4m0.23it/s[0m  
[2KStep 0 - TTA Loss: 1.0253â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 141/203 [2m0:07:38 â€¢ 0:04:30[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.6393, 0.3607]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 141/203 [2m0:07:38 â€¢ 0:04:30[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.6409, 0.3591]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 141/203 [2m0:07:38 â€¢ 0:04:30[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.6458, 0.3542]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 141/203 [2m0:07:38 â€¢ 0:04:30[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.6543, 0.3457]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 141/203 [2m0:07:38 â€¢ 0:04:30[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.6633, 0.3367]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 141/203 [2m0:07:38 â€¢ 0:04:30[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.6731, 0.3269]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 141/203 [2m0:07:38 â€¢ 0:04:30[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.6753, 0.3247]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 141/203 [2m0:07:38 â€¢ 0:04:30[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.6711, 0.3289]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 141/203 [2m0:07:38 â€¢ 0:04:30[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.6582, 0.3418]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 141/203 [2m0:07:38 â€¢ 0:04:30[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.5580, 0.4420]], device='cuda:0');224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 141/203 [2m0:07:38 â€¢ 0:04:30[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.7801, 0.2199],â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 141/203 [2m0:07:38 â€¢ 0:04:30[0m [2;4m0.23it/s[0m  
        [0.6300, 0.3700],
        [0.7386, 0.2614],
        [0.7214, 0.2786],
        [0.7919, 0.2081],
        [0.7929, 0.2071],
        [0.7432, 0.2568],
        [0.7891, 0.2109],
        [0.6754, 0.3246],
[2K        [0.7669, 0.2331]], device='cuda:0')â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 141/203 [2m0:07:38 â€¢ 0:04:30[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.5601, 0.4399],â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 142/203 [2m0:07:39 â€¢ 0:03:58[0m [2;4m0.26it/s[0m  
        [0.5541, 0.4459],
        [0.5564, 0.4436],
        [0.5582, 0.4418],
        [0.5564, 0.4436],
        [0.5552, 0.4448],
        [0.5541, 0.4459],
        [0.5595, 0.4405],
        [0.5583, 0.4417],
[2K        [0.5569, 0.4431]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 142/203 [2m0:07:39 â€¢ 0:03:58[0m [2;4m0.26it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 142/203 [2m0:07:39 â€¢ 0:03:58[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.5647, 0.4353]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 142/203 [2m0:07:39 â€¢ 0:03:58[0m [2;4m0.26it/s[0m  
[2KStep 0 - TTA Loss: 0.7421â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 142/203 [2m0:07:39 â€¢ 0:03:58[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.6131, 0.3869]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 142/203 [2m0:07:39 â€¢ 0:03:58[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.6710, 0.3290]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 142/203 [2m0:07:39 â€¢ 0:03:58[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.7300, 0.2700]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 142/203 [2m0:07:39 â€¢ 0:03:58[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.7751, 0.2249]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 142/203 [2m0:07:39 â€¢ 0:03:58[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.8034, 0.1966]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 142/203 [2m0:07:39 â€¢ 0:03:58[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.8203, 0.1797]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 142/203 [2m0:07:39 â€¢ 0:03:58[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.8315, 0.1685]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 142/203 [2m0:07:39 â€¢ 0:03:58[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.8395, 0.1605]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 142/203 [2m0:07:39 â€¢ 0:03:58[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.8444, 0.1556]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 142/203 [2m0:07:39 â€¢ 0:03:58[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.5631, 0.4369]], device='cuda:0');224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 142/203 [2m0:07:39 â€¢ 0:03:58[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.8325, 0.1675],â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 142/203 [2m0:07:39 â€¢ 0:03:58[0m [2;4m0.26it/s[0m  
        [0.7077, 0.2923],
        [0.7918, 0.2082],
        [0.7988, 0.2012],
        [0.8421, 0.1579],
        [0.8617, 0.1383],
        [0.8462, 0.1538],
        [0.8388, 0.1612],
        [0.7976, 0.2024],
[2K        [0.8470, 0.1530]], device='cuda:0')â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”[0m 142/203 [2m0:07:39 â€¢ 0:03:58[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.5601, 0.4399],â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 143/203 [2m0:07:42 â€¢ 0:03:24[0m [2;4m0.30it/s[0m   
        [0.5541, 0.4459],
        [0.5563, 0.4437],
        [0.5582, 0.4418],
        [0.5563, 0.4437],
        [0.5551, 0.4449],
        [0.5541, 0.4459],
        [0.5595, 0.4405],
        [0.5583, 0.4417],
[2K        [0.5569, 0.4431]], device='cuda:0', grad_fn=<SoftmaxBackward0>)8;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 143/203 [2m0:07:42 â€¢ 0:03:24[0m [2;4m0.30it/s[0m  
[2KClass label: HammerThrowâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 143/203 [2m0:07:42 â€¢ 0:03:24[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.5625, 0.4375]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 143/203 [2m0:07:42 â€¢ 0:03:24[0m [2;4m0.30it/s[0m  
[2KStep 0 - TTA Loss: 0.5899â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 143/203 [2m0:07:42 â€¢ 0:03:24[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.5625, 0.4375]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 143/203 [2m0:07:42 â€¢ 0:03:24[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.5667, 0.4333]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 143/203 [2m0:07:42 â€¢ 0:03:24[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.5861, 0.4139]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 143/203 [2m0:07:42 â€¢ 0:03:24[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.6323, 0.3677]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 143/203 [2m0:07:42 â€¢ 0:03:24[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.7132, 0.2868]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 143/203 [2m0:07:42 â€¢ 0:03:24[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.8217, 0.1783]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 143/203 [2m0:07:42 â€¢ 0:03:24[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.9245, 0.0755]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 143/203 [2m0:07:42 â€¢ 0:03:24[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.9826, 0.0174]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 143/203 [2m0:07:42 â€¢ 0:03:24[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.9962, 0.0038]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 143/203 [2m0:07:42 â€¢ 0:03:24[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.5888, 0.4112]], device='cuda:0')mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 143/203 [2m0:07:42 â€¢ 0:03:24[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.9960, 0.0040],â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 143/203 [2m0:07:42 â€¢ 0:03:24[0m [2;4m0.30it/s[0m  
        [0.9474, 0.0526],
        [0.9881, 0.0119],
        [0.9820, 0.0180],
        [0.9990, 0.0010],
        [0.9951, 0.0049],
        [0.9930, 0.0070],
        [0.9900, 0.0100],
        [0.9867, 0.0133],
[2K        [0.9951, 0.0049]], device='cuda:0')â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 143/203 [2m0:07:42 â€¢ 0:03:24[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.5601, 0.4399],â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 144/203 [2m0:07:56 â€¢ 0:04:31[0m [2;4m0.22it/s[0m  
        [0.5542, 0.4458],
        [0.5564, 0.4436],
        [0.5582, 0.4418],
        [0.5565, 0.4435],
        [0.5552, 0.4448],
        [0.5542, 0.4458],
        [0.5595, 0.4405],
        [0.5583, 0.4417],
[2K        [0.5570, 0.4430]], device='cuda:0', grad_fn=<SoftmaxBackward0>)8;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 144/203 [2m0:07:56 â€¢ 0:04:31[0m [2;4m0.22it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 144/203 [2m0:07:56 â€¢ 0:04:31[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.5662, 0.4338]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 144/203 [2m0:07:56 â€¢ 0:04:31[0m [2;4m0.22it/s[0m  
[2KStep 0 - TTA Loss: 0.5900â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 144/203 [2m0:07:56 â€¢ 0:04:31[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.7679, 0.2321]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 144/203 [2m0:07:56 â€¢ 0:04:31[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.8972, 0.1028]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 144/203 [2m0:07:56 â€¢ 0:04:31[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.9596, 0.0404]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 144/203 [2m0:07:56 â€¢ 0:04:31[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.9834, 0.0166]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 144/203 [2m0:07:56 â€¢ 0:04:31[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.9928, 0.0072]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 144/203 [2m0:07:56 â€¢ 0:04:31[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.9962, 0.0038]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 144/203 [2m0:07:56 â€¢ 0:04:31[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.9975, 0.0025]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 144/203 [2m0:07:56 â€¢ 0:04:31[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.9981, 0.0019]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 144/203 [2m0:07:56 â€¢ 0:04:31[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.9982, 0.0018]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 144/203 [2m0:07:56 â€¢ 0:04:31[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.5887, 0.4113]], device='cuda:0')mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 144/203 [2m0:07:56 â€¢ 0:04:31[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[9.9718e-01, 2.8158e-03],[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 144/203 [2m0:07:56 â€¢ 0:04:31[0m [2;4m0.22it/s[0m  
        [9.6791e-01, 3.2089e-02],
        [9.9321e-01, 6.7865e-03],
        [9.8929e-01, 1.0713e-02],
        [9.9930e-01, 6.9647e-04],
        [9.9821e-01, 1.7910e-03],
        [9.9830e-01, 1.6982e-03],
        [9.9385e-01, 6.1488e-03],
        [9.9290e-01, 7.0959e-03],
[2K        [9.9783e-01, 2.1720e-03]], device='cuda:0')m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 144/203 [2m0:07:56 â€¢ 0:04:31[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.5602, 0.4398],â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 145/203 [2m0:08:09 â€¢ 0:09:39[0m [2;4m0.10it/s[0m  
        [0.5542, 0.4458],
        [0.5565, 0.4435],
        [0.5582, 0.4418],
        [0.5565, 0.4435],
        [0.5553, 0.4447],
        [0.5543, 0.4457],
        [0.5595, 0.4405],
        [0.5584, 0.4416],
[2K        [0.5570, 0.4430]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 145/203 [2m0:08:09 â€¢ 0:09:39[0m [2;4m0.10it/s[0m  
[2KClass label: TennisSwingâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 145/203 [2m0:08:09 â€¢ 0:09:39[0m [2;4m0.10it/s[0m  
[2KAttention weights: tensor([[0.6420, 0.3580]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 145/203 [2m0:08:09 â€¢ 0:09:39[0m [2;4m0.10it/s[0m  
[2KStep 0 - TTA Loss: 0.7386â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 145/203 [2m0:08:09 â€¢ 0:09:39[0m [2;4m0.10it/s[0m  
[2KAttention weights: tensor([[0.6420, 0.3580]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 145/203 [2m0:08:09 â€¢ 0:09:39[0m [2;4m0.10it/s[0m  
[2KAttention weights: tensor([[0.6457, 0.3543]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 145/203 [2m0:08:09 â€¢ 0:09:39[0m [2;4m0.10it/s[0m  
[2KAttention weights: tensor([[0.6600, 0.3400]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 145/203 [2m0:08:09 â€¢ 0:09:39[0m [2;4m0.10it/s[0m  
[2KAttention weights: tensor([[0.6855, 0.3145]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 145/203 [2m0:08:09 â€¢ 0:09:39[0m [2;4m0.10it/s[0m  
[2KAttention weights: tensor([[0.7203, 0.2797]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 145/203 [2m0:08:09 â€¢ 0:09:39[0m [2;4m0.10it/s[0m  
[2KAttention weights: tensor([[0.7688, 0.2312]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 145/203 [2m0:08:09 â€¢ 0:09:39[0m [2;4m0.10it/s[0m  
[2KAttention weights: tensor([[0.8133, 0.1867]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 145/203 [2m0:08:09 â€¢ 0:09:39[0m [2;4m0.10it/s[0m  
[2KAttention weights: tensor([[0.8422, 0.1578]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 145/203 [2m0:08:09 â€¢ 0:09:39[0m [2;4m0.10it/s[0m  
[2KAttention weights: tensor([[0.8709, 0.1291]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 145/203 [2m0:08:09 â€¢ 0:09:39[0m [2;4m0.10it/s[0m  
[2KAttention weights: tensor([[0.5667, 0.4333]], device='cuda:0')6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 145/203 [2m0:08:09 â€¢ 0:09:39[0m [2;4m0.10it/s[0m  
[2KAttention weights: tensor([[0.8993, 0.1007],â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 145/203 [2m0:08:09 â€¢ 0:09:39[0m [2;4m0.10it/s[0m  
        [0.7906, 0.2094],
        [0.8553, 0.1447],
        [0.8742, 0.1258],
        [0.9104, 0.0896],
        [0.9150, 0.0850],
        [0.9155, 0.0845],
        [0.8905, 0.1095],
        [0.8961, 0.1039],
[2K        [0.9139, 0.0861]], device='cuda:0')â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 145/203 [2m0:08:09 â€¢ 0:09:39[0m [2;4m0.10it/s[0m  
[2KAttention weights: tensor([[0.5602, 0.4398],â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 146/203 [2m0:08:12 â€¢ 0:09:26[0m [2;4m0.10it/s[0m  
        [0.5542, 0.4458],
        [0.5565, 0.4435],
        [0.5582, 0.4418],
        [0.5565, 0.4435],
        [0.5553, 0.4447],
        [0.5543, 0.4457],
        [0.5595, 0.4405],
        [0.5584, 0.4416],
[2K        [0.5570, 0.4430]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 146/203 [2m0:08:12 â€¢ 0:09:26[0m [2;4m0.10it/s[0m  
[2KClass label: HammerThrowâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 146/203 [2m0:08:12 â€¢ 0:09:26[0m [2;4m0.10it/s[0m  
[2KAttention weights: tensor([[0.5617, 0.4383]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 146/203 [2m0:08:12 â€¢ 0:09:26[0m [2;4m0.10it/s[0m  
[2KStep 0 - TTA Loss: 0.6578â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 146/203 [2m0:08:12 â€¢ 0:09:26[0m [2;4m0.10it/s[0m  
[2KAttention weights: tensor([[0.6672, 0.3328]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 146/203 [2m0:08:12 â€¢ 0:09:26[0m [2;4m0.10it/s[0m  
[2KAttention weights: tensor([[0.7821, 0.2179]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 146/203 [2m0:08:12 â€¢ 0:09:26[0m [2;4m0.10it/s[0m  
[2KAttention weights: tensor([[0.8845, 0.1155]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 146/203 [2m0:08:12 â€¢ 0:09:26[0m [2;4m0.10it/s[0m  
[2KAttention weights: tensor([[0.9491, 0.0509]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 146/203 [2m0:08:12 â€¢ 0:09:26[0m [2;4m0.10it/s[0m  
[2KAttention weights: tensor([[0.9774, 0.0226]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 146/203 [2m0:08:12 â€¢ 0:09:26[0m [2;4m0.10it/s[0m  
[2KAttention weights: tensor([[0.9882, 0.0118]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 146/203 [2m0:08:12 â€¢ 0:09:26[0m [2;4m0.10it/s[0m  
[2KAttention weights: tensor([[0.9910, 0.0090]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 146/203 [2m0:08:12 â€¢ 0:09:26[0m [2;4m0.10it/s[0m  
[2KAttention weights: tensor([[0.9905, 0.0095]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 146/203 [2m0:08:12 â€¢ 0:09:26[0m [2;4m0.10it/s[0m  
[2KAttention weights: tensor([[0.9894, 0.0106]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 146/203 [2m0:08:12 â€¢ 0:09:26[0m [2;4m0.10it/s[0m  
[2KAttention weights: tensor([[0.5816, 0.4184]], device='cuda:0')6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 146/203 [2m0:08:12 â€¢ 0:09:26[0m [2;4m0.10it/s[0m  
[2KAttention weights: tensor([[0.9781, 0.0219],â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 146/203 [2m0:08:12 â€¢ 0:09:26[0m [2;4m0.10it/s[0m  
        [0.8851, 0.1149],
        [0.9523, 0.0477],
        [0.9468, 0.0532],
        [0.9892, 0.0108],
        [0.9755, 0.0245],
        [0.9698, 0.0302],
        [0.9638, 0.0362],
        [0.9577, 0.0423],
[2K        [0.9757, 0.0243]], device='cuda:0')â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 146/203 [2m0:08:12 â€¢ 0:09:26[0m [2;4m0.10it/s[0m  
[2KAttention weights: tensor([[0.5602, 0.4398],â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 147/203 [2m0:08:16 â€¢ 0:06:14[0m [2;4m0.15it/s[0m  
        [0.5542, 0.4458],
        [0.5565, 0.4435],
        [0.5582, 0.4418],
        [0.5566, 0.4434],
        [0.5553, 0.4447],
        [0.5543, 0.4457],
        [0.5595, 0.4405],
        [0.5583, 0.4417],
[2K        [0.5571, 0.4429]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 147/203 [2m0:08:16 â€¢ 0:06:14[0m [2;4m0.15it/s[0m  
[2KClass label: GolfSwingâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 147/203 [2m0:08:16 â€¢ 0:06:14[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.6495, 0.3505]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 147/203 [2m0:08:16 â€¢ 0:06:14[0m [2;4m0.15it/s[0m  
[2KStep 0 - TTA Loss: 0.2929â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 147/203 [2m0:08:16 â€¢ 0:06:14[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.6495, 0.3505]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 147/203 [2m0:08:16 â€¢ 0:06:14[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.6477, 0.3523]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 147/203 [2m0:08:16 â€¢ 0:06:14[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.6421, 0.3579]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 147/203 [2m0:08:16 â€¢ 0:06:14[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.6343, 0.3657]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 147/203 [2m0:08:16 â€¢ 0:06:14[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.6270, 0.3730]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 147/203 [2m0:08:16 â€¢ 0:06:14[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.6169, 0.3831]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 147/203 [2m0:08:16 â€¢ 0:06:14[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.6017, 0.3983]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 147/203 [2m0:08:16 â€¢ 0:06:14[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.5883, 0.4117]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 147/203 [2m0:08:16 â€¢ 0:06:14[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.5787, 0.4213]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 147/203 [2m0:08:16 â€¢ 0:06:14[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.5555, 0.4445]], device='cuda:0')6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 147/203 [2m0:08:16 â€¢ 0:06:14[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.3399, 0.6601],â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 147/203 [2m0:08:16 â€¢ 0:06:14[0m [2;4m0.15it/s[0m  
        [0.4355, 0.5645],
        [0.3189, 0.6811],
        [0.5825, 0.4175],
        [0.1651, 0.8349],
        [0.3263, 0.6737],
        [0.3234, 0.6766],
        [0.4397, 0.5603],
        [0.4727, 0.5273],
[2K        [0.3740, 0.6260]], device='cuda:0')â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”[0m 147/203 [2m0:08:16 â€¢ 0:06:14[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.5601, 0.4399],â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 148/203 [2m0:08:19 â€¢ 0:05:21[0m [2;4m0.17it/s[0m   
        [0.5542, 0.4458],
        [0.5564, 0.4436],
        [0.5582, 0.4418],
        [0.5565, 0.4435],
        [0.5552, 0.4448],
        [0.5542, 0.4458],
        [0.5595, 0.4405],
        [0.5583, 0.4417],
[2K        [0.5570, 0.4430]], device='cuda:0', grad_fn=<SoftmaxBackward0>)38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 148/203 [2m0:08:19 â€¢ 0:05:21[0m [2;4m0.17it/s[0m  
[2KClass label: SoccerPenaltyâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 148/203 [2m0:08:19 â€¢ 0:05:21[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.6660, 0.3340]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 148/203 [2m0:08:19 â€¢ 0:05:21[0m [2;4m0.17it/s[0m  
[2KStep 0 - TTA Loss: 0.8433â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 148/203 [2m0:08:19 â€¢ 0:05:21[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.6900, 0.3100]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 148/203 [2m0:08:19 â€¢ 0:05:21[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.7461, 0.2539]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 148/203 [2m0:08:19 â€¢ 0:05:21[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.8200, 0.1800]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 148/203 [2m0:08:19 â€¢ 0:05:21[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.8840, 0.1160]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 148/203 [2m0:08:19 â€¢ 0:05:21[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.9217, 0.0783]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 148/203 [2m0:08:19 â€¢ 0:05:21[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.9386, 0.0614]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 148/203 [2m0:08:19 â€¢ 0:05:21[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.9420, 0.0580]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 148/203 [2m0:08:19 â€¢ 0:05:21[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.9414, 0.0586]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 148/203 [2m0:08:19 â€¢ 0:05:21[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.9404, 0.0596]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 148/203 [2m0:08:19 â€¢ 0:05:21[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.5704, 0.4296]], device='cuda:0')7mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 148/203 [2m0:08:19 â€¢ 0:05:21[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.7802, 0.2198],â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 148/203 [2m0:08:19 â€¢ 0:05:21[0m [2;4m0.17it/s[0m  
        [0.7290, 0.2710],
        [0.5860, 0.4140],
        [0.8218, 0.1782],
        [0.5678, 0.4322],
        [0.7357, 0.2643],
        [0.6501, 0.3499],
        [0.9402, 0.0598],
        [0.7896, 0.2104],
[2K        [0.6947, 0.3053]], device='cuda:0')â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 148/203 [2m0:08:19 â€¢ 0:05:21[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.5601, 0.4399],â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 149/203 [2m0:08:22 â€¢ 0:04:47[0m [2;4m0.19it/s[0m  
        [0.5541, 0.4459],
        [0.5564, 0.4436],
        [0.5582, 0.4418],
        [0.5565, 0.4435],
        [0.5552, 0.4448],
        [0.5542, 0.4458],
        [0.5595, 0.4405],
        [0.5583, 0.4417],
[2K        [0.5570, 0.4430]], device='cuda:0', grad_fn=<SoftmaxBackward0>)38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 149/203 [2m0:08:22 â€¢ 0:04:47[0m [2;4m0.19it/s[0m  
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 149/203 [2m0:08:22 â€¢ 0:04:47[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.6044, 0.3956]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 149/203 [2m0:08:22 â€¢ 0:04:47[0m [2;4m0.19it/s[0m  
[2KStep 0 - TTA Loss: 0.7219â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 149/203 [2m0:08:22 â€¢ 0:04:47[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.6044, 0.3956]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 149/203 [2m0:08:22 â€¢ 0:04:47[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.6044, 0.3956]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 149/203 [2m0:08:22 â€¢ 0:04:47[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.6051, 0.3949]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 149/203 [2m0:08:22 â€¢ 0:04:47[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.6075, 0.3925]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 149/203 [2m0:08:22 â€¢ 0:04:47[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.6133, 0.3867]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 149/203 [2m0:08:22 â€¢ 0:04:47[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.6239, 0.3761]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 149/203 [2m0:08:22 â€¢ 0:04:47[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.6431, 0.3569]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 149/203 [2m0:08:22 â€¢ 0:04:47[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.6707, 0.3293]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 149/203 [2m0:08:22 â€¢ 0:04:47[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.7010, 0.2990]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 149/203 [2m0:08:22 â€¢ 0:04:47[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.5584, 0.4416]], device='cuda:0')7mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 149/203 [2m0:08:22 â€¢ 0:04:47[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.6936, 0.3064],â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 149/203 [2m0:08:22 â€¢ 0:04:47[0m [2;4m0.19it/s[0m  
        [0.6201, 0.3799],
        [0.6483, 0.3517],
        [0.7235, 0.2765],
        [0.6322, 0.3678],
        [0.7272, 0.2728],
        [0.6711, 0.3289],
        [0.7179, 0.2821],
        [0.7193, 0.2807],
[2K        [0.7123, 0.2877]], device='cuda:0')â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 149/203 [2m0:08:22 â€¢ 0:04:47[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.5601, 0.4399],â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 150/203 [2m0:08:26 â€¢ 0:02:56[0m [2;4m0.30it/s[0m  
        [0.5541, 0.4459],
        [0.5564, 0.4436],
        [0.5582, 0.4418],
        [0.5564, 0.4436],
        [0.5552, 0.4448],
        [0.5542, 0.4458],
        [0.5595, 0.4405],
        [0.5583, 0.4417],
[2K        [0.5570, 0.4430]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 150/203 [2m0:08:26 â€¢ 0:02:56[0m [2;4m0.30it/s[0m  
[2KClass label: BaseballPitchâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 150/203 [2m0:08:26 â€¢ 0:02:56[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.6451, 0.3549]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 150/203 [2m0:08:26 â€¢ 0:02:56[0m [2;4m0.30it/s[0m  
[2KStep 0 - TTA Loss: 0.5938â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 150/203 [2m0:08:26 â€¢ 0:02:56[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.6968, 0.3032]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 150/203 [2m0:08:26 â€¢ 0:02:56[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.7732, 0.2268]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 150/203 [2m0:08:26 â€¢ 0:02:56[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.8352, 0.1648]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 150/203 [2m0:08:26 â€¢ 0:02:56[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.8732, 0.1268]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 150/203 [2m0:08:26 â€¢ 0:02:56[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.8932, 0.1068]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 150/203 [2m0:08:26 â€¢ 0:02:56[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.9011, 0.0989]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 150/203 [2m0:08:26 â€¢ 0:02:56[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.9036, 0.0964]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 150/203 [2m0:08:26 â€¢ 0:02:56[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.9031, 0.0969]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 150/203 [2m0:08:26 â€¢ 0:02:56[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.9021, 0.0979]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 150/203 [2m0:08:26 â€¢ 0:02:56[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.5702, 0.4298]], device='cuda:0');6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 150/203 [2m0:08:26 â€¢ 0:02:56[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.9017, 0.0983],â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 150/203 [2m0:08:26 â€¢ 0:02:56[0m [2;4m0.30it/s[0m  
        [0.7188, 0.2812],
        [0.7212, 0.2788],
        [0.8199, 0.1801],
        [0.7933, 0.2067],
        [0.8202, 0.1798],
        [0.7686, 0.2314],
        [0.8443, 0.1557],
        [0.8218, 0.1782],
[2K        [0.8170, 0.1830]], device='cuda:0')â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 150/203 [2m0:08:26 â€¢ 0:02:56[0m [2;4m0.30it/s[0m  
[2KAttention weights: tensor([[0.5601, 0.4399],â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 151/203 [2m0:08:28 â€¢ 0:02:38[0m [2;4m0.33it/s[0m  
        [0.5541, 0.4459],
        [0.5564, 0.4436],
        [0.5582, 0.4418],
        [0.5564, 0.4436],
        [0.5552, 0.4448],
        [0.5542, 0.4458],
        [0.5595, 0.4405],
        [0.5583, 0.4417],
[2K        [0.5570, 0.4430]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 151/203 [2m0:08:28 â€¢ 0:02:38[0m [2;4m0.33it/s[0m  
[2KClass label: GolfSwingâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 151/203 [2m0:08:28 â€¢ 0:02:38[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.6544, 0.3456]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 151/203 [2m0:08:28 â€¢ 0:02:38[0m [2;4m0.33it/s[0m  
[2KStep 0 - TTA Loss: 0.7005â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 151/203 [2m0:08:28 â€¢ 0:02:38[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.6544, 0.3456]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 151/203 [2m0:08:28 â€¢ 0:02:38[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.6548, 0.3452]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 151/203 [2m0:08:28 â€¢ 0:02:38[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.6576, 0.3424]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 151/203 [2m0:08:28 â€¢ 0:02:38[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.6659, 0.3341]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 151/203 [2m0:08:28 â€¢ 0:02:38[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.6816, 0.3184]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 151/203 [2m0:08:28 â€¢ 0:02:38[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.7080, 0.2920]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 151/203 [2m0:08:28 â€¢ 0:02:38[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.7461, 0.2539]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 151/203 [2m0:08:28 â€¢ 0:02:38[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.7895, 0.2105]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 151/203 [2m0:08:28 â€¢ 0:02:38[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.8362, 0.1638]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 151/203 [2m0:08:28 â€¢ 0:02:38[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.5669, 0.4331]], device='cuda:0');6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 151/203 [2m0:08:28 â€¢ 0:02:38[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.7239, 0.2761],â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 151/203 [2m0:08:28 â€¢ 0:02:38[0m [2;4m0.33it/s[0m  
        [0.6794, 0.3206],
        [0.6705, 0.3295],
        [0.8780, 0.1220],
        [0.6754, 0.3246],
        [0.7229, 0.2771],
        [0.6875, 0.3125],
        [0.7507, 0.2493],
        [0.7912, 0.2088],
[2K        [0.7773, 0.2227]], device='cuda:0')â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 151/203 [2m0:08:28 â€¢ 0:02:38[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.5601, 0.4399],â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 152/203 [2m0:08:29 â€¢ 0:02:27[0m [2;4m0.35it/s[0m  
        [0.5542, 0.4458],
        [0.5564, 0.4436],
        [0.5582, 0.4418],
        [0.5564, 0.4436],
        [0.5552, 0.4448],
        [0.5542, 0.4458],
        [0.5595, 0.4405],
        [0.5583, 0.4417],
[2K        [0.5570, 0.4430]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 152/203 [2m0:08:29 â€¢ 0:02:27[0m [2;4m0.35it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 152/203 [2m0:08:29 â€¢ 0:02:27[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5684, 0.4316]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 152/203 [2m0:08:29 â€¢ 0:02:27[0m [2;4m0.35it/s[0m  
[2KStep 0 - TTA Loss: 0.6337â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 152/203 [2m0:08:29 â€¢ 0:02:27[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6215, 0.3785]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 152/203 [2m0:08:29 â€¢ 0:02:27[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6901, 0.3099]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 152/203 [2m0:08:29 â€¢ 0:02:27[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.7626, 0.2374]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 152/203 [2m0:08:29 â€¢ 0:02:27[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.8265, 0.1735]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 152/203 [2m0:08:29 â€¢ 0:02:27[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.8714, 0.1286]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 152/203 [2m0:08:29 â€¢ 0:02:27[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.9033, 0.0967]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 152/203 [2m0:08:29 â€¢ 0:02:27[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.9277, 0.0723]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 152/203 [2m0:08:29 â€¢ 0:02:27[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.9415, 0.0585]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 152/203 [2m0:08:29 â€¢ 0:02:27[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.9472, 0.0528]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 152/203 [2m0:08:29 â€¢ 0:02:27[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5721, 0.4279]], device='cuda:0');6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 152/203 [2m0:08:29 â€¢ 0:02:27[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.9065, 0.0935],â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 152/203 [2m0:08:29 â€¢ 0:02:27[0m [2;4m0.35it/s[0m  
        [0.8468, 0.1532],
        [0.8800, 0.1200],
        [0.9545, 0.0455],
        [0.9116, 0.0884],
        [0.9404, 0.0596],
        [0.9487, 0.0513],
        [0.9065, 0.0935],
        [0.9302, 0.0698],
[2K        [0.9463, 0.0537]], device='cuda:0')â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”[0m 152/203 [2m0:08:29 â€¢ 0:02:27[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5602, 0.4398],â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 153/203 [2m0:08:56 â€¢ 0:08:20[0m [2;4m0.10it/s[0m   
        [0.5542, 0.4458],
        [0.5564, 0.4436],
        [0.5583, 0.4417],
        [0.5565, 0.4435],
        [0.5553, 0.4447],
        [0.5543, 0.4457],
        [0.5595, 0.4405],
        [0.5584, 0.4416],
[2K        [0.5570, 0.4430]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 153/203 [2m0:08:56 â€¢ 0:08:20[0m [2;4m0.10it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 153/203 [2m0:08:56 â€¢ 0:08:20[0m [2;4m0.10it/s[0m  
[2KAttention weights: tensor([[0.6293, 0.3707]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 153/203 [2m0:08:56 â€¢ 0:08:20[0m [2;4m0.10it/s[0m  
[2KStep 0 - TTA Loss: 0.7567â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 153/203 [2m0:08:56 â€¢ 0:08:20[0m [2;4m0.10it/s[0m  
[2KAttention weights: tensor([[0.6293, 0.3707]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 153/203 [2m0:08:56 â€¢ 0:08:20[0m [2;4m0.10it/s[0m  
[2KAttention weights: tensor([[0.6348, 0.3652]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 153/203 [2m0:08:56 â€¢ 0:08:20[0m [2;4m0.10it/s[0m  
[2KAttention weights: tensor([[0.6572, 0.3428]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 153/203 [2m0:08:56 â€¢ 0:08:20[0m [2;4m0.10it/s[0m  
[2KAttention weights: tensor([[0.7051, 0.2949]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 153/203 [2m0:08:56 â€¢ 0:08:20[0m [2;4m0.10it/s[0m  
[2KAttention weights: tensor([[0.7783, 0.2217]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 153/203 [2m0:08:56 â€¢ 0:08:20[0m [2;4m0.10it/s[0m  
[2KAttention weights: tensor([[0.8661, 0.1339]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 153/203 [2m0:08:56 â€¢ 0:08:20[0m [2;4m0.10it/s[0m  
[2KAttention weights: tensor([[0.9449, 0.0551]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 153/203 [2m0:08:56 â€¢ 0:08:20[0m [2;4m0.10it/s[0m  
[2KAttention weights: tensor([[0.9879, 0.0121]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 153/203 [2m0:08:56 â€¢ 0:08:20[0m [2;4m0.10it/s[0m  
[2KAttention weights: tensor([[0.9974, 0.0026]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 153/203 [2m0:08:56 â€¢ 0:08:20[0m [2;4m0.10it/s[0m  
[2KAttention weights: tensor([[0.5926, 0.4074]], device='cuda:0')37mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 153/203 [2m0:08:56 â€¢ 0:08:20[0m [2;4m0.10it/s[0m  
[2KAttention weights: tensor([[9.9374e-01, 6.2561e-03],0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 153/203 [2m0:08:56 â€¢ 0:08:20[0m [2;4m0.10it/s[0m  
        [9.6321e-01, 3.6793e-02],
        [9.8324e-01, 1.6758e-02],
        [9.9524e-01, 4.7628e-03],
        [9.9641e-01, 3.5887e-03],
        [9.9666e-01, 3.3424e-03],
        [9.9746e-01, 2.5371e-03],
        [9.8545e-01, 1.4553e-02],
        [9.9540e-01, 4.6011e-03],
[2K        [9.9902e-01, 9.8038e-04]], device='cuda:0')[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 153/203 [2m0:08:56 â€¢ 0:08:20[0m [2;4m0.10it/s[0m  
[2KAttention weights: tensor([[0.5602, 0.4398],â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 154/203 [2m0:08:59 â€¢ 0:12:01[0m [2;4m0.07it/s[0m  
        [0.5542, 0.4458],
        [0.5565, 0.4435],
        [0.5583, 0.4417],
        [0.5566, 0.4434],
        [0.5553, 0.4447],
        [0.5544, 0.4456],
        [0.5596, 0.4404],
        [0.5584, 0.4416],
[2K        [0.5571, 0.4429]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 154/203 [2m0:08:59 â€¢ 0:12:01[0m [2;4m0.07it/s[0m  
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 154/203 [2m0:08:59 â€¢ 0:12:01[0m [2;4m0.07it/s[0m  
[2KAttention weights: tensor([[0.6047, 0.3953]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 154/203 [2m0:08:59 â€¢ 0:12:01[0m [2;4m0.07it/s[0m  
[2KStep 0 - TTA Loss: 0.6007â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 154/203 [2m0:08:59 â€¢ 0:12:01[0m [2;4m0.07it/s[0m  
[2KAttention weights: tensor([[0.7881, 0.2119]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 154/203 [2m0:08:59 â€¢ 0:12:01[0m [2;4m0.07it/s[0m  
[2KAttention weights: tensor([[0.9114, 0.0886]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 154/203 [2m0:08:59 â€¢ 0:12:01[0m [2;4m0.07it/s[0m  
[2KAttention weights: tensor([[0.9732, 0.0268]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 154/203 [2m0:08:59 â€¢ 0:12:01[0m [2;4m0.07it/s[0m  
[2KAttention weights: tensor([[0.9907, 0.0093]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 154/203 [2m0:08:59 â€¢ 0:12:01[0m [2;4m0.07it/s[0m  
[2KAttention weights: tensor([[0.9936, 0.0064]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 154/203 [2m0:08:59 â€¢ 0:12:01[0m [2;4m0.07it/s[0m  
[2KAttention weights: tensor([[0.9929, 0.0071]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 154/203 [2m0:08:59 â€¢ 0:12:01[0m [2;4m0.07it/s[0m  
[2KAttention weights: tensor([[0.9903, 0.0097]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 154/203 [2m0:08:59 â€¢ 0:12:01[0m [2;4m0.07it/s[0m  
[2KAttention weights: tensor([[0.9876, 0.0124]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 154/203 [2m0:08:59 â€¢ 0:12:01[0m [2;4m0.07it/s[0m  
[2KAttention weights: tensor([[0.9858, 0.0142]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 154/203 [2m0:08:59 â€¢ 0:12:01[0m [2;4m0.07it/s[0m  
[2KAttention weights: tensor([[0.5779, 0.4221]], device='cuda:0')37mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 154/203 [2m0:08:59 â€¢ 0:12:01[0m [2;4m0.07it/s[0m  
[2KAttention weights: tensor([[0.9745, 0.0255],â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 154/203 [2m0:08:59 â€¢ 0:12:01[0m [2;4m0.07it/s[0m  
        [0.9108, 0.0892],
        [0.9503, 0.0497],
        [0.9769, 0.0231],
        [0.9809, 0.0191],
        [0.9853, 0.0147],
        [0.9853, 0.0147],
        [0.9627, 0.0373],
        [0.9789, 0.0211],
[2K        [0.9914, 0.0086]], device='cuda:0')â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 154/203 [2m0:08:59 â€¢ 0:12:01[0m [2;4m0.07it/s[0m  
[2KAttention weights: tensor([[0.5602, 0.4398],â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 155/203 [2m0:09:02 â€¢ 0:02:33[0m [2;4m0.32it/s[0m  
        [0.5543, 0.4457],
        [0.5565, 0.4435],
        [0.5583, 0.4417],
        [0.5566, 0.4434],
        [0.5554, 0.4446],
        [0.5544, 0.4456],
        [0.5596, 0.4404],
        [0.5584, 0.4416],
[2K        [0.5571, 0.4429]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 155/203 [2m0:09:02 â€¢ 0:02:33[0m [2;4m0.32it/s[0m  
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 155/203 [2m0:09:02 â€¢ 0:02:33[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.5998, 0.4002]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 155/203 [2m0:09:02 â€¢ 0:02:33[0m [2;4m0.32it/s[0m  
[2KStep 0 - TTA Loss: 0.5471â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 155/203 [2m0:09:02 â€¢ 0:02:33[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.5998, 0.4002]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 155/203 [2m0:09:02 â€¢ 0:02:33[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.5921, 0.4079]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 155/203 [2m0:09:02 â€¢ 0:02:33[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.5653, 0.4347]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 155/203 [2m0:09:02 â€¢ 0:02:33[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.5129, 0.4871]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 155/203 [2m0:09:02 â€¢ 0:02:33[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.4354, 0.5646]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 155/203 [2m0:09:02 â€¢ 0:02:33[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.3414, 0.6586]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 155/203 [2m0:09:02 â€¢ 0:02:33[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.2483, 0.7517]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 155/203 [2m0:09:02 â€¢ 0:02:33[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.1685, 0.8315]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 155/203 [2m0:09:02 â€¢ 0:02:33[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.1088, 0.8912]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 155/203 [2m0:09:02 â€¢ 0:02:33[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.5390, 0.4610]], device='cuda:0')8;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 155/203 [2m0:09:02 â€¢ 0:02:33[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.2417, 0.7583],â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 155/203 [2m0:09:02 â€¢ 0:02:33[0m [2;4m0.32it/s[0m  
        [0.2606, 0.7394],
        [0.1788, 0.8212],
        [0.3313, 0.6687],
        [0.1228, 0.8772],
        [0.0675, 0.9325],
        [0.1145, 0.8855],
        [0.1881, 0.8119],
        [0.2300, 0.7700],
[2K        [0.2107, 0.7893]], device='cuda:0')â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 155/203 [2m0:09:02 â€¢ 0:02:33[0m [2;4m0.32it/s[0m  
[2KAttention weights: tensor([[0.5602, 0.4398],â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 156/203 [2m0:09:09 â€¢ 0:03:33[0m [2;4m0.22it/s[0m  
        [0.5542, 0.4458],
        [0.5565, 0.4435],
        [0.5583, 0.4417],
        [0.5566, 0.4434],
        [0.5553, 0.4447],
        [0.5544, 0.4456],
        [0.5596, 0.4404],
        [0.5584, 0.4416],
[2K        [0.5571, 0.4429]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 156/203 [2m0:09:09 â€¢ 0:03:33[0m [2;4m0.22it/s[0m  
[2KClass label: CleanAndJerkâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 156/203 [2m0:09:09 â€¢ 0:03:33[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.5714, 0.4286]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 156/203 [2m0:09:09 â€¢ 0:03:33[0m [2;4m0.22it/s[0m  
[2KStep 0 - TTA Loss: 0.5094â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 156/203 [2m0:09:09 â€¢ 0:03:33[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.5239, 0.4761]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 156/203 [2m0:09:09 â€¢ 0:03:33[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.4978, 0.5022]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 156/203 [2m0:09:09 â€¢ 0:03:33[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.4840, 0.5160]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 156/203 [2m0:09:09 â€¢ 0:03:33[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.4766, 0.5234]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 156/203 [2m0:09:09 â€¢ 0:03:33[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.4719, 0.5281]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 156/203 [2m0:09:09 â€¢ 0:03:33[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.4687, 0.5313]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 156/203 [2m0:09:09 â€¢ 0:03:33[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.4669, 0.5331]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 156/203 [2m0:09:09 â€¢ 0:03:33[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.4662, 0.5338]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 156/203 [2m0:09:09 â€¢ 0:03:33[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.4660, 0.5340]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 156/203 [2m0:09:09 â€¢ 0:03:33[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.5538, 0.4462]], device='cuda:0')8;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 156/203 [2m0:09:09 â€¢ 0:03:33[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.4057, 0.5943],â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 156/203 [2m0:09:09 â€¢ 0:03:33[0m [2;4m0.22it/s[0m  
        [0.4068, 0.5932],
        [0.4660, 0.5340],
        [0.4840, 0.5160],
        [0.2932, 0.7068],
        [0.2158, 0.7842],
        [0.2744, 0.7256],
        [0.3652, 0.6348],
        [0.4012, 0.5988],
[2K        [0.3887, 0.6113]], device='cuda:0')â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 156/203 [2m0:09:09 â€¢ 0:03:33[0m [2;4m0.22it/s[0m  
[2KAttention weights: tensor([[0.5602, 0.4398],â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 157/203 [2m0:09:17 â€¢ 0:04:06[0m [2;4m0.19it/s[0m  
        [0.5542, 0.4458],
        [0.5565, 0.4435],
        [0.5583, 0.4417],
        [0.5565, 0.4435],
        [0.5553, 0.4447],
        [0.5544, 0.4456],
        [0.5595, 0.4405],
        [0.5584, 0.4416],
[2K        [0.5571, 0.4429]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 157/203 [2m0:09:17 â€¢ 0:04:06[0m [2;4m0.19it/s[0m  
[2KClass label: GolfSwingâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 157/203 [2m0:09:17 â€¢ 0:04:06[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.6534, 0.3466]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 157/203 [2m0:09:17 â€¢ 0:04:06[0m [2;4m0.19it/s[0m  
[2KStep 0 - TTA Loss: 0.6299â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 157/203 [2m0:09:17 â€¢ 0:04:06[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.6534, 0.3466]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 157/203 [2m0:09:17 â€¢ 0:04:06[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.6537, 0.3463]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 157/203 [2m0:09:17 â€¢ 0:04:06[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.6560, 0.3440]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 157/203 [2m0:09:17 â€¢ 0:04:06[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.6613, 0.3387]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 157/203 [2m0:09:17 â€¢ 0:04:06[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.6720, 0.3280]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 157/203 [2m0:09:17 â€¢ 0:04:06[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.6886, 0.3114]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 157/203 [2m0:09:17 â€¢ 0:04:06[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.7077, 0.2923]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 157/203 [2m0:09:17 â€¢ 0:04:06[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.7270, 0.2730]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 157/203 [2m0:09:17 â€¢ 0:04:06[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.7336, 0.2664]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 157/203 [2m0:09:17 â€¢ 0:04:06[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.5595, 0.4405]], device='cuda:0')8;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 157/203 [2m0:09:17 â€¢ 0:04:06[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.6633, 0.3367],â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 157/203 [2m0:09:17 â€¢ 0:04:06[0m [2;4m0.19it/s[0m  
        [0.5912, 0.4088],
        [0.6114, 0.3886],
        [0.7323, 0.2677],
        [0.5747, 0.4253],
        [0.5870, 0.4130],
        [0.5668, 0.4332],
        [0.6654, 0.3346],
        [0.6727, 0.3273],
[2K        [0.6580, 0.3420]], device='cuda:0')â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”[0m 157/203 [2m0:09:17 â€¢ 0:04:06[0m [2;4m0.19it/s[0m  
[2KAttention weights: tensor([[0.5602, 0.4398],â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 158/203 [2m0:09:21 â€¢ 0:03:50[0m [2;4m0.20it/s[0m   
        [0.5542, 0.4458],
        [0.5565, 0.4435],
        [0.5583, 0.4417],
        [0.5565, 0.4435],
        [0.5553, 0.4447],
        [0.5544, 0.4456],
        [0.5595, 0.4405],
        [0.5584, 0.4416],
[2K        [0.5571, 0.4429]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[38;5;237mâ”â”â”â”â”â”â”â”[0m 158/203 [2m0:09:21 â€¢ 0:03:50[0m [2;4m0.20it/s[0m  
[2KClass label: BaseballPitchâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 158/203 [2m0:09:21 â€¢ 0:03:50[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.6425, 0.3575]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 158/203 [2m0:09:21 â€¢ 0:03:50[0m [2;4m0.20it/s[0m  
[2KStep 0 - TTA Loss: 0.6315â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 158/203 [2m0:09:21 â€¢ 0:03:50[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.6680, 0.3320]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 158/203 [2m0:09:21 â€¢ 0:03:50[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.7187, 0.2813]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 158/203 [2m0:09:21 â€¢ 0:03:50[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.7812, 0.2188]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 158/203 [2m0:09:21 â€¢ 0:03:50[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.8365, 0.1635]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 158/203 [2m0:09:21 â€¢ 0:03:50[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.8760, 0.1240]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 158/203 [2m0:09:21 â€¢ 0:03:50[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.9013, 0.0987]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 158/203 [2m0:09:21 â€¢ 0:03:50[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.9143, 0.0857]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 158/203 [2m0:09:21 â€¢ 0:03:50[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.9206, 0.0794]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 158/203 [2m0:09:21 â€¢ 0:03:50[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.9216, 0.0784]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 158/203 [2m0:09:21 â€¢ 0:03:50[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.5712, 0.4288]], device='cuda:0')237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 158/203 [2m0:09:21 â€¢ 0:03:50[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.9218, 0.0782],â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 158/203 [2m0:09:21 â€¢ 0:03:50[0m [2;4m0.20it/s[0m  
        [0.7014, 0.2986],
        [0.6933, 0.3067],
        [0.8106, 0.1894],
        [0.7808, 0.2192],
        [0.7422, 0.2578],
        [0.7049, 0.2951],
        [0.8334, 0.1666],
        [0.7978, 0.2022],
[2K        [0.7899, 0.2101]], device='cuda:0')â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 158/203 [2m0:09:21 â€¢ 0:03:50[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.5602, 0.4398],â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 159/203 [2m0:09:22 â€¢ 0:03:14[0m [2;4m0.23it/s[0m  
        [0.5542, 0.4458],
        [0.5565, 0.4435],
        [0.5583, 0.4417],
        [0.5566, 0.4434],
        [0.5553, 0.4447],
        [0.5544, 0.4456],
        [0.5596, 0.4404],
        [0.5584, 0.4416],
[2K        [0.5571, 0.4429]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[38;5;237mâ”â”â”â”â”â”â”â”[0m 159/203 [2m0:09:22 â€¢ 0:03:14[0m [2;4m0.23it/s[0m  
[2KClass label: Billiardsâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 159/203 [2m0:09:22 â€¢ 0:03:14[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.5593, 0.4407]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 159/203 [2m0:09:22 â€¢ 0:03:14[0m [2;4m0.23it/s[0m  
[2KStep 0 - TTA Loss: 0.5299â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 159/203 [2m0:09:22 â€¢ 0:03:14[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.5593, 0.4407]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 159/203 [2m0:09:22 â€¢ 0:03:14[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.5606, 0.4394]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 159/203 [2m0:09:22 â€¢ 0:03:14[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.5680, 0.4320]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 159/203 [2m0:09:22 â€¢ 0:03:14[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.5876, 0.4124]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 159/203 [2m0:09:22 â€¢ 0:03:14[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.6229, 0.3771]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 159/203 [2m0:09:22 â€¢ 0:03:14[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.6758, 0.3242]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 159/203 [2m0:09:22 â€¢ 0:03:14[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.7401, 0.2599]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 159/203 [2m0:09:22 â€¢ 0:03:14[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.8009, 0.1991]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 159/203 [2m0:09:22 â€¢ 0:03:14[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.8475, 0.1525]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 159/203 [2m0:09:22 â€¢ 0:03:14[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.5626, 0.4374]], device='cuda:0')237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 159/203 [2m0:09:22 â€¢ 0:03:14[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.7945, 0.2055],â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 159/203 [2m0:09:22 â€¢ 0:03:14[0m [2;4m0.23it/s[0m  
        [0.8771, 0.1229],
        [0.7069, 0.2931],
        [0.7858, 0.2142],
        [0.7004, 0.2996],
        [0.7308, 0.2692],
        [0.6947, 0.3053],
        [0.8062, 0.1938],
        [0.7895, 0.2105],
[2K        [0.7456, 0.2544]], device='cuda:0')â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 159/203 [2m0:09:22 â€¢ 0:03:14[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.5603, 0.4397],â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 160/203 [2m0:09:24 â€¢ 0:02:53[0m [2;4m0.25it/s[0m  
        [0.5543, 0.4457],
        [0.5565, 0.4435],
        [0.5583, 0.4417],
        [0.5566, 0.4434],
        [0.5553, 0.4447],
        [0.5544, 0.4456],
        [0.5596, 0.4404],
        [0.5584, 0.4416],
[2K        [0.5571, 0.4429]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 160/203 [2m0:09:24 â€¢ 0:02:53[0m [2;4m0.25it/s[0m  
[2KClass label: Billiardsâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 160/203 [2m0:09:24 â€¢ 0:02:53[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.5625, 0.4375]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 160/203 [2m0:09:24 â€¢ 0:02:53[0m [2;4m0.25it/s[0m  
[2KStep 0 - TTA Loss: 0.6987â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 160/203 [2m0:09:24 â€¢ 0:02:53[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6288, 0.3712]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 160/203 [2m0:09:24 â€¢ 0:02:53[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.6895, 0.3105]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 160/203 [2m0:09:24 â€¢ 0:02:53[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.7363, 0.2637]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 160/203 [2m0:09:24 â€¢ 0:02:53[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.7667, 0.2333]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 160/203 [2m0:09:24 â€¢ 0:02:53[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.7852, 0.2148]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 160/203 [2m0:09:24 â€¢ 0:02:53[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.7974, 0.2026]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 160/203 [2m0:09:24 â€¢ 0:02:53[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.8051, 0.1949]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 160/203 [2m0:09:24 â€¢ 0:02:53[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.8117, 0.1883]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 160/203 [2m0:09:24 â€¢ 0:02:53[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.8153, 0.1847]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 160/203 [2m0:09:24 â€¢ 0:02:53[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.5602, 0.4398]], device='cuda:0')98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 160/203 [2m0:09:24 â€¢ 0:02:53[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.7544, 0.2456],â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 160/203 [2m0:09:24 â€¢ 0:02:53[0m [2;4m0.25it/s[0m  
        [0.8165, 0.1835],
        [0.6690, 0.3310],
        [0.7518, 0.2482],
        [0.6589, 0.3411],
        [0.6961, 0.3039],
        [0.6579, 0.3421],
        [0.7713, 0.2287],
        [0.7525, 0.2475],
[2K        [0.7127, 0.2873]], device='cuda:0')â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 160/203 [2m0:09:24 â€¢ 0:02:53[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.5603, 0.4397],â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 161/203 [2m0:09:26 â€¢ 0:02:43[0m [2;4m0.26it/s[0m  
        [0.5543, 0.4457],
        [0.5565, 0.4435],
        [0.5583, 0.4417],
        [0.5566, 0.4434],
        [0.5553, 0.4447],
        [0.5544, 0.4456],
        [0.5596, 0.4404],
        [0.5584, 0.4416],
[2K        [0.5571, 0.4429]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 161/203 [2m0:09:26 â€¢ 0:02:43[0m [2;4m0.26it/s[0m  
[2KClass label: Billiardsâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 161/203 [2m0:09:26 â€¢ 0:02:43[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.5611, 0.4389]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 161/203 [2m0:09:26 â€¢ 0:02:43[0m [2;4m0.26it/s[0m  
[2KStep 0 - TTA Loss: 0.7055â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 161/203 [2m0:09:26 â€¢ 0:02:43[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.5611, 0.4389]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 161/203 [2m0:09:26 â€¢ 0:02:43[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.5638, 0.4362]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 161/203 [2m0:09:26 â€¢ 0:02:43[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.5755, 0.4245]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 161/203 [2m0:09:26 â€¢ 0:02:43[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.6003, 0.3997]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 161/203 [2m0:09:26 â€¢ 0:02:43[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.6397, 0.3603]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 161/203 [2m0:09:26 â€¢ 0:02:43[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.6912, 0.3088]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 161/203 [2m0:09:26 â€¢ 0:02:43[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.7480, 0.2520]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 161/203 [2m0:09:26 â€¢ 0:02:43[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.8231, 0.1769]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 161/203 [2m0:09:26 â€¢ 0:02:43[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.9247, 0.0753]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 161/203 [2m0:09:26 â€¢ 0:02:43[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.5748, 0.4252]], device='cuda:0')98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 161/203 [2m0:09:26 â€¢ 0:02:43[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.8854, 0.1146],â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 161/203 [2m0:09:26 â€¢ 0:02:43[0m [2;4m0.26it/s[0m  
        [0.9886, 0.0114],
        [0.8391, 0.1609],
        [0.8934, 0.1066],
        [0.8218, 0.1782],
        [0.8539, 0.1461],
        [0.8262, 0.1738],
        [0.9096, 0.0904],
        [0.9063, 0.0937],
[2K        [0.8522, 0.1478]], device='cuda:0')â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 161/203 [2m0:09:26 â€¢ 0:02:43[0m [2;4m0.26it/s[0m  
[2KAttention weights: tensor([[0.5603, 0.4397],â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 162/203 [2m0:09:28 â€¢ 0:02:31[0m [2;4m0.27it/s[0m  
        [0.5543, 0.4457],
        [0.5565, 0.4435],
        [0.5583, 0.4417],
        [0.5566, 0.4434],
        [0.5553, 0.4447],
        [0.5544, 0.4456],
        [0.5596, 0.4404],
        [0.5584, 0.4416],
[2K        [0.5571, 0.4429]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 162/203 [2m0:09:28 â€¢ 0:02:31[0m [2;4m0.27it/s[0m  
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 162/203 [2m0:09:28 â€¢ 0:02:31[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6044, 0.3956]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 162/203 [2m0:09:28 â€¢ 0:02:31[0m [2;4m0.27it/s[0m  
[2KStep 0 - TTA Loss: 0.5241â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 162/203 [2m0:09:28 â€¢ 0:02:31[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.7478, 0.2522]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 162/203 [2m0:09:28 â€¢ 0:02:31[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.8585, 0.1415]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 162/203 [2m0:09:28 â€¢ 0:02:31[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.9308, 0.0692]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 162/203 [2m0:09:28 â€¢ 0:02:31[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.9677, 0.0323]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 162/203 [2m0:09:28 â€¢ 0:02:31[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.9847, 0.0153]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 162/203 [2m0:09:28 â€¢ 0:02:31[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.9918, 0.0082]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 162/203 [2m0:09:28 â€¢ 0:02:31[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.9944, 0.0056]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 162/203 [2m0:09:28 â€¢ 0:02:31[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.9953, 0.0047]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 162/203 [2m0:09:28 â€¢ 0:02:31[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.9954, 0.0046]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 162/203 [2m0:09:28 â€¢ 0:02:31[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.5867, 0.4133]], device='cuda:0')98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 162/203 [2m0:09:28 â€¢ 0:02:31[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[9.9175e-01, 8.2502e-03],[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 162/203 [2m0:09:28 â€¢ 0:02:31[0m [2;4m0.27it/s[0m  
        [9.9988e-01, 1.2090e-04],
        [9.8676e-01, 1.3244e-02],
        [9.9046e-01, 9.5436e-03],
        [9.8955e-01, 1.0450e-02],
        [9.9548e-01, 4.5190e-03],
        [9.9076e-01, 9.2406e-03],
        [9.9557e-01, 4.4274e-03],
        [9.9487e-01, 5.1341e-03],
[2K        [9.8939e-01, 1.0608e-02]], device='cuda:0')[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”[0m 162/203 [2m0:09:28 â€¢ 0:02:31[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.5603, 0.4397],â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”[0m 163/203 [2m0:09:35 â€¢ 0:02:27[0m [2;4m0.27it/s[0m   
        [0.5544, 0.4456],
        [0.5566, 0.4434],
        [0.5584, 0.4416],
        [0.5566, 0.4434],
        [0.5554, 0.4446],
        [0.5545, 0.4455],
        [0.5596, 0.4404],
        [0.5585, 0.4415],
[2K        [0.5572, 0.4428]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m[38;5;237mâ”â”â”â”â”â”â”[0m 163/203 [2m0:09:35 â€¢ 0:02:27[0m [2;4m0.27it/s[0m  
[2KClass label: BaseballPitchâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”[0m 163/203 [2m0:09:35 â€¢ 0:02:27[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6391, 0.3609]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 163/203 [2m0:09:35 â€¢ 0:02:27[0m [2;4m0.27it/s[0m  
[2KStep 0 - TTA Loss: 0.5836â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”[0m 163/203 [2m0:09:35 â€¢ 0:02:27[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6391, 0.3609]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 163/203 [2m0:09:35 â€¢ 0:02:27[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6414, 0.3586]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 163/203 [2m0:09:35 â€¢ 0:02:27[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6527, 0.3473]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 163/203 [2m0:09:35 â€¢ 0:02:27[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.6813, 0.3187]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 163/203 [2m0:09:35 â€¢ 0:02:27[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.7317, 0.2683]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 163/203 [2m0:09:35 â€¢ 0:02:27[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.8012, 0.1988]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 163/203 [2m0:09:35 â€¢ 0:02:27[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.8710, 0.1290]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 163/203 [2m0:09:35 â€¢ 0:02:27[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.9277, 0.0723]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 163/203 [2m0:09:35 â€¢ 0:02:27[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.9599, 0.0401]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 163/203 [2m0:09:35 â€¢ 0:02:27[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.5738, 0.4262]], device='cuda:0');237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”[0m 163/203 [2m0:09:35 â€¢ 0:02:27[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.9710, 0.0290],â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”[0m 163/203 [2m0:09:35 â€¢ 0:02:27[0m [2;4m0.27it/s[0m  
        [0.9198, 0.0802],
        [0.7875, 0.2125],
        [0.8982, 0.1018],
        [0.8820, 0.1180],
        [0.8539, 0.1461],
        [0.8204, 0.1796],
        [0.9216, 0.0784],
        [0.8988, 0.1012],
[2K        [0.8786, 0.1214]], device='cuda:0')â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”[0m 163/203 [2m0:09:35 â€¢ 0:02:27[0m [2;4m0.27it/s[0m  
[2KAttention weights: tensor([[0.5604, 0.4396],â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”[0m 164/203 [2m0:09:37 â€¢ 0:02:14[0m [2;4m0.29it/s[0m  
        [0.5545, 0.4455],
        [0.5566, 0.4434],
        [0.5584, 0.4416],
        [0.5567, 0.4433],
        [0.5555, 0.4445],
        [0.5545, 0.4455],
        [0.5597, 0.4403],
        [0.5585, 0.4415],
[2K        [0.5572, 0.4428]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m[38;5;237mâ”â”â”â”â”â”â”[0m 164/203 [2m0:09:37 â€¢ 0:02:14[0m [2;4m0.29it/s[0m  
[2KClass label: Billiardsâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”[0m 164/203 [2m0:09:37 â€¢ 0:02:14[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.5616, 0.4384]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 164/203 [2m0:09:37 â€¢ 0:02:14[0m [2;4m0.29it/s[0m  
[2KStep 0 - TTA Loss: 0.7342â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”[0m 164/203 [2m0:09:37 â€¢ 0:02:14[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.6417, 0.3583]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 164/203 [2m0:09:37 â€¢ 0:02:14[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.7167, 0.2833]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 164/203 [2m0:09:37 â€¢ 0:02:14[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.7744, 0.2256]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 164/203 [2m0:09:37 â€¢ 0:02:14[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.8118, 0.1882]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 164/203 [2m0:09:37 â€¢ 0:02:14[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.8310, 0.1690]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 164/203 [2m0:09:37 â€¢ 0:02:14[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.8356, 0.1644]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 164/203 [2m0:09:37 â€¢ 0:02:14[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.8326, 0.1674]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 164/203 [2m0:09:37 â€¢ 0:02:14[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.8271, 0.1729]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 164/203 [2m0:09:37 â€¢ 0:02:14[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.8230, 0.1770]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 164/203 [2m0:09:37 â€¢ 0:02:14[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.5621, 0.4379]], device='cuda:0');237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”[0m 164/203 [2m0:09:37 â€¢ 0:02:14[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.8701, 0.1299],â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”[0m 164/203 [2m0:09:37 â€¢ 0:02:14[0m [2;4m0.29it/s[0m  
        [0.8216, 0.1784],
        [0.6959, 0.3041],
        [0.7997, 0.2003],
        [0.7448, 0.2552],
        [0.7454, 0.2546],
        [0.7075, 0.2925],
        [0.8281, 0.1719],
        [0.8003, 0.1997],
[2K        [0.7715, 0.2285]], device='cuda:0')â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”[0m 164/203 [2m0:09:37 â€¢ 0:02:14[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.5603, 0.4397],â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”[0m 165/203 [2m0:09:41 â€¢ 0:01:52[0m [2;4m0.34it/s[0m  
        [0.5545, 0.4455],
        [0.5566, 0.4434],
        [0.5584, 0.4416],
        [0.5567, 0.4433],
        [0.5555, 0.4445],
        [0.5545, 0.4455],
        [0.5597, 0.4403],
        [0.5585, 0.4415],
[2K        [0.5572, 0.4428]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”[0m 165/203 [2m0:09:41 â€¢ 0:01:52[0m [2;4m0.34it/s[0m  
[2KClass label: BaseballPitchâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”[0m 165/203 [2m0:09:41 â€¢ 0:01:52[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6345, 0.3655]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 165/203 [2m0:09:41 â€¢ 0:01:52[0m [2;4m0.34it/s[0m  
[2KStep 0 - TTA Loss: 0.4143â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”[0m 165/203 [2m0:09:41 â€¢ 0:01:52[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6345, 0.3655]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 165/203 [2m0:09:41 â€¢ 0:01:52[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6349, 0.3651]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 165/203 [2m0:09:41 â€¢ 0:01:52[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6361, 0.3639]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 165/203 [2m0:09:41 â€¢ 0:01:52[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6410, 0.3590]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 165/203 [2m0:09:41 â€¢ 0:01:52[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6521, 0.3479]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 165/203 [2m0:09:41 â€¢ 0:01:52[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6700, 0.3300]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 165/203 [2m0:09:41 â€¢ 0:01:52[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6917, 0.3083]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 165/203 [2m0:09:41 â€¢ 0:01:52[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.7170, 0.2830]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 165/203 [2m0:09:41 â€¢ 0:01:52[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.7517, 0.2483]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 165/203 [2m0:09:41 â€¢ 0:01:52[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.5636, 0.4364]], device='cuda:0');98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”[0m 165/203 [2m0:09:41 â€¢ 0:01:52[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.8000, 0.2000],â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”[0m 165/203 [2m0:09:41 â€¢ 0:01:52[0m [2;4m0.34it/s[0m  
        [0.4390, 0.5610],
        [0.5790, 0.4210],
        [0.6930, 0.3070],
        [0.6507, 0.3493],
        [0.6463, 0.3537],
        [0.6036, 0.3964],
        [0.7231, 0.2769],
        [0.6763, 0.3237],
[2K        [0.6842, 0.3158]], device='cuda:0')â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”[0m 165/203 [2m0:09:41 â€¢ 0:01:52[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.5603, 0.4397],â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”[0m 166/203 [2m0:09:44 â€¢ 0:01:50[0m [2;4m0.34it/s[0m  
        [0.5544, 0.4456],
        [0.5566, 0.4434],
        [0.5584, 0.4416],
        [0.5567, 0.4433],
        [0.5555, 0.4445],
        [0.5545, 0.4455],
        [0.5597, 0.4403],
        [0.5585, 0.4415],
[2K        [0.5572, 0.4428]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”[0m 166/203 [2m0:09:44 â€¢ 0:01:50[0m [2;4m0.34it/s[0m  
[2KClass label: TennisSwingâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”[0m 166/203 [2m0:09:44 â€¢ 0:01:50[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6454, 0.3546]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 166/203 [2m0:09:44 â€¢ 0:01:50[0m [2;4m0.34it/s[0m  
[2KStep 0 - TTA Loss: 1.0974â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”[0m 166/203 [2m0:09:44 â€¢ 0:01:50[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.6815, 0.3185]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 166/203 [2m0:09:44 â€¢ 0:01:50[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.7390, 0.2610]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 166/203 [2m0:09:44 â€¢ 0:01:50[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.8024, 0.1976]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 166/203 [2m0:09:44 â€¢ 0:01:50[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.8536, 0.1464]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 166/203 [2m0:09:44 â€¢ 0:01:50[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.8870, 0.1130]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 166/203 [2m0:09:44 â€¢ 0:01:50[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.9056, 0.0944]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 166/203 [2m0:09:44 â€¢ 0:01:50[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.9152, 0.0848]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 166/203 [2m0:09:44 â€¢ 0:01:50[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.9190, 0.0810]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 166/203 [2m0:09:44 â€¢ 0:01:50[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.9226, 0.0774]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 166/203 [2m0:09:44 â€¢ 0:01:50[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.5692, 0.4308]], device='cuda:0');98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”[0m 166/203 [2m0:09:44 â€¢ 0:01:50[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.9273, 0.0727],â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”[0m 166/203 [2m0:09:44 â€¢ 0:01:50[0m [2;4m0.34it/s[0m  
        [0.7054, 0.2946],
        [0.7328, 0.2672],
        [0.8777, 0.1223],
        [0.8380, 0.1620],
        [0.8384, 0.1616],
        [0.7930, 0.2070],
        [0.8764, 0.1236],
        [0.9237, 0.0763],
[2K        [0.8702, 0.1298]], device='cuda:0')â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”[0m 166/203 [2m0:09:44 â€¢ 0:01:50[0m [2;4m0.34it/s[0m  
[2KAttention weights: tensor([[0.5604, 0.4396],â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”[0m 167/203 [2m0:09:46 â€¢ 0:01:44[0m [2;4m0.35it/s[0m  
        [0.5544, 0.4456],
        [0.5566, 0.4434],
        [0.5585, 0.4415],
        [0.5567, 0.4433],
        [0.5555, 0.4445],
        [0.5545, 0.4455],
        [0.5597, 0.4403],
        [0.5586, 0.4414],
[2K        [0.5572, 0.4428]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”[0m 167/203 [2m0:09:46 â€¢ 0:01:44[0m [2;4m0.35it/s[0m  
[2KClass label: BaseballPitchâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”[0m 167/203 [2m0:09:46 â€¢ 0:01:44[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6434, 0.3566]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 167/203 [2m0:09:46 â€¢ 0:01:44[0m [2;4m0.35it/s[0m  
[2KStep 0 - TTA Loss: 0.5676â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”[0m 167/203 [2m0:09:46 â€¢ 0:01:44[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6434, 0.3566]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 167/203 [2m0:09:46 â€¢ 0:01:44[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6469, 0.3531]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 167/203 [2m0:09:46 â€¢ 0:01:44[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6622, 0.3378]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 167/203 [2m0:09:46 â€¢ 0:01:44[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6975, 0.3025]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 167/203 [2m0:09:46 â€¢ 0:01:44[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.7551, 0.2449]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 167/203 [2m0:09:46 â€¢ 0:01:44[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.8256, 0.1744]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 167/203 [2m0:09:46 â€¢ 0:01:44[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.8897, 0.1103]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 167/203 [2m0:09:46 â€¢ 0:01:44[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.9344, 0.0656]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 167/203 [2m0:09:46 â€¢ 0:01:44[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.9617, 0.0383]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 167/203 [2m0:09:46 â€¢ 0:01:44[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5764, 0.4236]], device='cuda:0');98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”[0m 167/203 [2m0:09:46 â€¢ 0:01:44[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.9801, 0.0199],â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”[0m 167/203 [2m0:09:46 â€¢ 0:01:44[0m [2;4m0.35it/s[0m  
        [0.8300, 0.1700],
        [0.8028, 0.1972],
        [0.9232, 0.0768],
        [0.9162, 0.0838],
        [0.8980, 0.1020],
        [0.8616, 0.1384],
        [0.9362, 0.0638],
        [0.9462, 0.0538],
[2K        [0.9196, 0.0804]], device='cuda:0')â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”[0m 167/203 [2m0:09:46 â€¢ 0:01:44[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5604, 0.4396],â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”[0m 168/203 [2m0:09:49 â€¢ 0:01:38[0m [2;4m0.36it/s[0m   
        [0.5545, 0.4455],
        [0.5566, 0.4434],
        [0.5585, 0.4415],
        [0.5567, 0.4433],
        [0.5555, 0.4445],
        [0.5545, 0.4455],
        [0.5597, 0.4403],
        [0.5586, 0.4414],
[2K        [0.5573, 0.4427]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0m[38;5;237mâ”â”â”â”â”â”[0m 168/203 [2m0:09:49 â€¢ 0:01:38[0m [2;4m0.36it/s[0m  
[2KClass label: SoccerPenaltyâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”[0m 168/203 [2m0:09:49 â€¢ 0:01:38[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.6680, 0.3320]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 168/203 [2m0:09:49 â€¢ 0:01:38[0m [2;4m0.36it/s[0m  
[2KStep 0 - TTA Loss: 0.8345â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”[0m 168/203 [2m0:09:49 â€¢ 0:01:38[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.7867, 0.2133]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 168/203 [2m0:09:49 â€¢ 0:01:38[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.8929, 0.1071]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 168/203 [2m0:09:49 â€¢ 0:01:38[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.9592, 0.0408]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 168/203 [2m0:09:49 â€¢ 0:01:38[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.9836, 0.0164]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 168/203 [2m0:09:49 â€¢ 0:01:38[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.9928, 0.0072]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 168/203 [2m0:09:49 â€¢ 0:01:38[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.9958, 0.0042]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 168/203 [2m0:09:49 â€¢ 0:01:38[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.9971, 0.0029]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 168/203 [2m0:09:49 â€¢ 0:01:38[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.9976, 0.0024]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 168/203 [2m0:09:49 â€¢ 0:01:38[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.9978, 0.0022]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 168/203 [2m0:09:49 â€¢ 0:01:38[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.5872, 0.4128]], device='cuda:0')5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”[0m 168/203 [2m0:09:49 â€¢ 0:01:38[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.9949, 0.0051],â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”[0m 168/203 [2m0:09:49 â€¢ 0:01:38[0m [2;4m0.36it/s[0m  
        [0.9394, 0.0606],
        [0.8995, 0.1005],
        [0.9660, 0.0340],
        [0.9754, 0.0246],
        [0.9757, 0.0243],
        [0.9473, 0.0527],
        [0.9979, 0.0021],
        [0.9800, 0.0200],
[2K        [0.9643, 0.0357]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”[0m 168/203 [2m0:09:49 â€¢ 0:01:38[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.5605, 0.4395],â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”[0m 169/203 [2m0:09:51 â€¢ 0:01:31[0m [2;4m0.38it/s[0m  
        [0.5545, 0.4455],
        [0.5567, 0.4433],
        [0.5585, 0.4415],
        [0.5567, 0.4433],
        [0.5556, 0.4444],
        [0.5546, 0.4454],
        [0.5597, 0.4403],
        [0.5586, 0.4414],
[2K        [0.5573, 0.4427]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0m[38;5;237mâ”â”â”â”â”â”[0m 169/203 [2m0:09:51 â€¢ 0:01:31[0m [2;4m0.38it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”[0m 169/203 [2m0:09:51 â€¢ 0:01:31[0m [2;4m0.38it/s[0m  
[2KAttention weights: tensor([[0.6303, 0.3697]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 169/203 [2m0:09:51 â€¢ 0:01:31[0m [2;4m0.38it/s[0m  
[2KStep 0 - TTA Loss: 0.4056â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”[0m 169/203 [2m0:09:51 â€¢ 0:01:31[0m [2;4m0.38it/s[0m  
[2KAttention weights: tensor([[0.6303, 0.3697]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 169/203 [2m0:09:51 â€¢ 0:01:31[0m [2;4m0.38it/s[0m  
[2KAttention weights: tensor([[0.6331, 0.3669]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 169/203 [2m0:09:51 â€¢ 0:01:31[0m [2;4m0.38it/s[0m  
[2KAttention weights: tensor([[0.6449, 0.3551]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 169/203 [2m0:09:51 â€¢ 0:01:31[0m [2;4m0.38it/s[0m  
[2KAttention weights: tensor([[0.6724, 0.3276]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 169/203 [2m0:09:51 â€¢ 0:01:31[0m [2;4m0.38it/s[0m  
[2KAttention weights: tensor([[0.7175, 0.2825]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 169/203 [2m0:09:51 â€¢ 0:01:31[0m [2;4m0.38it/s[0m  
[2KAttention weights: tensor([[0.7773, 0.2227]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 169/203 [2m0:09:51 â€¢ 0:01:31[0m [2;4m0.38it/s[0m  
[2KAttention weights: tensor([[0.8409, 0.1591]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 169/203 [2m0:09:51 â€¢ 0:01:31[0m [2;4m0.38it/s[0m  
[2KAttention weights: tensor([[0.9042, 0.0958]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 169/203 [2m0:09:51 â€¢ 0:01:31[0m [2;4m0.38it/s[0m  
[2KAttention weights: tensor([[0.9434, 0.0566]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 169/203 [2m0:09:51 â€¢ 0:01:31[0m [2;4m0.38it/s[0m  
[2KAttention weights: tensor([[0.5726, 0.4274]], device='cuda:0')5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”[0m 169/203 [2m0:09:51 â€¢ 0:01:31[0m [2;4m0.38it/s[0m  
[2KAttention weights: tensor([[0.9625, 0.0375],â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”[0m 169/203 [2m0:09:51 â€¢ 0:01:31[0m [2;4m0.38it/s[0m  
        [0.8650, 0.1350],
        [0.8722, 0.1278],
        [0.9463, 0.0537],
        [0.9458, 0.0542],
        [0.9473, 0.0527],
        [0.9294, 0.0706],
        [0.9694, 0.0306],
        [0.9536, 0.0464],
[2K        [0.9659, 0.0341]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”[0m 169/203 [2m0:09:51 â€¢ 0:01:31[0m [2;4m0.38it/s[0m  
[2KAttention weights: tensor([[0.5605, 0.4395],â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”[0m 170/203 [2m0:09:55 â€¢ 0:01:47[0m [2;4m0.31it/s[0m  
        [0.5545, 0.4455],
        [0.5567, 0.4433],
        [0.5585, 0.4415],
        [0.5568, 0.4432],
        [0.5556, 0.4444],
        [0.5546, 0.4454],
        [0.5597, 0.4403],
        [0.5586, 0.4414],
[2K        [0.5573, 0.4427]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0m[38;5;237mâ”â”â”â”â”â”[0m 170/203 [2m0:09:55 â€¢ 0:01:47[0m [2;4m0.31it/s[0m  
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”[0m 170/203 [2m0:09:55 â€¢ 0:01:47[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.6103, 0.3897]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 170/203 [2m0:09:55 â€¢ 0:01:47[0m [2;4m0.31it/s[0m  
[2KStep 0 - TTA Loss: 0.7500â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”[0m 170/203 [2m0:09:55 â€¢ 0:01:47[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.7246, 0.2754]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 170/203 [2m0:09:55 â€¢ 0:01:47[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.8224, 0.1776]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 170/203 [2m0:09:55 â€¢ 0:01:47[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.8940, 0.1060]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 170/203 [2m0:09:55 â€¢ 0:01:47[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.9369, 0.0631]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 170/203 [2m0:09:55 â€¢ 0:01:47[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.9600, 0.0400]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 170/203 [2m0:09:55 â€¢ 0:01:47[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.9708, 0.0292]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 170/203 [2m0:09:55 â€¢ 0:01:47[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.9755, 0.0245]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 170/203 [2m0:09:55 â€¢ 0:01:47[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.9785, 0.0215]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 170/203 [2m0:09:55 â€¢ 0:01:47[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.9800, 0.0200]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 170/203 [2m0:09:55 â€¢ 0:01:47[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5749, 0.4251]], device='cuda:0')5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”[0m 170/203 [2m0:09:55 â€¢ 0:01:47[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.9714, 0.0286],â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”[0m 170/203 [2m0:09:55 â€¢ 0:01:47[0m [2;4m0.31it/s[0m  
        [0.8958, 0.1042],
        [0.9279, 0.0721],
        [0.9583, 0.0417],
        [0.9700, 0.0300],
        [0.9805, 0.0195],
        [0.9659, 0.0341],
        [0.9786, 0.0214],
        [0.9688, 0.0312],
[2K        [0.9781, 0.0219]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”[0m 170/203 [2m0:09:55 â€¢ 0:01:47[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5605, 0.4395],â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”[0m 171/203 [2m0:09:59 â€¢ 0:01:36[0m [2;4m0.33it/s[0m  
        [0.5545, 0.4455],
        [0.5567, 0.4433],
        [0.5585, 0.4415],
        [0.5568, 0.4432],
        [0.5556, 0.4444],
        [0.5546, 0.4454],
        [0.5597, 0.4403],
        [0.5586, 0.4414],
[2K        [0.5573, 0.4427]], device='cuda:0', grad_fn=<SoftmaxBackward0>)4mâ•¸[0m[38;5;237mâ”â”â”â”â”â”[0m 171/203 [2m0:09:59 â€¢ 0:01:36[0m [2;4m0.33it/s[0m  
[2KClass label: BaseballPitchâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”[0m 171/203 [2m0:09:59 â€¢ 0:01:36[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.6401, 0.3599]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 171/203 [2m0:09:59 â€¢ 0:01:36[0m [2;4m0.33it/s[0m  
[2KStep 0 - TTA Loss: 0.9880â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”[0m 171/203 [2m0:09:59 â€¢ 0:01:36[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.6401, 0.3599]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 171/203 [2m0:09:59 â€¢ 0:01:36[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.6439, 0.3561]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 171/203 [2m0:09:59 â€¢ 0:01:36[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.6599, 0.3401]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 171/203 [2m0:09:59 â€¢ 0:01:36[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.6959, 0.3041]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 171/203 [2m0:09:59 â€¢ 0:01:36[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.7519, 0.2481]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 171/203 [2m0:09:59 â€¢ 0:01:36[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.8261, 0.1739]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 171/203 [2m0:09:59 â€¢ 0:01:36[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.9016, 0.0984]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 171/203 [2m0:09:59 â€¢ 0:01:36[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.9541, 0.0459]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 171/203 [2m0:09:59 â€¢ 0:01:36[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.9851, 0.0149]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 171/203 [2m0:09:59 â€¢ 0:01:36[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.5796, 0.4204]], device='cuda:0')2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”[0m 171/203 [2m0:09:59 â€¢ 0:01:36[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.9913, 0.0087],â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”[0m 171/203 [2m0:09:59 â€¢ 0:01:36[0m [2;4m0.33it/s[0m  
        [0.8898, 0.1102],
        [0.9014, 0.0986],
        [0.9510, 0.0490],
        [0.9687, 0.0313],
        [0.9690, 0.0310],
        [0.9466, 0.0534],
        [0.9743, 0.0257],
        [0.9589, 0.0411],
[2K        [0.9635, 0.0365]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”[0m 171/203 [2m0:09:59 â€¢ 0:01:36[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.5605, 0.4395],â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”[0m 172/203 [2m0:10:00 â€¢ 0:01:28[0m [2;4m0.36it/s[0m  
        [0.5545, 0.4455],
        [0.5567, 0.4433],
        [0.5585, 0.4415],
        [0.5568, 0.4432],
        [0.5556, 0.4444],
        [0.5546, 0.4454],
        [0.5597, 0.4403],
        [0.5586, 0.4414],
[2K        [0.5573, 0.4427]], device='cuda:0', grad_fn=<SoftmaxBackward0>)4mâ•¸[0m[38;5;237mâ”â”â”â”â”â”[0m 172/203 [2m0:10:00 â€¢ 0:01:28[0m [2;4m0.36it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”[0m 172/203 [2m0:10:00 â€¢ 0:01:28[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.5636, 0.4364]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 172/203 [2m0:10:00 â€¢ 0:01:28[0m [2;4m0.36it/s[0m  
[2KStep 0 - TTA Loss: 0.7564â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”[0m 172/203 [2m0:10:00 â€¢ 0:01:28[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.6756, 0.3244]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 172/203 [2m0:10:00 â€¢ 0:01:28[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.7932, 0.2068]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 172/203 [2m0:10:00 â€¢ 0:01:28[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.8933, 0.1067]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 172/203 [2m0:10:00 â€¢ 0:01:28[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.9559, 0.0441]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 172/203 [2m0:10:00 â€¢ 0:01:28[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.9865, 0.0135]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 172/203 [2m0:10:00 â€¢ 0:01:28[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.9956, 0.0044]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 172/203 [2m0:10:00 â€¢ 0:01:28[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.9979, 0.0021]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 172/203 [2m0:10:00 â€¢ 0:01:28[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.9986, 0.0014]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 172/203 [2m0:10:00 â€¢ 0:01:28[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.9989, 0.0011]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 172/203 [2m0:10:00 â€¢ 0:01:28[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.5943, 0.4057]], device='cuda:0')2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”[0m 172/203 [2m0:10:00 â€¢ 0:01:28[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.9962, 0.0038],â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”[0m 172/203 [2m0:10:00 â€¢ 0:01:28[0m [2;4m0.36it/s[0m  
        [0.9710, 0.0290],
        [0.9888, 0.0112],
        [0.9890, 0.0110],
        [0.9968, 0.0032],
        [0.9984, 0.0016],
        [0.9989, 0.0011],
        [0.9935, 0.0065],
        [0.9932, 0.0068],
[2K        [0.9973, 0.0027]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”[0m 172/203 [2m0:10:00 â€¢ 0:01:28[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.5604, 0.4396],â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”[0m 173/203 [2m0:10:06 â€¢ 0:01:37[0m [2;4m0.31it/s[0m   
        [0.5545, 0.4455],
        [0.5567, 0.4433],
        [0.5585, 0.4415],
        [0.5568, 0.4432],
        [0.5556, 0.4444],
        [0.5547, 0.4453],
        [0.5597, 0.4403],
        [0.5586, 0.4414],
[2K        [0.5573, 0.4427]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m[38;5;237mâ”â”â”â”â”[0m 173/203 [2m0:10:06 â€¢ 0:01:37[0m [2;4m0.31it/s[0m  
[2KClass label: HammerThrowâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”[0m 173/203 [2m0:10:06 â€¢ 0:01:37[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5616, 0.4384]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 173/203 [2m0:10:06 â€¢ 0:01:37[0m [2;4m0.31it/s[0m  
[2KStep 0 - TTA Loss: 0.5337â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”[0m 173/203 [2m0:10:06 â€¢ 0:01:37[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5616, 0.4384]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 173/203 [2m0:10:06 â€¢ 0:01:37[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5700, 0.4300]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 173/203 [2m0:10:06 â€¢ 0:01:37[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.6024, 0.3976]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 173/203 [2m0:10:06 â€¢ 0:01:37[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.6695, 0.3305]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 173/203 [2m0:10:06 â€¢ 0:01:37[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.7709, 0.2291]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 173/203 [2m0:10:06 â€¢ 0:01:37[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.8818, 0.1182]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 173/203 [2m0:10:06 â€¢ 0:01:37[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.9624, 0.0376]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 173/203 [2m0:10:06 â€¢ 0:01:37[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.9941, 0.0059]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 173/203 [2m0:10:06 â€¢ 0:01:37[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[9.9916e-01, 8.3692e-04]], device='cuda:0', grad_fn=<SoftmaxBackward0>)/203 [2m0:10:06 â€¢ 0:01:37[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5976, 0.4024]], device='cuda:0');5;237mâ•º[0m[38;5;237mâ”â”â”â”â”[0m 173/203 [2m0:10:06 â€¢ 0:01:37[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[9.9908e-01, 9.1597e-04],â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”[0m 173/203 [2m0:10:06 â€¢ 0:01:37[0m [2;4m0.31it/s[0m  
        [9.8294e-01, 1.7062e-02],
        [9.9736e-01, 2.6367e-03],
        [9.9512e-01, 4.8782e-03],
        [9.9983e-01, 1.7287e-04],
        [9.9942e-01, 5.8221e-04],
        [9.9942e-01, 5.8222e-04],
        [9.9758e-01, 2.4238e-03],
        [9.9716e-01, 2.8423e-03],
[2K        [9.9926e-01, 7.3616e-04]], device='cuda:0')â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”[0m 173/203 [2m0:10:06 â€¢ 0:01:37[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5605, 0.4395],â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”[0m 174/203 [2m0:10:08 â€¢ 0:01:28[0m [2;4m0.33it/s[0m  
        [0.5546, 0.4454],
        [0.5568, 0.4432],
        [0.5586, 0.4414],
        [0.5569, 0.4431],
        [0.5557, 0.4443],
        [0.5548, 0.4452],
        [0.5598, 0.4402],
        [0.5587, 0.4413],
[2K        [0.5574, 0.4426]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m[38;5;237mâ”â”â”â”â”[0m 174/203 [2m0:10:08 â€¢ 0:01:28[0m [2;4m0.33it/s[0m  
[2KClass label: TennisSwingâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”[0m 174/203 [2m0:10:08 â€¢ 0:01:28[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.6426, 0.3574]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 174/203 [2m0:10:08 â€¢ 0:01:28[0m [2;4m0.33it/s[0m  
[2KStep 0 - TTA Loss: 0.7763â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”[0m 174/203 [2m0:10:08 â€¢ 0:01:28[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.8097, 0.1903]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 174/203 [2m0:10:08 â€¢ 0:01:28[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.8988, 0.1012]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 174/203 [2m0:10:08 â€¢ 0:01:28[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.9395, 0.0605]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 174/203 [2m0:10:08 â€¢ 0:01:28[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.9547, 0.0453]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 174/203 [2m0:10:08 â€¢ 0:01:28[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.9549, 0.0451]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 174/203 [2m0:10:08 â€¢ 0:01:28[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.9480, 0.0520]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 174/203 [2m0:10:08 â€¢ 0:01:28[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.9364, 0.0636]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 174/203 [2m0:10:08 â€¢ 0:01:28[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.9249, 0.0751]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 174/203 [2m0:10:08 â€¢ 0:01:28[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.9173, 0.0827]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 174/203 [2m0:10:08 â€¢ 0:01:28[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.5702, 0.4298]], device='cuda:0');5;237mâ•º[0m[38;5;237mâ”â”â”â”â”[0m 174/203 [2m0:10:08 â€¢ 0:01:28[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.9843, 0.0157],â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”[0m 174/203 [2m0:10:08 â€¢ 0:01:28[0m [2;4m0.33it/s[0m  
        [0.8802, 0.1198],
        [0.9745, 0.0255],
        [0.9398, 0.0602],
        [0.9952, 0.0048],
        [0.9870, 0.0130],
        [0.9877, 0.0123],
        [0.9714, 0.0286],
        [0.9148, 0.0852],
[2K        [0.9838, 0.0162]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”[0m 174/203 [2m0:10:08 â€¢ 0:01:28[0m [2;4m0.33it/s[0m  
[2KAttention weights: tensor([[0.5605, 0.4395],â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”[0m 175/203 [2m0:10:13 â€¢ 0:01:32[0m [2;4m0.31it/s[0m  
        [0.5546, 0.4454],
        [0.5568, 0.4432],
        [0.5586, 0.4414],
        [0.5570, 0.4430],
        [0.5558, 0.4442],
        [0.5549, 0.4451],
        [0.5598, 0.4402],
        [0.5587, 0.4413],
[2K        [0.5575, 0.4425]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m[38;5;237mâ”â”â”â”â”[0m 175/203 [2m0:10:13 â€¢ 0:01:32[0m [2;4m0.31it/s[0m  
[2KClass label: BaseballPitchâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”[0m 175/203 [2m0:10:13 â€¢ 0:01:32[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.6387, 0.3613]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 175/203 [2m0:10:13 â€¢ 0:01:32[0m [2;4m0.31it/s[0m  
[2KStep 0 - TTA Loss: 0.8170â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”[0m 175/203 [2m0:10:13 â€¢ 0:01:32[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.6387, 0.3613]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 175/203 [2m0:10:13 â€¢ 0:01:32[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.6394, 0.3606]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 175/203 [2m0:10:13 â€¢ 0:01:32[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.6476, 0.3524]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 175/203 [2m0:10:13 â€¢ 0:01:32[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.6756, 0.3244]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 175/203 [2m0:10:13 â€¢ 0:01:32[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.7351, 0.2649]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 175/203 [2m0:10:13 â€¢ 0:01:32[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.8228, 0.1772]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 175/203 [2m0:10:13 â€¢ 0:01:32[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.9079, 0.0921]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 175/203 [2m0:10:13 â€¢ 0:01:32[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.9563, 0.0437]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 175/203 [2m0:10:13 â€¢ 0:01:32[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.9759, 0.0241]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 175/203 [2m0:10:13 â€¢ 0:01:32[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5776, 0.4224]], device='cuda:0');5;237mâ•º[0m[38;5;237mâ”â”â”â”â”[0m 175/203 [2m0:10:13 â€¢ 0:01:32[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.9897, 0.0103],â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”[0m 175/203 [2m0:10:13 â€¢ 0:01:32[0m [2;4m0.31it/s[0m  
        [0.6855, 0.3145],
        [0.7660, 0.2340],
        [0.7945, 0.2055],
        [0.9247, 0.0753],
        [0.8329, 0.1671],
        [0.8130, 0.1870],
        [0.9136, 0.0864],
        [0.5243, 0.4757],
[2K        [0.8377, 0.1623]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”[0m 175/203 [2m0:10:13 â€¢ 0:01:32[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5605, 0.4395],â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”[0m 176/203 [2m0:10:15 â€¢ 0:01:29[0m [2;4m0.31it/s[0m  
        [0.5546, 0.4454],
        [0.5568, 0.4432],
        [0.5586, 0.4414],
        [0.5570, 0.4430],
        [0.5558, 0.4442],
        [0.5549, 0.4451],
        [0.5598, 0.4402],
        [0.5586, 0.4414],
[2K        [0.5575, 0.4425]], device='cuda:0', grad_fn=<SoftmaxBackward0>)24mâ•¸[0m[38;5;237mâ”â”â”â”â”[0m 176/203 [2m0:10:15 â€¢ 0:01:29[0m [2;4m0.31it/s[0m  
[2KClass label: BaseballPitchâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”[0m 176/203 [2m0:10:15 â€¢ 0:01:29[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.6402, 0.3598]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 176/203 [2m0:10:15 â€¢ 0:01:29[0m [2;4m0.31it/s[0m  
[2KStep 0 - TTA Loss: 0.6985â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”[0m 176/203 [2m0:10:15 â€¢ 0:01:29[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.8312, 0.1688]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 176/203 [2m0:10:15 â€¢ 0:01:29[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.9440, 0.0560]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 176/203 [2m0:10:15 â€¢ 0:01:29[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.9832, 0.0168]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 176/203 [2m0:10:15 â€¢ 0:01:29[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.9958, 0.0042]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 176/203 [2m0:10:15 â€¢ 0:01:29[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.9983, 0.0017]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 176/203 [2m0:10:15 â€¢ 0:01:29[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.9990, 0.0010]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 176/203 [2m0:10:15 â€¢ 0:01:29[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[9.9916e-01, 8.4300e-04]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 176/203 [2m0:10:15 â€¢ 0:01:29[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[9.9921e-01, 7.8570e-04]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 176/203 [2m0:10:15 â€¢ 0:01:29[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[9.9923e-01, 7.6910e-04]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 176/203 [2m0:10:15 â€¢ 0:01:29[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5894, 0.4106]], device='cuda:0');2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”[0m 176/203 [2m0:10:15 â€¢ 0:01:29[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[9.9924e-01, 7.6500e-04],â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”[0m 176/203 [2m0:10:15 â€¢ 0:01:29[0m [2;4m0.31it/s[0m  
        [8.7638e-01, 1.2362e-01],
        [8.7914e-01, 1.2086e-01],
        [9.4778e-01, 5.2223e-02],
        [9.8306e-01, 1.6944e-02],
        [9.4860e-01, 5.1396e-02],
        [9.3031e-01, 6.9689e-02],
        [9.8079e-01, 1.9207e-02],
        [8.8933e-01, 1.1067e-01],
[2K        [9.5732e-01, 4.2682e-02]], device='cuda:0')â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”[0m 176/203 [2m0:10:15 â€¢ 0:01:29[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5605, 0.4395],â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”[0m 177/203 [2m0:10:19 â€¢ 0:01:26[0m [2;4m0.31it/s[0m  
        [0.5546, 0.4454],
        [0.5568, 0.4432],
        [0.5585, 0.4415],
        [0.5570, 0.4430],
        [0.5558, 0.4442],
        [0.5549, 0.4451],
        [0.5598, 0.4402],
        [0.5586, 0.4414],
[2K        [0.5575, 0.4425]], device='cuda:0', grad_fn=<SoftmaxBackward0>)24mâ•¸[0m[38;5;237mâ”â”â”â”â”[0m 177/203 [2m0:10:19 â€¢ 0:01:26[0m [2;4m0.31it/s[0m  
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”[0m 177/203 [2m0:10:19 â€¢ 0:01:26[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.6064, 0.3936]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 177/203 [2m0:10:19 â€¢ 0:01:26[0m [2;4m0.31it/s[0m  
[2KStep 0 - TTA Loss: 0.6345â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”[0m 177/203 [2m0:10:19 â€¢ 0:01:26[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.6064, 0.3936]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 177/203 [2m0:10:19 â€¢ 0:01:26[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.6080, 0.3920]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 177/203 [2m0:10:19 â€¢ 0:01:26[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.6167, 0.3833]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 177/203 [2m0:10:19 â€¢ 0:01:26[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.6402, 0.3598]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 177/203 [2m0:10:19 â€¢ 0:01:26[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.6865, 0.3135]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 177/203 [2m0:10:19 â€¢ 0:01:26[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.7549, 0.2451]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 177/203 [2m0:10:19 â€¢ 0:01:26[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.8334, 0.1666]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 177/203 [2m0:10:19 â€¢ 0:01:26[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.9054, 0.0946]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 177/203 [2m0:10:19 â€¢ 0:01:26[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.9514, 0.0486]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 177/203 [2m0:10:19 â€¢ 0:01:26[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5730, 0.4270]], device='cuda:0');2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”[0m 177/203 [2m0:10:19 â€¢ 0:01:26[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.9413, 0.0587],â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”[0m 177/203 [2m0:10:19 â€¢ 0:01:26[0m [2;4m0.31it/s[0m  
        [0.8415, 0.1585],
        [0.9127, 0.0873],
        [0.9055, 0.0945],
        [0.9463, 0.0537],
        [0.9755, 0.0245],
        [0.9498, 0.0502],
        [0.9553, 0.0447],
        [0.9237, 0.0763],
[2K        [0.9434, 0.0566]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”[0m 177/203 [2m0:10:19 â€¢ 0:01:26[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5605, 0.4395],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”[0m 178/203 [2m0:10:23 â€¢ 0:01:26[0m [2;4m0.29it/s[0m   
        [0.5545, 0.4455],
        [0.5568, 0.4432],
        [0.5585, 0.4415],
        [0.5570, 0.4430],
        [0.5558, 0.4442],
        [0.5549, 0.4451],
        [0.5598, 0.4402],
        [0.5586, 0.4414],
[2K        [0.5575, 0.4425]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m[38;5;237mâ”â”â”â”[0m 178/203 [2m0:10:23 â€¢ 0:01:26[0m [2;4m0.29it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”[0m 178/203 [2m0:10:23 â€¢ 0:01:26[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.6255, 0.3745]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 178/203 [2m0:10:23 â€¢ 0:01:26[0m [2;4m0.29it/s[0m  
[2KStep 0 - TTA Loss: 0.4150â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”[0m 178/203 [2m0:10:23 â€¢ 0:01:26[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.7399, 0.2601]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 178/203 [2m0:10:23 â€¢ 0:01:26[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.8366, 0.1634]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 178/203 [2m0:10:23 â€¢ 0:01:26[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.9096, 0.0904]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 178/203 [2m0:10:23 â€¢ 0:01:26[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.9536, 0.0464]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 178/203 [2m0:10:23 â€¢ 0:01:26[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.9731, 0.0269]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 178/203 [2m0:10:23 â€¢ 0:01:26[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.9816, 0.0184]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 178/203 [2m0:10:23 â€¢ 0:01:26[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.9857, 0.0143]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 178/203 [2m0:10:23 â€¢ 0:01:26[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.9876, 0.0124]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 178/203 [2m0:10:23 â€¢ 0:01:26[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.9884, 0.0116]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 178/203 [2m0:10:23 â€¢ 0:01:26[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.5807, 0.4193]], device='cuda:0')8;5;237mâ•º[0m[38;5;237mâ”â”â”â”[0m 178/203 [2m0:10:23 â€¢ 0:01:26[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.9770, 0.0230],â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”[0m 178/203 [2m0:10:23 â€¢ 0:01:26[0m [2;4m0.29it/s[0m  
        [0.9089, 0.0911],
        [0.9566, 0.0434],
        [0.9694, 0.0306],
        [0.9829, 0.0171],
        [0.9905, 0.0095],
        [0.9826, 0.0174],
        [0.9749, 0.0251],
        [0.9756, 0.0244],
[2K        [0.9886, 0.0114]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”[0m 178/203 [2m0:10:23 â€¢ 0:01:26[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.5604, 0.4396],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”[0m 179/203 [2m0:10:36 â€¢ 0:02:17[0m [2;4m0.18it/s[0m  
        [0.5545, 0.4455],
        [0.5568, 0.4432],
        [0.5585, 0.4415],
        [0.5570, 0.4430],
        [0.5558, 0.4442],
        [0.5549, 0.4451],
        [0.5597, 0.4403],
        [0.5585, 0.4415],
[2K        [0.5575, 0.4425]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m[38;5;237mâ”â”â”â”[0m 179/203 [2m0:10:36 â€¢ 0:02:17[0m [2;4m0.18it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”[0m 179/203 [2m0:10:36 â€¢ 0:02:17[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.5631, 0.4369]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 179/203 [2m0:10:36 â€¢ 0:02:17[0m [2;4m0.18it/s[0m  
[2KStep 0 - TTA Loss: 0.4575â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”[0m 179/203 [2m0:10:36 â€¢ 0:02:17[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.5631, 0.4369]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 179/203 [2m0:10:36 â€¢ 0:02:17[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.5668, 0.4332]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 179/203 [2m0:10:36 â€¢ 0:02:17[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.5811, 0.4189]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 179/203 [2m0:10:36 â€¢ 0:02:17[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.6120, 0.3880]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 179/203 [2m0:10:36 â€¢ 0:02:17[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.6626, 0.3374]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 179/203 [2m0:10:36 â€¢ 0:02:17[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.7303, 0.2697]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 179/203 [2m0:10:36 â€¢ 0:02:17[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.8096, 0.1904]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 179/203 [2m0:10:36 â€¢ 0:02:17[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.8758, 0.1242]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 179/203 [2m0:10:36 â€¢ 0:02:17[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.9224, 0.0776]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 179/203 [2m0:10:36 â€¢ 0:02:17[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.5711, 0.4289]], device='cuda:0')8;5;237mâ•º[0m[38;5;237mâ”â”â”â”[0m 179/203 [2m0:10:36 â€¢ 0:02:17[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.9222, 0.0778],â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”[0m 179/203 [2m0:10:36 â€¢ 0:02:17[0m [2;4m0.18it/s[0m  
        [0.8312, 0.1688],
        [0.8905, 0.1095],
        [0.9164, 0.0836],
        [0.9316, 0.0684],
        [0.9521, 0.0479],
        [0.9532, 0.0468],
        [0.9178, 0.0822],
        [0.9267, 0.0733],
[2K        [0.9553, 0.0447]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”[0m 179/203 [2m0:10:36 â€¢ 0:02:17[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.5604, 0.4396],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”[0m 180/203 [2m0:10:42 â€¢ 0:02:14[0m [2;4m0.17it/s[0m  
        [0.5545, 0.4455],
        [0.5568, 0.4432],
        [0.5585, 0.4415],
        [0.5570, 0.4430],
        [0.5558, 0.4442],
        [0.5549, 0.4451],
        [0.5597, 0.4403],
        [0.5586, 0.4414],
[2K        [0.5575, 0.4425]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m[38;5;237mâ”â”â”â”[0m 180/203 [2m0:10:42 â€¢ 0:02:14[0m [2;4m0.17it/s[0m  
[2KClass label: CleanAndJerkâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”[0m 180/203 [2m0:10:42 â€¢ 0:02:14[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.5697, 0.4303]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 180/203 [2m0:10:42 â€¢ 0:02:14[0m [2;4m0.17it/s[0m  
[2KStep 0 - TTA Loss: 0.4062â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”[0m 180/203 [2m0:10:42 â€¢ 0:02:14[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.6682, 0.3318]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 180/203 [2m0:10:42 â€¢ 0:02:14[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.7562, 0.2438]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 180/203 [2m0:10:42 â€¢ 0:02:14[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.8174, 0.1826]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 180/203 [2m0:10:42 â€¢ 0:02:14[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.8560, 0.1440]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 180/203 [2m0:10:42 â€¢ 0:02:14[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.8861, 0.1139]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 180/203 [2m0:10:42 â€¢ 0:02:14[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.9135, 0.0865]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 180/203 [2m0:10:42 â€¢ 0:02:14[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.9352, 0.0648]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 180/203 [2m0:10:42 â€¢ 0:02:14[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.9489, 0.0511]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 180/203 [2m0:10:42 â€¢ 0:02:14[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.9555, 0.0445]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 180/203 [2m0:10:42 â€¢ 0:02:14[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.5732, 0.4268]], device='cuda:0')8;5;237mâ•º[0m[38;5;237mâ”â”â”â”[0m 180/203 [2m0:10:42 â€¢ 0:02:14[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.9153, 0.0847],â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”[0m 180/203 [2m0:10:42 â€¢ 0:02:14[0m [2;4m0.17it/s[0m  
        [0.8487, 0.1513],
        [0.9573, 0.0427],
        [0.9121, 0.0879],
        [0.9371, 0.0629],
        [0.9520, 0.0480],
        [0.9506, 0.0494],
        [0.9142, 0.0858],
        [0.9218, 0.0782],
[2K        [0.9491, 0.0509]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”[0m 180/203 [2m0:10:42 â€¢ 0:02:14[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.5604, 0.4396],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”[0m 181/203 [2m0:10:48 â€¢ 0:02:43[0m [2;4m0.14it/s[0m  
        [0.5545, 0.4455],
        [0.5568, 0.4432],
        [0.5585, 0.4415],
        [0.5570, 0.4430],
        [0.5558, 0.4442],
        [0.5549, 0.4451],
        [0.5598, 0.4402],
        [0.5586, 0.4414],
[2K        [0.5575, 0.4425]], device='cuda:0', grad_fn=<SoftmaxBackward0>)224mâ•¸[0m[38;5;237mâ”â”â”â”[0m 181/203 [2m0:10:48 â€¢ 0:02:43[0m [2;4m0.14it/s[0m  
[2KClass label: GolfSwingâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”[0m 181/203 [2m0:10:48 â€¢ 0:02:43[0m [2;4m0.14it/s[0m  
[2KAttention weights: tensor([[0.6500, 0.3500]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 181/203 [2m0:10:48 â€¢ 0:02:43[0m [2;4m0.14it/s[0m  
[2KStep 0 - TTA Loss: 0.3376â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”[0m 181/203 [2m0:10:48 â€¢ 0:02:43[0m [2;4m0.14it/s[0m  
[2KAttention weights: tensor([[0.6500, 0.3500]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 181/203 [2m0:10:48 â€¢ 0:02:43[0m [2;4m0.14it/s[0m  
[2KAttention weights: tensor([[0.6537, 0.3463]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 181/203 [2m0:10:48 â€¢ 0:02:43[0m [2;4m0.14it/s[0m  
[2KAttention weights: tensor([[0.6688, 0.3312]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 181/203 [2m0:10:48 â€¢ 0:02:43[0m [2;4m0.14it/s[0m  
[2KAttention weights: tensor([[0.7017, 0.2983]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 181/203 [2m0:10:48 â€¢ 0:02:43[0m [2;4m0.14it/s[0m  
[2KAttention weights: tensor([[0.7544, 0.2456]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 181/203 [2m0:10:48 â€¢ 0:02:43[0m [2;4m0.14it/s[0m  
[2KAttention weights: tensor([[0.8197, 0.1803]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 181/203 [2m0:10:48 â€¢ 0:02:43[0m [2;4m0.14it/s[0m  
[2KAttention weights: tensor([[0.8823, 0.1177]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 181/203 [2m0:10:48 â€¢ 0:02:43[0m [2;4m0.14it/s[0m  
[2KAttention weights: tensor([[0.9284, 0.0716]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 181/203 [2m0:10:48 â€¢ 0:02:43[0m [2;4m0.14it/s[0m  
[2KAttention weights: tensor([[0.9603, 0.0397]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 181/203 [2m0:10:48 â€¢ 0:02:43[0m [2;4m0.14it/s[0m  
[2KAttention weights: tensor([[0.5754, 0.4246]], device='cuda:0')8;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”[0m 181/203 [2m0:10:48 â€¢ 0:02:43[0m [2;4m0.14it/s[0m  
[2KAttention weights: tensor([[0.9471, 0.0529],â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”[0m 181/203 [2m0:10:48 â€¢ 0:02:43[0m [2;4m0.14it/s[0m  
        [0.9172, 0.0828],
        [0.9903, 0.0097],
        [0.9821, 0.0179],
        [0.9600, 0.0400],
        [0.9606, 0.0394],
        [0.9497, 0.0503],
        [0.9377, 0.0623],
        [0.9589, 0.0411],
[2K        [0.9655, 0.0345]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”[0m 181/203 [2m0:10:48 â€¢ 0:02:43[0m [2;4m0.14it/s[0m  
[2KAttention weights: tensor([[0.5605, 0.4395],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”[0m 182/203 [2m0:10:54 â€¢ 0:02:03[0m [2;4m0.17it/s[0m  
        [0.5546, 0.4454],
        [0.5569, 0.4431],
        [0.5585, 0.4415],
        [0.5571, 0.4429],
        [0.5559, 0.4441],
        [0.5550, 0.4450],
        [0.5598, 0.4402],
        [0.5586, 0.4414],
[2K        [0.5576, 0.4424]], device='cuda:0', grad_fn=<SoftmaxBackward0>)224mâ•¸[0m[38;5;237mâ”â”â”â”[0m 182/203 [2m0:10:54 â€¢ 0:02:03[0m [2;4m0.17it/s[0m  
[2KClass label: CleanAndJerkâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”[0m 182/203 [2m0:10:54 â€¢ 0:02:03[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.5743, 0.4257]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 182/203 [2m0:10:54 â€¢ 0:02:03[0m [2;4m0.17it/s[0m  
[2KStep 0 - TTA Loss: 0.8152â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”[0m 182/203 [2m0:10:54 â€¢ 0:02:03[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.7456, 0.2544]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 182/203 [2m0:10:54 â€¢ 0:02:03[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.8589, 0.1411]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 182/203 [2m0:10:54 â€¢ 0:02:03[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.9171, 0.0829]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 182/203 [2m0:10:54 â€¢ 0:02:03[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.9460, 0.0540]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 182/203 [2m0:10:54 â€¢ 0:02:03[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.9582, 0.0418]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 182/203 [2m0:10:54 â€¢ 0:02:03[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.9637, 0.0363]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 182/203 [2m0:10:54 â€¢ 0:02:03[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.9670, 0.0330]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 182/203 [2m0:10:54 â€¢ 0:02:03[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.9685, 0.0315]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 182/203 [2m0:10:54 â€¢ 0:02:03[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.9691, 0.0309]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ”â”â”â”[0m 182/203 [2m0:10:54 â€¢ 0:02:03[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.5746, 0.4254]], device='cuda:0')8;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”[0m 182/203 [2m0:10:54 â€¢ 0:02:03[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.9310, 0.0690],â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”[0m 182/203 [2m0:10:54 â€¢ 0:02:03[0m [2;4m0.17it/s[0m  
        [0.8882, 0.1118],
        [0.9694, 0.0306],
        [0.9787, 0.0213],
        [0.9318, 0.0682],
        [0.9342, 0.0658],
        [0.9188, 0.0812],
        [0.9166, 0.0834],
        [0.9458, 0.0542],
[2K        [0.9497, 0.0503]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”[0m 182/203 [2m0:10:54 â€¢ 0:02:03[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.5605, 0.4395],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”[0m 183/203 [2m0:10:55 â€¢ 0:01:36[0m [2;4m0.21it/s[0m   
        [0.5546, 0.4454],
        [0.5570, 0.4430],
        [0.5586, 0.4414],
        [0.5571, 0.4429],
        [0.5559, 0.4441],
        [0.5550, 0.4450],
        [0.5598, 0.4402],
        [0.5587, 0.4413],
[2K        [0.5576, 0.4424]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â•º[0m[38;5;237mâ”â”â”[0m 183/203 [2m0:10:55 â€¢ 0:01:36[0m [2;4m0.21it/s[0m  
[2KClass label: BaseballPitchâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”[0m 183/203 [2m0:10:55 â€¢ 0:01:36[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.6433, 0.3567]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 183/203 [2m0:10:55 â€¢ 0:01:36[0m [2;4m0.21it/s[0m  
[2KStep 0 - TTA Loss: 0.4461â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”[0m 183/203 [2m0:10:55 â€¢ 0:01:36[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.6433, 0.3567]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 183/203 [2m0:10:55 â€¢ 0:01:36[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.6469, 0.3531]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 183/203 [2m0:10:55 â€¢ 0:01:36[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.6649, 0.3351]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 183/203 [2m0:10:55 â€¢ 0:01:36[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.7096, 0.2904]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 183/203 [2m0:10:55 â€¢ 0:01:36[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.7862, 0.2138]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 183/203 [2m0:10:55 â€¢ 0:01:36[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.8780, 0.1220]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 183/203 [2m0:10:55 â€¢ 0:01:36[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.9479, 0.0521]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 183/203 [2m0:10:55 â€¢ 0:01:36[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.9796, 0.0204]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 183/203 [2m0:10:55 â€¢ 0:01:36[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.9933, 0.0067]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 183/203 [2m0:10:55 â€¢ 0:01:36[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.5857, 0.4143]], device='cuda:0')38;5;237mâ•º[0m[38;5;237mâ”â”â”[0m 183/203 [2m0:10:55 â€¢ 0:01:36[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.9982, 0.0018],â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”[0m 183/203 [2m0:10:55 â€¢ 0:01:36[0m [2;4m0.21it/s[0m  
        [0.9195, 0.0805],
        [0.9135, 0.0865],
        [0.9748, 0.0252],
        [0.9788, 0.0212],
        [0.9599, 0.0401],
        [0.9407, 0.0593],
        [0.9808, 0.0192],
        [0.9697, 0.0303],
[2K        [0.9703, 0.0297]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”[0m 183/203 [2m0:10:55 â€¢ 0:01:36[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.5605, 0.4395],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”[0m 184/203 [2m0:10:58 â€¢ 0:01:24[0m [2;4m0.23it/s[0m  
        [0.5546, 0.4454],
        [0.5570, 0.4430],
        [0.5586, 0.4414],
        [0.5571, 0.4429],
        [0.5559, 0.4441],
        [0.5550, 0.4450],
        [0.5598, 0.4402],
        [0.5587, 0.4413],
[2K        [0.5576, 0.4424]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â•º[0m[38;5;237mâ”â”â”[0m 184/203 [2m0:10:58 â€¢ 0:01:24[0m [2;4m0.23it/s[0m  
[2KClass label: SoccerPenaltyâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”[0m 184/203 [2m0:10:58 â€¢ 0:01:24[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.6704, 0.3296]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 184/203 [2m0:10:58 â€¢ 0:01:24[0m [2;4m0.23it/s[0m  
[2KStep 0 - TTA Loss: 0.5965â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”[0m 184/203 [2m0:10:58 â€¢ 0:01:24[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.8183, 0.1817]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 184/203 [2m0:10:58 â€¢ 0:01:24[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.9132, 0.0868]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 184/203 [2m0:10:58 â€¢ 0:01:24[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.9565, 0.0435]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 184/203 [2m0:10:58 â€¢ 0:01:24[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.9719, 0.0281]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 184/203 [2m0:10:58 â€¢ 0:01:24[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.9741, 0.0259]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 184/203 [2m0:10:58 â€¢ 0:01:24[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.9683, 0.0317]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 184/203 [2m0:10:58 â€¢ 0:01:24[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.9584, 0.0416]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 184/203 [2m0:10:58 â€¢ 0:01:24[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.9495, 0.0505]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 184/203 [2m0:10:58 â€¢ 0:01:24[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.9448, 0.0552]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 184/203 [2m0:10:58 â€¢ 0:01:24[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.5713, 0.4287]], device='cuda:0')38;5;237mâ•º[0m[38;5;237mâ”â”â”[0m 184/203 [2m0:10:58 â€¢ 0:01:24[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.9953, 0.0047],â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”[0m 184/203 [2m0:10:58 â€¢ 0:01:24[0m [2;4m0.23it/s[0m  
        [0.8669, 0.1331],
        [0.8608, 0.1392],
        [0.9519, 0.0481],
        [0.9560, 0.0440],
        [0.9215, 0.0785],
        [0.8970, 0.1030],
        [0.9436, 0.0564],
        [0.9427, 0.0573],
[2K        [0.9461, 0.0539]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”[0m 184/203 [2m0:10:58 â€¢ 0:01:24[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.5605, 0.4395],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”[0m 185/203 [2m0:11:01 â€¢ 0:01:14[0m [2;4m0.25it/s[0m  
        [0.5546, 0.4454],
        [0.5569, 0.4431],
        [0.5586, 0.4414],
        [0.5571, 0.4429],
        [0.5559, 0.4441],
        [0.5550, 0.4450],
        [0.5598, 0.4402],
        [0.5587, 0.4413],
[2K        [0.5576, 0.4424]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â•º[0m[38;5;237mâ”â”â”[0m 185/203 [2m0:11:01 â€¢ 0:01:14[0m [2;4m0.25it/s[0m  
[2KClass label: Billiardsâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”[0m 185/203 [2m0:11:01 â€¢ 0:01:14[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.5611, 0.4389]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 185/203 [2m0:11:01 â€¢ 0:01:14[0m [2;4m0.25it/s[0m  
[2KStep 0 - TTA Loss: 0.5753â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”[0m 185/203 [2m0:11:01 â€¢ 0:01:14[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.5611, 0.4389]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 185/203 [2m0:11:01 â€¢ 0:01:14[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.5594, 0.4406]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 185/203 [2m0:11:01 â€¢ 0:01:14[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.5532, 0.4468]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 185/203 [2m0:11:01 â€¢ 0:01:14[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.5405, 0.4595]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 185/203 [2m0:11:01 â€¢ 0:01:14[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.5173, 0.4827]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 185/203 [2m0:11:01 â€¢ 0:01:14[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.4802, 0.5198]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 185/203 [2m0:11:01 â€¢ 0:01:14[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.4282, 0.5718]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 185/203 [2m0:11:01 â€¢ 0:01:14[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.3719, 0.6281]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 185/203 [2m0:11:01 â€¢ 0:01:14[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.3300, 0.6700]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 185/203 [2m0:11:01 â€¢ 0:01:14[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.5497, 0.4503]], device='cuda:0')38;5;237mâ•º[0m[38;5;237mâ”â”â”[0m 185/203 [2m0:11:01 â€¢ 0:01:14[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.5163, 0.4837],â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”[0m 185/203 [2m0:11:01 â€¢ 0:01:14[0m [2;4m0.25it/s[0m  
        [0.3186, 0.6814],
        [0.4455, 0.5545],
        [0.5292, 0.4708],
        [0.3912, 0.6088],
        [0.3702, 0.6298],
        [0.4033, 0.5967],
        [0.1854, 0.8146],
        [0.4601, 0.5399],
[2K        [0.5091, 0.4909]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”[0m 185/203 [2m0:11:01 â€¢ 0:01:14[0m [2;4m0.25it/s[0m  
[2KAttention weights: tensor([[0.5605, 0.4395],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”[0m 186/203 [2m0:11:01 â€¢ 0:01:01[0m [2;4m0.28it/s[0m  
        [0.5546, 0.4454],
        [0.5569, 0.4431],
        [0.5586, 0.4414],
        [0.5571, 0.4429],
        [0.5559, 0.4441],
        [0.5550, 0.4450],
        [0.5598, 0.4402],
        [0.5586, 0.4414],
[2K        [0.5576, 0.4424]], device='cuda:0', grad_fn=<SoftmaxBackward0>);224mâ•¸[0m[38;5;237mâ”â”â”[0m 186/203 [2m0:11:01 â€¢ 0:01:01[0m [2;4m0.28it/s[0m  
[2KClass label: SoccerPenaltyâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”[0m 186/203 [2m0:11:01 â€¢ 0:01:01[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6673, 0.3327]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 186/203 [2m0:11:01 â€¢ 0:01:01[0m [2;4m0.28it/s[0m  
[2KStep 0 - TTA Loss: 0.6446â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”[0m 186/203 [2m0:11:01 â€¢ 0:01:01[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6478, 0.3522]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 186/203 [2m0:11:01 â€¢ 0:01:01[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.6775, 0.3225]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 186/203 [2m0:11:01 â€¢ 0:01:01[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.7375, 0.2625]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 186/203 [2m0:11:01 â€¢ 0:01:01[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.8021, 0.1979]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 186/203 [2m0:11:01 â€¢ 0:01:01[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.8534, 0.1466]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 186/203 [2m0:11:01 â€¢ 0:01:01[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.8868, 0.1132]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 186/203 [2m0:11:01 â€¢ 0:01:01[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.9055, 0.0945]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 186/203 [2m0:11:01 â€¢ 0:01:01[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.9134, 0.0866]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 186/203 [2m0:11:01 â€¢ 0:01:01[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.9159, 0.0841]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 186/203 [2m0:11:01 â€¢ 0:01:01[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.5682, 0.4318]], device='cuda:0')38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”[0m 186/203 [2m0:11:01 â€¢ 0:01:01[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.8730, 0.1270],â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”[0m 186/203 [2m0:11:01 â€¢ 0:01:01[0m [2;4m0.28it/s[0m  
        [0.7565, 0.2435],
        [0.7114, 0.2886],
        [0.8050, 0.1950],
        [0.7781, 0.2219],
        [0.8009, 0.1991],
        [0.7398, 0.2602],
        [0.9165, 0.0835],
        [0.8158, 0.1842],
[2K        [0.7876, 0.2124]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”[0m 186/203 [2m0:11:01 â€¢ 0:01:01[0m [2;4m0.28it/s[0m  
[2KAttention weights: tensor([[0.5605, 0.4395],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”[0m 187/203 [2m0:11:04 â€¢ 0:00:55[0m [2;4m0.29it/s[0m  
        [0.5546, 0.4454],
        [0.5569, 0.4431],
        [0.5586, 0.4414],
        [0.5571, 0.4429],
        [0.5559, 0.4441],
        [0.5550, 0.4450],
        [0.5598, 0.4402],
        [0.5586, 0.4414],
[2K        [0.5576, 0.4424]], device='cuda:0', grad_fn=<SoftmaxBackward0>);224mâ•¸[0m[38;5;237mâ”â”â”[0m 187/203 [2m0:11:04 â€¢ 0:00:55[0m [2;4m0.29it/s[0m  
[2KClass label: CleanAndJerkâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”[0m 187/203 [2m0:11:04 â€¢ 0:00:55[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.5715, 0.4285]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 187/203 [2m0:11:04 â€¢ 0:00:55[0m [2;4m0.29it/s[0m  
[2KStep 0 - TTA Loss: 0.7914â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”[0m 187/203 [2m0:11:04 â€¢ 0:00:55[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.5715, 0.4285]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 187/203 [2m0:11:04 â€¢ 0:00:55[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.5722, 0.4278]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 187/203 [2m0:11:04 â€¢ 0:00:55[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.5756, 0.4244]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 187/203 [2m0:11:04 â€¢ 0:00:55[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.5858, 0.4142]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 187/203 [2m0:11:04 â€¢ 0:00:55[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.6040, 0.3960]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 187/203 [2m0:11:04 â€¢ 0:00:55[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.6307, 0.3693]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 187/203 [2m0:11:04 â€¢ 0:00:55[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.6666, 0.3334]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 187/203 [2m0:11:04 â€¢ 0:00:55[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.7077, 0.2923]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 187/203 [2m0:11:04 â€¢ 0:00:55[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.7539, 0.2461]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ”â”â”[0m 187/203 [2m0:11:04 â€¢ 0:00:55[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.5642, 0.4358]], device='cuda:0')38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”[0m 187/203 [2m0:11:04 â€¢ 0:00:55[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.7604, 0.2396],â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”[0m 187/203 [2m0:11:04 â€¢ 0:00:55[0m [2;4m0.29it/s[0m  
        [0.6785, 0.3215],
        [0.7903, 0.2097],
        [0.7429, 0.2571],
        [0.7162, 0.2838],
        [0.7451, 0.2549],
        [0.6972, 0.3028],
        [0.8084, 0.1916],
        [0.7465, 0.2535],
[2K        [0.7381, 0.2619]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”[0m 187/203 [2m0:11:04 â€¢ 0:00:55[0m [2;4m0.29it/s[0m  
[2KAttention weights: tensor([[0.5605, 0.4395],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”[0m 188/203 [2m0:11:05 â€¢ 0:00:48[0m [2;4m0.31it/s[0m   
        [0.5546, 0.4454],
        [0.5569, 0.4431],
        [0.5586, 0.4414],
        [0.5571, 0.4429],
        [0.5559, 0.4441],
        [0.5550, 0.4450],
        [0.5598, 0.4402],
        [0.5586, 0.4414],
[2K        [0.5576, 0.4424]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ•º[0m[38;5;237mâ”â”[0m 188/203 [2m0:11:05 â€¢ 0:00:48[0m [2;4m0.31it/s[0m  
[2KClass label: TennisSwingâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”[0m 188/203 [2m0:11:05 â€¢ 0:00:48[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.6411, 0.3589]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 188/203 [2m0:11:05 â€¢ 0:00:48[0m [2;4m0.31it/s[0m  
[2KStep 0 - TTA Loss: 0.6485â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”[0m 188/203 [2m0:11:05 â€¢ 0:00:48[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.6883, 0.3117]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 188/203 [2m0:11:05 â€¢ 0:00:48[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.7508, 0.2492]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 188/203 [2m0:11:05 â€¢ 0:00:48[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.8164, 0.1836]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 188/203 [2m0:11:05 â€¢ 0:00:48[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.8723, 0.1277]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 188/203 [2m0:11:05 â€¢ 0:00:48[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.9113, 0.0887]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 188/203 [2m0:11:05 â€¢ 0:00:48[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.9332, 0.0668]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 188/203 [2m0:11:05 â€¢ 0:00:48[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.9439, 0.0561]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 188/203 [2m0:11:05 â€¢ 0:00:48[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.9479, 0.0521]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 188/203 [2m0:11:05 â€¢ 0:00:48[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.9494, 0.0506]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 188/203 [2m0:11:05 â€¢ 0:00:48[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5734, 0.4266]], device='cuda:0')[38;5;237mâ•º[0m[38;5;237mâ”â”[0m 188/203 [2m0:11:05 â€¢ 0:00:48[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.8892, 0.1108],â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”[0m 188/203 [2m0:11:05 â€¢ 0:00:48[0m [2;4m0.31it/s[0m  
        [0.8328, 0.1672],
        [0.8607, 0.1393],
        [0.9001, 0.0999],
        [0.8570, 0.1430],
        [0.8819, 0.1181],
        [0.8402, 0.1598],
        [0.9033, 0.0967],
        [0.9496, 0.0504],
[2K        [0.8923, 0.1077]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”[0m 188/203 [2m0:11:05 â€¢ 0:00:48[0m [2;4m0.31it/s[0m  
[2KAttention weights: tensor([[0.5605, 0.4395],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”[0m 189/203 [2m0:11:08 â€¢ 0:00:41[0m [2;4m0.35it/s[0m  
        [0.5546, 0.4454],
        [0.5569, 0.4431],
        [0.5586, 0.4414],
        [0.5571, 0.4429],
        [0.5559, 0.4441],
        [0.5550, 0.4450],
        [0.5598, 0.4402],
        [0.5587, 0.4413],
[2K        [0.5576, 0.4424]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ•º[0m[38;5;237mâ”â”[0m 189/203 [2m0:11:08 â€¢ 0:00:41[0m [2;4m0.35it/s[0m  
[2KClass label: GolfSwingâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”[0m 189/203 [2m0:11:08 â€¢ 0:00:41[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6494, 0.3506]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 189/203 [2m0:11:08 â€¢ 0:00:41[0m [2;4m0.35it/s[0m  
[2KStep 0 - TTA Loss: 0.8635â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”[0m 189/203 [2m0:11:08 â€¢ 0:00:41[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6494, 0.3506]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 189/203 [2m0:11:08 â€¢ 0:00:41[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6498, 0.3502]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 189/203 [2m0:11:08 â€¢ 0:00:41[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6515, 0.3485]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 189/203 [2m0:11:08 â€¢ 0:00:41[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6549, 0.3451]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 189/203 [2m0:11:08 â€¢ 0:00:41[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6608, 0.3392]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 189/203 [2m0:11:08 â€¢ 0:00:41[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6702, 0.3298]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 189/203 [2m0:11:08 â€¢ 0:00:41[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6824, 0.3176]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 189/203 [2m0:11:08 â€¢ 0:00:41[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6952, 0.3048]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 189/203 [2m0:11:08 â€¢ 0:00:41[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.7066, 0.2934]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 189/203 [2m0:11:08 â€¢ 0:00:41[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5611, 0.4389]], device='cuda:0')[38;5;237mâ•º[0m[38;5;237mâ”â”[0m 189/203 [2m0:11:08 â€¢ 0:00:41[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.6917, 0.3083],â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”[0m 189/203 [2m0:11:08 â€¢ 0:00:41[0m [2;4m0.35it/s[0m  
        [0.6153, 0.3847],
        [0.6308, 0.3692],
        [0.7134, 0.2866],
        [0.6168, 0.3832],
        [0.6559, 0.3441],
        [0.6149, 0.3851],
        [0.7179, 0.2821],
        [0.7122, 0.2878],
[2K        [0.6805, 0.3195]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”[0m 189/203 [2m0:11:08 â€¢ 0:00:41[0m [2;4m0.35it/s[0m  
[2KAttention weights: tensor([[0.5605, 0.4395],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”[0m 190/203 [2m0:11:10 â€¢ 0:00:37[0m [2;4m0.36it/s[0m  
        [0.5546, 0.4454],
        [0.5569, 0.4431],
        [0.5586, 0.4414],
        [0.5571, 0.4429],
        [0.5559, 0.4441],
        [0.5550, 0.4450],
        [0.5598, 0.4402],
        [0.5586, 0.4414],
[2K        [0.5576, 0.4424]], device='cuda:0', grad_fn=<SoftmaxBackward0>)mâ•º[0m[38;5;237mâ”â”[0m 190/203 [2m0:11:10 â€¢ 0:00:37[0m [2;4m0.36it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”[0m 190/203 [2m0:11:10 â€¢ 0:00:37[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.5694, 0.4306]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 190/203 [2m0:11:10 â€¢ 0:00:37[0m [2;4m0.36it/s[0m  
[2KStep 0 - TTA Loss: 0.7126â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”[0m 190/203 [2m0:11:10 â€¢ 0:00:37[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.5969, 0.4031]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 190/203 [2m0:11:10 â€¢ 0:00:37[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.6508, 0.3492]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 190/203 [2m0:11:10 â€¢ 0:00:37[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.7153, 0.2847]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 190/203 [2m0:11:10 â€¢ 0:00:37[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.7750, 0.2250]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 190/203 [2m0:11:10 â€¢ 0:00:37[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.8189, 0.1811]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 190/203 [2m0:11:10 â€¢ 0:00:37[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.8458, 0.1542]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 190/203 [2m0:11:10 â€¢ 0:00:37[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.8606, 0.1394]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 190/203 [2m0:11:10 â€¢ 0:00:37[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.8680, 0.1320]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 190/203 [2m0:11:10 â€¢ 0:00:37[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.8708, 0.1292]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 190/203 [2m0:11:10 â€¢ 0:00:37[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.5658, 0.4342]], device='cuda:0')[38;5;237mâ•º[0m[38;5;237mâ”â”[0m 190/203 [2m0:11:10 â€¢ 0:00:37[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.8296, 0.1704],â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”[0m 190/203 [2m0:11:10 â€¢ 0:00:37[0m [2;4m0.36it/s[0m  
        [0.7410, 0.2590],
        [0.7988, 0.2012],
        [0.8239, 0.1761],
        [0.8210, 0.1790],
        [0.8648, 0.1352],
        [0.8717, 0.1283],
        [0.8410, 0.1590],
        [0.8437, 0.1563],
[2K        [0.8603, 0.1397]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”[0m 190/203 [2m0:11:10 â€¢ 0:00:37[0m [2;4m0.36it/s[0m  
[2KAttention weights: tensor([[0.5605, 0.4395],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”[0m 191/203 [2m0:11:28 â€¢ 0:00:52[0m [2;4m0.23it/s[0m  
        [0.5546, 0.4454],
        [0.5569, 0.4431],
        [0.5586, 0.4414],
        [0.5571, 0.4429],
        [0.5559, 0.4441],
        [0.5550, 0.4450],
        [0.5598, 0.4402],
        [0.5586, 0.4414],
[2K        [0.5576, 0.4424]], device='cuda:0', grad_fn=<SoftmaxBackward0>)6;224mâ•¸[0m[38;5;237mâ”â”[0m 191/203 [2m0:11:28 â€¢ 0:00:52[0m [2;4m0.23it/s[0m  
[2KClass label: CleanAndJerkâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”[0m 191/203 [2m0:11:28 â€¢ 0:00:52[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.5755, 0.4245]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 191/203 [2m0:11:28 â€¢ 0:00:52[0m [2;4m0.23it/s[0m  
[2KStep 0 - TTA Loss: 0.7012â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”[0m 191/203 [2m0:11:28 â€¢ 0:00:52[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.5755, 0.4245]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 191/203 [2m0:11:28 â€¢ 0:00:52[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.5771, 0.4229]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 191/203 [2m0:11:28 â€¢ 0:00:52[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.5849, 0.4151]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 191/203 [2m0:11:28 â€¢ 0:00:52[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.6044, 0.3956]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 191/203 [2m0:11:28 â€¢ 0:00:52[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.6374, 0.3626]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 191/203 [2m0:11:28 â€¢ 0:00:52[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.6870, 0.3130]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 191/203 [2m0:11:28 â€¢ 0:00:52[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.7415, 0.2585]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 191/203 [2m0:11:28 â€¢ 0:00:52[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.7916, 0.2084]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 191/203 [2m0:11:28 â€¢ 0:00:52[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.8201, 0.1799]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 191/203 [2m0:11:28 â€¢ 0:00:52[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.5627, 0.4373]], device='cuda:0')[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”[0m 191/203 [2m0:11:28 â€¢ 0:00:52[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.7577, 0.2423],â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”[0m 191/203 [2m0:11:28 â€¢ 0:00:52[0m [2;4m0.23it/s[0m  
        [0.6892, 0.3108],
        [0.8286, 0.1714],
        [0.7625, 0.2375],
        [0.7491, 0.2509],
        [0.7826, 0.2174],
        [0.7648, 0.2352],
        [0.7770, 0.2230],
        [0.7679, 0.2321],
[2K        [0.7792, 0.2208]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”[0m 191/203 [2m0:11:28 â€¢ 0:00:52[0m [2;4m0.23it/s[0m  
[2KAttention weights: tensor([[0.5605, 0.4395],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”[0m 192/203 [2m0:11:32 â€¢ 0:01:03[0m [2;4m0.17it/s[0m  
        [0.5546, 0.4454],
        [0.5569, 0.4431],
        [0.5586, 0.4414],
        [0.5571, 0.4429],
        [0.5559, 0.4441],
        [0.5550, 0.4450],
        [0.5598, 0.4402],
        [0.5586, 0.4414],
[2K        [0.5576, 0.4424]], device='cuda:0', grad_fn=<SoftmaxBackward0>)6;224mâ•¸[0m[38;5;237mâ”â”[0m 192/203 [2m0:11:32 â€¢ 0:01:03[0m [2;4m0.17it/s[0m  
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”[0m 192/203 [2m0:11:32 â€¢ 0:01:03[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.6012, 0.3988]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 192/203 [2m0:11:32 â€¢ 0:01:03[0m [2;4m0.17it/s[0m  
[2KStep 0 - TTA Loss: 0.8136â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”[0m 192/203 [2m0:11:32 â€¢ 0:01:03[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.6492, 0.3508]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 192/203 [2m0:11:32 â€¢ 0:01:03[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.7157, 0.2843]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 192/203 [2m0:11:32 â€¢ 0:01:03[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.7907, 0.2093]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 192/203 [2m0:11:32 â€¢ 0:01:03[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.8402, 0.1598]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 192/203 [2m0:11:32 â€¢ 0:01:03[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.8775, 0.1225]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 192/203 [2m0:11:32 â€¢ 0:01:03[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.9210, 0.0790]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 192/203 [2m0:11:32 â€¢ 0:01:03[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.9498, 0.0502]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 192/203 [2m0:11:32 â€¢ 0:01:03[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.9630, 0.0370]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 192/203 [2m0:11:32 â€¢ 0:01:03[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.9678, 0.0322]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ”â”[0m 192/203 [2m0:11:32 â€¢ 0:01:03[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.5748, 0.4252]], device='cuda:0')[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”[0m 192/203 [2m0:11:32 â€¢ 0:01:03[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.9186, 0.0814],â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”[0m 192/203 [2m0:11:32 â€¢ 0:01:03[0m [2;4m0.17it/s[0m  
        [0.8357, 0.1643],
        [0.9016, 0.0984],
        [0.8995, 0.1005],
        [0.9321, 0.0679],
        [0.9689, 0.0311],
        [0.9438, 0.0562],
        [0.9436, 0.0564],
        [0.9270, 0.0730],
[2K        [0.9374, 0.0626]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”[0m 192/203 [2m0:11:32 â€¢ 0:01:03[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.5605, 0.4395],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”[0m 193/203 [2m0:11:36 â€¢ 0:01:11[0m [2;4m0.14it/s[0m   
        [0.5546, 0.4454],
        [0.5569, 0.4431],
        [0.5586, 0.4414],
        [0.5571, 0.4429],
        [0.5559, 0.4441],
        [0.5550, 0.4450],
        [0.5598, 0.4402],
        [0.5586, 0.4414],
[2K        [0.5576, 0.4424]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ•º[0m[38;5;237mâ”[0m 193/203 [2m0:11:36 â€¢ 0:01:11[0m [2;4m0.14it/s[0m  
[2KClass label: BaseballPitchâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”[0m 193/203 [2m0:11:36 â€¢ 0:01:11[0m [2;4m0.14it/s[0m  
[2KAttention weights: tensor([[0.6420, 0.3580]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 193/203 [2m0:11:36 â€¢ 0:01:11[0m [2;4m0.14it/s[0m  
[2KStep 0 - TTA Loss: 1.0778â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”[0m 193/203 [2m0:11:36 â€¢ 0:01:11[0m [2;4m0.14it/s[0m  
[2KAttention weights: tensor([[0.6420, 0.3580]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 193/203 [2m0:11:36 â€¢ 0:01:11[0m [2;4m0.14it/s[0m  
[2KAttention weights: tensor([[0.6462, 0.3538]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 193/203 [2m0:11:36 â€¢ 0:01:11[0m [2;4m0.14it/s[0m  
[2KAttention weights: tensor([[0.6623, 0.3377]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 193/203 [2m0:11:36 â€¢ 0:01:11[0m [2;4m0.14it/s[0m  
[2KAttention weights: tensor([[0.6961, 0.3039]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 193/203 [2m0:11:36 â€¢ 0:01:11[0m [2;4m0.14it/s[0m  
[2KAttention weights: tensor([[0.7490, 0.2510]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 193/203 [2m0:11:36 â€¢ 0:01:11[0m [2;4m0.14it/s[0m  
[2KAttention weights: tensor([[0.8122, 0.1878]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 193/203 [2m0:11:36 â€¢ 0:01:11[0m [2;4m0.14it/s[0m  
[2KAttention weights: tensor([[0.8724, 0.1276]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 193/203 [2m0:11:36 â€¢ 0:01:11[0m [2;4m0.14it/s[0m  
[2KAttention weights: tensor([[0.9171, 0.0829]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 193/203 [2m0:11:36 â€¢ 0:01:11[0m [2;4m0.14it/s[0m  
[2KAttention weights: tensor([[0.9410, 0.0590]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 193/203 [2m0:11:36 â€¢ 0:01:11[0m [2;4m0.14it/s[0m  
[2KAttention weights: tensor([[0.5706, 0.4294]], device='cuda:0')[38;5;237mâ•º[0m[38;5;237mâ”[0m 193/203 [2m0:11:36 â€¢ 0:01:11[0m [2;4m0.14it/s[0m  
[2KAttention weights: tensor([[0.9475, 0.0525],â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”[0m 193/203 [2m0:11:36 â€¢ 0:01:11[0m [2;4m0.14it/s[0m  
        [0.8466, 0.1534],
        [0.9018, 0.0982],
        [0.9127, 0.0873],
        [0.9429, 0.0571],
        [0.9706, 0.0294],
        [0.9438, 0.0562],
        [0.9526, 0.0474],
        [0.9356, 0.0644],
[2K        [0.9436, 0.0564]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”[0m 193/203 [2m0:11:36 â€¢ 0:01:11[0m [2;4m0.14it/s[0m  
[2KAttention weights: tensor([[0.5605, 0.4395],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”[0m 194/203 [2m0:11:37 â€¢ 0:00:53[0m [2;4m0.17it/s[0m  
        [0.5546, 0.4454],
        [0.5570, 0.4430],
        [0.5586, 0.4414],
        [0.5571, 0.4429],
        [0.5560, 0.4440],
        [0.5551, 0.4449],
        [0.5598, 0.4402],
        [0.5587, 0.4413],
[2K        [0.5576, 0.4424]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ•º[0m[38;5;237mâ”[0m 194/203 [2m0:11:37 â€¢ 0:00:53[0m [2;4m0.17it/s[0m  
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”[0m 194/203 [2m0:11:37 â€¢ 0:00:53[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.6035, 0.3965]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 194/203 [2m0:11:37 â€¢ 0:00:53[0m [2;4m0.17it/s[0m  
[2KStep 0 - TTA Loss: 0.4944â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”[0m 194/203 [2m0:11:37 â€¢ 0:00:53[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.6906, 0.3094]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 194/203 [2m0:11:37 â€¢ 0:00:53[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.7598, 0.2402]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 194/203 [2m0:11:37 â€¢ 0:00:53[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.8072, 0.1928]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 194/203 [2m0:11:37 â€¢ 0:00:53[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.8389, 0.1611]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 194/203 [2m0:11:37 â€¢ 0:00:53[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.8617, 0.1383]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 194/203 [2m0:11:37 â€¢ 0:00:53[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.8809, 0.1191]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 194/203 [2m0:11:37 â€¢ 0:00:53[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.8966, 0.1034]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 194/203 [2m0:11:37 â€¢ 0:00:53[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.9070, 0.0930]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 194/203 [2m0:11:37 â€¢ 0:00:53[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.9122, 0.0878]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 194/203 [2m0:11:37 â€¢ 0:00:53[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.5671, 0.4329]], device='cuda:0')[38;5;237mâ•º[0m[38;5;237mâ”[0m 194/203 [2m0:11:37 â€¢ 0:00:53[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.7827, 0.2173],â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”[0m 194/203 [2m0:11:37 â€¢ 0:00:53[0m [2;4m0.17it/s[0m  
        [0.7341, 0.2659],
        [0.8189, 0.1811],
        [0.8159, 0.1841],
        [0.8315, 0.1685],
        [0.9138, 0.0862],
        [0.8626, 0.1374],
        [0.8711, 0.1289],
        [0.8487, 0.1513],
[2K        [0.8587, 0.1413]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”[0m 194/203 [2m0:11:37 â€¢ 0:00:53[0m [2;4m0.17it/s[0m  
[2KAttention weights: tensor([[0.5605, 0.4395],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”[0m 195/203 [2m0:11:49 â€¢ 0:00:41[0m [2;4m0.20it/s[0m  
        [0.5547, 0.4453],
        [0.5570, 0.4430],
        [0.5586, 0.4414],
        [0.5571, 0.4429],
        [0.5560, 0.4440],
        [0.5551, 0.4449],
        [0.5598, 0.4402],
        [0.5587, 0.4413],
[2K        [0.5576, 0.4424]], device='cuda:0', grad_fn=<SoftmaxBackward0>)7mâ•º[0m[38;5;237mâ”[0m 195/203 [2m0:11:49 â€¢ 0:00:41[0m [2;4m0.20it/s[0m  
[2KClass label: GolfSwingâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”[0m 195/203 [2m0:11:49 â€¢ 0:00:41[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.6511, 0.3489]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 195/203 [2m0:11:49 â€¢ 0:00:41[0m [2;4m0.20it/s[0m  
[2KStep 0 - TTA Loss: 0.5220â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”[0m 195/203 [2m0:11:49 â€¢ 0:00:41[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.6511, 0.3489]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 195/203 [2m0:11:49 â€¢ 0:00:41[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.6534, 0.3466]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 195/203 [2m0:11:49 â€¢ 0:00:41[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.6628, 0.3372]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 195/203 [2m0:11:49 â€¢ 0:00:41[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.6842, 0.3158]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 195/203 [2m0:11:49 â€¢ 0:00:41[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.7209, 0.2791]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 195/203 [2m0:11:49 â€¢ 0:00:41[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.7690, 0.2310]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 195/203 [2m0:11:49 â€¢ 0:00:41[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.8201, 0.1799]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 195/203 [2m0:11:49 â€¢ 0:00:41[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.8557, 0.1443]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 195/203 [2m0:11:49 â€¢ 0:00:41[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.8764, 0.1236]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 195/203 [2m0:11:49 â€¢ 0:00:41[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.5654, 0.4346]], device='cuda:0')[38;5;237mâ•º[0m[38;5;237mâ”[0m 195/203 [2m0:11:49 â€¢ 0:00:41[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.8519, 0.1481],â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”[0m 195/203 [2m0:11:49 â€¢ 0:00:41[0m [2;4m0.20it/s[0m  
        [0.7606, 0.2394],
        [0.8107, 0.1893],
        [0.8818, 0.1182],
        [0.8471, 0.1529],
        [0.9033, 0.0967],
        [0.8555, 0.1445],
        [0.8811, 0.1189],
        [0.8731, 0.1269],
[2K        [0.8762, 0.1238]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”[0m 195/203 [2m0:11:49 â€¢ 0:00:41[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.5605, 0.4395],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”[0m 196/203 [2m0:11:53 â€¢ 0:00:35[0m [2;4m0.20it/s[0m  
        [0.5547, 0.4453],
        [0.5570, 0.4430],
        [0.5586, 0.4414],
        [0.5571, 0.4429],
        [0.5560, 0.4440],
        [0.5551, 0.4449],
        [0.5598, 0.4402],
        [0.5587, 0.4413],
[2K        [0.5576, 0.4424]], device='cuda:0', grad_fn=<SoftmaxBackward0>);6;224mâ•¸[0m[38;5;237mâ”[0m 196/203 [2m0:11:53 â€¢ 0:00:35[0m [2;4m0.20it/s[0m  
[2KClass label: GolfSwingâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”[0m 196/203 [2m0:11:53 â€¢ 0:00:35[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.6527, 0.3473]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 196/203 [2m0:11:53 â€¢ 0:00:35[0m [2;4m0.20it/s[0m  
[2KStep 0 - TTA Loss: 1.2605â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”[0m 196/203 [2m0:11:53 â€¢ 0:00:35[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.6787, 0.3213]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 196/203 [2m0:11:53 â€¢ 0:00:35[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.7065, 0.2935]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 196/203 [2m0:11:53 â€¢ 0:00:35[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.7331, 0.2669]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 196/203 [2m0:11:53 â€¢ 0:00:35[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.7539, 0.2461]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 196/203 [2m0:11:53 â€¢ 0:00:35[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.7737, 0.2263]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 196/203 [2m0:11:53 â€¢ 0:00:35[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.7922, 0.2078]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 196/203 [2m0:11:53 â€¢ 0:00:35[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.8075, 0.1925]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 196/203 [2m0:11:53 â€¢ 0:00:35[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.8167, 0.1833]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 196/203 [2m0:11:53 â€¢ 0:00:35[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.8209, 0.1791]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 196/203 [2m0:11:53 â€¢ 0:00:35[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.5620, 0.4380]], device='cuda:0')[38;2;98;6;224mâ•¸[0m[38;5;237mâ”[0m 196/203 [2m0:11:53 â€¢ 0:00:35[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.7937, 0.2063],â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”[0m 196/203 [2m0:11:53 â€¢ 0:00:35[0m [2;4m0.20it/s[0m  
        [0.7002, 0.2998],
        [0.7392, 0.2608],
        [0.8221, 0.1779],
        [0.7687, 0.2313],
        [0.8324, 0.1676],
        [0.7766, 0.2234],
        [0.8251, 0.1749],
        [0.8130, 0.1870],
[2K        [0.8114, 0.1886]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”[0m 196/203 [2m0:11:53 â€¢ 0:00:35[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.5605, 0.4395],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”[0m 197/203 [2m0:11:57 â€¢ 0:00:29[0m [2;4m0.21it/s[0m  
        [0.5547, 0.4453],
        [0.5570, 0.4430],
        [0.5586, 0.4414],
        [0.5571, 0.4429],
        [0.5560, 0.4440],
        [0.5551, 0.4449],
        [0.5598, 0.4402],
        [0.5587, 0.4413],
[2K        [0.5576, 0.4424]], device='cuda:0', grad_fn=<SoftmaxBackward0>);6;224mâ•¸[0m[38;5;237mâ”[0m 197/203 [2m0:11:57 â€¢ 0:00:29[0m [2;4m0.21it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”[0m 197/203 [2m0:11:57 â€¢ 0:00:29[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.5662, 0.4338]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 197/203 [2m0:11:57 â€¢ 0:00:29[0m [2;4m0.21it/s[0m  
[2KStep 0 - TTA Loss: 0.8837â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”[0m 197/203 [2m0:11:57 â€¢ 0:00:29[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.5662, 0.4338]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 197/203 [2m0:11:57 â€¢ 0:00:29[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.5674, 0.4326]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 197/203 [2m0:11:57 â€¢ 0:00:29[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.5722, 0.4278]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 197/203 [2m0:11:57 â€¢ 0:00:29[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.5822, 0.4178]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 197/203 [2m0:11:57 â€¢ 0:00:29[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.5998, 0.4002]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 197/203 [2m0:11:57 â€¢ 0:00:29[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.6230, 0.3770]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 197/203 [2m0:11:57 â€¢ 0:00:29[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.6488, 0.3512]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 197/203 [2m0:11:57 â€¢ 0:00:29[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.6778, 0.3222]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 197/203 [2m0:11:57 â€¢ 0:00:29[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.7037, 0.2963]], device='cuda:0', grad_fn=<SoftmaxBackward0>)237mâ”[0m 197/203 [2m0:11:57 â€¢ 0:00:29[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.5592, 0.4408]], device='cuda:0')[38;2;98;6;224mâ•¸[0m[38;5;237mâ”[0m 197/203 [2m0:11:57 â€¢ 0:00:29[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.7564, 0.2436],â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”[0m 197/203 [2m0:11:57 â€¢ 0:00:29[0m [2;4m0.21it/s[0m  
        [0.6671, 0.3329],
        [0.6869, 0.3131],
        [0.8046, 0.1954],
        [0.7049, 0.2951],
        [0.7509, 0.2491],
        [0.7213, 0.2787],
        [0.7687, 0.2313],
        [0.7692, 0.2308],
[2K        [0.7692, 0.2308]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”[0m 197/203 [2m0:11:57 â€¢ 0:00:29[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.5605, 0.4395],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m 198/203 [2m0:12:02 â€¢ 0:00:25[0m [2;4m0.20it/s[0m  4m0.21it/s[0m  
        [0.5547, 0.4453],
        [0.5570, 0.4430],
        [0.5586, 0.4414],
        [0.5571, 0.4429],
        [0.5560, 0.4440],
        [0.5551, 0.4449],
        [0.5598, 0.4402],
        [0.5587, 0.4413],
[2K        [0.5576, 0.4424]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ•º[0m 198/203 [2m0:12:02 â€¢ 0:00:25[0m [2;4m0.20it/s[0m  
[2KClass label: SoccerPenaltyâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m 198/203 [2m0:12:02 â€¢ 0:00:25[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.6676, 0.3324]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:12:02 â€¢ 0:00:25[0m [2;4m0.20it/s[0m  
[2KStep 0 - TTA Loss: 0.5730â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m 198/203 [2m0:12:02 â€¢ 0:00:25[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.7345, 0.2655]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:12:02 â€¢ 0:00:25[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.8243, 0.1757]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:12:02 â€¢ 0:00:25[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.9075, 0.0925]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:12:02 â€¢ 0:00:25[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.9601, 0.0399]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:12:02 â€¢ 0:00:25[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.9830, 0.0170]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:12:02 â€¢ 0:00:25[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.9908, 0.0092]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:12:02 â€¢ 0:00:25[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.9938, 0.0062]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:12:02 â€¢ 0:00:25[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.9950, 0.0050]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:12:02 â€¢ 0:00:25[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.9954, 0.0046]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:12:02 â€¢ 0:00:25[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.5845, 0.4155]], device='cuda:0')m[38;5;237mâ•º[0m 198/203 [2m0:12:02 â€¢ 0:00:25[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.9613, 0.0387],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m 198/203 [2m0:12:02 â€¢ 0:00:25[0m [2;4m0.20it/s[0m  
        [0.8951, 0.1049],
        [0.8481, 0.1519],
        [0.9271, 0.0729],
        [0.9275, 0.0725],
        [0.9498, 0.0502],
        [0.8998, 0.1002],
        [0.9955, 0.0045],
        [0.9391, 0.0609],
[2K        [0.9125, 0.0875]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m 198/203 [2m0:12:02 â€¢ 0:00:25[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.5606, 0.4394],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m 199/203 [2m0:12:05 â€¢ 0:00:20[0m [2;4m0.21it/s[0m  
        [0.5547, 0.4453],
        [0.5570, 0.4430],
        [0.5586, 0.4414],
        [0.5572, 0.4428],
        [0.5561, 0.4439],
        [0.5551, 0.4449],
        [0.5599, 0.4401],
        [0.5587, 0.4413],
[2K        [0.5577, 0.4423]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ•º[0m 199/203 [2m0:12:05 â€¢ 0:00:20[0m [2;4m0.21it/s[0m  
[2KClass label: PoleVaultâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m 199/203 [2m0:12:05 â€¢ 0:00:20[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.5620, 0.4380]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:12:05 â€¢ 0:00:20[0m [2;4m0.21it/s[0m  
[2KStep 0 - TTA Loss: 0.5874â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m 199/203 [2m0:12:05 â€¢ 0:00:20[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.5620, 0.4380]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:12:05 â€¢ 0:00:20[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.5641, 0.4359]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:12:05 â€¢ 0:00:20[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.5720, 0.4280]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:12:05 â€¢ 0:00:20[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.5900, 0.4100]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:12:05 â€¢ 0:00:20[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.6213, 0.3787]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:12:05 â€¢ 0:00:20[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.6624, 0.3376]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:12:05 â€¢ 0:00:20[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.7124, 0.2876]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:12:05 â€¢ 0:00:20[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.7668, 0.2332]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:12:05 â€¢ 0:00:20[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.8136, 0.1864]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:12:05 â€¢ 0:00:20[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.5638, 0.4362]], device='cuda:0')m[38;5;237mâ•º[0m 199/203 [2m0:12:05 â€¢ 0:00:20[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.8764, 0.1236],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m 199/203 [2m0:12:05 â€¢ 0:00:20[0m [2;4m0.21it/s[0m  
        [0.7810, 0.2190],
        [0.7911, 0.2089],
        [0.8452, 0.1548],
        [0.8458, 0.1542],
        [0.8851, 0.1149],
        [0.8644, 0.1356],
        [0.9383, 0.0617],
        [0.8620, 0.1380],
[2K        [0.8624, 0.1376]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m 199/203 [2m0:12:05 â€¢ 0:00:20[0m [2;4m0.21it/s[0m  
[2KAttention weights: tensor([[0.5606, 0.4394],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m 200/203 [2m0:12:19 â€¢ 0:00:20[0m [2;4m0.15it/s[0m  
        [0.5547, 0.4453],
        [0.5570, 0.4430],
        [0.5587, 0.4413],
        [0.5572, 0.4428],
        [0.5561, 0.4439],
        [0.5551, 0.4449],
        [0.5599, 0.4401],
        [0.5587, 0.4413],
[2K        [0.5577, 0.4423]], device='cuda:0', grad_fn=<SoftmaxBackward0>)37mâ•º[0m 200/203 [2m0:12:19 â€¢ 0:00:20[0m [2;4m0.15it/s[0m  
[2KClass label: CleanAndJerkâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m 200/203 [2m0:12:19 â€¢ 0:00:20[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.5750, 0.4250]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:12:19 â€¢ 0:00:20[0m [2;4m0.15it/s[0m  
[2KStep 0 - TTA Loss: 0.9140â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m 200/203 [2m0:12:19 â€¢ 0:00:20[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.6310, 0.3690]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:12:19 â€¢ 0:00:20[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.6822, 0.3178]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:12:19 â€¢ 0:00:20[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.7246, 0.2754]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:12:19 â€¢ 0:00:20[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.7606, 0.2394]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:12:19 â€¢ 0:00:20[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.7872, 0.2128]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:12:19 â€¢ 0:00:20[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.8057, 0.1943]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:12:19 â€¢ 0:00:20[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.8168, 0.1832]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:12:19 â€¢ 0:00:20[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.8224, 0.1776]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:12:19 â€¢ 0:00:20[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.8244, 0.1756]], device='cuda:0', grad_fn=<SoftmaxBackward0>)2m0:12:19 â€¢ 0:00:20[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.5662, 0.4338]], device='cuda:0')m[38;5;237mâ•º[0m 200/203 [2m0:12:19 â€¢ 0:00:20[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.8607, 0.1393],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m 200/203 [2m0:12:19 â€¢ 0:00:20[0m [2;4m0.15it/s[0m  
        [0.7719, 0.2281],
        [0.8250, 0.1750],
        [0.8372, 0.1628],
        [0.8430, 0.1570],
        [0.8794, 0.1206],
        [0.8668, 0.1332],
        [0.9125, 0.0875],
        [0.8525, 0.1475],
[2K        [0.8623, 0.1377]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m 200/203 [2m0:12:19 â€¢ 0:00:20[0m [2;4m0.15it/s[0m  
[2KAttention weights: tensor([[0.5606, 0.4394],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m 201/203 [2m0:12:20 â€¢ 0:00:11[0m [2;4m0.18it/s[0m  
        [0.5547, 0.4453],
        [0.5570, 0.4430],
        [0.5587, 0.4413],
        [0.5572, 0.4428],
        [0.5561, 0.4439],
        [0.5552, 0.4448],
        [0.5599, 0.4401],
        [0.5587, 0.4413],
[2K        [0.5577, 0.4423]], device='cuda:0', grad_fn=<SoftmaxBackward0>)8;6;224mâ•¸[0m 201/203 [2m0:12:20 â€¢ 0:00:11[0m [2;4m0.18it/s[0m  
[2KClass label: CleanAndJerkâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m 201/203 [2m0:12:20 â€¢ 0:00:11[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.5691, 0.4309]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:12:20 â€¢ 0:00:11[0m [2;4m0.18it/s[0m  
[2KStep 0 - TTA Loss: 0.7781â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m 201/203 [2m0:12:20 â€¢ 0:00:11[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.5691, 0.4309]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:12:20 â€¢ 0:00:11[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.5699, 0.4301]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:12:20 â€¢ 0:00:11[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.5728, 0.4272]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:12:20 â€¢ 0:00:11[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.5784, 0.4216]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:12:20 â€¢ 0:00:11[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.5871, 0.4129]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:12:20 â€¢ 0:00:11[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.6006, 0.3994]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:12:20 â€¢ 0:00:11[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.6178, 0.3822]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:12:20 â€¢ 0:00:11[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.6377, 0.3623]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:12:20 â€¢ 0:00:11[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.6586, 0.3414]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:12:20 â€¢ 0:00:11[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.5604, 0.4396]], device='cuda:0')m[38;2;98;6;224mâ•¸[0m 201/203 [2m0:12:20 â€¢ 0:00:11[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.7055, 0.2945],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m 201/203 [2m0:12:20 â€¢ 0:00:11[0m [2;4m0.18it/s[0m  
        [0.6206, 0.3794],
        [0.6761, 0.3239],
        [0.7069, 0.2931],
        [0.6482, 0.3518],
        [0.6919, 0.3081],
        [0.6555, 0.3445],
        [0.7459, 0.2541],
        [0.7041, 0.2959],
[2K        [0.6992, 0.3008]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m 201/203 [2m0:12:20 â€¢ 0:00:11[0m [2;4m0.18it/s[0m  
[2KAttention weights: tensor([[0.5606, 0.4394],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m 202/203 [2m0:12:23 â€¢ 0:00:05[0m [2;4m0.20it/s[0m  
        [0.5547, 0.4453],
        [0.5570, 0.4430],
        [0.5587, 0.4413],
        [0.5572, 0.4428],
        [0.5561, 0.4439],
        [0.5552, 0.4448],
        [0.5599, 0.4401],
        [0.5587, 0.4413],
[2K        [0.5577, 0.4423]], device='cuda:0', grad_fn=<SoftmaxBackward0>)8;6;224mâ•¸[0m 202/203 [2m0:12:23 â€¢ 0:00:05[0m [2;4m0.20it/s[0m  
[2KClass label: HammerThrowâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m 202/203 [2m0:12:23 â€¢ 0:00:05[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.5604, 0.4396]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:12:23 â€¢ 0:00:05[0m [2;4m0.20it/s[0m  
[2KStep 0 - TTA Loss: 0.4353â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m 202/203 [2m0:12:23 â€¢ 0:00:05[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.5956, 0.4044]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:12:23 â€¢ 0:00:05[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.6452, 0.3548]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:12:23 â€¢ 0:00:05[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.6966, 0.3034]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:12:23 â€¢ 0:00:05[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.7412, 0.2588]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:12:23 â€¢ 0:00:05[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.7727, 0.2273]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:12:23 â€¢ 0:00:05[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.7918, 0.2082]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:12:23 â€¢ 0:00:05[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.8025, 0.1975]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:12:23 â€¢ 0:00:05[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.8082, 0.1918]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:12:23 â€¢ 0:00:05[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.8105, 0.1895]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:12:23 â€¢ 0:00:05[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.5658, 0.4342]], device='cuda:0')m[38;2;98;6;224mâ•¸[0m 202/203 [2m0:12:23 â€¢ 0:00:05[0m [2;4m0.20it/s[0m  
[2KAttention weights: tensor([[0.8123, 0.1877],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m 202/203 [2m0:12:23 â€¢ 0:00:05[0m [2;4m0.20it/s[0m  
        [0.6803, 0.3197],
        [0.7643, 0.2357],
        [0.7783, 0.2217],
        [0.8111, 0.1889],
        [0.7936, 0.2064],
        [0.7571, 0.2429],
        [0.8169, 0.1831],
        [0.7843, 0.2157],
[2K        [0.8020, 0.1980]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m 202/203 [2m0:12:23 â€¢ 0:00:05[0m [2;4m0.20it/s[0m  
[2K                   video-id  t-start  t-end     labelâ”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:12:28 â€¢ 0:00:00[0m [2;4m0.19it/s[0m  4m0.20it/s[0m  
0  video_validation_0000365     18.1   24.3  HighJump
1  video_validation_0000365     29.6   33.3  HighJump
2  video_validation_0000365     69.7   77.3  HighJump
3  video_validation_0000365     80.8   84.3  HighJump
[2K4  video_validation_0000365    110.4  116.2  HighJumpâ”â”â”â”â”â”â”[0m 203/203 [2m0:12:28 â€¢ 0:00:00[0m [2;4m0.19it/s[0m  
[2K                   video-id     t-start       t-end      score     label [2m0:12:28 â€¢ 0:00:00[0m [2;4m0.19it/s[0m  
0  video_validation_0000365   18.833333   37.666667  2.1432376  HighJump
1  video_validation_0000365   66.366667   88.866667   2.399738  HighJump
2  video_validation_0000365  111.533333  124.133333  2.0450263  HighJump
3  video_validation_0000365  137.766667  156.000000   1.850547  HighJump
[2K4  video_validation_0000365  156.733333  159.100000   0.760611  HighJump [2m0:12:28 â€¢ 0:00:00[0m [2;4m0.19it/s[0m  
[2KGround truth labels:  ['HighJump' 'PoleVault' 'TennisSwing' 'GolfSwing' 'HammerThrow'â€¢ 0:00:00[0m [2;4m0.19it/s[0m  
[2K 'Billiards' 'BaseballPitch' 'CleanAndJerk' 'ThrowDiscus' 'SoccerPenalty'][2m0:12:28 â€¢ 0:00:00[0m [2;4m0.19it/s[0m  
[2KPredicted labels:  ['HighJump' 'HammerThrow' 'TennisSwing' 'GolfSwing' 'Billiards'28 â€¢ 0:00:00[0m [2;4m0.19it/s[0m  
[2K 'BaseballPitch' 'CleanAndJerk' 'ThrowDiscus' 'SoccerPenalty' 'PoleVault'][2m0:12:28 â€¢ 0:00:00[0m [2;4m0.19it/s[0m  
[2King [38;2;98;6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:12:28 â€¢ 0:00:00[0m [2;4m0.19it/s[0m  
[2King [38;2;98;6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:12:28 â€¢ 0:00:00[0m [2;4m0.19it/s[0m  
[2KBaseballPitch [9.3, 2.1, 0.9, 0.2, 0.1]â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:12:28 â€¢ 0:00:00[0m [2;4m0.19it/s[0m  
[2King [38;2;98;6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:12:28 â€¢ 0:00:00[0m [2;4m0.19it/s[0m  
[2King [38;2;98;6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:12:28 â€¢ 0:00:00[0m [2;4m0.19it/s[0m  
[2KBilliards [6.0, 4.4, 1.8, 1.1, 0.1]â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:12:28 â€¢ 0:00:00[0m [2;4m0.19it/s[0m  
[2King [38;2;98;6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:12:28 â€¢ 0:00:00[0m [2;4m0.19it/s[0m  
[2King [38;2;98;6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:12:28 â€¢ 0:00:00[0m [2;4m0.19it/s[0m  
[2KCleanAndJerk [37.8, 26.7, 15.9, 9.4, 4.6]â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:12:28 â€¢ 0:00:00[0m [2;4m0.19it/s[0m  
[2King [38;2;98;6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:12:28 â€¢ 0:00:00[0m [2;4m0.19it/s[0m  
[2King [38;2;98;6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:12:28 â€¢ 0:00:00[0m [2;4m0.19it/s[0m  
[2KGolfSwing [15.5, 10.0, 5.6, 1.6, 0.5]â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:12:28 â€¢ 0:00:00[0m [2;4m0.19it/s[0m  
[2King [38;2;98;6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:12:28 â€¢ 0:00:00[0m [2;4m0.19it/s[0m  
[2King [38;2;98;6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:12:28 â€¢ 0:00:00[0m [2;4m0.19it/s[0m  
[2KHammerThrow [27.9, 22.4, 16.2, 11.3, 8.4]â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:12:28 â€¢ 0:00:00[0m [2;4m0.19it/s[0m  
[2King [38;2;98;6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:12:28 â€¢ 0:00:00[0m [2;4m0.19it/s[0m  
[2King [38;2;98;6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:12:28 â€¢ 0:00:00[0m [2;4m0.19it/s[0m  
[2KHighJump [30.2, 18.3, 10.3, 5.4, 2.8]â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:12:28 â€¢ 0:00:00[0m [2;4m0.19it/s[0m  
[2King [38;2;98;6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:12:28 â€¢ 0:00:00[0m [2;4m0.19it/s[0m  
[2King [38;2;98;6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:12:28 â€¢ 0:00:00[0m [2;4m0.19it/s[0m  
[2KPoleVault [47.4, 39.8, 28.9, 22.5, 13.0]â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:12:28 â€¢ 0:00:00[0m [2;4m0.19it/s[0m  
[2King [38;2;98;6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:12:28 â€¢ 0:00:00[0m [2;4m0.19it/s[0m  
[2King [38;2;98;6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:12:28 â€¢ 0:00:00[0m [2;4m0.19it/s[0m  
[2KSoccerPenalty [30.3, 13.1, 7.5, 3.1, 1.1]â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:12:28 â€¢ 0:00:00[0m [2;4m0.19it/s[0m  
[2King [38;2;98;6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:12:28 â€¢ 0:00:00[0m [2;4m0.19it/s[0m  
[2King [38;2;98;6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:12:28 â€¢ 0:00:00[0m [2;4m0.19it/s[0m  
[2KTennisSwing [1.6, 0.8, 0.6, 0.2, 0.0]â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:12:28 â€¢ 0:00:00[0m [2;4m0.19it/s[0m  
[2King [38;2;98;6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:12:28 â€¢ 0:00:00[0m [2;4m0.19it/s[0m  
[2King [38;2;98;6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:12:28 â€¢ 0:00:00[0m [2;4m0.19it/s[0m  
[2KThrowDiscus [3.2, 2.1, 1.2, 0.8, 0.3]â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:12:28 â€¢ 0:00:00[0m [2;4m0.19it/s[0m  
[2King [38;2;98;6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:12:28 â€¢ 0:00:00[0m [2;4m0.19it/s[0m  
[2King [38;2;98;6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:12:28 â€¢ 0:00:00[0m [2;4m0.19it/s[0m  
[2K\begin{tabular}{lrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr}
\hline
 Class         &   AP@0% &   AP@0% &     AP@0% &     AP@0% &   AP@0% \\
\hline
 BaseballPitch &    9.3  &     2.1 &  0.9      &  0.2      &     0.1 \\
 Billiards     &    6    &     4.4 &  1.8      &  1.1      &     0.1 \\
 CleanAndJerk  &   37.8  &    26.7 & 15.9      &  9.4      &     4.6 \\
 GolfSwing     &   15.5  &    10   &  5.6      &  1.6      &     0.5 \\
 HammerThrow   &   27.9  &    22.4 & 16.2      & 11.3      &     8.4 \\
 HighJump      &   30.2  &    18.3 & 10.3      &  5.4      &     2.8 \\
 PoleVault     &   47.4  &    39.8 & 28.9      & 22.5      &    13   \\
 SoccerPenalty &   30.3  &    13.1 &  7.5      &  3.1      &     1.1 \\
 TennisSwing   &    1.6  &     0.8 &  0.6      &  0.2      &     0   \\
 ThrowDiscus   &    3.2  &     2.1 &  1.2      &  0.8      &     0.3 \\
 IoU           &    0.39 &     0   &  0.266067 &  0.419048 &     0   \\
\hline
[2K\end{tabular};6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:12:28 â€¢ 0:00:00[0m [2;4m0.19it/s[0m  
[2K[20.92 13.97  8.89  5.56  3.09]â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:12:28 â€¢ 0:00:00[0m [2;4m0.19it/s[0m  
[2KAverage TIOU:  0.010022553699136634â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:12:28 â€¢ 0:00:00[0m [2;4m0.19it/s[0m  
[2Kpreds_tensor: torch.Size([203, 10])â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:12:28 â€¢ 0:00:00[0m [2;4m0.19it/s[0m  
[2KTop-1 accuracy: 0.8374384236453202â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:12:28 â€¢ 0:00:00[0m [2;4m0.19it/s[0m  
[2Kpreds_tensor: torch.Size([203, 10])â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:12:28 â€¢ 0:00:00[0m [2;4m0.19it/s[0m  
[2KTop-3 accuracy: 0.9507389162561576â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:12:28 â€¢ 0:00:00[0m [2;4m0.19it/s[0m  
[2Kpreds_tensor: torch.Size([203, 10])â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:12:28 â€¢ 0:00:00[0m [2;4m0.19it/s[0m  
[2KTop-5 accuracy: 0.9950738916256158â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:12:28 â€¢ 0:00:00[0m [2;4m0.19it/s[0m  
[2Kâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“â”â”â”[0m 203/203 [2m0:12:28 â€¢ 0:00:00[0m [2;4m0.19it/s[0m  
â”ƒ[1m [0m[1m       Test metric       [0m[1m [0mâ”ƒ[1m [0m[1m      DataLoader 0       [0m[1m [0mâ”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚[36m [0m[36m          AP_0           [0m[36m [0mâ”‚[35m [0m[35m   20.920000076293945    [0m[35m [0mâ”‚
â”‚[36m [0m[36m         avg_AP          [0m[36m [0mâ”‚[35m [0m[35m   10.486000061035156    [0m[35m [0mâ”‚
â”‚[36m [0m[36m  label top-1 accuracy   [0m[36m [0mâ”‚[35m [0m[35m   0.8374384045600891    [0m[35m [0mâ”‚
â”‚[36m [0m[36m  label top-3 accuracy   [0m[36m [0mâ”‚[35m [0m[35m   0.9507389068603516    [0m[35m [0mâ”‚
â”‚[36m [0m[36m  label top-5 accuracy   [0m[36m [0mâ”‚[35m [0m[35m   0.9950739145278931    [0m[35m [0mâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
[2KTesting [38;2;98;6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 203/203 [2m0:12:28 â€¢ 0:00:00[0m [2;4m0.19it/s[0m
[?25h[[36m2025-02-14 10:00:38,620[0m][[34m__main__[0m][[32mINFO[0m] - Best ckpt path: None[0m
[[36m2025-02-14 10:00:38,632[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/def/fewshot/logs/train/runs/2025-02-14_09-47-47[0m
[[36m2025-02-14 10:00:38,632[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
