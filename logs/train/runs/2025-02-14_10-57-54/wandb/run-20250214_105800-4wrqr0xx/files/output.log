[[36m2025-02-14 10:58:01,519[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
You are using a CUDA device ('NVIDIA GeForce RTX 4070 SUPER') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
No of videos in train is 214
Loading train Video Information ...
No of class 10
No of videos in validation is 203
Loading validation Video Information ...
No of class 10
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”“
â”ƒ[1;35m [0m[1;35m   [0m[1;35m [0mâ”ƒ[1;35m [0m[1;35mName                                                   [0m[1;35m [0mâ”ƒ[1;35m [0m[1;35mType                           [0m[1;35m [0mâ”ƒ[1;35m [0m[1;35mParams[0m[1;35m [0mâ”ƒ[1;35m [0m[1;35mMode [0m[1;35m [0mâ”ƒ
â”¡â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”©
â”‚[2m [0m[2m0  [0m[2m [0mâ”‚ net                                                     â”‚ T3ALNet                         â”‚  639 M â”‚ train â”‚
â”‚[2m [0m[2m1  [0m[2m [0mâ”‚ net.model                                               â”‚ CoCa                            â”‚  638 M â”‚ train â”‚
â”‚[2m [0m[2m2  [0m[2m [0mâ”‚ net.model.text                                          â”‚ TextTransformer                 â”‚  123 M â”‚ train â”‚
â”‚[2m [0m[2m3  [0m[2m [0mâ”‚ net.model.text.token_embedding                          â”‚ Embedding                       â”‚ 37.9 M â”‚ train â”‚
â”‚[2m [0m[2m4  [0m[2m [0mâ”‚ net.model.text.transformer                              â”‚ Transformer                     â”‚ 85.1 M â”‚ train â”‚
â”‚[2m [0m[2m5  [0m[2m [0mâ”‚ net.model.text.transformer.resblocks                    â”‚ ModuleList                      â”‚ 85.1 M â”‚ train â”‚
â”‚[2m [0m[2m6  [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m7  [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m8  [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m9  [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m10 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m11 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m12 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m13 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m14 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m15 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m16 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m17 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m18 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m19 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m20 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m21 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m22 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m23 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m24 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m25 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m26 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m27 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m28 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m29 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m30 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m31 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m32 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m33 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m34 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m35 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m36 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m37 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m38 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m39 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m40 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m41 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m42 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m43 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m44 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m45 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m46 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m47 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m48 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m49 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m50 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m51 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m52 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m53 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m54 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m55 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m56 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m57 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m58 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m59 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m60 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m61 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m62 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m63 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m64 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m65 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m66 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m67 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m68 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m69 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m70 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m71 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m72 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m73 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m74 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m75 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m76 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m77 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m78 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m79 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m80 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m81 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m82 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m83 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m84 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m85 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m86 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m87 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m88 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m89 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m90 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m91 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m92 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m93 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m94 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m95 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m96 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m97 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m98 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m99 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m100[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m101[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m102[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m103[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m104[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m105[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m106[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m107[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m108[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m109[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m110[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m111[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m112[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m113[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m114[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m115[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m116[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10                 â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m117[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.ln_1            â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m118[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.attn            â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m119[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.attn.out_proj   â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m120[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.ls_1            â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m121[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.ln_2            â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m122[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.mlp             â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m123[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.mlp.c_fc        â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m124[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.mlp.gelu        â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m125[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.mlp.c_proj      â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m126[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.ls_2            â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m127[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11                 â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m128[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.ln_1            â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m129[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.attn            â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m130[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.attn.out_proj   â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m131[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.ls_1            â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m132[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.ln_2            â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m133[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.mlp             â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m134[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.mlp.c_fc        â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m135[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.mlp.gelu        â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m136[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.mlp.c_proj      â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m137[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.ls_2            â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m138[0m[2m [0mâ”‚ net.model.text.ln_final                                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m139[0m[2m [0mâ”‚ net.model.visual                                        â”‚ VisionTransformer               â”‚  306 M â”‚ train â”‚
â”‚[2m [0m[2m140[0m[2m [0mâ”‚ net.model.visual.conv1                                  â”‚ Conv2d                          â”‚  602 K â”‚ train â”‚
â”‚[2m [0m[2m141[0m[2m [0mâ”‚ net.model.visual.patch_dropout                          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m142[0m[2m [0mâ”‚ net.model.visual.ln_pre                                 â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m143[0m[2m [0mâ”‚ net.model.visual.transformer                            â”‚ Transformer                     â”‚  302 M â”‚ train â”‚
â”‚[2m [0m[2m144[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks                  â”‚ ModuleList                      â”‚  302 M â”‚ train â”‚
â”‚[2m [0m[2m145[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m146[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m147[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m148[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m149[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m150[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m151[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m152[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m153[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m154[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m155[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m156[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m157[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m158[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m159[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m160[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m161[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m162[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m163[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m164[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m165[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m166[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m167[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m168[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m169[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m170[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m171[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m172[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m173[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m174[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m175[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m176[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m177[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m178[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m179[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m180[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m181[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m182[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m183[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m184[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m185[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m186[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m187[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m188[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m189[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m190[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m191[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m192[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m193[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m194[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m195[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m196[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m197[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m198[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m199[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m200[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m201[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m202[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m203[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m204[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m205[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m206[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m207[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m208[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m209[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m210[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m211[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m212[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m213[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m214[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m215[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m216[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m217[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m218[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m219[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m220[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m221[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m222[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m223[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m224[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m225[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m226[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m227[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m228[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m229[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m230[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m231[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m232[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m233[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m234[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m235[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m236[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m237[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m238[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m239[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m240[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m241[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m242[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m243[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m244[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m245[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m246[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m247[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m248[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m249[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m250[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m251[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m252[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m253[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m254[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m255[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m256[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m257[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m258[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m259[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m260[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m261[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m262[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m263[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m264[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m265[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m266[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m267[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m268[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m269[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m270[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m271[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m272[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m273[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m274[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m275[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m276[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m277[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m278[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m279[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m280[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m281[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m282[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m283[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m284[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m285[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m286[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m287[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m288[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m289[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m290[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m291[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m292[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m293[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m294[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m295[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m296[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m297[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m298[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m299[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m300[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m301[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m302[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m303[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m304[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m305[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m306[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m307[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m308[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m309[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m310[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m311[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m312[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m313[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m314[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m315[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m316[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m317[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m318[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m319[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m320[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m321[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m322[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m323[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m324[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m325[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m326[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m327[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m328[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m329[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m330[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m331[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m332[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m333[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m334[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m335[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m336[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m337[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m338[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m339[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m340[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m341[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m342[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m343[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m344[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m345[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m346[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m347[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m348[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m349[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m350[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m351[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m352[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m353[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m354[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m355[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m356[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m357[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m358[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m359[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m360[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m361[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m362[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m363[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m364[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m365[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m366[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m367[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m368[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m369[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m370[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m371[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m372[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m373[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m374[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m375[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m376[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m377[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m378[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m379[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m380[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m381[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m382[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m383[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m384[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m385[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m386[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m387[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m388[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m389[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m390[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m391[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m392[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m393[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m394[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m395[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m396[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m397[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m398[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m399[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m400[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m401[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m402[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m403[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m404[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m405[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m406[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m407[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m408[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m409[0m[2m [0mâ”‚ net.model.visual.attn_pool                              â”‚ AttentionalPooler               â”‚  3.0 M â”‚ train â”‚
â”‚[2m [0m[2m410[0m[2m [0mâ”‚ net.model.visual.attn_pool.attn                         â”‚ MultiheadAttention              â”‚  2.8 M â”‚ train â”‚
â”‚[2m [0m[2m411[0m[2m [0mâ”‚ net.model.visual.attn_pool.attn.out_proj                â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m412[0m[2m [0mâ”‚ net.model.visual.attn_pool.ln_q                         â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m413[0m[2m [0mâ”‚ net.model.visual.attn_pool.ln_k                         â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m414[0m[2m [0mâ”‚ net.model.visual.ln_post                                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m415[0m[2m [0mâ”‚ net.model.text_decoder                                  â”‚ MultimodalTransformer           â”‚  208 M â”‚ train â”‚
â”‚[2m [0m[2m416[0m[2m [0mâ”‚ net.model.text_decoder.resblocks                        â”‚ ModuleList                      â”‚ 85.1 M â”‚ train â”‚
â”‚[2m [0m[2m417[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m418[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m419[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m420[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m421[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m422[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m423[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m424[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m425[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m426[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m427[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m428[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m429[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m430[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m431[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m432[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m433[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m434[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m435[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m436[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m437[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m438[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m439[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m440[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m441[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m442[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m443[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m444[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m445[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m446[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m447[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m448[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m449[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m450[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m451[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m452[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m453[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m454[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m455[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m456[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m457[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m458[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m459[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m460[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m461[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m462[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m463[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m464[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m465[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m466[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m467[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m468[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m469[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m470[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m471[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m472[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m473[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m474[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m475[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m476[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m477[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m478[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m479[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m480[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m481[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m482[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m483[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m484[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m485[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m486[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m487[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m488[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m489[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m490[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m491[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m492[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m493[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m494[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m495[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m496[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m497[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m498[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m499[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m500[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m501[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m502[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m503[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m504[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m505[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m506[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m507[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m508[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m509[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m510[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m511[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m512[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m513[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m514[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m515[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m516[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m517[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m518[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m519[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m520[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m521[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m522[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m523[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m524[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m525[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m526[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m527[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m528[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m529[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m530[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m531[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m532[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m533[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m534[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m535[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m536[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m537[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m538[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m539[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m540[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m541[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m542[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m543[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m544[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m545[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m546[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m547[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m548[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m549[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn                       â”‚ ModuleList                      â”‚ 85.1 M â”‚ train â”‚
â”‚[2m [0m[2m550[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m551[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m552[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m553[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m554[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m555[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m556[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m557[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m558[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m559[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m560[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m561[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m562[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m563[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m564[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m565[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m566[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m567[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m568[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m569[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m570[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m571[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m572[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m573[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m574[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m575[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m576[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m577[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m578[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m579[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m580[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m581[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m582[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m583[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m584[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m585[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m586[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m587[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m588[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m589[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m590[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m591[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m592[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m593[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m594[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m595[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m596[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m597[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m598[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m599[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m600[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m601[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m602[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m603[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m604[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m605[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m606[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m607[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m608[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m609[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m610[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m611[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m612[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m613[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m614[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m615[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m616[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m617[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m618[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m619[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m620[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m621[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m622[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m623[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m624[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m625[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m626[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m627[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m628[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m629[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m630[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m631[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m632[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m633[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m634[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m635[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m636[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m637[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m638[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m639[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m640[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m641[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m642[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m643[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m644[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m645[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m646[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m647[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m648[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m649[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m650[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m651[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m652[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m653[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m654[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m655[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m656[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m657[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m658[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m659[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m660[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m661[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m662[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m663[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m664[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m665[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m666[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m667[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m668[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m669[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m670[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10                    â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m671[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.ln_1               â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m672[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.attn               â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m673[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.attn.out_proj      â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m674[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.ls_1               â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m675[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.ln_1_kv            â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m676[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.ln_2               â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m677[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.mlp                â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m678[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.mlp.c_fc           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m679[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.mlp.gelu           â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m680[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.mlp.c_proj         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m681[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.ls_2               â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m682[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11                    â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m683[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.ln_1               â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m684[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.attn               â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m685[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.attn.out_proj      â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m686[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.ls_1               â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m687[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.ln_1_kv            â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m688[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.ln_2               â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m689[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.mlp                â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m690[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.mlp.c_fc           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m691[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.mlp.gelu           â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m692[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.mlp.c_proj         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m693[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.ls_2               â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m694[0m[2m [0mâ”‚ net.model.text_decoder.ln_final                         â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m695[0m[2m [0mâ”‚ net.tta_loss                                            â”‚ ByolLoss                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m696[0m[2m [0mâ”‚ net.video_proj                                          â”‚ VideoProjector                  â”‚  591 K â”‚ train â”‚
â”‚[2m [0m[2m697[0m[2m [0mâ”‚ net.video_proj.transform                                â”‚ Sequential                      â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m698[0m[2m [0mâ”‚ net.video_proj.transform.0                              â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m699[0m[2m [0mâ”‚ net.video_proj.transform.1                              â”‚ Dropout                         â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m700[0m[2m [0mâ”‚ net.visual_proj                                         â”‚ VideoProjector                  â”‚  591 K â”‚ train â”‚
â”‚[2m [0m[2m701[0m[2m [0mâ”‚ net.visual_proj.transform                               â”‚ Sequential                      â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m702[0m[2m [0mâ”‚ net.visual_proj.transform.0                             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m703[0m[2m [0mâ”‚ net.visual_proj.transform.1                             â”‚ Dropout                         â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m704[0m[2m [0mâ”‚ net.fusion                                              â”‚ Fusion                          â”‚  6.2 K â”‚ train â”‚
â”‚[2m [0m[2m705[0m[2m [0mâ”‚ net.fusion.attn                                         â”‚ Sequential                      â”‚  6.2 K â”‚ train â”‚
â”‚[2m [0m[2m706[0m[2m [0mâ”‚ net.fusion.attn.0                                       â”‚ Linear                          â”‚  6.1 K â”‚ train â”‚
â”‚[2m [0m[2m707[0m[2m [0mâ”‚ net.fusion.attn.1                                       â”‚ Linear                          â”‚     10 â”‚ train â”‚
â”‚[2m [0m[2m708[0m[2m [0mâ”‚ net.fusion.attn.2                                       â”‚ Softmax                         â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m709[0m[2m [0mâ”‚ binary_acc                                              â”‚ BinaryAccuracy                  â”‚      0 â”‚ train â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜
[1mTrainable params[0m: 1.2 M
[1mNon-trainable params[0m: 638 M
[1mTotal params[0m: 639 M
[1mTotal estimated model params size (MB)[0m: 2.6 K
[1mModules in train mode[0m: 710
[1mModules in eval mode[0m: 0
/home/def/miniforge3/envs/fewshot/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.
`Trainer.fit` stopped: `max_epochs=0` reached.
[[36m2025-02-14 10:58:03,145[0m][[34m__main__[0m][[32mINFO[0m] - Starting testing![0m
[[36m2025-02-14 10:58:03,145[0m][[34m__main__[0m][[33mWARNING[0m] - Best ckpt not found! Using current weights for testing...[0m
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/def/miniforge3/envs/fewshot/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.
[2KStart testing...
[2KAttention weights: tensor([[0.5204, 0.4796],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 0/203 [2m0:00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
        [0.5123, 0.4877],
        [0.5140, 0.4860],
        [0.5197, 0.4803],
        [0.5214, 0.4786],
        [0.5241, 0.4759],
        [0.5214, 0.4786],
        [0.5185, 0.4815],
        [0.5154, 0.4846],
[2K        [0.5216, 0.4784]], device='cuda:0', grad_fn=<SoftmaxBackward0>):00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2K/home/def/fewshot/src/models/components/tt_method.py:319: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or
`x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3683.)
  dot_product = (x @ y.T)
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 0/203 [2m0:00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.6850, 0.3150]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KStep 0 - TTA Loss: 1.2758â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 0/203 [2m0:00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.7505, 0.2495]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.7371, 0.2629]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.7367, 0.2633]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.7245, 0.2755]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.7219, 0.2781]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.7213, 0.2787]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.7210, 0.2790]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.7206, 0.2794]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.7204, 0.2796]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5236, 0.4764]], device='cuda:0')203 [2m0:00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.6113, 0.3887],â”â”â”â”â”â”â”â”â”â”â”[0m 0/203 [2m0:00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
        [0.4476, 0.5524],
        [0.5013, 0.4987],
        [0.6151, 0.3849],
        [0.6023, 0.3977],
        [0.7211, 0.2789],
        [0.6703, 0.3297],
        [0.6185, 0.3815],
        [0.5317, 0.4683],
[2K        [0.6104, 0.3896]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”[0m 0/203 [2m0:00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5203, 0.4797],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 1/203 [2m0:00:01 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
        [0.5122, 0.4878],
        [0.5139, 0.4861],
        [0.5196, 0.4804],
        [0.5213, 0.4787],
        [0.5240, 0.4760],
        [0.5213, 0.4787],
        [0.5184, 0.4816],
        [0.5153, 0.4847],
[2K        [0.5215, 0.4785]], device='cuda:0', grad_fn=<SoftmaxBackward0>):00:01 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 1/203 [2m0:00:01 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.6825, 0.3175]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KStep 0 - TTA Loss: 1.7808â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 1/203 [2m0:00:01 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.6825, 0.3175]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.6826, 0.3174]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.6831, 0.3169]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.6872, 0.3128]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.6984, 0.3016]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.7167, 0.2833]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.7514, 0.2486]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.7939, 0.2061]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.8348, 0.1652]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5309, 0.4691]], device='cuda:0')203 [2m0:00:01 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.6935, 0.3065],â”â”â”â”â”â”â”â”â”â”â”[0m 1/203 [2m0:00:01 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
        [0.4602, 0.5398],
        [0.5393, 0.4607],
        [0.6813, 0.3187],
        [0.7369, 0.2631],
        [0.8683, 0.1317],
        [0.7945, 0.2055],
        [0.6958, 0.3042],
        [0.6189, 0.3811],
[2K        [0.7503, 0.2497]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”[0m 1/203 [2m0:00:01 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5203, 0.4797],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 2/203 [2m0:00:01 â€¢ 0:01:14[0m [2;4m2.75it/s[0m  
        [0.5122, 0.4878],
        [0.5140, 0.4860],
        [0.5196, 0.4804],
        [0.5213, 0.4787],
        [0.5240, 0.4760],
        [0.5214, 0.4786],
        [0.5184, 0.4816],
        [0.5153, 0.4847],
[2K        [0.5215, 0.4785]], device='cuda:0', grad_fn=<SoftmaxBackward0>):00:01 â€¢ 0:01:14[0m [2;4m2.75it/s[0m  
[2KClass label: TennisSwingâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 2/203 [2m0:00:01 â€¢ 0:01:14[0m [2;4m2.75it/s[0m  
[2KAttention weights: tensor([[0.5189, 0.4811]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m2.75it/s[0m  
[2KStep 0 - TTA Loss: 1.5833â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 2/203 [2m0:00:01 â€¢ 0:01:14[0m [2;4m2.75it/s[0m  
[2KAttention weights: tensor([[0.5301, 0.4699]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m2.75it/s[0m  
[2KAttention weights: tensor([[0.5390, 0.4610]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m2.75it/s[0m  
[2KAttention weights: tensor([[0.5466, 0.4534]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m2.75it/s[0m  
[2KAttention weights: tensor([[0.5547, 0.4453]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m2.75it/s[0m  
[2KAttention weights: tensor([[0.5612, 0.4388]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m2.75it/s[0m  
[2KAttention weights: tensor([[0.5689, 0.4311]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m2.75it/s[0m  
[2KAttention weights: tensor([[0.5775, 0.4225]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m2.75it/s[0m  
[2KAttention weights: tensor([[0.5816, 0.4184]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m2.75it/s[0m  
[2KAttention weights: tensor([[0.5833, 0.4167]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m2.75it/s[0m  
[2KAttention weights: tensor([[0.5173, 0.4827]], device='cuda:0')203 [2m0:00:01 â€¢ 0:01:14[0m [2;4m2.75it/s[0m  
[2KAttention weights: tensor([[0.6802, 0.3198],â”â”â”â”â”â”â”â”â”â”â”[0m 2/203 [2m0:00:01 â€¢ 0:01:14[0m [2;4m2.75it/s[0m  
        [0.4528, 0.5472],
        [0.5314, 0.4686],
        [0.6700, 0.3300],
        [0.7171, 0.2829],
        [0.8537, 0.1463],
        [0.7819, 0.2181],
        [0.6845, 0.3155],
        [0.5838, 0.4162],
[2K        [0.7284, 0.2716]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”[0m 2/203 [2m0:00:01 â€¢ 0:01:14[0m [2;4m2.75it/s[0m  
[2KAttention weights: tensor([[0.5204, 0.4796],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 3/203 [2m0:00:02 â€¢ 0:01:21[0m [2;4m2.48it/s[0m  
        [0.5123, 0.4877],
        [0.5140, 0.4860],
        [0.5197, 0.4803],
        [0.5214, 0.4786],
        [0.5241, 0.4759],
        [0.5215, 0.4785],
        [0.5185, 0.4815],
        [0.5154, 0.4846],
[2K        [0.5216, 0.4784]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”[0m 3/203 [2m0:00:02 â€¢ 0:01:21[0m [2;4m2.48it/s[0m  
[2KClass label: HighJump[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 3/203 [2m0:00:02 â€¢ 0:01:21[0m [2;4m2.48it/s[0m  
[2KAttention weights: tensor([[0.6886, 0.3114]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:02 â€¢ 0:01:21[0m [2;4m2.48it/s[0m  
[2KStep 0 - TTA Loss: 1.2512[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 3/203 [2m0:00:02 â€¢ 0:01:21[0m [2;4m2.48it/s[0m  
[2KAttention weights: tensor([[0.6886, 0.3114]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:02 â€¢ 0:01:21[0m [2;4m2.48it/s[0m  
[2KAttention weights: tensor([[0.6890, 0.3110]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:02 â€¢ 0:01:21[0m [2;4m2.48it/s[0m  
[2KAttention weights: tensor([[0.6904, 0.3096]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:02 â€¢ 0:01:21[0m [2;4m2.48it/s[0m  
[2KAttention weights: tensor([[0.6926, 0.3074]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:02 â€¢ 0:01:21[0m [2;4m2.48it/s[0m  
[2KAttention weights: tensor([[0.6962, 0.3038]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:02 â€¢ 0:01:21[0m [2;4m2.48it/s[0m  
[2KAttention weights: tensor([[0.7009, 0.2991]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:02 â€¢ 0:01:21[0m [2;4m2.48it/s[0m  
[2KAttention weights: tensor([[0.7068, 0.2932]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:02 â€¢ 0:01:21[0m [2;4m2.48it/s[0m  
[2KAttention weights: tensor([[0.7206, 0.2794]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:02 â€¢ 0:01:21[0m [2;4m2.48it/s[0m  
[2KAttention weights: tensor([[0.7391, 0.2609]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:02 â€¢ 0:01:21[0m [2;4m2.48it/s[0m  
[2KAttention weights: tensor([[0.5277, 0.4723]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 3/203 [2m0:00:02 â€¢ 0:01:21[0m [2;4m2.48it/s[0m  
[2KAttention weights: tensor([[0.6332, 0.3668],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 3/203 [2m0:00:02 â€¢ 0:01:21[0m [2;4m2.48it/s[0m  
        [0.4508, 0.5492],
        [0.5085, 0.4915],
        [0.6301, 0.3699],
        [0.6349, 0.3651],
        [0.7602, 0.2398],
        [0.6965, 0.3035],
        [0.6354, 0.3646],
        [0.5639, 0.4361],
[2K        [0.6450, 0.3550]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 3/203 [2m0:00:02 â€¢ 0:01:21[0m [2;4m2.48it/s[0m  
[2KAttention weights: tensor([[0.5204, 0.4796],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 4/203 [2m0:00:03 â€¢ 0:02:18[0m [2;4m1.45it/s[0m  
        [0.5123, 0.4877],
        [0.5140, 0.4860],
        [0.5197, 0.4803],
        [0.5214, 0.4786],
        [0.5241, 0.4759],
        [0.5215, 0.4785],
        [0.5185, 0.4815],
        [0.5154, 0.4846],
[2K        [0.5216, 0.4784]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”[0m 4/203 [2m0:00:03 â€¢ 0:02:18[0m [2;4m1.45it/s[0m  
[2KClass label: GolfSwing[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 4/203 [2m0:00:03 â€¢ 0:02:18[0m [2;4m1.45it/s[0m  
[2KAttention weights: tensor([[0.5972, 0.4028]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:03 â€¢ 0:02:18[0m [2;4m1.45it/s[0m  
[2KStep 0 - TTA Loss: 1.1763[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 4/203 [2m0:00:03 â€¢ 0:02:18[0m [2;4m1.45it/s[0m  
[2KAttention weights: tensor([[0.6039, 0.3961]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:03 â€¢ 0:02:18[0m [2;4m1.45it/s[0m  
[2KAttention weights: tensor([[0.6116, 0.3884]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:03 â€¢ 0:02:18[0m [2;4m1.45it/s[0m  
[2KAttention weights: tensor([[0.6234, 0.3766]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:03 â€¢ 0:02:18[0m [2;4m1.45it/s[0m  
[2KAttention weights: tensor([[0.6300, 0.3700]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:03 â€¢ 0:02:18[0m [2;4m1.45it/s[0m  
[2KAttention weights: tensor([[0.6359, 0.3641]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:03 â€¢ 0:02:18[0m [2;4m1.45it/s[0m  
[2KAttention weights: tensor([[0.6404, 0.3596]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:03 â€¢ 0:02:18[0m [2;4m1.45it/s[0m  
[2KAttention weights: tensor([[0.6435, 0.3565]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:03 â€¢ 0:02:18[0m [2;4m1.45it/s[0m  
[2KAttention weights: tensor([[0.6451, 0.3549]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:03 â€¢ 0:02:18[0m [2;4m1.45it/s[0m  
[2KAttention weights: tensor([[0.6457, 0.3543]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:03 â€¢ 0:02:18[0m [2;4m1.45it/s[0m  
[2KAttention weights: tensor([[0.5220, 0.4780]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 4/203 [2m0:00:03 â€¢ 0:02:18[0m [2;4m1.45it/s[0m  
[2KAttention weights: tensor([[0.6411, 0.3589],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 4/203 [2m0:00:03 â€¢ 0:02:18[0m [2;4m1.45it/s[0m  
        [0.4562, 0.5438],
        [0.5094, 0.4906],
        [0.6462, 0.3538],
        [0.6420, 0.3580],
        [0.7716, 0.2284],
        [0.7086, 0.2914],
        [0.6421, 0.3579],
        [0.5604, 0.4396],
[2K        [0.6556, 0.3444]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 4/203 [2m0:00:03 â€¢ 0:02:18[0m [2;4m1.45it/s[0m  
[2KAttention weights: tensor([[0.5204, 0.4796],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 5/203 [2m0:00:03 â€¢ 0:02:03[0m [2;4m1.62it/s[0m  
        [0.5123, 0.4877],
        [0.5140, 0.4860],
        [0.5197, 0.4803],
        [0.5214, 0.4786],
        [0.5241, 0.4759],
        [0.5215, 0.4785],
        [0.5185, 0.4815],
        [0.5154, 0.4846],
[2K        [0.5216, 0.4784]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”[0m 5/203 [2m0:00:03 â€¢ 0:02:03[0m [2;4m1.62it/s[0m  
[2KClass label: HammerThrowm[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 5/203 [2m0:00:03 â€¢ 0:02:03[0m [2;4m1.62it/s[0m  
[2KAttention weights: tensor([[0.5782, 0.4218]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:03 â€¢ 0:02:03[0m [2;4m1.62it/s[0m  
[2KStep 0 - TTA Loss: 1.3421[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 5/203 [2m0:00:03 â€¢ 0:02:03[0m [2;4m1.62it/s[0m  
[2KAttention weights: tensor([[0.5782, 0.4218]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:03 â€¢ 0:02:03[0m [2;4m1.62it/s[0m  
[2KAttention weights: tensor([[0.5787, 0.4213]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:03 â€¢ 0:02:03[0m [2;4m1.62it/s[0m  
[2KAttention weights: tensor([[0.5816, 0.4184]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:03 â€¢ 0:02:03[0m [2;4m1.62it/s[0m  
[2KAttention weights: tensor([[0.5873, 0.4127]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:03 â€¢ 0:02:03[0m [2;4m1.62it/s[0m  
[2KAttention weights: tensor([[0.5975, 0.4025]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:03 â€¢ 0:02:03[0m [2;4m1.62it/s[0m  
[2KAttention weights: tensor([[0.6147, 0.3853]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:03 â€¢ 0:02:03[0m [2;4m1.62it/s[0m  
[2KAttention weights: tensor([[0.6343, 0.3657]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:03 â€¢ 0:02:03[0m [2;4m1.62it/s[0m  
[2KAttention weights: tensor([[0.6580, 0.3420]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:03 â€¢ 0:02:03[0m [2;4m1.62it/s[0m  
[2KAttention weights: tensor([[0.6841, 0.3159]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:03 â€¢ 0:02:03[0m [2;4m1.62it/s[0m  
[2KAttention weights: tensor([[0.5252, 0.4748]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 5/203 [2m0:00:03 â€¢ 0:02:03[0m [2;4m1.62it/s[0m  
[2KAttention weights: tensor([[0.6738, 0.3262],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 5/203 [2m0:00:03 â€¢ 0:02:03[0m [2;4m1.62it/s[0m  
        [0.4619, 0.5381],
        [0.5258, 0.4742],
        [0.6671, 0.3329],
        [0.7138, 0.2862],
        [0.7534, 0.2466],
        [0.7067, 0.2933],
        [0.6406, 0.3594],
        [0.5556, 0.4444],
[2K        [0.6935, 0.3065]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 5/203 [2m0:00:03 â€¢ 0:02:03[0m [2;4m1.62it/s[0m  
[2KAttention weights: tensor([[0.5204, 0.4796],m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:04 â€¢ 0:02:00[0m [2;4m1.65it/s[0m  
        [0.5123, 0.4877],
        [0.5140, 0.4860],
        [0.5197, 0.4803],
        [0.5214, 0.4786],
        [0.5241, 0.4759],
        [0.5215, 0.4785],
        [0.5185, 0.4815],
        [0.5154, 0.4846],
[2K        [0.5216, 0.4784]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:04 â€¢ 0:02:00[0m [2;4m1.65it/s[0m  
[2KClass label: HammerThrowm[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:04 â€¢ 0:02:00[0m [2;4m1.65it/s[0m  
[2KAttention weights: tensor([[0.5722, 0.4278]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:04 â€¢ 0:02:00[0m [2;4m1.65it/s[0m  
[2KStep 0 - TTA Loss: 0.9956[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:04 â€¢ 0:02:00[0m [2;4m1.65it/s[0m  
[2KAttention weights: tensor([[0.5952, 0.4048]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:04 â€¢ 0:02:00[0m [2;4m1.65it/s[0m  
[2KAttention weights: tensor([[0.6133, 0.3867]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:04 â€¢ 0:02:00[0m [2;4m1.65it/s[0m  
[2KAttention weights: tensor([[0.6300, 0.3700]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:04 â€¢ 0:02:00[0m [2;4m1.65it/s[0m  
[2KAttention weights: tensor([[0.6488, 0.3512]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:04 â€¢ 0:02:00[0m [2;4m1.65it/s[0m  
[2KAttention weights: tensor([[0.6657, 0.3343]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:04 â€¢ 0:02:00[0m [2;4m1.65it/s[0m  
[2KAttention weights: tensor([[0.6790, 0.3210]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:04 â€¢ 0:02:00[0m [2;4m1.65it/s[0m  
[2KAttention weights: tensor([[0.6880, 0.3120]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:04 â€¢ 0:02:00[0m [2;4m1.65it/s[0m  
[2KAttention weights: tensor([[0.6921, 0.3079]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:04 â€¢ 0:02:00[0m [2;4m1.65it/s[0m  
[2KAttention weights: tensor([[0.6938, 0.3062]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:04 â€¢ 0:02:00[0m [2;4m1.65it/s[0m  
[2KAttention weights: tensor([[0.5247, 0.4753]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:04 â€¢ 0:02:00[0m [2;4m1.65it/s[0m  
[2KAttention weights: tensor([[0.6645, 0.3355],8;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:04 â€¢ 0:02:00[0m [2;4m1.65it/s[0m  
        [0.4595, 0.5405],
        [0.5258, 0.4742],
        [0.6527, 0.3473],
        [0.6942, 0.3058],
        [0.7413, 0.2587],
        [0.6952, 0.3048],
        [0.6343, 0.3657],
        [0.5491, 0.4509],
[2K        [0.6787, 0.3213]], device='cuda:0')38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:04 â€¢ 0:02:00[0m [2;4m1.65it/s[0m  
[2KAttention weights: tensor([[0.5204, 0.4796],m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:05 â€¢ 0:02:21[0m [2;4m1.40it/s[0m  
        [0.5123, 0.4877],
        [0.5141, 0.4859],
        [0.5197, 0.4803],
        [0.5214, 0.4786],
        [0.5241, 0.4759],
        [0.5215, 0.4785],
        [0.5185, 0.4815],
        [0.5154, 0.4846],
[2K        [0.5216, 0.4784]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:05 â€¢ 0:02:21[0m [2;4m1.40it/s[0m  
[2KClass label: Billiards[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:05 â€¢ 0:02:21[0m [2;4m1.40it/s[0m  
[2KAttention weights: tensor([[0.4434, 0.5566]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:05 â€¢ 0:02:21[0m [2;4m1.40it/s[0m  
[2KStep 0 - TTA Loss: 0.7608[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:05 â€¢ 0:02:21[0m [2;4m1.40it/s[0m  
[2KAttention weights: tensor([[0.4434, 0.5566]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:05 â€¢ 0:02:21[0m [2;4m1.40it/s[0m  
[2KAttention weights: tensor([[0.4433, 0.5567]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:05 â€¢ 0:02:21[0m [2;4m1.40it/s[0m  
[2KAttention weights: tensor([[0.4431, 0.5569]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:05 â€¢ 0:02:21[0m [2;4m1.40it/s[0m  
[2KAttention weights: tensor([[0.4423, 0.5577]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:05 â€¢ 0:02:21[0m [2;4m1.40it/s[0m  
[2KAttention weights: tensor([[0.4400, 0.5600]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:05 â€¢ 0:02:21[0m [2;4m1.40it/s[0m  
[2KAttention weights: tensor([[0.4356, 0.5644]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:05 â€¢ 0:02:21[0m [2;4m1.40it/s[0m  
[2KAttention weights: tensor([[0.4318, 0.5682]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:05 â€¢ 0:02:21[0m [2;4m1.40it/s[0m  
[2KAttention weights: tensor([[0.4237, 0.5763]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:05 â€¢ 0:02:21[0m [2;4m1.40it/s[0m  
[2KAttention weights: tensor([[0.4126, 0.5874]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:05 â€¢ 0:02:21[0m [2;4m1.40it/s[0m  
[2KAttention weights: tensor([[0.5112, 0.4888]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:05 â€¢ 0:02:21[0m [2;4m1.40it/s[0m  
[2KAttention weights: tensor([[0.6014, 0.3986],8;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:05 â€¢ 0:02:21[0m [2;4m1.40it/s[0m  
        [0.3995, 0.6005],
        [0.4945, 0.5055],
        [0.5965, 0.4035],
        [0.5804, 0.4196],
        [0.6891, 0.3109],
        [0.6448, 0.3552],
        [0.5989, 0.4011],
        [0.5139, 0.4861],
[2K        [0.5869, 0.4131]], device='cuda:0')38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:05 â€¢ 0:02:21[0m [2;4m1.40it/s[0m  
[2KAttention weights: tensor([[0.5204, 0.4796],mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:06 â€¢ 0:02:17[0m [2;4m1.43it/s[0m  
        [0.5123, 0.4877],
        [0.5140, 0.4860],
        [0.5197, 0.4803],
        [0.5214, 0.4786],
        [0.5241, 0.4759],
        [0.5215, 0.4785],
        [0.5185, 0.4815],
        [0.5154, 0.4846],
[2K        [0.5216, 0.4784]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:06 â€¢ 0:02:17[0m [2;4m1.43it/s[0m  
[2KClass label: BaseballPitch[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:06 â€¢ 0:02:17[0m [2;4m1.43it/s[0m  
[2KAttention weights: tensor([[0.6037, 0.3963]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:06 â€¢ 0:02:17[0m [2;4m1.43it/s[0m  
[2KStep 0 - TTA Loss: 1.6968[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:06 â€¢ 0:02:17[0m [2;4m1.43it/s[0m  
[2KAttention weights: tensor([[0.5969, 0.4031]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:06 â€¢ 0:02:17[0m [2;4m1.43it/s[0m  
[2KAttention weights: tensor([[0.5799, 0.4201]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:06 â€¢ 0:02:17[0m [2;4m1.43it/s[0m  
[2KAttention weights: tensor([[0.5598, 0.4402]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:06 â€¢ 0:02:17[0m [2;4m1.43it/s[0m  
[2KAttention weights: tensor([[0.5388, 0.4612]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:06 â€¢ 0:02:17[0m [2;4m1.43it/s[0m  
[2KAttention weights: tensor([[0.5180, 0.4820]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:06 â€¢ 0:02:17[0m [2;4m1.43it/s[0m  
[2KAttention weights: tensor([[0.5072, 0.4928]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:06 â€¢ 0:02:17[0m [2;4m1.43it/s[0m  
[2KAttention weights: tensor([[0.5002, 0.4998]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:06 â€¢ 0:02:17[0m [2;4m1.43it/s[0m  
[2KAttention weights: tensor([[0.4963, 0.5037]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:06 â€¢ 0:02:17[0m [2;4m1.43it/s[0m  
[2KAttention weights: tensor([[0.4950, 0.5050]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:06 â€¢ 0:02:17[0m [2;4m1.43it/s[0m  
[2KAttention weights: tensor([[0.5174, 0.4826]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:06 â€¢ 0:02:17[0m [2;4m1.43it/s[0m  
[2KAttention weights: tensor([[0.4947, 0.5053],0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:06 â€¢ 0:02:17[0m [2;4m1.43it/s[0m  
        [0.3857, 0.6143],
        [0.4723, 0.5277],
        [0.5626, 0.4374],
        [0.5314, 0.4686],
        [0.6633, 0.3367],
        [0.6194, 0.3806],
        [0.6003, 0.3997],
        [0.4847, 0.5153],
[2K        [0.5335, 0.4665]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:06 â€¢ 0:02:17[0m [2;4m1.43it/s[0m  
[2KAttention weights: tensor([[0.5204, 0.4796],mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 9/203 [2m0:00:06 â€¢ 0:02:11[0m [2;4m1.48it/s[0m  
        [0.5122, 0.4878],
        [0.5140, 0.4860],
        [0.5197, 0.4803],
        [0.5214, 0.4786],
        [0.5241, 0.4759],
        [0.5215, 0.4785],
        [0.5185, 0.4815],
        [0.5154, 0.4846],
[2K        [0.5216, 0.4784]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 9/203 [2m0:00:06 â€¢ 0:02:11[0m [2;4m1.48it/s[0m  
[2KClass label: BaseballPitch[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 9/203 [2m0:00:06 â€¢ 0:02:11[0m [2;4m1.48it/s[0m  
[2KAttention weights: tensor([[0.5943, 0.4057]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:06 â€¢ 0:02:11[0m [2;4m1.48it/s[0m  
[2KStep 0 - TTA Loss: 2.7567[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 9/203 [2m0:00:06 â€¢ 0:02:11[0m [2;4m1.48it/s[0m  
[2KAttention weights: tensor([[0.5943, 0.4057]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:06 â€¢ 0:02:11[0m [2;4m1.48it/s[0m  
[2KAttention weights: tensor([[0.5933, 0.4067]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:06 â€¢ 0:02:11[0m [2;4m1.48it/s[0m  
[2KAttention weights: tensor([[0.5890, 0.4110]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:06 â€¢ 0:02:11[0m [2;4m1.48it/s[0m  
[2KAttention weights: tensor([[0.5787, 0.4213]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:06 â€¢ 0:02:11[0m [2;4m1.48it/s[0m  
[2KAttention weights: tensor([[0.5580, 0.4420]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:06 â€¢ 0:02:11[0m [2;4m1.48it/s[0m  
[2KAttention weights: tensor([[0.5343, 0.4657]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:06 â€¢ 0:02:11[0m [2;4m1.48it/s[0m  
[2KAttention weights: tensor([[0.4962, 0.5038]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:06 â€¢ 0:02:11[0m [2;4m1.48it/s[0m  
[2KAttention weights: tensor([[0.4554, 0.5446]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:06 â€¢ 0:02:11[0m [2;4m1.48it/s[0m  
[2KAttention weights: tensor([[0.4214, 0.5786]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:06 â€¢ 0:02:11[0m [2;4m1.48it/s[0m  
[2KAttention weights: tensor([[0.5157, 0.4843]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 9/203 [2m0:00:06 â€¢ 0:02:11[0m [2;4m1.48it/s[0m  
[2KAttention weights: tensor([[0.3941, 0.6059],0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 9/203 [2m0:00:06 â€¢ 0:02:11[0m [2;4m1.48it/s[0m  
        [0.4167, 0.5833],
        [0.4564, 0.5436],
        [0.5312, 0.4688],
        [0.4850, 0.5150],
        [0.6366, 0.3634],
        [0.5907, 0.4093],
        [0.6019, 0.3981],
        [0.4625, 0.5375],
[2K        [0.4877, 0.5123]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 9/203 [2m0:00:06 â€¢ 0:02:11[0m [2;4m1.48it/s[0m  
[2KAttention weights: tensor([[0.5204, 0.4796],mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 10/203 [2m0:00:07 â€¢ 0:02:05[0m [2;4m1.55it/s[0m  
        [0.5123, 0.4877],
        [0.5140, 0.4860],
        [0.5197, 0.4803],
        [0.5214, 0.4786],
        [0.5241, 0.4759],
        [0.5215, 0.4785],
        [0.5185, 0.4815],
        [0.5154, 0.4846],
[2K        [0.5216, 0.4784]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 10/203 [2m0:00:07 â€¢ 0:02:05[0m [2;4m1.55it/s[0m  
[2KClass label: TennisSwingm[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 10/203 [2m0:00:07 â€¢ 0:02:05[0m [2;4m1.55it/s[0m  
[2KAttention weights: tensor([[0.5146, 0.4854]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:07 â€¢ 0:02:05[0m [2;4m1.55it/s[0m  
[2KStep 0 - TTA Loss: 1.2985[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 10/203 [2m0:00:07 â€¢ 0:02:05[0m [2;4m1.55it/s[0m  
[2KAttention weights: tensor([[0.5117, 0.4883]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:07 â€¢ 0:02:05[0m [2;4m1.55it/s[0m  
[2KAttention weights: tensor([[0.5018, 0.4982]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:07 â€¢ 0:02:05[0m [2;4m1.55it/s[0m  
[2KAttention weights: tensor([[0.4980, 0.5020]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:07 â€¢ 0:02:05[0m [2;4m1.55it/s[0m  
[2KAttention weights: tensor([[0.4972, 0.5028]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:07 â€¢ 0:02:05[0m [2;4m1.55it/s[0m  
[2KAttention weights: tensor([[0.4971, 0.5029]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:07 â€¢ 0:02:05[0m [2;4m1.55it/s[0m  
[2KAttention weights: tensor([[0.4982, 0.5018]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:07 â€¢ 0:02:05[0m [2;4m1.55it/s[0m  
[2KAttention weights: tensor([[0.5011, 0.4989]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:07 â€¢ 0:02:05[0m [2;4m1.55it/s[0m  
[2KAttention weights: tensor([[0.5019, 0.4981]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:07 â€¢ 0:02:05[0m [2;4m1.55it/s[0m  
[2KAttention weights: tensor([[0.5021, 0.4979]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:07 â€¢ 0:02:05[0m [2;4m1.55it/s[0m  
[2KAttention weights: tensor([[0.5150, 0.4850]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 10/203 [2m0:00:07 â€¢ 0:02:05[0m [2;4m1.55it/s[0m  
[2KAttention weights: tensor([[0.4943, 0.5057],0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 10/203 [2m0:00:07 â€¢ 0:02:05[0m [2;4m1.55it/s[0m  
        [0.4347, 0.5653],
        [0.4798, 0.5202],
        [0.5699, 0.4301],
        [0.5335, 0.4665],
        [0.6674, 0.3326],
        [0.6196, 0.3804],
        [0.6060, 0.3940],
        [0.5020, 0.4980],
[2K        [0.5409, 0.4591]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 10/203 [2m0:00:07 â€¢ 0:02:05[0m [2;4m1.55it/s[0m  
[2KAttention weights: tensor([[0.5205, 0.4795],0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 11/203 [2m0:00:07 â€¢ 0:02:00[0m [2;4m1.61it/s[0m   
        [0.5123, 0.4877],
        [0.5140, 0.4860],
        [0.5197, 0.4803],
        [0.5214, 0.4786],
        [0.5241, 0.4759],
        [0.5215, 0.4785],
        [0.5185, 0.4815],
        [0.5154, 0.4846],
[2K        [0.5216, 0.4784]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 11/203 [2m0:00:07 â€¢ 0:02:00[0m [2;4m1.61it/s[0m  
[2KClass label: HammerThrow0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 11/203 [2m0:00:07 â€¢ 0:02:00[0m [2;4m1.61it/s[0m  
[2KAttention weights: tensor([[0.5752, 0.4248]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:07 â€¢ 0:02:00[0m [2;4m1.61it/s[0m  
[2KStep 0 - TTA Loss: 1.8248m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 11/203 [2m0:00:07 â€¢ 0:02:00[0m [2;4m1.61it/s[0m  
[2KAttention weights: tensor([[0.5752, 0.4248]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:07 â€¢ 0:02:00[0m [2;4m1.61it/s[0m  
[2KAttention weights: tensor([[0.5750, 0.4250]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:07 â€¢ 0:02:00[0m [2;4m1.61it/s[0m  
[2KAttention weights: tensor([[0.5730, 0.4270]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:07 â€¢ 0:02:00[0m [2;4m1.61it/s[0m  
[2KAttention weights: tensor([[0.5686, 0.4314]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:07 â€¢ 0:02:00[0m [2;4m1.61it/s[0m  
[2KAttention weights: tensor([[0.5635, 0.4365]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:07 â€¢ 0:02:00[0m [2;4m1.61it/s[0m  
[2KAttention weights: tensor([[0.5496, 0.4504]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:07 â€¢ 0:02:00[0m [2;4m1.61it/s[0m  
[2KAttention weights: tensor([[0.5312, 0.4688]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:07 â€¢ 0:02:00[0m [2;4m1.61it/s[0m  
[2KAttention weights: tensor([[0.5090, 0.4910]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:07 â€¢ 0:02:00[0m [2;4m1.61it/s[0m  
[2KAttention weights: tensor([[0.4888, 0.5112]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:07 â€¢ 0:02:00[0m [2;4m1.61it/s[0m  
[2KAttention weights: tensor([[0.5185, 0.4815]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 11/203 [2m0:00:07 â€¢ 0:02:00[0m [2;4m1.61it/s[0m  
[2KAttention weights: tensor([[0.5428, 0.4572],38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 11/203 [2m0:00:07 â€¢ 0:02:00[0m [2;4m1.61it/s[0m  
        [0.4273, 0.5727],
        [0.4692, 0.5308],
        [0.5559, 0.4441],
        [0.4719, 0.5281],
        [0.6378, 0.3622],
        [0.5933, 0.4067],
        [0.5747, 0.4253],
        [0.4852, 0.5148],
[2K        [0.5002, 0.4998]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 11/203 [2m0:00:07 â€¢ 0:02:00[0m [2;4m1.61it/s[0m  
[2KAttention weights: tensor([[0.5205, 0.4795],0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 12/203 [2m0:00:07 â€¢ 0:01:58[0m [2;4m1.63it/s[0m  
        [0.5123, 0.4877],
        [0.5140, 0.4860],
        [0.5198, 0.4802],
        [0.5214, 0.4786],
        [0.5242, 0.4758],
        [0.5215, 0.4785],
        [0.5185, 0.4815],
        [0.5155, 0.4845],
[2K        [0.5216, 0.4784]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 12/203 [2m0:00:07 â€¢ 0:01:58[0m [2;4m1.63it/s[0m  
[2KClass label: CleanAndJerkm[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 12/203 [2m0:00:07 â€¢ 0:01:58[0m [2;4m1.63it/s[0m  
[2KAttention weights: tensor([[0.4931, 0.5069]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:07 â€¢ 0:01:58[0m [2;4m1.63it/s[0m  
[2KStep 0 - TTA Loss: 1.9259m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 12/203 [2m0:00:07 â€¢ 0:01:58[0m [2;4m1.63it/s[0m  
[2KAttention weights: tensor([[0.4741, 0.5259]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:07 â€¢ 0:01:58[0m [2;4m1.63it/s[0m  
[2KAttention weights: tensor([[0.4482, 0.5518]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:07 â€¢ 0:01:58[0m [2;4m1.63it/s[0m  
[2KAttention weights: tensor([[0.4161, 0.5839]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:07 â€¢ 0:01:58[0m [2;4m1.63it/s[0m  
[2KAttention weights: tensor([[0.3902, 0.6098]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:07 â€¢ 0:01:58[0m [2;4m1.63it/s[0m  
[2KAttention weights: tensor([[0.3673, 0.6327]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:07 â€¢ 0:01:58[0m [2;4m1.63it/s[0m  
[2KAttention weights: tensor([[0.3497, 0.6503]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:07 â€¢ 0:01:58[0m [2;4m1.63it/s[0m  
[2KAttention weights: tensor([[0.3393, 0.6607]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:07 â€¢ 0:01:58[0m [2;4m1.63it/s[0m  
[2KAttention weights: tensor([[0.3337, 0.6663]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:07 â€¢ 0:01:58[0m [2;4m1.63it/s[0m  
[2KAttention weights: tensor([[0.3316, 0.6684]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:07 â€¢ 0:01:58[0m [2;4m1.63it/s[0m  
[2KAttention weights: tensor([[0.5095, 0.4905]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 12/203 [2m0:00:07 â€¢ 0:01:58[0m [2;4m1.63it/s[0m  
[2KAttention weights: tensor([[0.5205, 0.4795],4mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 13/203 [2m0:00:08 â€¢ 0:01:54[0m [2;4m1.68it/s[0m  
        [0.5123, 0.4877],
        [0.5141, 0.4859],
        [0.5198, 0.4802],
        [0.5215, 0.4785],
        [0.5242, 0.4758],
        [0.5215, 0.4785],
        [0.5186, 0.4814],
        [0.5155, 0.4845],
[2K        [0.5217, 0.4783]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 13/203 [2m0:00:08 â€¢ 0:01:54[0m [2;4m1.68it/s[0m  
[2KClass label: BaseballPitch[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 13/203 [2m0:00:08 â€¢ 0:01:54[0m [2;4m1.68it/s[0m  
[2KAttention weights: tensor([[0.5992, 0.4008]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:08 â€¢ 0:01:54[0m [2;4m1.68it/s[0m  
[2KStep 0 - TTA Loss: 2.0440m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 13/203 [2m0:00:08 â€¢ 0:01:54[0m [2;4m1.68it/s[0m  
[2KAttention weights: tensor([[0.5992, 0.4008]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:08 â€¢ 0:01:54[0m [2;4m1.68it/s[0m  
[2KAttention weights: tensor([[0.5993, 0.4007]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:08 â€¢ 0:01:54[0m [2;4m1.68it/s[0m  
[2KAttention weights: tensor([[0.6005, 0.3995]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:08 â€¢ 0:01:54[0m [2;4m1.68it/s[0m  
[2KAttention weights: tensor([[0.6041, 0.3959]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:08 â€¢ 0:01:54[0m [2;4m1.68it/s[0m  
[2KAttention weights: tensor([[0.6119, 0.3881]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:08 â€¢ 0:01:54[0m [2;4m1.68it/s[0m  
[2KAttention weights: tensor([[0.6248, 0.3752]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:08 â€¢ 0:01:54[0m [2;4m1.68it/s[0m  
[2KAttention weights: tensor([[0.6385, 0.3615]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:08 â€¢ 0:01:54[0m [2;4m1.68it/s[0m  
[2KAttention weights: tensor([[0.6520, 0.3480]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:08 â€¢ 0:01:54[0m [2;4m1.68it/s[0m  
[2KAttention weights: tensor([[0.6633, 0.3367]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:08 â€¢ 0:01:54[0m [2;4m1.68it/s[0m  
[2KAttention weights: tensor([[0.5218, 0.4782]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 13/203 [2m0:00:08 â€¢ 0:01:54[0m [2;4m1.68it/s[0m  
[2KAttention weights: tensor([[0.6725, 0.3275],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 13/203 [2m0:00:08 â€¢ 0:01:54[0m [2;4m1.68it/s[0m  
        [0.4482, 0.5518],
        [0.4718, 0.5282],
        [0.6208, 0.3792],
        [0.5989, 0.4011],
        [0.7042, 0.2958],
        [0.6562, 0.3438],
        [0.6063, 0.3937],
        [0.5384, 0.4616],
[2K        [0.6117, 0.3883]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 13/203 [2m0:00:08 â€¢ 0:01:54[0m [2;4m1.68it/s[0m  
[2KAttention weights: tensor([[0.5205, 0.4795],4mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:08 â€¢ 0:01:52[0m [2;4m1.70it/s[0m  
        [0.5123, 0.4877],
        [0.5141, 0.4859],
        [0.5198, 0.4802],
        [0.5215, 0.4785],
        [0.5242, 0.4758],
        [0.5216, 0.4784],
        [0.5186, 0.4814],
        [0.5155, 0.4845],
[2K        [0.5217, 0.4783]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:08 â€¢ 0:01:52[0m [2;4m1.70it/s[0m  
[2KClass label: GolfSwing[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:08 â€¢ 0:01:52[0m [2;4m1.70it/s[0m  
[2KAttention weights: tensor([[0.5975, 0.4025]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:08 â€¢ 0:01:52[0m [2;4m1.70it/s[0m  
[2KStep 0 - TTA Loss: 1.4273m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:08 â€¢ 0:01:52[0m [2;4m1.70it/s[0m  
[2KAttention weights: tensor([[0.5992, 0.4008]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:08 â€¢ 0:01:52[0m [2;4m1.70it/s[0m  
[2KAttention weights: tensor([[0.6041, 0.3959]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:08 â€¢ 0:01:52[0m [2;4m1.70it/s[0m  
[2KAttention weights: tensor([[0.6090, 0.3910]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:08 â€¢ 0:01:52[0m [2;4m1.70it/s[0m  
[2KAttention weights: tensor([[0.6124, 0.3876]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:08 â€¢ 0:01:52[0m [2;4m1.70it/s[0m  
[2KAttention weights: tensor([[0.6128, 0.3872]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:08 â€¢ 0:01:52[0m [2;4m1.70it/s[0m  
[2KAttention weights: tensor([[0.6119, 0.3881]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:08 â€¢ 0:01:52[0m [2;4m1.70it/s[0m  
[2KAttention weights: tensor([[0.6121, 0.3879]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:08 â€¢ 0:01:52[0m [2;4m1.70it/s[0m  
[2KAttention weights: tensor([[0.6129, 0.3871]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:08 â€¢ 0:01:52[0m [2;4m1.70it/s[0m  
[2KAttention weights: tensor([[0.6134, 0.3866]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:08 â€¢ 0:01:52[0m [2;4m1.70it/s[0m  
[2KAttention weights: tensor([[0.5209, 0.4791]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:08 â€¢ 0:01:52[0m [2;4m1.70it/s[0m  
[2KAttention weights: tensor([[0.6358, 0.3642],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:08 â€¢ 0:01:52[0m [2;4m1.70it/s[0m  
        [0.4486, 0.5514],
        [0.4789, 0.5211],
        [0.6137, 0.3863],
        [0.5880, 0.4120],
        [0.6938, 0.3062],
        [0.6494, 0.3506],
        [0.6067, 0.3933],
        [0.5202, 0.4798],
[2K        [0.5963, 0.4037]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:08 â€¢ 0:01:52[0m [2;4m1.70it/s[0m  
[2KAttention weights: tensor([[0.5206, 0.4794],4mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 15/203 [2m0:00:09 â€¢ 0:01:49[0m [2;4m1.73it/s[0m  
        [0.5123, 0.4877],
        [0.5141, 0.4859],
        [0.5198, 0.4802],
        [0.5215, 0.4785],
        [0.5242, 0.4758],
        [0.5216, 0.4784],
        [0.5186, 0.4814],
        [0.5155, 0.4845],
[2K        [0.5217, 0.4783]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 15/203 [2m0:00:09 â€¢ 0:01:49[0m [2;4m1.73it/s[0m  
[2KClass label: HighJumpâ”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 15/203 [2m0:00:09 â€¢ 0:01:49[0m [2;4m1.73it/s[0m  
[2KAttention weights: tensor([[0.6870, 0.3130]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:09 â€¢ 0:01:49[0m [2;4m1.73it/s[0m  
[2KStep 0 - TTA Loss: 2.9027m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 15/203 [2m0:00:09 â€¢ 0:01:49[0m [2;4m1.73it/s[0m  
[2KAttention weights: tensor([[0.6870, 0.3130]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:09 â€¢ 0:01:49[0m [2;4m1.73it/s[0m  
[2KAttention weights: tensor([[0.6875, 0.3125]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:09 â€¢ 0:01:49[0m [2;4m1.73it/s[0m  
[2KAttention weights: tensor([[0.6922, 0.3078]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:09 â€¢ 0:01:49[0m [2;4m1.73it/s[0m  
[2KAttention weights: tensor([[0.7052, 0.2948]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:09 â€¢ 0:01:49[0m [2;4m1.73it/s[0m  
[2KAttention weights: tensor([[0.7293, 0.2707]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:09 â€¢ 0:01:49[0m [2;4m1.73it/s[0m  
[2KAttention weights: tensor([[0.7587, 0.2413]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:09 â€¢ 0:01:49[0m [2;4m1.73it/s[0m  
[2KAttention weights: tensor([[0.7872, 0.2128]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:09 â€¢ 0:01:49[0m [2;4m1.73it/s[0m  
[2KAttention weights: tensor([[0.8143, 0.1857]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:09 â€¢ 0:01:49[0m [2;4m1.73it/s[0m  
[2KAttention weights: tensor([[0.8407, 0.1593]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:09 â€¢ 0:01:49[0m [2;4m1.73it/s[0m  
[2KAttention weights: tensor([[0.5276, 0.4724]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 15/203 [2m0:00:09 â€¢ 0:01:49[0m [2;4m1.73it/s[0m  
[2KAttention weights: tensor([[0.5206, 0.4794],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 16/203 [2m0:00:09 â€¢ 0:01:45[0m [2;4m1.78it/s[0m   
        [0.5124, 0.4876],
        [0.5142, 0.4858],
        [0.5199, 0.4801],
        [0.5216, 0.4784],
        [0.5243, 0.4757],
        [0.5217, 0.4783],
        [0.5187, 0.4813],
        [0.5156, 0.4844],
[2K        [0.5218, 0.4782]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 16/203 [2m0:00:09 â€¢ 0:01:45[0m [2;4m1.78it/s[0m  
[2KClass label: HighJumpâ”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 16/203 [2m0:00:09 â€¢ 0:01:45[0m [2;4m1.78it/s[0m  
[2KAttention weights: tensor([[0.6837, 0.3163]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:09 â€¢ 0:01:45[0m [2;4m1.78it/s[0m  
[2KStep 0 - TTA Loss: 1.25550m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 16/203 [2m0:00:09 â€¢ 0:01:45[0m [2;4m1.78it/s[0m  
[2KAttention weights: tensor([[0.7164, 0.2836]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:09 â€¢ 0:01:45[0m [2;4m1.78it/s[0m  
[2KAttention weights: tensor([[0.7454, 0.2546]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:09 â€¢ 0:01:45[0m [2;4m1.78it/s[0m  
[2KAttention weights: tensor([[0.7670, 0.2330]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:09 â€¢ 0:01:45[0m [2;4m1.78it/s[0m  
[2KAttention weights: tensor([[0.7829, 0.2171]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:09 â€¢ 0:01:45[0m [2;4m1.78it/s[0m  
[2KAttention weights: tensor([[0.7968, 0.2032]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:09 â€¢ 0:01:45[0m [2;4m1.78it/s[0m  
[2KAttention weights: tensor([[0.8060, 0.1940]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:09 â€¢ 0:01:45[0m [2;4m1.78it/s[0m  
[2KAttention weights: tensor([[0.8120, 0.1880]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:09 â€¢ 0:01:45[0m [2;4m1.78it/s[0m  
[2KAttention weights: tensor([[0.8150, 0.1850]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:09 â€¢ 0:01:45[0m [2;4m1.78it/s[0m  
[2KAttention weights: tensor([[0.8163, 0.1837]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:09 â€¢ 0:01:45[0m [2;4m1.78it/s[0m  
[2KAttention weights: tensor([[0.5275, 0.4725]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 16/203 [2m0:00:09 â€¢ 0:01:45[0m [2;4m1.78it/s[0m  
[2KAttention weights: tensor([[0.5207, 0.4793],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 17/203 [2m0:00:10 â€¢ 0:01:44[0m [2;4m1.80it/s[0m  
        [0.5124, 0.4876],
        [0.5142, 0.4858],
        [0.5199, 0.4801],
        [0.5216, 0.4784],
        [0.5244, 0.4756],
        [0.5217, 0.4783],
        [0.5187, 0.4813],
        [0.5156, 0.4844],
[2K        [0.5218, 0.4782]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 17/203 [2m0:00:10 â€¢ 0:01:44[0m [2;4m1.80it/s[0m  
[2KClass label: GolfSwingâ”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 17/203 [2m0:00:10 â€¢ 0:01:44[0m [2;4m1.80it/s[0m  
[2KAttention weights: tensor([[0.6005, 0.3995]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:10 â€¢ 0:01:44[0m [2;4m1.80it/s[0m  
[2KStep 0 - TTA Loss: 1.68390m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 17/203 [2m0:00:10 â€¢ 0:01:44[0m [2;4m1.80it/s[0m  
[2KAttention weights: tensor([[0.6005, 0.3995]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:10 â€¢ 0:01:44[0m [2;4m1.80it/s[0m  
[2KAttention weights: tensor([[0.6006, 0.3994]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:10 â€¢ 0:01:44[0m [2;4m1.80it/s[0m  
[2KAttention weights: tensor([[0.6010, 0.3990]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:10 â€¢ 0:01:44[0m [2;4m1.80it/s[0m  
[2KAttention weights: tensor([[0.6017, 0.3983]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:10 â€¢ 0:01:44[0m [2;4m1.80it/s[0m  
[2KAttention weights: tensor([[0.6024, 0.3976]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:10 â€¢ 0:01:44[0m [2;4m1.80it/s[0m  
[2KAttention weights: tensor([[0.6037, 0.3963]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:10 â€¢ 0:01:44[0m [2;4m1.80it/s[0m  
[2KAttention weights: tensor([[0.6056, 0.3944]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:10 â€¢ 0:01:44[0m [2;4m1.80it/s[0m  
[2KAttention weights: tensor([[0.6080, 0.3920]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:10 â€¢ 0:01:44[0m [2;4m1.80it/s[0m  
[2KAttention weights: tensor([[0.6110, 0.3890]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:10 â€¢ 0:01:44[0m [2;4m1.80it/s[0m  
[2KAttention weights: tensor([[0.5201, 0.4799]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 17/203 [2m0:00:10 â€¢ 0:01:44[0m [2;4m1.80it/s[0m  
[2KAttention weights: tensor([[0.6146, 0.3854],[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 17/203 [2m0:00:10 â€¢ 0:01:44[0m [2;4m1.80it/s[0m  
        [0.4499, 0.5501],
        [0.4968, 0.5032],
        [0.6144, 0.3856],
        [0.5960, 0.4040],
        [0.7152, 0.2848],
        [0.6616, 0.3384],
        [0.6207, 0.3793],
        [0.5333, 0.4667],
[2K        [0.6042, 0.3958]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 17/203 [2m0:00:10 â€¢ 0:01:44[0m [2;4m1.80it/s[0m  
[2KAttention weights: tensor([[0.5207, 0.4793],24mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 18/203 [2m0:00:10 â€¢ 0:01:43[0m [2;4m1.81it/s[0m  
        [0.5124, 0.4876],
        [0.5142, 0.4858],
        [0.5199, 0.4801],
        [0.5217, 0.4783],
        [0.5244, 0.4756],
        [0.5217, 0.4783],
        [0.5187, 0.4813],
        [0.5156, 0.4844],
[2K        [0.5218, 0.4782]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 18/203 [2m0:00:10 â€¢ 0:01:43[0m [2;4m1.81it/s[0m  
[2KClass label: ThrowDiscus[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 18/203 [2m0:00:10 â€¢ 0:01:43[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.5834, 0.4166]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:10 â€¢ 0:01:43[0m [2;4m1.81it/s[0m  
[2KStep 0 - TTA Loss: 2.20680m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 18/203 [2m0:00:10 â€¢ 0:01:43[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.5831, 0.4169]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:10 â€¢ 0:01:43[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.5775, 0.4225]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:10 â€¢ 0:01:43[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.5706, 0.4294]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:10 â€¢ 0:01:43[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.5650, 0.4350]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:10 â€¢ 0:01:43[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.5635, 0.4365]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:10 â€¢ 0:01:43[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.5603, 0.4397]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:10 â€¢ 0:01:43[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.5599, 0.4401]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:10 â€¢ 0:01:43[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.5601, 0.4399]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:10 â€¢ 0:01:43[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.5602, 0.4398]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:10 â€¢ 0:01:43[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.5213, 0.4787]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 18/203 [2m0:00:10 â€¢ 0:01:43[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.5910, 0.4090],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 18/203 [2m0:00:10 â€¢ 0:01:43[0m [2;4m1.81it/s[0m  
        [0.4475, 0.5525],
        [0.4890, 0.5110],
        [0.5980, 0.4020],
        [0.5610, 0.4390],
        [0.6868, 0.3132],
        [0.6374, 0.3626],
        [0.6071, 0.3929],
        [0.5138, 0.4862],
[2K        [0.5605, 0.4395]], device='cuda:0')â•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 18/203 [2m0:00:10 â€¢ 0:01:43[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.5207, 0.4793],24mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 19/203 [2m0:00:11 â€¢ 0:01:41[0m [2;4m1.83it/s[0m  
        [0.5124, 0.4876],
        [0.5143, 0.4857],
        [0.5199, 0.4801],
        [0.5217, 0.4783],
        [0.5244, 0.4756],
        [0.5218, 0.4782],
        [0.5187, 0.4813],
        [0.5156, 0.4844],
[2K        [0.5219, 0.4781]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 19/203 [2m0:00:11 â€¢ 0:01:41[0m [2;4m1.83it/s[0m  
[2KClass label: SoccerPenaltym[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 19/203 [2m0:00:11 â€¢ 0:01:41[0m [2;4m1.83it/s[0m  
[2KAttention weights: tensor([[0.6078, 0.3922]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:11 â€¢ 0:01:41[0m [2;4m1.83it/s[0m  
[2KStep 0 - TTA Loss: 2.13240m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 19/203 [2m0:00:11 â€¢ 0:01:41[0m [2;4m1.83it/s[0m  
[2KAttention weights: tensor([[0.6078, 0.3922]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:11 â€¢ 0:01:41[0m [2;4m1.83it/s[0m  
[2KAttention weights: tensor([[0.6082, 0.3918]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:11 â€¢ 0:01:41[0m [2;4m1.83it/s[0m  
[2KAttention weights: tensor([[0.6119, 0.3881]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:11 â€¢ 0:01:41[0m [2;4m1.83it/s[0m  
[2KAttention weights: tensor([[0.6225, 0.3775]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:11 â€¢ 0:01:41[0m [2;4m1.83it/s[0m  
[2KAttention weights: tensor([[0.6373, 0.3627]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:11 â€¢ 0:01:41[0m [2;4m1.83it/s[0m  
[2KAttention weights: tensor([[0.6594, 0.3406]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:11 â€¢ 0:01:41[0m [2;4m1.83it/s[0m  
[2KAttention weights: tensor([[0.6824, 0.3176]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:11 â€¢ 0:01:41[0m [2;4m1.83it/s[0m  
[2KAttention weights: tensor([[0.7091, 0.2909]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:11 â€¢ 0:01:41[0m [2;4m1.83it/s[0m  
[2KAttention weights: tensor([[0.7340, 0.2660]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:11 â€¢ 0:01:41[0m [2;4m1.83it/s[0m  
[2KAttention weights: tensor([[0.5207, 0.4793]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 19/203 [2m0:00:11 â€¢ 0:01:41[0m [2;4m1.83it/s[0m  
[2KAttention weights: tensor([[0.5207, 0.4793],24mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 20/203 [2m0:00:11 â€¢ 0:01:39[0m [2;4m1.86it/s[0m  
        [0.5125, 0.4875],
        [0.5143, 0.4857],
        [0.5200, 0.4800],
        [0.5217, 0.4783],
        [0.5245, 0.4755],
        [0.5218, 0.4782],
        [0.5188, 0.4812],
        [0.5157, 0.4843],
[2K        [0.5219, 0.4781]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 20/203 [2m0:00:11 â€¢ 0:01:39[0m [2;4m1.86it/s[0m  
[2KClass label: HighJumpâ”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 20/203 [2m0:00:11 â€¢ 0:01:39[0m [2;4m1.86it/s[0m  
[2KAttention weights: tensor([[0.6888, 0.3112]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:11 â€¢ 0:01:39[0m [2;4m1.86it/s[0m  
[2KStep 0 - TTA Loss: 1.00090m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 20/203 [2m0:00:11 â€¢ 0:01:39[0m [2;4m1.86it/s[0m  
[2KAttention weights: tensor([[0.6928, 0.3072]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:11 â€¢ 0:01:39[0m [2;4m1.86it/s[0m  
[2KAttention weights: tensor([[0.6934, 0.3066]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:11 â€¢ 0:01:39[0m [2;4m1.86it/s[0m  
[2KAttention weights: tensor([[0.6922, 0.3078]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:11 â€¢ 0:01:39[0m [2;4m1.86it/s[0m  
[2KAttention weights: tensor([[0.6896, 0.3104]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:11 â€¢ 0:01:39[0m [2;4m1.86it/s[0m  
[2KAttention weights: tensor([[0.6867, 0.3133]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:11 â€¢ 0:01:39[0m [2;4m1.86it/s[0m  
[2KAttention weights: tensor([[0.6866, 0.3134]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:11 â€¢ 0:01:39[0m [2;4m1.86it/s[0m  
[2KAttention weights: tensor([[0.6851, 0.3149]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:11 â€¢ 0:01:39[0m [2;4m1.86it/s[0m  
[2KAttention weights: tensor([[0.6844, 0.3156]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:11 â€¢ 0:01:39[0m [2;4m1.86it/s[0m  
[2KAttention weights: tensor([[0.6834, 0.3166]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:11 â€¢ 0:01:39[0m [2;4m1.86it/s[0m  
[2KAttention weights: tensor([[0.5252, 0.4748]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 20/203 [2m0:00:11 â€¢ 0:01:39[0m [2;4m1.86it/s[0m  
[2KAttention weights: tensor([[0.5923, 0.4077],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 20/203 [2m0:00:11 â€¢ 0:01:39[0m [2;4m1.86it/s[0m  
        [0.4600, 0.5400],
        [0.4900, 0.5100],
        [0.6080, 0.3920],
        [0.5812, 0.4188],
        [0.6837, 0.3163],
        [0.6404, 0.3596],
        [0.6837, 0.3163],
        [0.5247, 0.4753],
[2K        [0.5815, 0.4185]], device='cuda:0')â•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 20/203 [2m0:00:11 â€¢ 0:01:39[0m [2;4m1.86it/s[0m  
[2KAttention weights: tensor([[0.5208, 0.4792],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 21/203 [2m0:00:12 â€¢ 0:01:44[0m [2;4m1.76it/s[0m   
        [0.5125, 0.4875],
        [0.5143, 0.4857],
        [0.5200, 0.4800],
        [0.5217, 0.4783],
        [0.5245, 0.4755],
        [0.5218, 0.4782],
        [0.5188, 0.4812],
        [0.5157, 0.4843],
[2K        [0.5219, 0.4781]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 21/203 [2m0:00:12 â€¢ 0:01:44[0m [2;4m1.76it/s[0m  
[2KClass label: BaseballPitch0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 21/203 [2m0:00:12 â€¢ 0:01:44[0m [2;4m1.76it/s[0m  
[2KAttention weights: tensor([[0.6001, 0.3999]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:12 â€¢ 0:01:44[0m [2;4m1.76it/s[0m  
[2KStep 0 - TTA Loss: 2.2284[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 21/203 [2m0:00:12 â€¢ 0:01:44[0m [2;4m1.76it/s[0m  
[2KAttention weights: tensor([[0.6001, 0.3999]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:12 â€¢ 0:01:44[0m [2;4m1.76it/s[0m  
[2KAttention weights: tensor([[0.6002, 0.3998]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:12 â€¢ 0:01:44[0m [2;4m1.76it/s[0m  
[2KAttention weights: tensor([[0.6010, 0.3990]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:12 â€¢ 0:01:44[0m [2;4m1.76it/s[0m  
[2KAttention weights: tensor([[0.6037, 0.3963]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:12 â€¢ 0:01:44[0m [2;4m1.76it/s[0m  
[2KAttention weights: tensor([[0.6097, 0.3903]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:12 â€¢ 0:01:44[0m [2;4m1.76it/s[0m  
[2KAttention weights: tensor([[0.6115, 0.3885]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:12 â€¢ 0:01:44[0m [2;4m1.76it/s[0m  
[2KAttention weights: tensor([[0.6120, 0.3880]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:12 â€¢ 0:01:44[0m [2;4m1.76it/s[0m  
[2KAttention weights: tensor([[0.6146, 0.3854]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:12 â€¢ 0:01:44[0m [2;4m1.76it/s[0m  
[2KAttention weights: tensor([[0.6194, 0.3806]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:12 â€¢ 0:01:44[0m [2;4m1.76it/s[0m  
[2KAttention weights: tensor([[0.5203, 0.4797]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 21/203 [2m0:00:12 â€¢ 0:01:44[0m [2;4m1.76it/s[0m  
[2KAttention weights: tensor([[0.6271, 0.3729],[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 21/203 [2m0:00:12 â€¢ 0:01:44[0m [2;4m1.76it/s[0m  
        [0.4471, 0.5529],
        [0.4959, 0.5041],
        [0.6120, 0.3880],
        [0.5835, 0.4165],
        [0.6788, 0.3212],
        [0.6364, 0.3636],
        [0.6126, 0.3874],
        [0.5236, 0.4764],
[2K        [0.5894, 0.4106]], device='cuda:0')m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 21/203 [2m0:00:12 â€¢ 0:01:44[0m [2;4m1.76it/s[0m  
[2KAttention weights: tensor([[0.5208, 0.4792],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 22/203 [2m0:00:13 â€¢ 0:01:43[0m [2;4m1.76it/s[0m  
        [0.5125, 0.4875],
        [0.5143, 0.4857],
        [0.5200, 0.4800],
        [0.5218, 0.4782],
        [0.5245, 0.4755],
        [0.5218, 0.4782],
        [0.5188, 0.4812],
        [0.5157, 0.4843],
[2K        [0.5220, 0.4780]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 22/203 [2m0:00:13 â€¢ 0:01:43[0m [2;4m1.76it/s[0m  
[2KClass label: CleanAndJerk[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 22/203 [2m0:00:13 â€¢ 0:01:43[0m [2;4m1.76it/s[0m  
[2KAttention weights: tensor([[0.4932, 0.5068]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:13 â€¢ 0:01:43[0m [2;4m1.76it/s[0m  
[2KStep 0 - TTA Loss: 0.9359[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 22/203 [2m0:00:13 â€¢ 0:01:43[0m [2;4m1.76it/s[0m  
[2KAttention weights: tensor([[0.4924, 0.5076]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:13 â€¢ 0:01:43[0m [2;4m1.76it/s[0m  
[2KAttention weights: tensor([[0.4892, 0.5108]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:13 â€¢ 0:01:43[0m [2;4m1.76it/s[0m  
[2KAttention weights: tensor([[0.4876, 0.5124]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:13 â€¢ 0:01:43[0m [2;4m1.76it/s[0m  
[2KAttention weights: tensor([[0.4871, 0.5129]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:13 â€¢ 0:01:43[0m [2;4m1.76it/s[0m  
[2KAttention weights: tensor([[0.4872, 0.5128]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:13 â€¢ 0:01:43[0m [2;4m1.76it/s[0m  
[2KAttention weights: tensor([[0.4887, 0.5113]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:13 â€¢ 0:01:43[0m [2;4m1.76it/s[0m  
[2KAttention weights: tensor([[0.4900, 0.5100]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:13 â€¢ 0:01:43[0m [2;4m1.76it/s[0m  
[2KAttention weights: tensor([[0.4908, 0.5092]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:13 â€¢ 0:01:43[0m [2;4m1.76it/s[0m  
[2KAttention weights: tensor([[0.4912, 0.5088]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:13 â€¢ 0:01:43[0m [2;4m1.76it/s[0m  
[2KAttention weights: tensor([[0.5141, 0.4859]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 22/203 [2m0:00:13 â€¢ 0:01:43[0m [2;4m1.76it/s[0m  
[2KAttention weights: tensor([[0.6288, 0.3712],[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 22/203 [2m0:00:13 â€¢ 0:01:43[0m [2;4m1.76it/s[0m  
        [0.4483, 0.5517],
        [0.4912, 0.5088],
        [0.6120, 0.3880],
        [0.5844, 0.4156],
        [0.6801, 0.3199],
        [0.6377, 0.3623],
        [0.6126, 0.3874],
        [0.5175, 0.4825],
[2K        [0.5885, 0.4115]], device='cuda:0')m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 22/203 [2m0:00:13 â€¢ 0:01:43[0m [2;4m1.76it/s[0m  
[2KAttention weights: tensor([[0.5208, 0.4792],224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 23/203 [2m0:00:13 â€¢ 0:01:41[0m [2;4m1.78it/s[0m  
        [0.5125, 0.4875],
        [0.5143, 0.4857],
        [0.5200, 0.4800],
        [0.5218, 0.4782],
        [0.5245, 0.4755],
        [0.5218, 0.4782],
        [0.5189, 0.4811],
        [0.5157, 0.4843],
[2K        [0.5220, 0.4780]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 23/203 [2m0:00:13 â€¢ 0:01:41[0m [2;4m1.78it/s[0m  
[2KClass label: TennisSwing[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 23/203 [2m0:00:13 â€¢ 0:01:41[0m [2;4m1.78it/s[0m  
[2KAttention weights: tensor([[0.5157, 0.4843]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:13 â€¢ 0:01:41[0m [2;4m1.78it/s[0m  
[2KStep 0 - TTA Loss: 2.1850[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 23/203 [2m0:00:13 â€¢ 0:01:41[0m [2;4m1.78it/s[0m  
[2KAttention weights: tensor([[0.5157, 0.4843]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:13 â€¢ 0:01:41[0m [2;4m1.78it/s[0m  
[2KAttention weights: tensor([[0.5165, 0.4835]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:13 â€¢ 0:01:41[0m [2;4m1.78it/s[0m  
[2KAttention weights: tensor([[0.5216, 0.4784]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:13 â€¢ 0:01:41[0m [2;4m1.78it/s[0m  
[2KAttention weights: tensor([[0.5332, 0.4668]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:13 â€¢ 0:01:41[0m [2;4m1.78it/s[0m  
[2KAttention weights: tensor([[0.5532, 0.4468]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:13 â€¢ 0:01:41[0m [2;4m1.78it/s[0m  
[2KAttention weights: tensor([[0.5836, 0.4164]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:13 â€¢ 0:01:41[0m [2;4m1.78it/s[0m  
[2KAttention weights: tensor([[0.6182, 0.3818]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:13 â€¢ 0:01:41[0m [2;4m1.78it/s[0m  
[2KAttention weights: tensor([[0.6557, 0.3443]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:13 â€¢ 0:01:41[0m [2;4m1.78it/s[0m  
[2KAttention weights: tensor([[0.6903, 0.3097]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:13 â€¢ 0:01:41[0m [2;4m1.78it/s[0m  
[2KAttention weights: tensor([[0.5203, 0.4797]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 23/203 [2m0:00:13 â€¢ 0:01:41[0m [2;4m1.78it/s[0m  
[2KAttention weights: tensor([[0.5208, 0.4792],224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 24/203 [2m0:00:13 â€¢ 0:01:39[0m [2;4m1.81it/s[0m  
        [0.5125, 0.4875],
        [0.5143, 0.4857],
        [0.5200, 0.4800],
        [0.5218, 0.4782],
        [0.5245, 0.4755],
        [0.5218, 0.4782],
        [0.5189, 0.4811],
        [0.5157, 0.4843],
[2K        [0.5220, 0.4780]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 24/203 [2m0:00:13 â€¢ 0:01:39[0m [2;4m1.81it/s[0m  
[2KClass label: Billiardsâ”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 24/203 [2m0:00:13 â€¢ 0:01:39[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.4449, 0.5551]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:13 â€¢ 0:01:39[0m [2;4m1.81it/s[0m  
[2KStep 0 - TTA Loss: 0.8729[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 24/203 [2m0:00:13 â€¢ 0:01:39[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.4477, 0.5523]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:13 â€¢ 0:01:39[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.4513, 0.5487]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:13 â€¢ 0:01:39[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.4550, 0.5450]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:13 â€¢ 0:01:39[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.4586, 0.5414]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:13 â€¢ 0:01:39[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.4616, 0.5384]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:13 â€¢ 0:01:39[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.4640, 0.5360]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:13 â€¢ 0:01:39[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.4659, 0.5341]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:13 â€¢ 0:01:39[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.4669, 0.5331]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:13 â€¢ 0:01:39[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.4674, 0.5326]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:13 â€¢ 0:01:39[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.5132, 0.4868]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 24/203 [2m0:00:13 â€¢ 0:01:39[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.6330, 0.3670],â•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 24/203 [2m0:00:13 â€¢ 0:01:39[0m [2;4m1.81it/s[0m  
        [0.4675, 0.5325],
        [0.5116, 0.4884],
        [0.6285, 0.3715],
        [0.6077, 0.3923],
        [0.7168, 0.2832],
        [0.6560, 0.3440],
        [0.6238, 0.3762],
        [0.6348, 0.3652],
[2K        [0.6247, 0.3753]], device='cuda:0')mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 24/203 [2m0:00:13 â€¢ 0:01:39[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.5208, 0.4792],224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 25/203 [2m0:00:14 â€¢ 0:01:39[0m [2;4m1.81it/s[0m  
        [0.5125, 0.4875],
        [0.5143, 0.4857],
        [0.5200, 0.4800],
        [0.5218, 0.4782],
        [0.5245, 0.4755],
        [0.5218, 0.4782],
        [0.5188, 0.4812],
        [0.5157, 0.4843],
[2K        [0.5219, 0.4781]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 25/203 [2m0:00:14 â€¢ 0:01:39[0m [2;4m1.81it/s[0m  
[2KClass label: GolfSwingâ”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 25/203 [2m0:00:14 â€¢ 0:01:39[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.6043, 0.3957]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:14 â€¢ 0:01:39[0m [2;4m1.81it/s[0m  
[2KStep 0 - TTA Loss: 1.2272[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 25/203 [2m0:00:14 â€¢ 0:01:39[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.6043, 0.3957]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:14 â€¢ 0:01:39[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.6044, 0.3956]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:14 â€¢ 0:01:39[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.6047, 0.3953]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:14 â€¢ 0:01:39[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.6053, 0.3947]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:14 â€¢ 0:01:39[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.6059, 0.3941]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:14 â€¢ 0:01:39[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.6080, 0.3920]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:14 â€¢ 0:01:39[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.6108, 0.3892]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:14 â€¢ 0:01:39[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.6151, 0.3849]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:14 â€¢ 0:01:39[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.6237, 0.3763]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:14 â€¢ 0:01:39[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.5219, 0.4781]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 25/203 [2m0:00:14 â€¢ 0:01:39[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.6160, 0.3840],â•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 25/203 [2m0:00:14 â€¢ 0:01:39[0m [2;4m1.81it/s[0m  
        [0.4608, 0.5392],
        [0.5022, 0.4978],
        [0.6338, 0.3662],
        [0.5925, 0.4075],
        [0.7018, 0.2982],
        [0.6504, 0.3496],
        [0.6107, 0.3893],
        [0.5470, 0.4530],
[2K        [0.6025, 0.3975]], device='cuda:0')mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 25/203 [2m0:00:14 â€¢ 0:01:39[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.5208, 0.4792],â•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 26/203 [2m0:00:14 â€¢ 0:01:38[0m [2;4m1.82it/s[0m   
        [0.5125, 0.4875],
        [0.5143, 0.4857],
        [0.5200, 0.4800],
        [0.5218, 0.4782],
        [0.5245, 0.4755],
        [0.5218, 0.4782],
        [0.5188, 0.4812],
        [0.5157, 0.4843],
[2K        [0.5219, 0.4781]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 26/203 [2m0:00:14 â€¢ 0:01:38[0m [2;4m1.82it/s[0m  
[2KClass label: HighJumpâ”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 26/203 [2m0:00:14 â€¢ 0:01:38[0m [2;4m1.82it/s[0m  
[2KAttention weights: tensor([[0.6886, 0.3114]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:14 â€¢ 0:01:38[0m [2;4m1.82it/s[0m  
[2KStep 0 - TTA Loss: 1.7745[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 26/203 [2m0:00:14 â€¢ 0:01:38[0m [2;4m1.82it/s[0m  
[2KAttention weights: tensor([[0.7049, 0.2951]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:14 â€¢ 0:01:38[0m [2;4m1.82it/s[0m  
[2KAttention weights: tensor([[0.7296, 0.2704]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:14 â€¢ 0:01:38[0m [2;4m1.82it/s[0m  
[2KAttention weights: tensor([[0.7440, 0.2560]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:14 â€¢ 0:01:38[0m [2;4m1.82it/s[0m  
[2KAttention weights: tensor([[0.7571, 0.2429]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:14 â€¢ 0:01:38[0m [2;4m1.82it/s[0m  
[2KAttention weights: tensor([[0.7791, 0.2209]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:14 â€¢ 0:01:38[0m [2;4m1.82it/s[0m  
[2KAttention weights: tensor([[0.7956, 0.2044]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:14 â€¢ 0:01:38[0m [2;4m1.82it/s[0m  
[2KAttention weights: tensor([[0.8145, 0.1855]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:14 â€¢ 0:01:38[0m [2;4m1.82it/s[0m  
[2KAttention weights: tensor([[0.8248, 0.1752]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:14 â€¢ 0:01:38[0m [2;4m1.82it/s[0m  
[2KAttention weights: tensor([[0.8289, 0.1711]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:14 â€¢ 0:01:38[0m [2;4m1.82it/s[0m  
[2KAttention weights: tensor([[0.5312, 0.4688]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 26/203 [2m0:00:14 â€¢ 0:01:38[0m [2;4m1.82it/s[0m  
[2KAttention weights: tensor([[0.6637, 0.3363],m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 26/203 [2m0:00:14 â€¢ 0:01:38[0m [2;4m1.82it/s[0m  
        [0.4614, 0.5386],
        [0.5354, 0.4646],
        [0.6848, 0.3152],
        [0.6913, 0.3087],
        [0.8310, 0.1690],
        [0.7608, 0.2392],
        [0.6761, 0.3239],
        [0.6079, 0.3921],
[2K        [0.7084, 0.2916]], device='cuda:0')0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 26/203 [2m0:00:14 â€¢ 0:01:38[0m [2;4m1.82it/s[0m  
[2KAttention weights: tensor([[0.5208, 0.4792],â•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 27/203 [2m0:00:15 â€¢ 0:01:40[0m [2;4m1.76it/s[0m  
        [0.5125, 0.4875],
        [0.5143, 0.4857],
        [0.5200, 0.4800],
        [0.5217, 0.4783],
        [0.5245, 0.4755],
        [0.5218, 0.4782],
        [0.5188, 0.4812],
        [0.5157, 0.4843],
[2K        [0.5219, 0.4781]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 27/203 [2m0:00:15 â€¢ 0:01:40[0m [2;4m1.76it/s[0m  
[2KClass label: GolfSwingâ”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 27/203 [2m0:00:15 â€¢ 0:01:40[0m [2;4m1.76it/s[0m  
[2KAttention weights: tensor([[0.6030, 0.3970]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:15 â€¢ 0:01:40[0m [2;4m1.76it/s[0m  
[2KStep 0 - TTA Loss: 1.0916[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 27/203 [2m0:00:15 â€¢ 0:01:40[0m [2;4m1.76it/s[0m  
[2KAttention weights: tensor([[0.6030, 0.3970]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:15 â€¢ 0:01:40[0m [2;4m1.76it/s[0m  
[2KAttention weights: tensor([[0.6036, 0.3964]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:15 â€¢ 0:01:40[0m [2;4m1.76it/s[0m  
[2KAttention weights: tensor([[0.6057, 0.3943]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:15 â€¢ 0:01:40[0m [2;4m1.76it/s[0m  
[2KAttention weights: tensor([[0.6102, 0.3898]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:15 â€¢ 0:01:40[0m [2;4m1.76it/s[0m  
[2KAttention weights: tensor([[0.6164, 0.3836]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:15 â€¢ 0:01:40[0m [2;4m1.76it/s[0m  
[2KAttention weights: tensor([[0.6242, 0.3758]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:15 â€¢ 0:01:40[0m [2;4m1.76it/s[0m  
[2KAttention weights: tensor([[0.6345, 0.3655]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:15 â€¢ 0:01:40[0m [2;4m1.76it/s[0m  
[2KAttention weights: tensor([[0.6499, 0.3501]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:15 â€¢ 0:01:40[0m [2;4m1.76it/s[0m  
[2KAttention weights: tensor([[0.6677, 0.3323]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:15 â€¢ 0:01:40[0m [2;4m1.76it/s[0m  
[2KAttention weights: tensor([[0.5240, 0.4760]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 27/203 [2m0:00:15 â€¢ 0:01:40[0m [2;4m1.76it/s[0m  
[2KAttention weights: tensor([[0.6668, 0.3332],m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 27/203 [2m0:00:15 â€¢ 0:01:40[0m [2;4m1.76it/s[0m  
        [0.4654, 0.5346],
        [0.5310, 0.4690],
        [0.6906, 0.3094],
        [0.6929, 0.3071],
        [0.8297, 0.1703],
        [0.7618, 0.2382],
        [0.6762, 0.3238],
        [0.5928, 0.4072],
[2K        [0.7086, 0.2914]], device='cuda:0')0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 27/203 [2m0:00:15 â€¢ 0:01:40[0m [2;4m1.76it/s[0m  
[2KAttention weights: tensor([[0.5208, 0.4792],;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 28/203 [2m0:00:16 â€¢ 0:01:40[0m [2;4m1.76it/s[0m  
        [0.5125, 0.4875],
        [0.5143, 0.4857],
        [0.5200, 0.4800],
        [0.5218, 0.4782],
        [0.5245, 0.4755],
        [0.5218, 0.4782],
        [0.5188, 0.4812],
        [0.5157, 0.4843],
[2K        [0.5219, 0.4781]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 28/203 [2m0:00:16 â€¢ 0:01:40[0m [2;4m1.76it/s[0m  
[2KClass label: CleanAndJerk[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 28/203 [2m0:00:16 â€¢ 0:01:40[0m [2;4m1.76it/s[0m  
[2KAttention weights: tensor([[0.4988, 0.5012]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:16 â€¢ 0:01:40[0m [2;4m1.76it/s[0m  
[2KStep 0 - TTA Loss: 2.6638[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 28/203 [2m0:00:16 â€¢ 0:01:40[0m [2;4m1.76it/s[0m  
[2KAttention weights: tensor([[0.5036, 0.4964]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:16 â€¢ 0:01:40[0m [2;4m1.76it/s[0m  
[2KAttention weights: tensor([[0.5117, 0.4883]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:16 â€¢ 0:01:40[0m [2;4m1.76it/s[0m  
[2KAttention weights: tensor([[0.5175, 0.4825]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:16 â€¢ 0:01:40[0m [2;4m1.76it/s[0m  
[2KAttention weights: tensor([[0.5259, 0.4741]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:16 â€¢ 0:01:40[0m [2;4m1.76it/s[0m  
[2KAttention weights: tensor([[0.5347, 0.4653]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:16 â€¢ 0:01:40[0m [2;4m1.76it/s[0m  
[2KAttention weights: tensor([[0.5417, 0.4583]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:16 â€¢ 0:01:40[0m [2;4m1.76it/s[0m  
[2KAttention weights: tensor([[0.5471, 0.4529]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:16 â€¢ 0:01:40[0m [2;4m1.76it/s[0m  
[2KAttention weights: tensor([[0.5508, 0.4492]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:16 â€¢ 0:01:40[0m [2;4m1.76it/s[0m  
[2KAttention weights: tensor([[0.5523, 0.4477]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:16 â€¢ 0:01:40[0m [2;4m1.76it/s[0m  
[2KAttention weights: tensor([[0.5159, 0.4841]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 28/203 [2m0:00:16 â€¢ 0:01:40[0m [2;4m1.76it/s[0m  
[2KAttention weights: tensor([[0.6522, 0.3478],mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 28/203 [2m0:00:16 â€¢ 0:01:40[0m [2;4m1.76it/s[0m  
        [0.4640, 0.5360],
        [0.5526, 0.4474],
        [0.6929, 0.3071],
        [0.6666, 0.3334],
        [0.7916, 0.2084],
        [0.7287, 0.2713],
        [0.6566, 0.3434],
        [0.5730, 0.4270],
[2K        [0.6791, 0.3209]], device='cuda:0')4mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 28/203 [2m0:00:16 â€¢ 0:01:40[0m [2;4m1.76it/s[0m  
[2KAttention weights: tensor([[0.5208, 0.4792],;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 29/203 [2m0:00:16 â€¢ 0:01:38[0m [2;4m1.78it/s[0m  
        [0.5125, 0.4875],
        [0.5143, 0.4857],
        [0.5200, 0.4800],
        [0.5218, 0.4782],
        [0.5245, 0.4755],
        [0.5218, 0.4782],
        [0.5189, 0.4811],
        [0.5157, 0.4843],
[2K        [0.5220, 0.4780]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 29/203 [2m0:00:16 â€¢ 0:01:38[0m [2;4m1.78it/s[0m  
[2KClass label: Billiardsâ”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 29/203 [2m0:00:16 â€¢ 0:01:38[0m [2;4m1.78it/s[0m  
[2KAttention weights: tensor([[0.4436, 0.5564]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:16 â€¢ 0:01:38[0m [2;4m1.78it/s[0m  
[2KStep 0 - TTA Loss: 1.6101[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 29/203 [2m0:00:16 â€¢ 0:01:38[0m [2;4m1.78it/s[0m  
[2KAttention weights: tensor([[0.4436, 0.5564]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:16 â€¢ 0:01:38[0m [2;4m1.78it/s[0m  
[2KAttention weights: tensor([[0.4428, 0.5572]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:16 â€¢ 0:01:38[0m [2;4m1.78it/s[0m  
[2KAttention weights: tensor([[0.4390, 0.5610]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:16 â€¢ 0:01:38[0m [2;4m1.78it/s[0m  
[2KAttention weights: tensor([[0.4266, 0.5734]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:16 â€¢ 0:01:38[0m [2;4m1.78it/s[0m  
[2KAttention weights: tensor([[0.4073, 0.5927]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:16 â€¢ 0:01:38[0m [2;4m1.78it/s[0m  
[2KAttention weights: tensor([[0.3800, 0.6200]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:16 â€¢ 0:01:38[0m [2;4m1.78it/s[0m  
[2KAttention weights: tensor([[0.3466, 0.6534]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:16 â€¢ 0:01:38[0m [2;4m1.78it/s[0m  
[2KAttention weights: tensor([[0.3074, 0.6926]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:16 â€¢ 0:01:38[0m [2;4m1.78it/s[0m  
[2KAttention weights: tensor([[0.2706, 0.7294]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:16 â€¢ 0:01:38[0m [2;4m1.78it/s[0m  
[2KAttention weights: tensor([[0.5094, 0.4906]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 29/203 [2m0:00:16 â€¢ 0:01:38[0m [2;4m1.78it/s[0m  
[2KAttention weights: tensor([[0.5935, 0.4065],mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 29/203 [2m0:00:16 â€¢ 0:01:38[0m [2;4m1.78it/s[0m  
        [0.2368, 0.7632],
        [0.5117, 0.4883],
        [0.5784, 0.4216],
        [0.5766, 0.4234],
        [0.7082, 0.2918],
        [0.6660, 0.3340],
        [0.5838, 0.4162],
        [0.5128, 0.4872],
[2K        [0.5950, 0.4050]], device='cuda:0')4mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 29/203 [2m0:00:16 â€¢ 0:01:38[0m [2;4m1.78it/s[0m  
[2KAttention weights: tensor([[0.5208, 0.4792],;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 30/203 [2m0:00:17 â€¢ 0:01:37[0m [2;4m1.79it/s[0m  
        [0.5125, 0.4875],
        [0.5144, 0.4856],
        [0.5200, 0.4800],
        [0.5218, 0.4782],
        [0.5245, 0.4755],
        [0.5218, 0.4782],
        [0.5189, 0.4811],
        [0.5157, 0.4843],
[2K        [0.5220, 0.4780]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 30/203 [2m0:00:17 â€¢ 0:01:37[0m [2;4m1.79it/s[0m  
[2KClass label: CleanAndJerk[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 30/203 [2m0:00:17 â€¢ 0:01:37[0m [2;4m1.79it/s[0m  
[2KAttention weights: tensor([[0.4923, 0.5077]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:17 â€¢ 0:01:37[0m [2;4m1.79it/s[0m  
[2KStep 0 - TTA Loss: 1.5719[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 30/203 [2m0:00:17 â€¢ 0:01:37[0m [2;4m1.79it/s[0m  
[2KAttention weights: tensor([[0.5005, 0.4995]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:17 â€¢ 0:01:37[0m [2;4m1.79it/s[0m  
[2KAttention weights: tensor([[0.5190, 0.4810]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:17 â€¢ 0:01:37[0m [2;4m1.79it/s[0m  
[2KAttention weights: tensor([[0.5337, 0.4663]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:17 â€¢ 0:01:37[0m [2;4m1.79it/s[0m  
[2KAttention weights: tensor([[0.5537, 0.4463]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:17 â€¢ 0:01:37[0m [2;4m1.79it/s[0m  
[2KAttention weights: tensor([[0.5666, 0.4334]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:17 â€¢ 0:01:37[0m [2;4m1.79it/s[0m  
[2KAttention weights: tensor([[0.5770, 0.4230]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:17 â€¢ 0:01:37[0m [2;4m1.79it/s[0m  
[2KAttention weights: tensor([[0.5849, 0.4151]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:17 â€¢ 0:01:37[0m [2;4m1.79it/s[0m  
[2KAttention weights: tensor([[0.5894, 0.4106]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:17 â€¢ 0:01:37[0m [2;4m1.79it/s[0m  
[2KAttention weights: tensor([[0.5919, 0.4081]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:17 â€¢ 0:01:37[0m [2;4m1.79it/s[0m  
[2KAttention weights: tensor([[0.5173, 0.4827]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 30/203 [2m0:00:17 â€¢ 0:01:37[0m [2;4m1.79it/s[0m  
[2KAttention weights: tensor([[0.6062, 0.3938],mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 30/203 [2m0:00:17 â€¢ 0:01:37[0m [2;4m1.79it/s[0m  
        [0.2842, 0.7158],
        [0.5926, 0.4074],
        [0.6015, 0.3985],
        [0.5932, 0.4068],
        [0.7104, 0.2896],
        [0.6694, 0.3306],
        [0.5952, 0.4048],
        [0.5231, 0.4769],
[2K        [0.6110, 0.3890]], device='cuda:0')4mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 30/203 [2m0:00:17 â€¢ 0:01:37[0m [2;4m1.79it/s[0m  
[2KAttention weights: tensor([[0.5208, 0.4792],mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 31/203 [2m0:00:17 â€¢ 0:01:36[0m [2;4m1.81it/s[0m   
        [0.5125, 0.4875],
        [0.5144, 0.4856],
        [0.5200, 0.4800],
        [0.5218, 0.4782],
        [0.5245, 0.4755],
        [0.5218, 0.4782],
        [0.5189, 0.4811],
        [0.5157, 0.4843],
[2K        [0.5220, 0.4780]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 31/203 [2m0:00:17 â€¢ 0:01:36[0m [2;4m1.81it/s[0m  
[2KClass label: CleanAndJerkâ”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 31/203 [2m0:00:17 â€¢ 0:01:36[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.4943, 0.5057]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:17 â€¢ 0:01:36[0m [2;4m1.81it/s[0m  
[2KStep 0 - TTA Loss: 0.7247â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 31/203 [2m0:00:17 â€¢ 0:01:36[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.4943, 0.5057]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:17 â€¢ 0:01:36[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.4948, 0.5052]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:17 â€¢ 0:01:36[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.4970, 0.5030]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:17 â€¢ 0:01:36[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.5014, 0.4986]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:17 â€¢ 0:01:36[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.5101, 0.4899]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:17 â€¢ 0:01:36[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.5250, 0.4750]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:17 â€¢ 0:01:36[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.5440, 0.4560]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:17 â€¢ 0:01:36[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.5698, 0.4302]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:17 â€¢ 0:01:36[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.5942, 0.4058]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:17 â€¢ 0:01:36[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.5184, 0.4816]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 31/203 [2m0:00:17 â€¢ 0:01:36[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.6169, 0.3831],0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 31/203 [2m0:00:17 â€¢ 0:01:36[0m [2;4m1.81it/s[0m  
        [0.4191, 0.5809],
        [0.6207, 0.3793],
        [0.6191, 0.3809],
        [0.6007, 0.3993],
        [0.7029, 0.2971],
        [0.6642, 0.3358],
        [0.6089, 0.3911],
        [0.5291, 0.4709],
[2K        [0.6121, 0.3879]], device='cuda:0')[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 31/203 [2m0:00:17 â€¢ 0:01:36[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.5208, 0.4792],mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 32/203 [2m0:00:18 â€¢ 0:01:35[0m [2;4m1.80it/s[0m  
        [0.5125, 0.4875],
        [0.5143, 0.4857],
        [0.5200, 0.4800],
        [0.5218, 0.4782],
        [0.5245, 0.4755],
        [0.5218, 0.4782],
        [0.5189, 0.4811],
        [0.5157, 0.4843],
[2K        [0.5220, 0.4780]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 32/203 [2m0:00:18 â€¢ 0:01:35[0m [2;4m1.80it/s[0m  
[2KClass label: TennisSwingâ”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 32/203 [2m0:00:18 â€¢ 0:01:35[0m [2;4m1.80it/s[0m  
[2KAttention weights: tensor([[0.5191, 0.4809]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:18 â€¢ 0:01:35[0m [2;4m1.80it/s[0m  
[2KStep 0 - TTA Loss: 1.4464â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 32/203 [2m0:00:18 â€¢ 0:01:35[0m [2;4m1.80it/s[0m  
[2KAttention weights: tensor([[0.5253, 0.4747]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:18 â€¢ 0:01:35[0m [2;4m1.80it/s[0m  
[2KAttention weights: tensor([[0.5441, 0.4559]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:18 â€¢ 0:01:35[0m [2;4m1.80it/s[0m  
[2KAttention weights: tensor([[0.5704, 0.4296]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:18 â€¢ 0:01:35[0m [2;4m1.80it/s[0m  
[2KAttention weights: tensor([[0.5956, 0.4044]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:18 â€¢ 0:01:35[0m [2;4m1.80it/s[0m  
[2KAttention weights: tensor([[0.6169, 0.3831]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:18 â€¢ 0:01:35[0m [2;4m1.80it/s[0m  
[2KAttention weights: tensor([[0.6329, 0.3671]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:18 â€¢ 0:01:35[0m [2;4m1.80it/s[0m  
[2KAttention weights: tensor([[0.6424, 0.3576]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:18 â€¢ 0:01:35[0m [2;4m1.80it/s[0m  
[2KAttention weights: tensor([[0.6476, 0.3524]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:18 â€¢ 0:01:35[0m [2;4m1.80it/s[0m  
[2KAttention weights: tensor([[0.6496, 0.3504]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:18 â€¢ 0:01:35[0m [2;4m1.80it/s[0m  
[2KAttention weights: tensor([[0.5198, 0.4802]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 32/203 [2m0:00:18 â€¢ 0:01:35[0m [2;4m1.80it/s[0m  
[2KAttention weights: tensor([[0.5208, 0.4792],6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 33/203 [2m0:00:18 â€¢ 0:01:34[0m [2;4m1.81it/s[0m  
        [0.5125, 0.4875],
        [0.5143, 0.4857],
        [0.5200, 0.4800],
        [0.5218, 0.4782],
        [0.5245, 0.4755],
        [0.5218, 0.4782],
        [0.5189, 0.4811],
        [0.5157, 0.4843],
[2K        [0.5220, 0.4780]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 33/203 [2m0:00:18 â€¢ 0:01:34[0m [2;4m1.81it/s[0m  
[2KClass label: CleanAndJerkâ”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 33/203 [2m0:00:18 â€¢ 0:01:34[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.4930, 0.5070]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:18 â€¢ 0:01:34[0m [2;4m1.81it/s[0m  
[2KStep 0 - TTA Loss: 1.4945â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 33/203 [2m0:00:18 â€¢ 0:01:34[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.4930, 0.5070]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:18 â€¢ 0:01:34[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.4928, 0.5072]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:18 â€¢ 0:01:34[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.4938, 0.5062]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:18 â€¢ 0:01:34[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.4943, 0.5057]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:18 â€¢ 0:01:34[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.5023, 0.4977]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:18 â€¢ 0:01:34[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.5105, 0.4895]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:18 â€¢ 0:01:34[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.5218, 0.4782]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:18 â€¢ 0:01:34[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.5412, 0.4588]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:18 â€¢ 0:01:34[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.5584, 0.4416]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:18 â€¢ 0:01:34[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.5167, 0.4833]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 33/203 [2m0:00:18 â€¢ 0:01:34[0m [2;4m1.81it/s[0m  
[2KAttention weights: tensor([[0.5208, 0.4792],6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 34/203 [2m0:00:19 â€¢ 0:01:33[0m [2;4m1.82it/s[0m  
        [0.5125, 0.4875],
        [0.5143, 0.4857],
        [0.5200, 0.4800],
        [0.5218, 0.4782],
        [0.5245, 0.4755],
        [0.5218, 0.4782],
        [0.5189, 0.4811],
        [0.5157, 0.4843],
[2K        [0.5220, 0.4780]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 34/203 [2m0:00:19 â€¢ 0:01:33[0m [2;4m1.82it/s[0m  
[2KClass label: HammerThrowâ”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 34/203 [2m0:00:19 â€¢ 0:01:33[0m [2;4m1.82it/s[0m  
[2KAttention weights: tensor([[0.5817, 0.4183]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:19 â€¢ 0:01:33[0m [2;4m1.82it/s[0m  
[2KStep 0 - TTA Loss: 2.1642â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 34/203 [2m0:00:19 â€¢ 0:01:33[0m [2;4m1.82it/s[0m  
[2KAttention weights: tensor([[0.5889, 0.4111]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:19 â€¢ 0:01:33[0m [2;4m1.82it/s[0m  
[2KAttention weights: tensor([[0.5901, 0.4099]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:19 â€¢ 0:01:33[0m [2;4m1.82it/s[0m  
[2KAttention weights: tensor([[0.5881, 0.4119]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:19 â€¢ 0:01:33[0m [2;4m1.82it/s[0m  
[2KAttention weights: tensor([[0.5892, 0.4108]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:19 â€¢ 0:01:33[0m [2;4m1.82it/s[0m  
[2KAttention weights: tensor([[0.5940, 0.4060]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:19 â€¢ 0:01:33[0m [2;4m1.82it/s[0m  
[2KAttention weights: tensor([[0.6089, 0.3911]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:19 â€¢ 0:01:33[0m [2;4m1.82it/s[0m  
[2KAttention weights: tensor([[0.6181, 0.3819]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:19 â€¢ 0:01:33[0m [2;4m1.82it/s[0m  
[2KAttention weights: tensor([[0.6240, 0.3760]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:19 â€¢ 0:01:33[0m [2;4m1.82it/s[0m  
[2KAttention weights: tensor([[0.6263, 0.3737]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:19 â€¢ 0:01:33[0m [2;4m1.82it/s[0m  
[2KAttention weights: tensor([[0.5234, 0.4766]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 34/203 [2m0:00:19 â€¢ 0:01:33[0m [2;4m1.82it/s[0m  
[2KTesting [38;2;98;6;224mâ”â”â”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 34/203 [2m0:00:19 â€¢ 0:01:33[0m [2;4m1.82it/s[0m

Detected KeyboardInterrupt, attempting graceful shutdown ...
[?25h[[36m2025-02-14 10:58:23,738[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/def/fewshot/logs/train/runs/2025-02-14_10-57-54[0m
[[36m2025-02-14 10:58:23,739[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
