[[36m2025-02-14 00:43:09,072[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
You are using a CUDA device ('NVIDIA GeForce RTX 4070 SUPER') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
No of videos in train is 214
Loading train Video Information ...
No of class 10
No of videos in validation is 203
Loading validation Video Information ...
No of class 10
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”“
â”ƒ[1;35m [0m[1;35m   [0m[1;35m [0mâ”ƒ[1;35m [0m[1;35mName                                                   [0m[1;35m [0mâ”ƒ[1;35m [0m[1;35mType                           [0m[1;35m [0mâ”ƒ[1;35m [0m[1;35mParams[0m[1;35m [0mâ”ƒ[1;35m [0m[1;35mMode [0m[1;35m [0mâ”ƒ
â”¡â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”©
â”‚[2m [0m[2m0  [0m[2m [0mâ”‚ net                                                     â”‚ T3ALNet                         â”‚  639 M â”‚ train â”‚
â”‚[2m [0m[2m1  [0m[2m [0mâ”‚ net.model                                               â”‚ CoCa                            â”‚  638 M â”‚ train â”‚
â”‚[2m [0m[2m2  [0m[2m [0mâ”‚ net.model.text                                          â”‚ TextTransformer                 â”‚  123 M â”‚ train â”‚
â”‚[2m [0m[2m3  [0m[2m [0mâ”‚ net.model.text.token_embedding                          â”‚ Embedding                       â”‚ 37.9 M â”‚ train â”‚
â”‚[2m [0m[2m4  [0m[2m [0mâ”‚ net.model.text.transformer                              â”‚ Transformer                     â”‚ 85.1 M â”‚ train â”‚
â”‚[2m [0m[2m5  [0m[2m [0mâ”‚ net.model.text.transformer.resblocks                    â”‚ ModuleList                      â”‚ 85.1 M â”‚ train â”‚
â”‚[2m [0m[2m6  [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m7  [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m8  [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m9  [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m10 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m11 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m12 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m13 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m14 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m15 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m16 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.0.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m17 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m18 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m19 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m20 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m21 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m22 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m23 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m24 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m25 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m26 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m27 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.1.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m28 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m29 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m30 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m31 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m32 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m33 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m34 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m35 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m36 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m37 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m38 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.2.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m39 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m40 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m41 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m42 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m43 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m44 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m45 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m46 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m47 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m48 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m49 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.3.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m50 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m51 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m52 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m53 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m54 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m55 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m56 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m57 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m58 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m59 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m60 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.4.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m61 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m62 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m63 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m64 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m65 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m66 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m67 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m68 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m69 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m70 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m71 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.5.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m72 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m73 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m74 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m75 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m76 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m77 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m78 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m79 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m80 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m81 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m82 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.6.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m83 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m84 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m85 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m86 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m87 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m88 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m89 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m90 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m91 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m92 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m93 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.7.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m94 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m95 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m96 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m97 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m98 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m99 [0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m100[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m101[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m102[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m103[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m104[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.8.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m105[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9                  â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m106[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.ln_1             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m107[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.attn             â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m108[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.attn.out_proj    â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m109[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.ls_1             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m110[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.ln_2             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m111[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.mlp              â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m112[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.mlp.c_fc         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m113[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.mlp.gelu         â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m114[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.mlp.c_proj       â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m115[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.9.ls_2             â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m116[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10                 â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m117[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.ln_1            â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m118[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.attn            â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m119[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.attn.out_proj   â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m120[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.ls_1            â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m121[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.ln_2            â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m122[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.mlp             â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m123[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.mlp.c_fc        â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m124[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.mlp.gelu        â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m125[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.mlp.c_proj      â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m126[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.10.ls_2            â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m127[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11                 â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m128[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.ln_1            â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m129[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.attn            â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m130[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.attn.out_proj   â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m131[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.ls_1            â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m132[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.ln_2            â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m133[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.mlp             â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m134[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.mlp.c_fc        â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m135[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.mlp.gelu        â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m136[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.mlp.c_proj      â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m137[0m[2m [0mâ”‚ net.model.text.transformer.resblocks.11.ls_2            â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m138[0m[2m [0mâ”‚ net.model.text.ln_final                                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m139[0m[2m [0mâ”‚ net.model.visual                                        â”‚ VisionTransformer               â”‚  306 M â”‚ train â”‚
â”‚[2m [0m[2m140[0m[2m [0mâ”‚ net.model.visual.conv1                                  â”‚ Conv2d                          â”‚  602 K â”‚ train â”‚
â”‚[2m [0m[2m141[0m[2m [0mâ”‚ net.model.visual.patch_dropout                          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m142[0m[2m [0mâ”‚ net.model.visual.ln_pre                                 â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m143[0m[2m [0mâ”‚ net.model.visual.transformer                            â”‚ Transformer                     â”‚  302 M â”‚ train â”‚
â”‚[2m [0m[2m144[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks                  â”‚ ModuleList                      â”‚  302 M â”‚ train â”‚
â”‚[2m [0m[2m145[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m146[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m147[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m148[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m149[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m150[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m151[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m152[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m153[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m154[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m155[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.0.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m156[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m157[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m158[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m159[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m160[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m161[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m162[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m163[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m164[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m165[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m166[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.1.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m167[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m168[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m169[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m170[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m171[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m172[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m173[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m174[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m175[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m176[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m177[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.2.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m178[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m179[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m180[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m181[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m182[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m183[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m184[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m185[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m186[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m187[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m188[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.3.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m189[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m190[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m191[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m192[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m193[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m194[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m195[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m196[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m197[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m198[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m199[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.4.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m200[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m201[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m202[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m203[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m204[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m205[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m206[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m207[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m208[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m209[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m210[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.5.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m211[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m212[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m213[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m214[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m215[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m216[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m217[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m218[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m219[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m220[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m221[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.6.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m222[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m223[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m224[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m225[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m226[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m227[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m228[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m229[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m230[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m231[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m232[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.7.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m233[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m234[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m235[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m236[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m237[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m238[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m239[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m240[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m241[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m242[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m243[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.8.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m244[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9                â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m245[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.ln_1           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m246[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.attn           â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m247[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.attn.out_proj  â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m248[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.ls_1           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m249[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.ln_2           â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m250[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.mlp            â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m251[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.mlp.c_fc       â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m252[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.mlp.gelu       â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m253[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.mlp.c_proj     â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m254[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.9.ls_2           â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m255[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m256[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m257[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m258[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m259[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m260[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m261[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m262[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m263[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m264[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m265[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.10.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m266[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m267[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m268[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m269[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m270[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m271[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m272[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m273[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m274[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m275[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m276[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.11.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m277[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m278[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m279[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m280[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m281[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m282[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m283[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m284[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m285[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m286[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m287[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.12.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m288[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m289[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m290[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m291[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m292[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m293[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m294[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m295[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m296[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m297[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m298[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.13.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m299[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m300[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m301[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m302[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m303[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m304[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m305[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m306[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m307[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m308[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m309[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.14.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m310[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m311[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m312[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m313[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m314[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m315[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m316[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m317[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m318[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m319[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m320[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.15.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m321[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m322[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m323[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m324[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m325[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m326[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m327[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m328[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m329[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m330[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m331[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.16.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m332[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m333[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m334[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m335[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m336[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m337[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m338[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m339[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m340[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m341[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m342[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.17.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m343[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m344[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m345[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m346[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m347[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m348[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m349[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m350[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m351[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m352[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m353[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.18.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m354[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m355[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m356[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m357[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m358[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m359[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m360[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m361[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m362[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m363[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m364[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.19.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m365[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m366[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m367[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m368[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m369[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m370[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m371[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m372[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m373[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m374[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m375[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.20.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m376[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m377[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m378[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m379[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m380[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m381[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m382[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m383[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m384[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m385[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m386[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.21.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m387[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m388[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m389[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m390[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m391[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m392[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m393[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m394[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m395[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m396[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m397[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.22.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m398[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23               â”‚ ResidualAttentionBlock          â”‚ 12.6 M â”‚ train â”‚
â”‚[2m [0m[2m399[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.ln_1          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m400[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.attn          â”‚ MultiheadAttention              â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m401[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.attn.out_proj â”‚ NonDynamicallyQuantizableLinear â”‚  1.0 M â”‚ train â”‚
â”‚[2m [0m[2m402[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.ls_1          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m403[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.ln_2          â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m404[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.mlp           â”‚ Sequential                      â”‚  8.4 M â”‚ train â”‚
â”‚[2m [0m[2m405[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.mlp.c_fc      â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m406[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.mlp.gelu      â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m407[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.mlp.c_proj    â”‚ Linear                          â”‚  4.2 M â”‚ train â”‚
â”‚[2m [0m[2m408[0m[2m [0mâ”‚ net.model.visual.transformer.resblocks.23.ls_2          â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m409[0m[2m [0mâ”‚ net.model.visual.attn_pool                              â”‚ AttentionalPooler               â”‚  3.0 M â”‚ train â”‚
â”‚[2m [0m[2m410[0m[2m [0mâ”‚ net.model.visual.attn_pool.attn                         â”‚ MultiheadAttention              â”‚  2.8 M â”‚ train â”‚
â”‚[2m [0m[2m411[0m[2m [0mâ”‚ net.model.visual.attn_pool.attn.out_proj                â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m412[0m[2m [0mâ”‚ net.model.visual.attn_pool.ln_q                         â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m413[0m[2m [0mâ”‚ net.model.visual.attn_pool.ln_k                         â”‚ LayerNorm                       â”‚  2.0 K â”‚ train â”‚
â”‚[2m [0m[2m414[0m[2m [0mâ”‚ net.model.visual.ln_post                                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m415[0m[2m [0mâ”‚ net.model.text_decoder                                  â”‚ MultimodalTransformer           â”‚  208 M â”‚ train â”‚
â”‚[2m [0m[2m416[0m[2m [0mâ”‚ net.model.text_decoder.resblocks                        â”‚ ModuleList                      â”‚ 85.1 M â”‚ train â”‚
â”‚[2m [0m[2m417[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m418[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m419[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m420[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m421[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m422[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m423[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m424[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m425[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m426[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m427[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.0.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m428[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m429[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m430[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m431[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m432[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m433[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m434[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m435[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m436[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m437[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m438[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.1.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m439[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m440[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m441[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m442[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m443[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m444[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m445[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m446[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m447[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m448[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m449[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.2.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m450[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m451[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m452[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m453[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m454[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m455[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m456[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m457[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m458[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m459[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m460[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.3.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m461[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m462[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m463[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m464[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m465[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m466[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m467[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m468[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m469[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m470[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m471[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.4.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m472[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m473[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m474[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m475[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m476[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m477[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m478[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m479[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m480[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m481[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m482[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.5.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m483[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m484[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m485[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m486[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m487[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m488[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m489[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m490[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m491[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m492[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m493[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.6.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m494[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m495[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m496[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m497[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m498[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m499[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m500[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m501[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m502[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m503[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m504[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.7.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m505[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m506[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m507[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m508[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m509[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m510[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m511[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m512[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m513[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m514[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m515[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.8.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m516[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9                      â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m517[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.ln_1                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m518[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.attn                 â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m519[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.attn.out_proj        â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m520[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.ls_1                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m521[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.ln_2                 â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m522[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.mlp                  â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m523[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.mlp.c_fc             â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m524[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.mlp.gelu             â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m525[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.mlp.c_proj           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m526[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.9.ls_2                 â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m527[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m528[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m529[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m530[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m531[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m532[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m533[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m534[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m535[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m536[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m537[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.10.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m538[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m539[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m540[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m541[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m542[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m543[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m544[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m545[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m546[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m547[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m548[0m[2m [0mâ”‚ net.model.text_decoder.resblocks.11.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m549[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn                       â”‚ ModuleList                      â”‚ 85.1 M â”‚ train â”‚
â”‚[2m [0m[2m550[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m551[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m552[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m553[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m554[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m555[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m556[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m557[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m558[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m559[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m560[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m561[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.0.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m562[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m563[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m564[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m565[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m566[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m567[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m568[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m569[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m570[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m571[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m572[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m573[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.1.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m574[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m575[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m576[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m577[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m578[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m579[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m580[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m581[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m582[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m583[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m584[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m585[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.2.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m586[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m587[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m588[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m589[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m590[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m591[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m592[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m593[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m594[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m595[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m596[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m597[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.3.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m598[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m599[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m600[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m601[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m602[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m603[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m604[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m605[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m606[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m607[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m608[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m609[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.4.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m610[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m611[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m612[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m613[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m614[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m615[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m616[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m617[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m618[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m619[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m620[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m621[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.5.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m622[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m623[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m624[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m625[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m626[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m627[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m628[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m629[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m630[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m631[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m632[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m633[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.6.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m634[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m635[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m636[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m637[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m638[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m639[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m640[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m641[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m642[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m643[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m644[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m645[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.7.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m646[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m647[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m648[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m649[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m650[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m651[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m652[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m653[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m654[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m655[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m656[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m657[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.8.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m658[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9                     â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m659[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.ln_1                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m660[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.attn                â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m661[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.attn.out_proj       â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m662[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.ls_1                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m663[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.ln_1_kv             â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m664[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.ln_2                â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m665[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.mlp                 â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m666[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.mlp.c_fc            â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m667[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.mlp.gelu            â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m668[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.mlp.c_proj          â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m669[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.9.ls_2                â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m670[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10                    â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m671[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.ln_1               â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m672[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.attn               â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m673[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.attn.out_proj      â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m674[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.ls_1               â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m675[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.ln_1_kv            â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m676[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.ln_2               â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m677[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.mlp                â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m678[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.mlp.c_fc           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m679[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.mlp.gelu           â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m680[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.mlp.c_proj         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m681[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.10.ls_2               â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m682[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11                    â”‚ ResidualAttentionBlock          â”‚  7.1 M â”‚ train â”‚
â”‚[2m [0m[2m683[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.ln_1               â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m684[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.attn               â”‚ MultiheadAttention              â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m685[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.attn.out_proj      â”‚ NonDynamicallyQuantizableLinear â”‚  590 K â”‚ train â”‚
â”‚[2m [0m[2m686[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.ls_1               â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m687[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.ln_1_kv            â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m688[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.ln_2               â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m689[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.mlp                â”‚ Sequential                      â”‚  4.7 M â”‚ train â”‚
â”‚[2m [0m[2m690[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.mlp.c_fc           â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m691[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.mlp.gelu           â”‚ GELU                            â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m692[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.mlp.c_proj         â”‚ Linear                          â”‚  2.4 M â”‚ train â”‚
â”‚[2m [0m[2m693[0m[2m [0mâ”‚ net.model.text_decoder.cross_attn.11.ls_2               â”‚ Identity                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m694[0m[2m [0mâ”‚ net.model.text_decoder.ln_final                         â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m695[0m[2m [0mâ”‚ net.tta_loss                                            â”‚ ByolLoss                        â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m696[0m[2m [0mâ”‚ net.video_proj                                          â”‚ VideoProjector                  â”‚  591 K â”‚ train â”‚
â”‚[2m [0m[2m697[0m[2m [0mâ”‚ net.video_proj.transform                                â”‚ Sequential                      â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m698[0m[2m [0mâ”‚ net.video_proj.transform.0                              â”‚ LayerNorm                       â”‚  1.5 K â”‚ train â”‚
â”‚[2m [0m[2m699[0m[2m [0mâ”‚ net.video_proj.transform.1                              â”‚ Dropout                         â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m700[0m[2m [0mâ”‚ net.fusion                                              â”‚ Fusion                          â”‚  6.2 K â”‚ train â”‚
â”‚[2m [0m[2m701[0m[2m [0mâ”‚ net.fusion.attn                                         â”‚ Sequential                      â”‚  6.2 K â”‚ train â”‚
â”‚[2m [0m[2m702[0m[2m [0mâ”‚ net.fusion.attn.0                                       â”‚ Linear                          â”‚  6.1 K â”‚ train â”‚
â”‚[2m [0m[2m703[0m[2m [0mâ”‚ net.fusion.attn.1                                       â”‚ Linear                          â”‚     10 â”‚ train â”‚
â”‚[2m [0m[2m704[0m[2m [0mâ”‚ net.fusion.attn.2                                       â”‚ Softmax                         â”‚      0 â”‚ train â”‚
â”‚[2m [0m[2m705[0m[2m [0mâ”‚ binary_acc                                              â”‚ BinaryAccuracy                  â”‚      0 â”‚ train â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜
[1mTrainable params[0m: 1.2 M
[1mNon-trainable params[0m: 637 M
[1mTotal params[0m: 639 M
[1mTotal estimated model params size (MB)[0m: 2.6 K
[1mModules in train mode[0m: 706
[1mModules in eval mode[0m: 0
/home/def/miniforge3/envs/fewshot/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.
`Trainer.fit` stopped: `max_epochs=0` reached.
[[36m2025-02-14 00:43:10,774[0m][[34m__main__[0m][[32mINFO[0m] - Starting testing![0m
[[36m2025-02-14 00:43:10,775[0m][[34m__main__[0m][[33mWARNING[0m] - Best ckpt not found! Using current weights for testing...[0m
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/def/miniforge3/envs/fewshot/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.
[2KStart testing...
[2KAttention weights: tensor([[0.5582, 0.4418],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 0/203 [2m0:00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
        [0.5540, 0.4460],
        [0.5547, 0.4453],
        [0.5570, 0.4430],
        [0.5559, 0.4441],
        [0.5534, 0.4466],
        [0.5526, 0.4474],
        [0.5577, 0.4423],
        [0.5582, 0.4418],
[2K        [0.5555, 0.4445]], device='cuda:0', grad_fn=<SoftmaxBackward0>):00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2K/home/def/fewshot/src/models/components/tt_method.py:317: UserWarning: The use of `x.T` on tensors of dimension other than 2
to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of
matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at
../aten/src/ATen/native/TensorShape.cpp:3683.)
  dot_product = (x @ y.T)
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 0/203 [2m0:00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5939, 0.4061]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KStep 0 - TTA Loss: 0.6590â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 0/203 [2m0:00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.8330, 0.1670]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 7.9677e-11]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 7.3080e-18]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.1630e-23]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 7.9891e-28]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.1009e-31]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.6975e-34]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.2810e-36]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.8095e-38]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 6.3575e-40]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KStep 10 - TTA Loss: 1.4684â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 0/203 [2m0:00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.7744e-41]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 6.7248e-42]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.5849e-42]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 5.6472e-43]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.8026e-43]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.7796e-43]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.3593e-43]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.1771e-43]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.1210e-43]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[0.6017, 0.3983]], device='cuda:0')203 [2m0:00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.1854e-32],â”â”â”[0m 0/203 [2m0:00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
        [1.0000e+00, 3.2071e-21],
        [1.0000e+00, 4.1189e-28],
        [1.0000e+00, 2.3243e-28],
        [1.0000e+00, 4.2871e-37],
        [1.0000e+00, 9.9492e-44],
        [1.0000e+00, 5.3729e-38],
        [1.0000e+00, 1.0458e-31],
        [1.0000e+00, 8.8884e-30],
[2K        [1.0000e+00, 6.0601e-36]], device='cuda:0')â”â”â”â”[0m 0/203 [2m0:00:00 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5582, 0.4418],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 1/203 [2m0:00:01 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
        [0.5540, 0.4460],
        [0.5548, 0.4452],
        [0.5570, 0.4430],
        [0.5560, 0.4440],
        [0.5534, 0.4466],
        [0.5527, 0.4473],
        [0.5578, 0.4422],
        [0.5582, 0.4418],
[2K        [0.5555, 0.4445]], device='cuda:0', grad_fn=<SoftmaxBackward0>):00:01 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KClass label: HighJumpâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 1/203 [2m0:00:01 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5913, 0.4087]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KStep 0 - TTA Loss: 0.8867â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 1/203 [2m0:00:01 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5913, 0.4087]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5953, 0.4047]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.6120, 0.3880]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.6476, 0.3524]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.7020, 0.2980]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.7620, 0.2380]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.8026, 0.1974]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.9472, 0.0528]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[9.9961e-01, 3.9108e-04]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[9.9999e-01, 9.6782e-06]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KStep 10 - TTA Loss: 1.5848â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 1/203 [2m0:00:01 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.9081e-07]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.0846e-09]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.4344e-11]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 6.1347e-13]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 8.7858e-15]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.3903e-16]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.5710e-18]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 5.8198e-20]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.6725e-21]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0.00it/s[0m  
[2KAttention weights: tensor([[0.5868, 0.4132]], device='cuda:0')203 [2m0:00:01 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.0659e-17],â”â”â”[0m 1/203 [2m0:00:01 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
        [1.0000e+00, 9.2735e-12],
        [1.0000e+00, 4.6072e-15],
        [1.0000e+00, 3.7190e-15],
        [1.0000e+00, 1.3157e-19],
        [1.0000e+00, 5.9610e-23],
        [1.0000e+00, 4.3487e-20],
        [1.0000e+00, 8.0897e-17],
        [1.0000e+00, 8.8325e-16],
[2K        [1.0000e+00, 4.7002e-19]], device='cuda:0')â”â”â”â”[0m 1/203 [2m0:00:01 â€¢ -:--:--[0m [2;4m0.00it/s[0m  
[2KAttention weights: tensor([[0.5581, 0.4419],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 2/203 [2m0:00:02 â€¢ 0:02:30[0m [2;4m1.34it/s[0m  
        [0.5539, 0.4461],
        [0.5547, 0.4453],
        [0.5570, 0.4430],
        [0.5559, 0.4441],
        [0.5533, 0.4467],
        [0.5525, 0.4475],
        [0.5577, 0.4423],
        [0.5581, 0.4419],
[2K        [0.5554, 0.4446]], device='cuda:0', grad_fn=<SoftmaxBackward0>):00:02 â€¢ 0:02:30[0m [2;4m1.34it/s[0m  
[2KClass label: TennisSwingâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 2/203 [2m0:00:02 â€¢ 0:02:30[0m [2;4m1.34it/s[0m  
[2KAttention weights: tensor([[0.6437, 0.3563]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m1.34it/s[0m  
[2KStep 0 - TTA Loss: 0.5263â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 2/203 [2m0:00:02 â€¢ 0:02:30[0m [2;4m1.34it/s[0m  
[2KAttention weights: tensor([[0.8938, 0.1062]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m1.34it/s[0m  
[2KAttention weights: tensor([[0.9968, 0.0032]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m [2;4m1.34it/s[0m  
[2KAttention weights: tensor([[1.4646e-04, 9.9985e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)1.34it/s[0m  
[2KAttention weights: tensor([[4.2633e-10, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)1.34it/s[0m  
[2KAttention weights: tensor([[6.9833e-15, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)1.34it/s[0m  
[2KAttention weights: tensor([[5.8643e-19, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)1.34it/s[0m  
[2KAttention weights: tensor([[2.2418e-22, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)1.34it/s[0m  
[2KAttention weights: tensor([[3.3641e-25, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)1.34it/s[0m  
[2KAttention weights: tensor([[1.7021e-27, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)1.34it/s[0m  
[2KAttention weights: tensor([[2.5031e-29, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)1.34it/s[0m  
[2KStep 10 - TTA Loss: 0.9721â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 2/203 [2m0:00:02 â€¢ 0:02:30[0m [2;4m1.34it/s[0m  
[2KAttention weights: tensor([[9.2390e-31, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)1.34it/s[0m  
[2KAttention weights: tensor([[7.4485e-32, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)1.34it/s[0m  
[2KAttention weights: tensor([[1.1506e-32, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)1.34it/s[0m  
[2KAttention weights: tensor([[3.0219e-33, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)1.34it/s[0m  
[2KAttention weights: tensor([[1.2112e-33, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)1.34it/s[0m  
[2KAttention weights: tensor([[6.7247e-34, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)1.34it/s[0m  
[2KAttention weights: tensor([[4.7506e-34, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)1.34it/s[0m  
[2KAttention weights: tensor([[3.9671e-34, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)1.34it/s[0m  
[2KAttention weights: tensor([[3.6865e-34, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)1.34it/s[0m  
[2KAttention weights: tensor([[0.4932, 0.5068]], device='cuda:0')203 [2m0:00:02 â€¢ 0:02:30[0m [2;4m1.34it/s[0m  
[2KAttention weights: tensor([[8.3785e-20, 1.0000e+00],â”â”â”[0m 2/203 [2m0:00:02 â€¢ 0:02:30[0m [2;4m1.34it/s[0m  
        [1.2673e-15, 1.0000e+00],
        [6.0913e-12, 1.0000e+00],
        [1.8712e-21, 1.0000e+00],
        [3.8303e-15, 1.0000e+00],
        [8.5588e-14, 1.0000e+00],
        [5.8037e-08, 1.0000e+00],
        [4.5897e-12, 1.0000e+00],
        [3.5558e-34, 1.0000e+00],
[2K        [9.2523e-18, 1.0000e+00]], device='cuda:0')â”â”â”â”[0m 2/203 [2m0:00:02 â€¢ 0:02:30[0m [2;4m1.34it/s[0m  
[2KAttention weights: tensor([[0.5580, 0.4420],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 3/203 [2m0:00:02 â€¢ 0:02:23[0m [2;4m1.40it/s[0m  
        [0.5538, 0.4462],
        [0.5546, 0.4454],
        [0.5568, 0.4432],
        [0.5557, 0.4443],
        [0.5531, 0.4469],
        [0.5524, 0.4476],
        [0.5576, 0.4424],
        [0.5580, 0.4420],
[2K        [0.5553, 0.4447]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”[0m 3/203 [2m0:00:02 â€¢ 0:02:23[0m [2;4m1.40it/s[0m  
[2KClass label: HighJump[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 3/203 [2m0:00:02 â€¢ 0:02:23[0m [2;4m1.40it/s[0m  
[2KAttention weights: tensor([[0.5921, 0.4079]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:02 â€¢ 0:02:23[0m [2;4m1.40it/s[0m  
[2KStep 0 - TTA Loss: 0.5903[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 3/203 [2m0:00:02 â€¢ 0:02:23[0m [2;4m1.40it/s[0m  
[2KAttention weights: tensor([[0.5921, 0.4079]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:02 â€¢ 0:02:23[0m [2;4m1.40it/s[0m  
[2KAttention weights: tensor([[0.5923, 0.4077]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:02 â€¢ 0:02:23[0m [2;4m1.40it/s[0m  
[2KAttention weights: tensor([[0.5968, 0.4032]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:02 â€¢ 0:02:23[0m [2;4m1.40it/s[0m  
[2KAttention weights: tensor([[0.6150, 0.3850]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:02 â€¢ 0:02:23[0m [2;4m1.40it/s[0m  
[2KAttention weights: tensor([[0.6551, 0.3449]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:02 â€¢ 0:02:23[0m [2;4m1.40it/s[0m  
[2KAttention weights: tensor([[0.7134, 0.2866]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:02 â€¢ 0:02:23[0m [2;4m1.40it/s[0m  
[2KAttention weights: tensor([[0.7731, 0.2269]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:02 â€¢ 0:02:23[0m [2;4m1.40it/s[0m  
[2KAttention weights: tensor([[0.8966, 0.1034]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:02 â€¢ 0:02:23[0m [2;4m1.40it/s[0m  
[2KAttention weights: tensor([[0.9969, 0.0031]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:02 â€¢ 0:02:23[0m [2;4m1.40it/s[0m  
[2KAttention weights: tensor([[0.9987, 0.0013]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:02 â€¢ 0:02:23[0m [2;4m1.40it/s[0m  
[2KStep 10 - TTA Loss: 1.1527[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 3/203 [2m0:00:02 â€¢ 0:02:23[0m [2;4m1.40it/s[0m  
[2KAttention weights: tensor([[0.9945, 0.0055]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:02 â€¢ 0:02:23[0m [2;4m1.40it/s[0m  
[2KAttention weights: tensor([[0.9585, 0.0415]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:02 â€¢ 0:02:23[0m [2;4m1.40it/s[0m  
[2KAttention weights: tensor([[0.9887, 0.0113]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:02 â€¢ 0:02:23[0m [2;4m1.40it/s[0m  
[2KAttention weights: tensor([[9.9982e-01, 1.8204e-04]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:02:23[0m [2;4m1.40it/s[0m  
[2KAttention weights: tensor([[9.9999e-01, 8.2241e-06]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:02:23[0m [2;4m1.40it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.3560e-07]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:02:23[0m [2;4m1.40it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.6512e-08]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:02:23[0m [2;4m1.40it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.9006e-09]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:02:23[0m [2;4m1.40it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.6404e-10]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:02:23[0m [2;4m1.40it/s[0m  
[2KAttention weights: tensor([[0.5738, 0.4262]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 3/203 [2m0:00:02 â€¢ 0:02:23[0m [2;4m1.40it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 9.3833e-08],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 3/203 [2m0:00:02 â€¢ 0:02:23[0m [2;4m1.40it/s[0m  
        [9.9997e-01, 3.4927e-05],
        [1.0000e+00, 2.3217e-07],
        [1.0000e+00, 2.1707e-06],
        [1.0000e+00, 1.5708e-09],
        [1.0000e+00, 1.6884e-11],
        [1.0000e+00, 2.1317e-10],
        [1.0000e+00, 3.0248e-08],
        [9.9998e-01, 1.6758e-05],
[2K        [1.0000e+00, 5.1895e-09]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 3/203 [2m0:00:02 â€¢ 0:02:23[0m [2;4m1.40it/s[0m  
[2KAttention weights: tensor([[0.5580, 0.4420],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 4/203 [2m0:00:04 â€¢ 0:03:42[0m [2;4m0.90it/s[0m  
        [0.5538, 0.4462],
        [0.5546, 0.4454],
        [0.5568, 0.4432],
        [0.5557, 0.4443],
        [0.5531, 0.4469],
        [0.5524, 0.4476],
        [0.5576, 0.4424],
        [0.5580, 0.4420],
[2K        [0.5553, 0.4447]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”[0m 4/203 [2m0:00:04 â€¢ 0:03:42[0m [2;4m0.90it/s[0m  
[2KClass label: GolfSwing[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 4/203 [2m0:00:04 â€¢ 0:03:42[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.6273, 0.3727]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:04 â€¢ 0:03:42[0m [2;4m0.90it/s[0m  
[2KStep 0 - TTA Loss: 0.8786[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 4/203 [2m0:00:04 â€¢ 0:03:42[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.8968, 0.1032]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:04 â€¢ 0:03:42[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.9600, 0.0400]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:04 â€¢ 0:03:42[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5557, 0.4443]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:04 â€¢ 0:03:42[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.0644, 0.9356]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:04 â€¢ 0:03:42[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.0057, 0.9943]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:04 â€¢ 0:03:42[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[7.7749e-04, 9.9922e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:03:42[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[1.5642e-04, 9.9984e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:03:42[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[4.3700e-05, 9.9996e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:03:42[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[1.6034e-05, 9.9998e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:03:42[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[7.3650e-06, 9.9999e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:03:42[0m [2;4m0.90it/s[0m  
[2KStep 10 - TTA Loss: 1.1175[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 4/203 [2m0:00:04 â€¢ 0:03:42[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[4.0663e-06, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:03:42[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[2.6196e-06, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:03:42[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[1.9104e-06, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:03:42[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[1.5324e-06, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:03:42[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[1.3257e-06, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:03:42[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[1.2123e-06, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:03:42[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[1.1528e-06, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:03:42[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[1.1244e-06, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:03:42[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[1.1137e-06, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:03:42[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5427, 0.4573]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 4/203 [2m0:00:04 â€¢ 0:03:42[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[3.8102e-04, 9.9962e-01],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 4/203 [2m0:00:04 â€¢ 0:03:42[0m [2;4m0.90it/s[0m  
        [1.5975e-01, 8.4025e-01],
        [1.5423e-01, 8.4577e-01],
        [1.0980e-06, 1.0000e+00],
        [5.1989e-02, 9.4801e-01],
        [9.3997e-01, 6.0031e-02],
        [7.4428e-01, 2.5572e-01],
        [5.7507e-01, 4.2493e-01],
        [2.7209e-03, 9.9728e-01],
[2K        [1.5394e-02, 9.8461e-01]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 4/203 [2m0:00:04 â€¢ 0:03:42[0m [2;4m0.90it/s[0m  
[2KAttention weights: tensor([[0.5580, 0.4420],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 5/203 [2m0:00:05 â€¢ 0:03:20[0m [2;4m0.99it/s[0m  
        [0.5538, 0.4462],
        [0.5546, 0.4454],
        [0.5568, 0.4432],
        [0.5557, 0.4443],
        [0.5531, 0.4469],
        [0.5524, 0.4476],
        [0.5576, 0.4424],
        [0.5580, 0.4420],
[2K        [0.5553, 0.4447]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”[0m 5/203 [2m0:00:05 â€¢ 0:03:20[0m [2;4m0.99it/s[0m  
[2KClass label: HammerThrowm[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 5/203 [2m0:00:05 â€¢ 0:03:20[0m [2;4m0.99it/s[0m  
[2KAttention weights: tensor([[0.5816, 0.4184]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:03:20[0m [2;4m0.99it/s[0m  
[2KStep 0 - TTA Loss: 0.6472[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 5/203 [2m0:00:05 â€¢ 0:03:20[0m [2;4m0.99it/s[0m  
[2KAttention weights: tensor([[0.5816, 0.4184]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:03:20[0m [2;4m0.99it/s[0m  
[2KAttention weights: tensor([[0.5819, 0.4181]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:03:20[0m [2;4m0.99it/s[0m  
[2KAttention weights: tensor([[0.5836, 0.4164]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:03:20[0m [2;4m0.99it/s[0m  
[2KAttention weights: tensor([[0.5863, 0.4137]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:03:20[0m [2;4m0.99it/s[0m  
[2KAttention weights: tensor([[0.5881, 0.4119]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:03:20[0m [2;4m0.99it/s[0m  
[2KAttention weights: tensor([[0.5809, 0.4191]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:03:20[0m [2;4m0.99it/s[0m  
[2KAttention weights: tensor([[0.5537, 0.4463]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:03:20[0m [2;4m0.99it/s[0m  
[2KAttention weights: tensor([[0.5374, 0.4626]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:03:20[0m [2;4m0.99it/s[0m  
[2KAttention weights: tensor([[0.6698, 0.3302]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:03:20[0m [2;4m0.99it/s[0m  
[2KAttention weights: tensor([[0.9218, 0.0782]], device='cuda:0', grad_fn=<SoftmaxBackward0>)0:00:05 â€¢ 0:03:20[0m [2;4m0.99it/s[0m  
[2KStep 10 - TTA Loss: 1.1813[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 5/203 [2m0:00:05 â€¢ 0:03:20[0m [2;4m0.99it/s[0m  
[2KAttention weights: tensor([[9.9952e-01, 4.7854e-04]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:03:20[0m [2;4m0.99it/s[0m  
[2KAttention weights: tensor([[9.9997e-01, 2.6044e-05]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:03:20[0m [2;4m0.99it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.6093e-06]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:03:20[0m [2;4m0.99it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.0221e-07]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:03:20[0m [2;4m0.99it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 6.8838e-09]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:03:20[0m [2;4m0.99it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 5.1101e-10]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:03:20[0m [2;4m0.99it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.3171e-11]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:03:20[0m [2;4m0.99it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.2544e-12]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:03:20[0m [2;4m0.99it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.9785e-13]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â€¢ 0:03:20[0m [2;4m0.99it/s[0m  
[2KAttention weights: tensor([[0.5769, 0.4231]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 5/203 [2m0:00:05 â€¢ 0:03:20[0m [2;4m0.99it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.4441e-10],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 5/203 [2m0:00:05 â€¢ 0:03:20[0m [2;4m0.99it/s[0m  
        [9.9999e-01, 9.7712e-06],
        [1.0000e+00, 5.7588e-09],
        [1.0000e+00, 9.9261e-09],
        [1.0000e+00, 6.8440e-14],
        [1.0000e+00, 6.7384e-10],
        [1.0000e+00, 1.9330e-09],
        [1.0000e+00, 9.1143e-09],
        [1.0000e+00, 1.3180e-07],
[2K        [1.0000e+00, 1.5612e-11]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 5/203 [2m0:00:05 â€¢ 0:03:20[0m [2;4m0.99it/s[0m  
[2KAttention weights: tensor([[0.5581, 0.4419],m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:06 â€¢ 0:03:11[0m [2;4m1.04it/s[0m  
        [0.5539, 0.4461],
        [0.5547, 0.4453],
        [0.5569, 0.4431],
        [0.5559, 0.4441],
        [0.5532, 0.4468],
        [0.5525, 0.4475],
        [0.5577, 0.4423],
        [0.5581, 0.4419],
[2K        [0.5554, 0.4446]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:06 â€¢ 0:03:11[0m [2;4m1.04it/s[0m  
[2KClass label: HammerThrowm[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:06 â€¢ 0:03:11[0m [2;4m1.04it/s[0m  
[2KAttention weights: tensor([[0.5845, 0.4155]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:06 â€¢ 0:03:11[0m [2;4m1.04it/s[0m  
[2KStep 0 - TTA Loss: 0.5510[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:06 â€¢ 0:03:11[0m [2;4m1.04it/s[0m  
[2KAttention weights: tensor([[0.9195, 0.0805]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:06 â€¢ 0:03:11[0m [2;4m1.04it/s[0m  
[2KAttention weights: tensor([[0.6782, 0.3218]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:06 â€¢ 0:03:11[0m [2;4m1.04it/s[0m  
[2KAttention weights: tensor([[0.3269, 0.6731]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:06 â€¢ 0:03:11[0m [2;4m1.04it/s[0m  
[2KAttention weights: tensor([[0.1195, 0.8805]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:06 â€¢ 0:03:11[0m [2;4m1.04it/s[0m  
[2KAttention weights: tensor([[0.0384, 0.9616]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:06 â€¢ 0:03:11[0m [2;4m1.04it/s[0m  
[2KAttention weights: tensor([[0.0131, 0.9869]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:06 â€¢ 0:03:11[0m [2;4m1.04it/s[0m  
[2KAttention weights: tensor([[0.0050, 0.9950]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:06 â€¢ 0:03:11[0m [2;4m1.04it/s[0m  
[2KAttention weights: tensor([[0.0023, 0.9977]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:06 â€¢ 0:03:11[0m [2;4m1.04it/s[0m  
[2KAttention weights: tensor([[0.0012, 0.9988]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 6/203 [2m0:00:06 â€¢ 0:03:11[0m [2;4m1.04it/s[0m  
[2KAttention weights: tensor([[6.7654e-04, 9.9932e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:00:06 â€¢ 0:03:11[0m [2;4m1.04it/s[0m  
[2KStep 10 - TTA Loss: 0.7943[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:06 â€¢ 0:03:11[0m [2;4m1.04it/s[0m  
[2KAttention weights: tensor([[4.3452e-04, 9.9957e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:00:06 â€¢ 0:03:11[0m [2;4m1.04it/s[0m  
[2KAttention weights: tensor([[3.1082e-04, 9.9969e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:00:06 â€¢ 0:03:11[0m [2;4m1.04it/s[0m  
[2KAttention weights: tensor([[2.4290e-04, 9.9976e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:00:06 â€¢ 0:03:11[0m [2;4m1.04it/s[0m  
[2KAttention weights: tensor([[2.0496e-04, 9.9980e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:00:06 â€¢ 0:03:11[0m [2;4m1.04it/s[0m  
[2KAttention weights: tensor([[1.8263e-04, 9.9982e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:00:06 â€¢ 0:03:11[0m [2;4m1.04it/s[0m  
[2KAttention weights: tensor([[1.6975e-04, 9.9983e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:00:06 â€¢ 0:03:11[0m [2;4m1.04it/s[0m  
[2KAttention weights: tensor([[1.6289e-04, 9.9984e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:00:06 â€¢ 0:03:11[0m [2;4m1.04it/s[0m  
[2KAttention weights: tensor([[1.5964e-04, 9.9984e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:00:06 â€¢ 0:03:11[0m [2;4m1.04it/s[0m  
[2KAttention weights: tensor([[1.5837e-04, 9.9984e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)03 [2m0:00:06 â€¢ 0:03:11[0m [2;4m1.04it/s[0m  
[2KAttention weights: tensor([[0.5460, 0.4540]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:06 â€¢ 0:03:11[0m [2;4m1.04it/s[0m  
[2KAttention weights: tensor([[1.9870e-03, 9.9801e-01],â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:06 â€¢ 0:03:11[0m [2;4m1.04it/s[0m  
        [4.1420e-02, 9.5858e-01],
        [4.2749e-03, 9.9573e-01],
        [5.8789e-03, 9.9412e-01],
        [1.5707e-04, 9.9984e-01],
        [2.4736e-03, 9.9753e-01],
        [2.6554e-03, 9.9734e-01],
        [7.4572e-03, 9.9254e-01],
        [1.5568e-02, 9.8443e-01],
[2K        [9.9582e-04, 9.9900e-01]], device='cuda:0')mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 6/203 [2m0:00:06 â€¢ 0:03:11[0m [2;4m1.04it/s[0m  
[2KAttention weights: tensor([[0.5582, 0.4418],m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:07 â€¢ 0:03:35[0m [2;4m0.91it/s[0m  
        [0.5540, 0.4460],
        [0.5548, 0.4452],
        [0.5570, 0.4430],
        [0.5561, 0.4439],
        [0.5533, 0.4467],
        [0.5526, 0.4474],
        [0.5578, 0.4422],
        [0.5582, 0.4418],
[2K        [0.5556, 0.4444]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:07 â€¢ 0:03:35[0m [2;4m0.91it/s[0m  
[2KClass label: Billiards[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:07 â€¢ 0:03:35[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5850, 0.4150]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:07 â€¢ 0:03:35[0m [2;4m0.91it/s[0m  
[2KStep 0 - TTA Loss: 0.4417[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:07 â€¢ 0:03:35[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5850, 0.4150]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:07 â€¢ 0:03:35[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5833, 0.4167]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:07 â€¢ 0:03:35[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5735, 0.4265]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:07 â€¢ 0:03:35[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5459, 0.4541]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:07 â€¢ 0:03:35[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.4818, 0.5182]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:07 â€¢ 0:03:35[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.3936, 0.6064]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:07 â€¢ 0:03:35[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.3552, 0.6448]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:07 â€¢ 0:03:35[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.3843, 0.6157]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:07 â€¢ 0:03:35[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.4890, 0.5110]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:07 â€¢ 0:03:35[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.6497, 0.3503]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:07 â€¢ 0:03:35[0m [2;4m0.91it/s[0m  
[2KStep 10 - TTA Loss: 0.6403[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:07 â€¢ 0:03:35[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.8156, 0.1844]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:07 â€¢ 0:03:35[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.9232, 0.0768]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:07 â€¢ 0:03:35[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.9608, 0.0392]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:07 â€¢ 0:03:35[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.9377, 0.0623]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:07 â€¢ 0:03:35[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.8370, 0.1630]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:07 â€¢ 0:03:35[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5903, 0.4097]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:07 â€¢ 0:03:35[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.2968, 0.7032]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:07 â€¢ 0:03:35[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.1163, 0.8837]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:07 â€¢ 0:03:35[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.0426, 0.9574]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 7/203 [2m0:00:07 â€¢ 0:03:35[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5503, 0.4497]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:07 â€¢ 0:03:35[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.2043, 0.7957],8;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:07 â€¢ 0:03:35[0m [2;4m0.91it/s[0m  
        [0.0164, 0.9836],
        [0.1925, 0.8075],
        [0.2369, 0.7631],
        [0.1578, 0.8422],
        [0.1816, 0.8184],
        [0.1639, 0.8361],
        [0.1973, 0.8027],
        [0.2236, 0.7764],
[2K        [0.2039, 0.7961]], device='cuda:0')38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 7/203 [2m0:00:07 â€¢ 0:03:35[0m [2;4m0.91it/s[0m  
[2KAttention weights: tensor([[0.5582, 0.4418],mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:08 â€¢ 0:03:28[0m [2;4m0.94it/s[0m  
        [0.5540, 0.4460],
        [0.5548, 0.4452],
        [0.5570, 0.4430],
        [0.5561, 0.4439],
        [0.5533, 0.4467],
        [0.5526, 0.4474],
        [0.5578, 0.4422],
        [0.5582, 0.4418],
[2K        [0.5556, 0.4444]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:08 â€¢ 0:03:28[0m [2;4m0.94it/s[0m  
[2KClass label: BaseballPitch[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:08 â€¢ 0:03:28[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.6084, 0.3916]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:08 â€¢ 0:03:28[0m [2;4m0.94it/s[0m  
[2KStep 0 - TTA Loss: 0.7419[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:08 â€¢ 0:03:28[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.7206, 0.2794]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:08 â€¢ 0:03:28[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.7407, 0.2593]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:08 â€¢ 0:03:28[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.9964, 0.0036]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 8/203 [2m0:00:08 â€¢ 0:03:28[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[4.2145e-04, 9.9958e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 8/203 [2m0:00:08 â€¢ 0:03:28[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[4.4314e-09, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 8/203 [2m0:00:08 â€¢ 0:03:28[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[2.9192e-13, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 8/203 [2m0:00:08 â€¢ 0:03:28[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[1.0081e-16, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 8/203 [2m0:00:08 â€¢ 0:03:28[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[1.5142e-19, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 8/203 [2m0:00:08 â€¢ 0:03:28[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[8.1243e-22, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 8/203 [2m0:00:08 â€¢ 0:03:28[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[1.3123e-23, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 8/203 [2m0:00:08 â€¢ 0:03:28[0m [2;4m0.94it/s[0m  
[2KStep 10 - TTA Loss: 1.1024[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:08 â€¢ 0:03:28[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[5.3728e-25, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 8/203 [2m0:00:08 â€¢ 0:03:28[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[4.7886e-26, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 8/203 [2m0:00:08 â€¢ 0:03:28[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[8.0829e-27, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 8/203 [2m0:00:08 â€¢ 0:03:28[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[2.2860e-27, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 8/203 [2m0:00:08 â€¢ 0:03:28[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[9.6878e-28, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 8/203 [2m0:00:08 â€¢ 0:03:28[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[5.6025e-28, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 8/203 [2m0:00:08 â€¢ 0:03:28[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[4.0675e-28, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 8/203 [2m0:00:08 â€¢ 0:03:28[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[3.4477e-28, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 8/203 [2m0:00:08 â€¢ 0:03:28[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[3.2249e-28, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 8/203 [2m0:00:08 â€¢ 0:03:28[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5016, 0.4984]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:08 â€¢ 0:03:28[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[3.0390e-28, 1.0000e+00],;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:08 â€¢ 0:03:28[0m [2;4m0.94it/s[0m  
        [4.4960e-12, 1.0000e+00],
        [1.0508e-10, 1.0000e+00],
        [1.6315e-18, 1.0000e+00],
        [1.9632e-18, 1.0000e+00],
        [1.1541e-15, 1.0000e+00],
        [2.7688e-14, 1.0000e+00],
        [4.4329e-15, 1.0000e+00],
        [8.2209e-16, 1.0000e+00],
[2K        [3.8251e-19, 1.0000e+00]], device='cuda:0')5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 8/203 [2m0:00:08 â€¢ 0:03:28[0m [2;4m0.94it/s[0m  
[2KAttention weights: tensor([[0.5582, 0.4418],mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 9/203 [2m0:00:09 â€¢ 0:03:19[0m [2;4m0.98it/s[0m  
        [0.5540, 0.4460],
        [0.5548, 0.4452],
        [0.5570, 0.4430],
        [0.5561, 0.4439],
        [0.5533, 0.4467],
        [0.5526, 0.4474],
        [0.5578, 0.4422],
        [0.5582, 0.4418],
[2K        [0.5556, 0.4444]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 9/203 [2m0:00:09 â€¢ 0:03:19[0m [2;4m0.98it/s[0m  
[2KClass label: BaseballPitch[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 9/203 [2m0:00:09 â€¢ 0:03:19[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6133, 0.3867]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:09 â€¢ 0:03:19[0m [2;4m0.98it/s[0m  
[2KStep 0 - TTA Loss: 0.5313[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 9/203 [2m0:00:09 â€¢ 0:03:19[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6133, 0.3867]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:09 â€¢ 0:03:19[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6120, 0.3880]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:09 â€¢ 0:03:19[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6112, 0.3888]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:09 â€¢ 0:03:19[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6144, 0.3856]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:09 â€¢ 0:03:19[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6202, 0.3798]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:09 â€¢ 0:03:19[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6245, 0.3755]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:09 â€¢ 0:03:19[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6183, 0.3817]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:09 â€¢ 0:03:19[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.6190, 0.3810]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:09 â€¢ 0:03:19[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.7299, 0.2701]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:09 â€¢ 0:03:19[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.8952, 0.1048]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:09 â€¢ 0:03:19[0m [2;4m0.98it/s[0m  
[2KStep 10 - TTA Loss: 1.0208[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 9/203 [2m0:00:09 â€¢ 0:03:19[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.9942, 0.0058]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:09 â€¢ 0:03:19[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.8849, 0.1151]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:09 â€¢ 0:03:19[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.3037, 0.6963]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:09 â€¢ 0:03:19[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.0242, 0.9758]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:09 â€¢ 0:03:19[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.0015, 0.9985]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 9/203 [2m0:00:09 â€¢ 0:03:19[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[9.8097e-05, 9.9990e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 9/203 [2m0:00:09 â€¢ 0:03:19[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[7.3859e-06, 9.9999e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 9/203 [2m0:00:09 â€¢ 0:03:19[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[6.5494e-07, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 9/203 [2m0:00:09 â€¢ 0:03:19[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[6.9673e-08, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 9/203 [2m0:00:09 â€¢ 0:03:19[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.5428, 0.4572]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 9/203 [2m0:00:09 â€¢ 0:03:19[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[8.9076e-09, 1.0000e+00],;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 9/203 [2m0:00:09 â€¢ 0:03:19[0m [2;4m0.98it/s[0m  
        [1.7181e-03, 9.9828e-01],
        [1.2648e-03, 9.9874e-01],
        [7.2679e-06, 9.9999e-01],
        [6.0693e-06, 9.9999e-01],
        [3.8246e-05, 9.9996e-01],
        [9.4990e-05, 9.9990e-01],
        [8.9850e-05, 9.9991e-01],
        [5.4066e-05, 9.9995e-01],
[2K        [4.3887e-06, 1.0000e+00]], device='cuda:0')5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 9/203 [2m0:00:09 â€¢ 0:03:19[0m [2;4m0.98it/s[0m  
[2KAttention weights: tensor([[0.5582, 0.4418],mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 10/203 [2m0:00:10 â€¢ 0:03:11[0m [2;4m1.01it/s[0m  
        [0.5540, 0.4460],
        [0.5548, 0.4452],
        [0.5570, 0.4430],
        [0.5561, 0.4439],
        [0.5533, 0.4467],
        [0.5526, 0.4474],
        [0.5577, 0.4423],
        [0.5582, 0.4418],
[2K        [0.5556, 0.4444]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 10/203 [2m0:00:10 â€¢ 0:03:11[0m [2;4m1.01it/s[0m  
[2KClass label: TennisSwingm[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 10/203 [2m0:00:10 â€¢ 0:03:11[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.6487, 0.3513]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:10 â€¢ 0:03:11[0m [2;4m1.01it/s[0m  
[2KStep 0 - TTA Loss: 0.4799[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 10/203 [2m0:00:10 â€¢ 0:03:11[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.2730, 0.7270]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:10 â€¢ 0:03:11[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.0934, 0.9066]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:10 â€¢ 0:03:11[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.0347, 0.9653]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:10 â€¢ 0:03:11[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.0158, 0.9842]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:10 â€¢ 0:03:11[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.0085, 0.9915]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:10 â€¢ 0:03:11[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.0049, 0.9951]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:10 â€¢ 0:03:11[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.0030, 0.9970]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:10 â€¢ 0:03:11[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.0019, 0.9981]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:10 â€¢ 0:03:11[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.0013, 0.9987]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 10/203 [2m0:00:10 â€¢ 0:03:11[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[9.6600e-04, 9.9903e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 10/203 [2m0:00:10 â€¢ 0:03:11[0m [2;4m1.01it/s[0m  
[2KStep 10 - TTA Loss: 0.7070[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 10/203 [2m0:00:10 â€¢ 0:03:11[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[7.4375e-04, 9.9926e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 10/203 [2m0:00:10 â€¢ 0:03:11[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[6.0032e-04, 9.9940e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 10/203 [2m0:00:10 â€¢ 0:03:11[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[5.0951e-04, 9.9949e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 10/203 [2m0:00:10 â€¢ 0:03:11[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[4.5000e-04, 9.9955e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 10/203 [2m0:00:10 â€¢ 0:03:11[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[4.1150e-04, 9.9959e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 10/203 [2m0:00:10 â€¢ 0:03:11[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[3.8708e-04, 9.9961e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 10/203 [2m0:00:10 â€¢ 0:03:11[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[3.7342e-04, 9.9963e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 10/203 [2m0:00:10 â€¢ 0:03:11[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[3.6628e-04, 9.9963e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 10/203 [2m0:00:10 â€¢ 0:03:11[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[3.6325e-04, 9.9964e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 10/203 [2m0:00:10 â€¢ 0:03:11[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.5438, 0.4562]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 10/203 [2m0:00:10 â€¢ 0:03:11[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[2.6562e-06, 1.0000e+00],;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 10/203 [2m0:00:10 â€¢ 0:03:11[0m [2;4m1.01it/s[0m  
        [8.0675e-03, 9.9193e-01],
        [6.1743e-03, 9.9383e-01],
        [1.8849e-04, 9.9981e-01],
        [1.7829e-04, 9.9982e-01],
        [5.3298e-04, 9.9947e-01],
        [1.2576e-03, 9.9874e-01],
        [1.2785e-03, 9.9872e-01],
        [3.6177e-04, 9.9964e-01],
[2K        [1.3869e-04, 9.9986e-01]], device='cuda:0')5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 10/203 [2m0:00:10 â€¢ 0:03:11[0m [2;4m1.01it/s[0m  
[2KAttention weights: tensor([[0.5582, 0.4418],0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 11/203 [2m0:00:10 â€¢ 0:03:03[0m [2;4m1.05it/s[0m   
        [0.5539, 0.4461],
        [0.5548, 0.4452],
        [0.5569, 0.4431],
        [0.5560, 0.4440],
        [0.5532, 0.4468],
        [0.5525, 0.4475],
        [0.5577, 0.4423],
        [0.5581, 0.4419],
[2K        [0.5556, 0.4444]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 11/203 [2m0:00:10 â€¢ 0:03:03[0m [2;4m1.05it/s[0m  
[2KClass label: HammerThrow0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 11/203 [2m0:00:10 â€¢ 0:03:03[0m [2;4m1.05it/s[0m  
[2KAttention weights: tensor([[0.5834, 0.4166]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:10 â€¢ 0:03:03[0m [2;4m1.05it/s[0m  
[2KStep 0 - TTA Loss: 0.5908m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 11/203 [2m0:00:10 â€¢ 0:03:03[0m [2;4m1.05it/s[0m  
[2KAttention weights: tensor([[0.5834, 0.4166]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:10 â€¢ 0:03:03[0m [2;4m1.05it/s[0m  
[2KAttention weights: tensor([[0.5847, 0.4153]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:10 â€¢ 0:03:03[0m [2;4m1.05it/s[0m  
[2KAttention weights: tensor([[0.5935, 0.4065]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:10 â€¢ 0:03:03[0m [2;4m1.05it/s[0m  
[2KAttention weights: tensor([[0.6170, 0.3830]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:10 â€¢ 0:03:03[0m [2;4m1.05it/s[0m  
[2KAttention weights: tensor([[0.6577, 0.3423]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:10 â€¢ 0:03:03[0m [2;4m1.05it/s[0m  
[2KAttention weights: tensor([[0.7029, 0.2971]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:10 â€¢ 0:03:03[0m [2;4m1.05it/s[0m  
[2KAttention weights: tensor([[0.7328, 0.2672]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:10 â€¢ 0:03:03[0m [2;4m1.05it/s[0m  
[2KAttention weights: tensor([[0.7326, 0.2674]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:10 â€¢ 0:03:03[0m [2;4m1.05it/s[0m  
[2KAttention weights: tensor([[0.8455, 0.1545]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:10 â€¢ 0:03:03[0m [2;4m1.05it/s[0m  
[2KAttention weights: tensor([[0.9897, 0.0103]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:10 â€¢ 0:03:03[0m [2;4m1.05it/s[0m  
[2KStep 10 - TTA Loss: 0.9588[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 11/203 [2m0:00:10 â€¢ 0:03:03[0m [2;4m1.05it/s[0m  
[2KAttention weights: tensor([[9.9970e-01, 2.9568e-04]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:10 â€¢ 0:03:03[0m [2;4m1.05it/s[0m  
[2KAttention weights: tensor([[9.9990e-01, 9.5832e-05]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:10 â€¢ 0:03:03[0m [2;4m1.05it/s[0m  
[2KAttention weights: tensor([[9.9990e-01, 1.0475e-04]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:10 â€¢ 0:03:03[0m [2;4m1.05it/s[0m  
[2KAttention weights: tensor([[9.9942e-01, 5.7930e-04]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:10 â€¢ 0:03:03[0m [2;4m1.05it/s[0m  
[2KAttention weights: tensor([[0.2707, 0.7293]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 11/203 [2m0:00:10 â€¢ 0:03:03[0m [2;4m1.05it/s[0m  
[2KAttention weights: tensor([[1.1082e-04, 9.9989e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:10 â€¢ 0:03:03[0m [2;4m1.05it/s[0m  
[2KAttention weights: tensor([[4.7004e-08, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:10 â€¢ 0:03:03[0m [2;4m1.05it/s[0m  
[2KAttention weights: tensor([[3.2079e-11, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:10 â€¢ 0:03:03[0m [2;4m1.05it/s[0m  
[2KAttention weights: tensor([[3.7272e-14, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:10 â€¢ 0:03:03[0m [2;4m1.05it/s[0m  
[2KAttention weights: tensor([[0.5306, 0.4694]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 11/203 [2m0:00:10 â€¢ 0:03:03[0m [2;4m1.05it/s[0m  
[2KAttention weights: tensor([[3.2106e-12, 1.0000e+00],mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 11/203 [2m0:00:10 â€¢ 0:03:03[0m [2;4m1.05it/s[0m  
        [9.3558e-07, 1.0000e+00],
        [5.0176e-11, 1.0000e+00],
        [7.8225e-11, 1.0000e+00],
        [7.5958e-17, 1.0000e+00],
        [2.0213e-12, 1.0000e+00],
        [1.1866e-11, 1.0000e+00],
        [3.2810e-10, 1.0000e+00],
        [1.4468e-09, 1.0000e+00],
[2K        [5.6361e-14, 1.0000e+00]], device='cuda:0')7mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 11/203 [2m0:00:10 â€¢ 0:03:03[0m [2;4m1.05it/s[0m  
[2KAttention weights: tensor([[0.5582, 0.4418],0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 12/203 [2m0:00:11 â€¢ 0:02:58[0m [2;4m1.07it/s[0m  
        [0.5540, 0.4460],
        [0.5548, 0.4452],
        [0.5570, 0.4430],
        [0.5561, 0.4439],
        [0.5533, 0.4467],
        [0.5526, 0.4474],
        [0.5578, 0.4422],
        [0.5582, 0.4418],
[2K        [0.5556, 0.4444]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 12/203 [2m0:00:11 â€¢ 0:02:58[0m [2;4m1.07it/s[0m  
[2KClass label: CleanAndJerkm[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 12/203 [2m0:00:11 â€¢ 0:02:58[0m [2;4m1.07it/s[0m  
[2KAttention weights: tensor([[0.5676, 0.4324]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:11 â€¢ 0:02:58[0m [2;4m1.07it/s[0m  
[2KStep 0 - TTA Loss: 0.6189m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 12/203 [2m0:00:11 â€¢ 0:02:58[0m [2;4m1.07it/s[0m  
[2KAttention weights: tensor([[0.0591, 0.9409]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:11 â€¢ 0:02:58[0m [2;4m1.07it/s[0m  
[2KAttention weights: tensor([[0.0045, 0.9955]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 12/203 [2m0:00:11 â€¢ 0:02:58[0m [2;4m1.07it/s[0m  
[2KAttention weights: tensor([[3.6572e-04, 9.9963e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:11 â€¢ 0:02:58[0m [2;4m1.07it/s[0m  
[2KAttention weights: tensor([[3.4892e-05, 9.9997e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:11 â€¢ 0:02:58[0m [2;4m1.07it/s[0m  
[2KAttention weights: tensor([[4.2869e-06, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:11 â€¢ 0:02:58[0m [2;4m1.07it/s[0m  
[2KAttention weights: tensor([[6.9639e-07, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:11 â€¢ 0:02:58[0m [2;4m1.07it/s[0m  
[2KAttention weights: tensor([[1.4976e-07, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:11 â€¢ 0:02:58[0m [2;4m1.07it/s[0m  
[2KAttention weights: tensor([[4.1995e-08, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:11 â€¢ 0:02:58[0m [2;4m1.07it/s[0m  
[2KAttention weights: tensor([[1.5047e-08, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:11 â€¢ 0:02:58[0m [2;4m1.07it/s[0m  
[2KAttention weights: tensor([[6.6870e-09, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:11 â€¢ 0:02:58[0m [2;4m1.07it/s[0m  
[2KStep 10 - TTA Loss: 0.6136[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 12/203 [2m0:00:11 â€¢ 0:02:58[0m [2;4m1.07it/s[0m  
[2KAttention weights: tensor([[3.5803e-09, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:11 â€¢ 0:02:58[0m [2;4m1.07it/s[0m  
[2KAttention weights: tensor([[2.2392e-09, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:11 â€¢ 0:02:58[0m [2;4m1.07it/s[0m  
[2KAttention weights: tensor([[1.5946e-09, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:11 â€¢ 0:02:58[0m [2;4m1.07it/s[0m  
[2KAttention weights: tensor([[1.2589e-09, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:11 â€¢ 0:02:58[0m [2;4m1.07it/s[0m  
[2KAttention weights: tensor([[1.0753e-09, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:11 â€¢ 0:02:58[0m [2;4m1.07it/s[0m  
[2KAttention weights: tensor([[9.7358e-10, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:11 â€¢ 0:02:58[0m [2;4m1.07it/s[0m  
[2KAttention weights: tensor([[9.1937e-10, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:11 â€¢ 0:02:58[0m [2;4m1.07it/s[0m  
[2KAttention weights: tensor([[8.9328e-10, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:11 â€¢ 0:02:58[0m [2;4m1.07it/s[0m  
[2KAttention weights: tensor([[8.8331e-10, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:11 â€¢ 0:02:58[0m [2;4m1.07it/s[0m  
[2KAttention weights: tensor([[0.5346, 0.4654]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 12/203 [2m0:00:11 â€¢ 0:02:58[0m [2;4m1.07it/s[0m  
[2KAttention weights: tensor([[1.3458e-11, 1.0000e+00],mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 12/203 [2m0:00:11 â€¢ 0:02:58[0m [2;4m1.07it/s[0m  
        [1.9585e-06, 1.0000e+00],
        [8.6927e-10, 1.0000e+00],
        [3.2412e-10, 1.0000e+00],
        [4.6625e-16, 1.0000e+00],
        [6.6530e-12, 1.0000e+00],
        [3.2407e-11, 1.0000e+00],
        [6.7687e-10, 1.0000e+00],
        [5.6139e-09, 1.0000e+00],
[2K        [2.9419e-13, 1.0000e+00]], device='cuda:0')7mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 12/203 [2m0:00:11 â€¢ 0:02:58[0m [2;4m1.07it/s[0m  
[2KAttention weights: tensor([[0.5582, 0.4418],4mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 13/203 [2m0:00:12 â€¢ 0:02:54[0m [2;4m1.09it/s[0m  
        [0.5540, 0.4460],
        [0.5548, 0.4452],
        [0.5570, 0.4430],
        [0.5561, 0.4439],
        [0.5533, 0.4467],
        [0.5526, 0.4474],
        [0.5578, 0.4422],
        [0.5582, 0.4418],
[2K        [0.5556, 0.4444]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 13/203 [2m0:00:12 â€¢ 0:02:54[0m [2;4m1.09it/s[0m  
[2KClass label: BaseballPitch[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 13/203 [2m0:00:12 â€¢ 0:02:54[0m [2;4m1.09it/s[0m  
[2KAttention weights: tensor([[0.6126, 0.3874]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:12 â€¢ 0:02:54[0m [2;4m1.09it/s[0m  
[2KStep 0 - TTA Loss: 0.8557m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 13/203 [2m0:00:12 â€¢ 0:02:54[0m [2;4m1.09it/s[0m  
[2KAttention weights: tensor([[0.6126, 0.3874]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:12 â€¢ 0:02:54[0m [2;4m1.09it/s[0m  
[2KAttention weights: tensor([[0.6173, 0.3827]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:12 â€¢ 0:02:54[0m [2;4m1.09it/s[0m  
[2KAttention weights: tensor([[0.6446, 0.3554]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:12 â€¢ 0:02:54[0m [2;4m1.09it/s[0m  
[2KAttention weights: tensor([[0.7145, 0.2855]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:12 â€¢ 0:02:54[0m [2;4m1.09it/s[0m  
[2KAttention weights: tensor([[0.8185, 0.1815]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:12 â€¢ 0:02:54[0m [2;4m1.09it/s[0m  
[2KAttention weights: tensor([[0.9106, 0.0894]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:12 â€¢ 0:02:54[0m [2;4m1.09it/s[0m  
[2KAttention weights: tensor([[0.9914, 0.0086]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 13/203 [2m0:00:12 â€¢ 0:02:54[0m [2;4m1.09it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 9.9360e-07]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 13/203 [2m0:00:12 â€¢ 0:02:54[0m [2;4m1.09it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.0550e-11]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 13/203 [2m0:00:12 â€¢ 0:02:54[0m [2;4m1.09it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.1742e-16]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 13/203 [2m0:00:12 â€¢ 0:02:54[0m [2;4m1.09it/s[0m  
[2KStep 10 - TTA Loss: 1.3470[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 13/203 [2m0:00:12 â€¢ 0:02:54[0m [2;4m1.09it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.4325e-21]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 13/203 [2m0:00:12 â€¢ 0:02:54[0m [2;4m1.09it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.6430e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 13/203 [2m0:00:12 â€¢ 0:02:54[0m [2;4m1.09it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 6.7551e-33]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 13/203 [2m0:00:12 â€¢ 0:02:54[0m [2;4m1.09it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.1670e-38]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 13/203 [2m0:00:12 â€¢ 0:02:54[0m [2;4m1.09it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.3822e-44]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 13/203 [2m0:00:12 â€¢ 0:02:54[0m [2;4m1.09it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 13/203 [2m0:00:12 â€¢ 0:02:54[0m [2;4m1.09it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 13/203 [2m0:00:12 â€¢ 0:02:54[0m [2;4m1.09it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 13/203 [2m0:00:12 â€¢ 0:02:54[0m [2;4m1.09it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 13/203 [2m0:00:12 â€¢ 0:02:54[0m [2;4m1.09it/s[0m  
[2KAttention weights: tensor([[0.6249, 0.3751]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 13/203 [2m0:00:12 â€¢ 0:02:54[0m [2;4m1.09it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 0.0000e+00],5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 13/203 [2m0:00:12 â€¢ 0:02:54[0m [2;4m1.09it/s[0m  
        [1.0000e+00, 6.8489e-26],
        [1.0000e+00, 1.0567e-26],
        [1.0000e+00, 0.0000e+00],
        [1.0000e+00, 2.8026e-45],
        [1.0000e+00, 1.6996e-39],
        [1.0000e+00, 2.2643e-35],
        [1.0000e+00, 2.8446e-37],
        [1.0000e+00, 4.6537e-40],
[2K        [1.0000e+00, 0.0000e+00]], device='cuda:0');5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 13/203 [2m0:00:12 â€¢ 0:02:54[0m [2;4m1.09it/s[0m  
[2KAttention weights: tensor([[0.5583, 0.4417],4mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:12 â€¢ 0:02:50[0m [2;4m1.11it/s[0m  
        [0.5541, 0.4459],
        [0.5549, 0.4451],
        [0.5571, 0.4429],
        [0.5562, 0.4438],
        [0.5534, 0.4466],
        [0.5527, 0.4473],
        [0.5579, 0.4421],
        [0.5583, 0.4417],
[2K        [0.5557, 0.4443]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:12 â€¢ 0:02:50[0m [2;4m1.11it/s[0m  
[2KClass label: GolfSwing[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:12 â€¢ 0:02:50[0m [2;4m1.11it/s[0m  
[2KAttention weights: tensor([[0.6326, 0.3674]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:12 â€¢ 0:02:50[0m [2;4m1.11it/s[0m  
[2KStep 0 - TTA Loss: 0.8579m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:12 â€¢ 0:02:50[0m [2;4m1.11it/s[0m  
[2KAttention weights: tensor([[0.9983, 0.0017]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 14/203 [2m0:00:12 â€¢ 0:02:50[0m [2;4m1.11it/s[0m  
[2KAttention weights: tensor([[9.9999e-01, 1.0356e-05]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 14/203 [2m0:00:12 â€¢ 0:02:50[0m [2;4m1.11it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.1515e-07]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 14/203 [2m0:00:12 â€¢ 0:02:50[0m [2;4m1.11it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.2428e-09]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 14/203 [2m0:00:12 â€¢ 0:02:50[0m [2;4m1.11it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 7.4671e-11]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 14/203 [2m0:00:12 â€¢ 0:02:50[0m [2;4m1.11it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.1135e-12]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 14/203 [2m0:00:12 â€¢ 0:02:50[0m [2;4m1.11it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.6076e-13]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 14/203 [2m0:00:12 â€¢ 0:02:50[0m [2;4m1.11it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.8264e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 14/203 [2m0:00:12 â€¢ 0:02:50[0m [2;4m1.11it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 9.4169e-15]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 14/203 [2m0:00:12 â€¢ 0:02:50[0m [2;4m1.11it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.5597e-15]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 14/203 [2m0:00:12 â€¢ 0:02:50[0m [2;4m1.11it/s[0m  
[2KStep 10 - TTA Loss: 1.8628[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:12 â€¢ 0:02:50[0m [2;4m1.11it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 9.2652e-16]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 14/203 [2m0:00:12 â€¢ 0:02:50[0m [2;4m1.11it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.2758e-16]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 14/203 [2m0:00:12 â€¢ 0:02:50[0m [2;4m1.11it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.4149e-16]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 14/203 [2m0:00:12 â€¢ 0:02:50[0m [2;4m1.11it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.6071e-16]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 14/203 [2m0:00:12 â€¢ 0:02:50[0m [2;4m1.11it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.2180e-16]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 14/203 [2m0:00:12 â€¢ 0:02:50[0m [2;4m1.11it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.0197e-16]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 14/203 [2m0:00:12 â€¢ 0:02:50[0m [2;4m1.11it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 9.1858e-17]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 14/203 [2m0:00:12 â€¢ 0:02:50[0m [2;4m1.11it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 8.7050e-17]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 14/203 [2m0:00:12 â€¢ 0:02:50[0m [2;4m1.11it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 8.5175e-17]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 14/203 [2m0:00:12 â€¢ 0:02:50[0m [2;4m1.11it/s[0m  
[2KAttention weights: tensor([[0.6177, 0.3823]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:12 â€¢ 0:02:50[0m [2;4m1.11it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 6.4809e-25],5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:12 â€¢ 0:02:50[0m [2;4m1.11it/s[0m  
        [1.0000e+00, 1.5060e-09],
        [1.0000e+00, 7.7390e-10],
        [1.0000e+00, 8.2176e-17],
        [1.0000e+00, 1.9455e-16],
        [1.0000e+00, 2.1736e-14],
        [1.0000e+00, 7.0191e-13],
        [1.0000e+00, 1.2542e-13],
        [1.0000e+00, 1.3430e-14],
[2K        [1.0000e+00, 2.4637e-17]], device='cuda:0');5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 14/203 [2m0:00:12 â€¢ 0:02:50[0m [2;4m1.11it/s[0m  
[2KAttention weights: tensor([[0.5585, 0.4415],4mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 15/203 [2m0:00:13 â€¢ 0:02:46[0m [2;4m1.14it/s[0m  
        [0.5542, 0.4458],
        [0.5550, 0.4450],
        [0.5572, 0.4428],
        [0.5563, 0.4437],
        [0.5535, 0.4465],
        [0.5528, 0.4472],
        [0.5580, 0.4420],
        [0.5584, 0.4416],
[2K        [0.5558, 0.4442]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 15/203 [2m0:00:13 â€¢ 0:02:46[0m [2;4m1.14it/s[0m  
[2KClass label: HighJumpâ”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 15/203 [2m0:00:13 â€¢ 0:02:46[0m [2;4m1.14it/s[0m  
[2KAttention weights: tensor([[0.6012, 0.3988]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:13 â€¢ 0:02:46[0m [2;4m1.14it/s[0m  
[2KStep 0 - TTA Loss: 0.8763m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 15/203 [2m0:00:13 â€¢ 0:02:46[0m [2;4m1.14it/s[0m  
[2KAttention weights: tensor([[0.6012, 0.3988]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:13 â€¢ 0:02:46[0m [2;4m1.14it/s[0m  
[2KAttention weights: tensor([[0.6021, 0.3979]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:13 â€¢ 0:02:46[0m [2;4m1.14it/s[0m  
[2KAttention weights: tensor([[0.6049, 0.3951]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:13 â€¢ 0:02:46[0m [2;4m1.14it/s[0m  
[2KAttention weights: tensor([[0.6108, 0.3892]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:13 â€¢ 0:02:46[0m [2;4m1.14it/s[0m  
[2KAttention weights: tensor([[0.6143, 0.3857]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:13 â€¢ 0:02:46[0m [2;4m1.14it/s[0m  
[2KAttention weights: tensor([[0.6144, 0.3856]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:13 â€¢ 0:02:46[0m [2;4m1.14it/s[0m  
[2KAttention weights: tensor([[0.6077, 0.3923]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:13 â€¢ 0:02:46[0m [2;4m1.14it/s[0m  
[2KAttention weights: tensor([[0.6201, 0.3799]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:13 â€¢ 0:02:46[0m [2;4m1.14it/s[0m  
[2KAttention weights: tensor([[0.6828, 0.3172]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:13 â€¢ 0:02:46[0m [2;4m1.14it/s[0m  
[2KAttention weights: tensor([[0.8602, 0.1398]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:13 â€¢ 0:02:46[0m [2;4m1.14it/s[0m  
[2KStep 10 - TTA Loss: 0.9678[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 15/203 [2m0:00:13 â€¢ 0:02:46[0m [2;4m1.14it/s[0m  
[2KAttention weights: tensor([[0.9733, 0.0267]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:13 â€¢ 0:02:46[0m [2;4m1.14it/s[0m  
[2KAttention weights: tensor([[0.9989, 0.0011]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:13 â€¢ 0:02:46[0m [2;4m1.14it/s[0m  
[2KAttention weights: tensor([[0.9951, 0.0049]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:13 â€¢ 0:02:46[0m [2;4m1.14it/s[0m  
[2KAttention weights: tensor([[0.3160, 0.6840]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:13 â€¢ 0:02:46[0m [2;4m1.14it/s[0m  
[2KAttention weights: tensor([[0.0012, 0.9988]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 15/203 [2m0:00:13 â€¢ 0:02:46[0m [2;4m1.14it/s[0m  
[2KAttention weights: tensor([[3.7130e-06, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 15/203 [2m0:00:13 â€¢ 0:02:46[0m [2;4m1.14it/s[0m  
[2KAttention weights: tensor([[1.5130e-08, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 15/203 [2m0:00:13 â€¢ 0:02:46[0m [2;4m1.14it/s[0m  
[2KAttention weights: tensor([[8.6594e-11, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 15/203 [2m0:00:13 â€¢ 0:02:46[0m [2;4m1.14it/s[0m  
[2KAttention weights: tensor([[7.3268e-13, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 15/203 [2m0:00:13 â€¢ 0:02:46[0m [2;4m1.14it/s[0m  
[2KAttention weights: tensor([[0.5303, 0.4697]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 15/203 [2m0:00:13 â€¢ 0:02:46[0m [2;4m1.14it/s[0m  
[2KAttention weights: tensor([[1.6319e-08, 1.0000e+00],5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 15/203 [2m0:00:13 â€¢ 0:02:46[0m [2;4m1.14it/s[0m  
        [6.1424e-07, 1.0000e+00],
        [1.6792e-09, 1.0000e+00],
        [6.8553e-08, 1.0000e+00],
        [2.3101e-11, 1.0000e+00],
        [9.2029e-15, 1.0000e+00],
        [1.1439e-12, 1.0000e+00],
        [8.3054e-10, 1.0000e+00],
        [6.6953e-09, 1.0000e+00],
[2K        [6.5210e-11, 1.0000e+00]], device='cuda:0');5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 15/203 [2m0:00:13 â€¢ 0:02:46[0m [2;4m1.14it/s[0m  
[2KAttention weights: tensor([[0.5585, 0.4415],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 16/203 [2m0:00:14 â€¢ 0:02:42[0m [2;4m1.16it/s[0m   
        [0.5542, 0.4458],
        [0.5550, 0.4450],
        [0.5573, 0.4427],
        [0.5563, 0.4437],
        [0.5535, 0.4465],
        [0.5528, 0.4472],
        [0.5580, 0.4420],
        [0.5584, 0.4416],
[2K        [0.5558, 0.4442]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 16/203 [2m0:00:14 â€¢ 0:02:42[0m [2;4m1.16it/s[0m  
[2KClass label: HighJumpâ”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 16/203 [2m0:00:14 â€¢ 0:02:42[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[0.5937, 0.4063]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:14 â€¢ 0:02:42[0m [2;4m1.16it/s[0m  
[2KStep 0 - TTA Loss: 0.74490m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 16/203 [2m0:00:14 â€¢ 0:02:42[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[0.0266, 0.9734]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 16/203 [2m0:00:14 â€¢ 0:02:42[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[7.9793e-04, 9.9920e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:14 â€¢ 0:02:42[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[3.5547e-05, 9.9996e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:14 â€¢ 0:02:42[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[2.3486e-06, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:14 â€¢ 0:02:42[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[2.0390e-07, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:14 â€¢ 0:02:42[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[2.4741e-08, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:14 â€¢ 0:02:42[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[4.2726e-09, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:14 â€¢ 0:02:42[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[9.9366e-10, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:14 â€¢ 0:02:42[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[3.0485e-10, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:14 â€¢ 0:02:42[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[1.1834e-10, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:14 â€¢ 0:02:42[0m [2;4m1.16it/s[0m  
[2KStep 10 - TTA Loss: 1.3170m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 16/203 [2m0:00:14 â€¢ 0:02:42[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[5.6395e-11, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:14 â€¢ 0:02:42[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[3.1985e-11, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:14 â€¢ 0:02:42[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[2.1076e-11, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:14 â€¢ 0:02:42[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[1.5692e-11, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:14 â€¢ 0:02:42[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[1.2828e-11, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:14 â€¢ 0:02:42[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[1.1274e-11, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:14 â€¢ 0:02:42[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[1.0443e-11, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:14 â€¢ 0:02:42[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[1.0038e-11, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:14 â€¢ 0:02:42[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[9.8763e-12, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:14 â€¢ 0:02:42[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[0.5275, 0.4725]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 16/203 [2m0:00:14 â€¢ 0:02:42[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[3.7279e-07, 1.0000e+00],7mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 16/203 [2m0:00:14 â€¢ 0:02:42[0m [2;4m1.16it/s[0m  
        [1.1799e-05, 9.9999e-01],
        [1.2069e-07, 1.0000e+00],
        [1.6462e-06, 1.0000e+00],
        [3.5927e-09, 1.0000e+00],
        [9.5948e-12, 1.0000e+00],
        [3.9893e-10, 1.0000e+00],
        [6.6683e-08, 1.0000e+00],
        [3.1272e-07, 1.0000e+00],
[2K        [7.9591e-09, 1.0000e+00]], device='cuda:0')37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 16/203 [2m0:00:14 â€¢ 0:02:42[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[0.5585, 0.4415],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 17/203 [2m0:00:14 â€¢ 0:02:40[0m [2;4m1.17it/s[0m  
        [0.5542, 0.4458],
        [0.5550, 0.4450],
        [0.5573, 0.4427],
        [0.5563, 0.4437],
        [0.5534, 0.4466],
        [0.5528, 0.4472],
        [0.5580, 0.4420],
        [0.5584, 0.4416],
[2K        [0.5558, 0.4442]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 17/203 [2m0:00:14 â€¢ 0:02:40[0m [2;4m1.17it/s[0m  
[2KClass label: GolfSwingâ”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 17/203 [2m0:00:14 â€¢ 0:02:40[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.6284, 0.3716]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:14 â€¢ 0:02:40[0m [2;4m1.17it/s[0m  
[2KStep 0 - TTA Loss: 0.68410m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 17/203 [2m0:00:14 â€¢ 0:02:40[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.6284, 0.3716]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:14 â€¢ 0:02:40[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.6277, 0.3723]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:14 â€¢ 0:02:40[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.6250, 0.3750]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:14 â€¢ 0:02:40[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.6178, 0.3822]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:14 â€¢ 0:02:40[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.6015, 0.3985]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:14 â€¢ 0:02:40[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.5700, 0.4300]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:14 â€¢ 0:02:40[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.5210, 0.4790]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:14 â€¢ 0:02:40[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.5022, 0.4978]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:14 â€¢ 0:02:40[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.6051, 0.3949]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:14 â€¢ 0:02:40[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.7666, 0.2334]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:14 â€¢ 0:02:40[0m [2;4m1.17it/s[0m  
[2KStep 10 - TTA Loss: 1.2949m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 17/203 [2m0:00:14 â€¢ 0:02:40[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.8065, 0.1935]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:14 â€¢ 0:02:40[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.6557, 0.3443]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:14 â€¢ 0:02:40[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.3535, 0.6465]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:14 â€¢ 0:02:40[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.1628, 0.8372]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:14 â€¢ 0:02:40[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.0745, 0.9255]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:14 â€¢ 0:02:40[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.0336, 0.9664]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:14 â€¢ 0:02:40[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.0154, 0.9846]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:14 â€¢ 0:02:40[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.0073, 0.9927]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:14 â€¢ 0:02:40[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.0036, 0.9964]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 17/203 [2m0:00:14 â€¢ 0:02:40[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.5497, 0.4503]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 17/203 [2m0:00:14 â€¢ 0:02:40[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.0082, 0.9918],[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 17/203 [2m0:00:14 â€¢ 0:02:40[0m [2;4m1.17it/s[0m  
        [0.0607, 0.9393],
        [0.0254, 0.9746],
        [0.0019, 0.9981],
        [0.0092, 0.9908],
        [0.0141, 0.9859],
        [0.0165, 0.9835],
        [0.0424, 0.9576],
        [0.0117, 0.9883],
[2K        [0.0090, 0.9910]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 17/203 [2m0:00:14 â€¢ 0:02:40[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.5585, 0.4415],24mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 18/203 [2m0:00:15 â€¢ 0:02:38[0m [2;4m1.17it/s[0m  
        [0.5542, 0.4458],
        [0.5550, 0.4450],
        [0.5573, 0.4427],
        [0.5563, 0.4437],
        [0.5534, 0.4466],
        [0.5528, 0.4472],
        [0.5580, 0.4420],
        [0.5584, 0.4416],
[2K        [0.5558, 0.4442]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 18/203 [2m0:00:15 â€¢ 0:02:38[0m [2;4m1.17it/s[0m  
[2KClass label: ThrowDiscus[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 18/203 [2m0:00:15 â€¢ 0:02:38[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.6355, 0.3645]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:15 â€¢ 0:02:38[0m [2;4m1.17it/s[0m  
[2KStep 0 - TTA Loss: 0.48010m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 18/203 [2m0:00:15 â€¢ 0:02:38[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.3962, 0.6038]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:15 â€¢ 0:02:38[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.1927, 0.8073]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:15 â€¢ 0:02:38[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.0960, 0.9040]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:15 â€¢ 0:02:38[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.0491, 0.9509]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:15 â€¢ 0:02:38[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.0264, 0.9736]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:15 â€¢ 0:02:38[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.0150, 0.9850]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:15 â€¢ 0:02:38[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.0090, 0.9910]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:15 â€¢ 0:02:38[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.0058, 0.9942]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:15 â€¢ 0:02:38[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.0040, 0.9960]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:15 â€¢ 0:02:38[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.0029, 0.9971]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:15 â€¢ 0:02:38[0m [2;4m1.17it/s[0m  
[2KStep 10 - TTA Loss: 0.8371m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 18/203 [2m0:00:15 â€¢ 0:02:38[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.0023, 0.9977]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:15 â€¢ 0:02:38[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.0019, 0.9981]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:15 â€¢ 0:02:38[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.0016, 0.9984]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:15 â€¢ 0:02:38[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.0015, 0.9985]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:15 â€¢ 0:02:38[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.0014, 0.9986]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:15 â€¢ 0:02:38[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.0013, 0.9987]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:15 â€¢ 0:02:38[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.0013, 0.9987]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:15 â€¢ 0:02:38[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.0012, 0.9988]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:15 â€¢ 0:02:38[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.0012, 0.9988]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 18/203 [2m0:00:15 â€¢ 0:02:38[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.5419, 0.4581]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 18/203 [2m0:00:15 â€¢ 0:02:38[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.0027, 0.9973],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 18/203 [2m0:00:15 â€¢ 0:02:38[0m [2;4m1.17it/s[0m  
        [0.0380, 0.9620],
        [0.0083, 0.9917],
        [0.0016, 0.9984],
        [0.0017, 0.9983],
        [0.0038, 0.9962],
        [0.0048, 0.9952],
        [0.0194, 0.9806],
        [0.0056, 0.9944],
[2K        [0.0012, 0.9988]], device='cuda:0')â•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 18/203 [2m0:00:15 â€¢ 0:02:38[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.5585, 0.4415],24mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 19/203 [2m0:00:16 â€¢ 0:02:36[0m [2;4m1.18it/s[0m  
        [0.5542, 0.4458],
        [0.5550, 0.4450],
        [0.5572, 0.4428],
        [0.5563, 0.4437],
        [0.5534, 0.4466],
        [0.5527, 0.4473],
        [0.5580, 0.4420],
        [0.5584, 0.4416],
[2K        [0.5558, 0.4442]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 19/203 [2m0:00:16 â€¢ 0:02:36[0m [2;4m1.18it/s[0m  
[2KClass label: SoccerPenaltym[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 19/203 [2m0:00:16 â€¢ 0:02:36[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.6391, 0.3609]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:16 â€¢ 0:02:36[0m [2;4m1.18it/s[0m  
[2KStep 0 - TTA Loss: 0.47470m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 19/203 [2m0:00:16 â€¢ 0:02:36[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.6391, 0.3609]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:16 â€¢ 0:02:36[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.6388, 0.3612]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:16 â€¢ 0:02:36[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.6385, 0.3615]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:16 â€¢ 0:02:36[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.6384, 0.3616]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:16 â€¢ 0:02:36[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.6304, 0.3696]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:16 â€¢ 0:02:36[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.6151, 0.3849]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:16 â€¢ 0:02:36[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.6377, 0.3623]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:16 â€¢ 0:02:36[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.7579, 0.2421]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:16 â€¢ 0:02:36[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.9356, 0.0644]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:16 â€¢ 0:02:36[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[9.9916e-01, 8.4501e-04]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 19/203 [2m0:00:16 â€¢ 0:02:36[0m [2;4m1.18it/s[0m  
[2KStep 10 - TTA Loss: 1.2859m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 19/203 [2m0:00:16 â€¢ 0:02:36[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.9904, 0.0096]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:16 â€¢ 0:02:36[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.9072, 0.0928]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:16 â€¢ 0:02:36[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.9104, 0.0896]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:16 â€¢ 0:02:36[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.9621, 0.0379]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:16 â€¢ 0:02:36[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.9956, 0.0044]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:16 â€¢ 0:02:36[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.9850, 0.0150]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:16 â€¢ 0:02:36[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.9864, 0.0136]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:16 â€¢ 0:02:36[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.9951, 0.0049]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:16 â€¢ 0:02:36[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.9908, 0.0092]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 19/203 [2m0:00:16 â€¢ 0:02:36[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.5579, 0.4421]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 19/203 [2m0:00:16 â€¢ 0:02:36[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.8181, 0.1819],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 19/203 [2m0:00:16 â€¢ 0:02:36[0m [2;4m1.18it/s[0m  
        [0.8211, 0.1789],
        [0.6542, 0.3458],
        [0.7726, 0.2274],
        [0.8048, 0.1952],
        [0.8291, 0.1709],
        [0.8044, 0.1956],
        [0.9893, 0.0107],
        [0.8121, 0.1879],
[2K        [0.7983, 0.2017]], device='cuda:0')â•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 19/203 [2m0:00:16 â€¢ 0:02:36[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.5585, 0.4415],24mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 20/203 [2m0:00:17 â€¢ 0:02:33[0m [2;4m1.20it/s[0m  
        [0.5541, 0.4459],
        [0.5550, 0.4450],
        [0.5572, 0.4428],
        [0.5563, 0.4437],
        [0.5534, 0.4466],
        [0.5527, 0.4473],
        [0.5579, 0.4421],
        [0.5584, 0.4416],
[2K        [0.5558, 0.4442]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 20/203 [2m0:00:17 â€¢ 0:02:33[0m [2;4m1.20it/s[0m  
[2KClass label: HighJumpâ”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 20/203 [2m0:00:17 â€¢ 0:02:33[0m [2;4m1.20it/s[0m  
[2KAttention weights: tensor([[0.5955, 0.4045]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:17 â€¢ 0:02:33[0m [2;4m1.20it/s[0m  
[2KStep 0 - TTA Loss: 0.63450m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 20/203 [2m0:00:17 â€¢ 0:02:33[0m [2;4m1.20it/s[0m  
[2KAttention weights: tensor([[0.7267, 0.2733]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:17 â€¢ 0:02:33[0m [2;4m1.20it/s[0m  
[2KAttention weights: tensor([[0.7997, 0.2003]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:17 â€¢ 0:02:33[0m [2;4m1.20it/s[0m  
[2KAttention weights: tensor([[0.9446, 0.0554]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 20/203 [2m0:00:17 â€¢ 0:02:33[0m [2;4m1.20it/s[0m  
[2KAttention weights: tensor([[9.9974e-01, 2.6268e-04]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:17 â€¢ 0:02:33[0m [2;4m1.20it/s[0m  
[2KAttention weights: tensor([[9.9998e-01, 1.6389e-05]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:17 â€¢ 0:02:33[0m [2;4m1.20it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.7810e-06]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:17 â€¢ 0:02:33[0m [2;4m1.20it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.8276e-07]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:17 â€¢ 0:02:33[0m [2;4m1.20it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 6.2246e-08]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:17 â€¢ 0:02:33[0m [2;4m1.20it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.8273e-08]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:17 â€¢ 0:02:33[0m [2;4m1.20it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 6.8958e-09]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:17 â€¢ 0:02:33[0m [2;4m1.20it/s[0m  
[2KStep 10 - TTA Loss: 1.3994m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 20/203 [2m0:00:17 â€¢ 0:02:33[0m [2;4m1.20it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.2291e-09]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:17 â€¢ 0:02:33[0m [2;4m1.20it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.8145e-09]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:17 â€¢ 0:02:33[0m [2;4m1.20it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.1859e-09]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:17 â€¢ 0:02:33[0m [2;4m1.20it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 8.7615e-10]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:17 â€¢ 0:02:33[0m [2;4m1.20it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 7.1308e-10]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:17 â€¢ 0:02:33[0m [2;4m1.20it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 6.2499e-10]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:17 â€¢ 0:02:33[0m [2;4m1.20it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 5.7839e-10]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:17 â€¢ 0:02:33[0m [2;4m1.20it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 5.5579e-10]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:17 â€¢ 0:02:33[0m [2;4m1.20it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 5.4689e-10]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 20/203 [2m0:00:17 â€¢ 0:02:33[0m [2;4m1.20it/s[0m  
[2KAttention weights: tensor([[0.5777, 0.4223]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 20/203 [2m0:00:17 â€¢ 0:02:33[0m [2;4m1.20it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.4579e-06],;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 20/203 [2m0:00:17 â€¢ 0:02:33[0m [2;4m1.20it/s[0m  
        [9.9995e-01, 5.4459e-05],
        [1.0000e+00, 1.2005e-06],
        [9.9999e-01, 5.2890e-06],
        [1.0000e+00, 5.9897e-08],
        [1.0000e+00, 5.3374e-10],
        [1.0000e+00, 1.7592e-08],
        [1.0000e+00, 1.2711e-06],
        [1.0000e+00, 1.1935e-06],
[2K        [1.0000e+00, 7.1246e-08]], device='cuda:0')8;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 20/203 [2m0:00:17 â€¢ 0:02:33[0m [2;4m1.20it/s[0m  
[2KAttention weights: tensor([[0.5584, 0.4416],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 21/203 [2m0:00:18 â€¢ 0:02:38[0m [2;4m1.15it/s[0m   
        [0.5541, 0.4459],
        [0.5549, 0.4451],
        [0.5572, 0.4428],
        [0.5563, 0.4437],
        [0.5534, 0.4466],
        [0.5527, 0.4473],
        [0.5579, 0.4421],
        [0.5584, 0.4416],
[2K        [0.5558, 0.4442]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 21/203 [2m0:00:18 â€¢ 0:02:38[0m [2;4m1.15it/s[0m  
[2KClass label: BaseballPitch0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 21/203 [2m0:00:18 â€¢ 0:02:38[0m [2;4m1.15it/s[0m  
[2KAttention weights: tensor([[0.6124, 0.3876]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:18 â€¢ 0:02:38[0m [2;4m1.15it/s[0m  
[2KStep 0 - TTA Loss: 0.6376[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 21/203 [2m0:00:18 â€¢ 0:02:38[0m [2;4m1.15it/s[0m  
[2KAttention weights: tensor([[0.6124, 0.3876]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:18 â€¢ 0:02:38[0m [2;4m1.15it/s[0m  
[2KAttention weights: tensor([[0.6123, 0.3877]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:18 â€¢ 0:02:38[0m [2;4m1.15it/s[0m  
[2KAttention weights: tensor([[0.6124, 0.3876]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:18 â€¢ 0:02:38[0m [2;4m1.15it/s[0m  
[2KAttention weights: tensor([[0.6045, 0.3955]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:18 â€¢ 0:02:38[0m [2;4m1.15it/s[0m  
[2KAttention weights: tensor([[0.5909, 0.4091]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:18 â€¢ 0:02:38[0m [2;4m1.15it/s[0m  
[2KAttention weights: tensor([[0.5712, 0.4288]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:18 â€¢ 0:02:38[0m [2;4m1.15it/s[0m  
[2KAttention weights: tensor([[0.5738, 0.4262]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:18 â€¢ 0:02:38[0m [2;4m1.15it/s[0m  
[2KAttention weights: tensor([[0.6642, 0.3358]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:18 â€¢ 0:02:38[0m [2;4m1.15it/s[0m  
[2KAttention weights: tensor([[0.8527, 0.1473]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:18 â€¢ 0:02:38[0m [2;4m1.15it/s[0m  
[2KAttention weights: tensor([[0.9878, 0.0122]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:18 â€¢ 0:02:38[0m [2;4m1.15it/s[0m  
[2KStep 10 - TTA Loss: 0.87340m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 21/203 [2m0:00:18 â€¢ 0:02:38[0m [2;4m1.15it/s[0m  
[2KAttention weights: tensor([[9.9976e-01, 2.3557e-04]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:18 â€¢ 0:02:38[0m [2;4m1.15it/s[0m  
[2KAttention weights: tensor([[9.9995e-01, 5.4805e-05]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:18 â€¢ 0:02:38[0m [2;4m1.15it/s[0m  
[2KAttention weights: tensor([[9.9997e-01, 2.7659e-05]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:18 â€¢ 0:02:38[0m [2;4m1.15it/s[0m  
[2KAttention weights: tensor([[9.9998e-01, 2.3214e-05]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:18 â€¢ 0:02:38[0m [2;4m1.15it/s[0m  
[2KAttention weights: tensor([[9.9997e-01, 3.3059e-05]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:18 â€¢ 0:02:38[0m [2;4m1.15it/s[0m  
[2KAttention weights: tensor([[9.9990e-01, 9.7122e-05]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:18 â€¢ 0:02:38[0m [2;4m1.15it/s[0m  
[2KAttention weights: tensor([[0.9962, 0.0038]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:18 â€¢ 0:02:38[0m [2;4m1.15it/s[0m  
[2KAttention weights: tensor([[0.0452, 0.9548]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 21/203 [2m0:00:18 â€¢ 0:02:38[0m [2;4m1.15it/s[0m  
[2KAttention weights: tensor([[1.6759e-05, 9.9998e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:18 â€¢ 0:02:38[0m [2;4m1.15it/s[0m  
[2KAttention weights: tensor([[0.5440, 0.4560]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 21/203 [2m0:00:18 â€¢ 0:02:38[0m [2;4m1.15it/s[0m  
[2KAttention weights: tensor([[1.2034e-08, 1.0000e+00],37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 21/203 [2m0:00:18 â€¢ 0:02:38[0m [2;4m1.15it/s[0m  
        [2.2882e-03, 9.9771e-01],
        [1.9944e-03, 9.9801e-01],
        [7.2820e-06, 9.9999e-01],
        [8.8407e-06, 9.9999e-01],
        [6.4672e-05, 9.9994e-01],
        [1.4015e-04, 9.9986e-01],
        [1.3360e-04, 9.9987e-01],
        [5.5917e-05, 9.9994e-01],
[2K        [6.4228e-06, 9.9999e-01]], device='cuda:0')237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 21/203 [2m0:00:18 â€¢ 0:02:38[0m [2;4m1.15it/s[0m  
[2KAttention weights: tensor([[0.5585, 0.4415],[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 22/203 [2m0:00:19 â€¢ 0:02:37[0m [2;4m1.16it/s[0m  
        [0.5542, 0.4458],
        [0.5550, 0.4450],
        [0.5573, 0.4427],
        [0.5563, 0.4437],
        [0.5535, 0.4465],
        [0.5528, 0.4472],
        [0.5580, 0.4420],
        [0.5585, 0.4415],
[2K        [0.5558, 0.4442]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 22/203 [2m0:00:19 â€¢ 0:02:37[0m [2;4m1.16it/s[0m  
[2KClass label: CleanAndJerk[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 22/203 [2m0:00:19 â€¢ 0:02:37[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[0.5700, 0.4300]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:19 â€¢ 0:02:37[0m [2;4m1.16it/s[0m  
[2KStep 0 - TTA Loss: 0.8443[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 22/203 [2m0:00:19 â€¢ 0:02:37[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[0.1067, 0.8933]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:19 â€¢ 0:02:37[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[0.0165, 0.9835]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:19 â€¢ 0:02:37[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[0.0034, 0.9966]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 22/203 [2m0:00:19 â€¢ 0:02:37[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[6.9402e-04, 9.9931e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:19 â€¢ 0:02:37[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[1.7067e-04, 9.9983e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:19 â€¢ 0:02:37[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[5.0770e-05, 9.9995e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:19 â€¢ 0:02:37[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[1.8207e-05, 9.9998e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:19 â€¢ 0:02:37[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[7.7614e-06, 9.9999e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:19 â€¢ 0:02:37[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[3.8709e-06, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:19 â€¢ 0:02:37[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[2.2178e-06, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:19 â€¢ 0:02:37[0m [2;4m1.16it/s[0m  
[2KStep 10 - TTA Loss: 0.89430m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 22/203 [2m0:00:19 â€¢ 0:02:37[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[1.4352e-06, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:19 â€¢ 0:02:37[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[1.0310e-06, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:19 â€¢ 0:02:37[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[8.0802e-07, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:19 â€¢ 0:02:37[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[6.7966e-07, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:19 â€¢ 0:02:37[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[6.0467e-07, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:19 â€¢ 0:02:37[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[5.6127e-07, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:19 â€¢ 0:02:37[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[5.3735e-07, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:19 â€¢ 0:02:37[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[5.2558e-07, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:19 â€¢ 0:02:37[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[5.2094e-07, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:19 â€¢ 0:02:37[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[0.5487, 0.4513]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 22/203 [2m0:00:19 â€¢ 0:02:37[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[2.7662e-18, 1.0000e+00],37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 22/203 [2m0:00:19 â€¢ 0:02:37[0m [2;4m1.16it/s[0m  
        [3.5163e-07, 1.0000e+00],
        [5.1522e-07, 1.0000e+00],
        [1.4593e-12, 1.0000e+00],
        [2.1370e-12, 1.0000e+00],
        [8.1375e-11, 1.0000e+00],
        [6.5926e-10, 1.0000e+00],
        [5.1555e-10, 1.0000e+00],
        [9.6351e-11, 1.0000e+00],
[2K        [9.2611e-13, 1.0000e+00]], device='cuda:0')237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 22/203 [2m0:00:19 â€¢ 0:02:37[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[0.5586, 0.4414],224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 23/203 [2m0:00:20 â€¢ 0:02:34[0m [2;4m1.17it/s[0m  
        [0.5542, 0.4458],
        [0.5550, 0.4450],
        [0.5573, 0.4427],
        [0.5564, 0.4436],
        [0.5535, 0.4465],
        [0.5528, 0.4472],
        [0.5580, 0.4420],
        [0.5585, 0.4415],
[2K        [0.5559, 0.4441]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 23/203 [2m0:00:20 â€¢ 0:02:34[0m [2;4m1.17it/s[0m  
[2KClass label: TennisSwing[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 23/203 [2m0:00:20 â€¢ 0:02:34[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.6502, 0.3498]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:20 â€¢ 0:02:34[0m [2;4m1.17it/s[0m  
[2KStep 0 - TTA Loss: 0.7318[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 23/203 [2m0:00:20 â€¢ 0:02:34[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.6502, 0.3498]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:20 â€¢ 0:02:34[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.6474, 0.3526]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:20 â€¢ 0:02:34[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.6342, 0.3658]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:20 â€¢ 0:02:34[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.5982, 0.4018]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:20 â€¢ 0:02:34[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.5182, 0.4818]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:20 â€¢ 0:02:34[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.3988, 0.6012]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:20 â€¢ 0:02:34[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.2744, 0.7256]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:20 â€¢ 0:02:34[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.1748, 0.8252]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:20 â€¢ 0:02:34[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.0985, 0.9015]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:20 â€¢ 0:02:34[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.0487, 0.9513]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:20 â€¢ 0:02:34[0m [2;4m1.17it/s[0m  
[2KStep 10 - TTA Loss: 0.75650m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 23/203 [2m0:00:20 â€¢ 0:02:34[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.0221, 0.9779]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:20 â€¢ 0:02:34[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.0097, 0.9903]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:20 â€¢ 0:02:34[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.0042, 0.9958]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:20 â€¢ 0:02:34[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.0019, 0.9981]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 23/203 [2m0:00:20 â€¢ 0:02:34[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[8.7137e-04, 9.9913e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 23/203 [2m0:00:20 â€¢ 0:02:34[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[4.2286e-04, 9.9958e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 23/203 [2m0:00:20 â€¢ 0:02:34[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[2.1777e-04, 9.9978e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 23/203 [2m0:00:20 â€¢ 0:02:34[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1.1869e-04, 9.9988e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 23/203 [2m0:00:20 â€¢ 0:02:34[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[6.9733e-05, 9.9993e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 23/203 [2m0:00:20 â€¢ 0:02:34[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.5481, 0.4519]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 23/203 [2m0:00:20 â€¢ 0:02:34[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[8.9152e-04, 9.9911e-01],8;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 23/203 [2m0:00:20 â€¢ 0:02:34[0m [2;4m1.17it/s[0m  
        [8.1388e-03, 9.9186e-01],
        [1.1497e-02, 9.8850e-01],
        [1.0828e-03, 9.9892e-01],
        [2.2352e-03, 9.9776e-01],
        [1.8357e-03, 9.9816e-01],
        [1.0167e-02, 9.8983e-01],
        [9.3734e-03, 9.9063e-01],
        [4.4639e-05, 9.9996e-01],
[2K        [1.3899e-03, 9.9861e-01]], device='cuda:0')38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 23/203 [2m0:00:20 â€¢ 0:02:34[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.5586, 0.4414],224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 24/203 [2m0:00:20 â€¢ 0:02:32[0m [2;4m1.18it/s[0m  
        [0.5542, 0.4458],
        [0.5550, 0.4450],
        [0.5573, 0.4427],
        [0.5564, 0.4436],
        [0.5535, 0.4465],
        [0.5528, 0.4472],
        [0.5580, 0.4420],
        [0.5585, 0.4415],
[2K        [0.5559, 0.4441]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 24/203 [2m0:00:20 â€¢ 0:02:32[0m [2;4m1.18it/s[0m  
[2KClass label: Billiardsâ”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 24/203 [2m0:00:20 â€¢ 0:02:32[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.5839, 0.4161]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:20 â€¢ 0:02:32[0m [2;4m1.18it/s[0m  
[2KStep 0 - TTA Loss: 0.9273[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 24/203 [2m0:00:20 â€¢ 0:02:32[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.5366, 0.4634]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:20 â€¢ 0:02:32[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.4638, 0.5362]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:20 â€¢ 0:02:32[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.3584, 0.6416]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:20 â€¢ 0:02:32[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.2457, 0.7543]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:20 â€¢ 0:02:32[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.1697, 0.8303]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:20 â€¢ 0:02:32[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.1088, 0.8912]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:20 â€¢ 0:02:32[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.0733, 0.9267]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:20 â€¢ 0:02:32[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.0527, 0.9473]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:20 â€¢ 0:02:32[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.0407, 0.9593]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:20 â€¢ 0:02:32[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.0333, 0.9667]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:20 â€¢ 0:02:32[0m [2;4m1.18it/s[0m  
[2KStep 10 - TTA Loss: 1.03840m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 24/203 [2m0:00:20 â€¢ 0:02:32[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.0286, 0.9714]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:20 â€¢ 0:02:32[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.0254, 0.9746]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:20 â€¢ 0:02:32[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.0234, 0.9766]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:20 â€¢ 0:02:32[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.0220, 0.9780]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:20 â€¢ 0:02:32[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.0212, 0.9788]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:20 â€¢ 0:02:32[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.0207, 0.9793]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:20 â€¢ 0:02:32[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.0204, 0.9796]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:20 â€¢ 0:02:32[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.0202, 0.9798]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:20 â€¢ 0:02:32[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.0202, 0.9798]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 24/203 [2m0:00:20 â€¢ 0:02:32[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.5505, 0.4495]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 24/203 [2m0:00:20 â€¢ 0:02:32[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.0851, 0.9149],â•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 24/203 [2m0:00:20 â€¢ 0:02:32[0m [2;4m1.18it/s[0m  
        [0.0201, 0.9799],
        [0.1670, 0.8330],
        [0.0968, 0.9032],
        [0.0983, 0.9017],
        [0.0922, 0.9078],
        [0.1272, 0.8728],
        [0.1379, 0.8621],
        [0.0417, 0.9583],
[2K        [0.1040, 0.8960]], device='cuda:0')mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 24/203 [2m0:00:20 â€¢ 0:02:32[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.5586, 0.4414],224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 25/203 [2m0:00:21 â€¢ 0:02:31[0m [2;4m1.18it/s[0m  
        [0.5542, 0.4458],
        [0.5550, 0.4450],
        [0.5573, 0.4427],
        [0.5564, 0.4436],
        [0.5535, 0.4465],
        [0.5528, 0.4472],
        [0.5580, 0.4420],
        [0.5585, 0.4415],
[2K        [0.5559, 0.4441]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 25/203 [2m0:00:21 â€¢ 0:02:31[0m [2;4m1.18it/s[0m  
[2KClass label: GolfSwingâ”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 25/203 [2m0:00:21 â€¢ 0:02:31[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.6336, 0.3664]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:21 â€¢ 0:02:31[0m [2;4m1.18it/s[0m  
[2KStep 0 - TTA Loss: 0.6457[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 25/203 [2m0:00:21 â€¢ 0:02:31[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.6336, 0.3664]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:21 â€¢ 0:02:31[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.6346, 0.3654]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:21 â€¢ 0:02:31[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.6401, 0.3599]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:21 â€¢ 0:02:31[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.6533, 0.3467]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:21 â€¢ 0:02:31[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.6763, 0.3237]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:21 â€¢ 0:02:31[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.7028, 0.2972]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:21 â€¢ 0:02:31[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.7052, 0.2948]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:21 â€¢ 0:02:31[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.6575, 0.3425]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:21 â€¢ 0:02:31[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.6386, 0.3614]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:21 â€¢ 0:02:31[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.8047, 0.1953]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:21 â€¢ 0:02:31[0m [2;4m1.18it/s[0m  
[2KStep 10 - TTA Loss: 1.42120m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 25/203 [2m0:00:21 â€¢ 0:02:31[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.9864, 0.0136]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:21 â€¢ 0:02:31[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.9963, 0.0037]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:21 â€¢ 0:02:31[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.0031, 0.9969]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 25/203 [2m0:00:21 â€¢ 0:02:31[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[4.0076e-08, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 25/203 [2m0:00:21 â€¢ 0:02:31[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[6.8470e-13, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 25/203 [2m0:00:21 â€¢ 0:02:31[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[1.8323e-17, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 25/203 [2m0:00:21 â€¢ 0:02:31[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[8.6227e-22, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 25/203 [2m0:00:21 â€¢ 0:02:31[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[7.8639e-26, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 25/203 [2m0:00:21 â€¢ 0:02:31[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[1.4845e-29, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 25/203 [2m0:00:21 â€¢ 0:02:31[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.5046, 0.4954]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 25/203 [2m0:00:21 â€¢ 0:02:31[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[1.6022e-25, 1.0000e+00],8;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 25/203 [2m0:00:21 â€¢ 0:02:31[0m [2;4m1.18it/s[0m  
        [9.3789e-14, 1.0000e+00],
        [8.7073e-17, 1.0000e+00],
        [5.6051e-33, 1.0000e+00],
        [7.5257e-23, 1.0000e+00],
        [1.6344e-19, 1.0000e+00],
        [1.9776e-18, 1.0000e+00],
        [6.7529e-16, 1.0000e+00],
        [8.8727e-24, 1.0000e+00],
[2K        [4.3067e-24, 1.0000e+00]], device='cuda:0')38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 25/203 [2m0:00:21 â€¢ 0:02:31[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.5586, 0.4414],â•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 26/203 [2m0:00:22 â€¢ 0:02:30[0m [2;4m1.19it/s[0m   
        [0.5542, 0.4458],
        [0.5550, 0.4450],
        [0.5573, 0.4427],
        [0.5564, 0.4436],
        [0.5535, 0.4465],
        [0.5528, 0.4472],
        [0.5580, 0.4420],
        [0.5585, 0.4415],
[2K        [0.5559, 0.4441]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 26/203 [2m0:00:22 â€¢ 0:02:30[0m [2;4m1.19it/s[0m  
[2KClass label: HighJumpâ”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 26/203 [2m0:00:22 â€¢ 0:02:30[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[0.5921, 0.4079]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:22 â€¢ 0:02:30[0m [2;4m1.19it/s[0m  
[2KStep 0 - TTA Loss: 0.5583[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 26/203 [2m0:00:22 â€¢ 0:02:30[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[0.0343, 0.9657]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 26/203 [2m0:00:22 â€¢ 0:02:30[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[4.5595e-04, 9.9954e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:22 â€¢ 0:02:30[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[1.1594e-05, 9.9999e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:22 â€¢ 0:02:30[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[5.4918e-07, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:22 â€¢ 0:02:30[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[4.5368e-08, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:22 â€¢ 0:02:30[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[5.9016e-09, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:22 â€¢ 0:02:30[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[1.1501e-09, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:22 â€¢ 0:02:30[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[3.0733e-10, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:22 â€¢ 0:02:30[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[1.0593e-10, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:22 â€¢ 0:02:30[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[4.5151e-11, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:22 â€¢ 0:02:30[0m [2;4m1.19it/s[0m  
[2KStep 10 - TTA Loss: 0.8914[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 26/203 [2m0:00:22 â€¢ 0:02:30[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[2.2945e-11, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:22 â€¢ 0:02:30[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[1.3574e-11, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:22 â€¢ 0:02:30[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[9.1269e-12, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:22 â€¢ 0:02:30[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[6.8297e-12, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:22 â€¢ 0:02:30[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[5.5696e-12, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:22 â€¢ 0:02:30[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[4.8657e-12, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:22 â€¢ 0:02:30[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[4.4839e-12, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:22 â€¢ 0:02:30[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[4.2994e-12, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:22 â€¢ 0:02:30[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[4.2248e-12, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:22 â€¢ 0:02:30[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[0.5209, 0.4791]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 26/203 [2m0:00:22 â€¢ 0:02:30[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[7.1149e-16, 1.0000e+00],237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 26/203 [2m0:00:22 â€¢ 0:02:30[0m [2;4m1.19it/s[0m  
        [1.3203e-08, 1.0000e+00],
        [1.6446e-10, 1.0000e+00],
        [1.6564e-20, 1.0000e+00],
        [3.0994e-14, 1.0000e+00],
        [4.0977e-12, 1.0000e+00],
        [1.5836e-11, 1.0000e+00],
        [6.0708e-10, 1.0000e+00],
        [9.4380e-15, 1.0000e+00],
[2K        [5.5575e-15, 1.0000e+00]], device='cuda:0');237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 26/203 [2m0:00:22 â€¢ 0:02:30[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[0.5586, 0.4414],â•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 27/203 [2m0:00:23 â€¢ 0:02:32[0m [2;4m1.16it/s[0m  
        [0.5542, 0.4458],
        [0.5550, 0.4450],
        [0.5573, 0.4427],
        [0.5563, 0.4437],
        [0.5535, 0.4465],
        [0.5528, 0.4472],
        [0.5580, 0.4420],
        [0.5584, 0.4416],
[2K        [0.5558, 0.4442]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 27/203 [2m0:00:23 â€¢ 0:02:32[0m [2;4m1.16it/s[0m  
[2KClass label: GolfSwingâ”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 27/203 [2m0:00:23 â€¢ 0:02:32[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[0.6231, 0.3769]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:23 â€¢ 0:02:32[0m [2;4m1.16it/s[0m  
[2KStep 0 - TTA Loss: 0.7077[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 27/203 [2m0:00:23 â€¢ 0:02:32[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[0.6231, 0.3769]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:23 â€¢ 0:02:32[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[0.6217, 0.3783]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:23 â€¢ 0:02:32[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[0.6168, 0.3832]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:23 â€¢ 0:02:32[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[0.6072, 0.3928]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:23 â€¢ 0:02:32[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[0.5906, 0.4094]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:23 â€¢ 0:02:32[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[0.5583, 0.4417]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:23 â€¢ 0:02:32[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[0.4990, 0.5010]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:23 â€¢ 0:02:32[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[0.4158, 0.5842]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:23 â€¢ 0:02:32[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[0.3339, 0.6661]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:23 â€¢ 0:02:32[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[0.2768, 0.7232]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:23 â€¢ 0:02:32[0m [2;4m1.16it/s[0m  
[2KStep 10 - TTA Loss: 1.2751[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 27/203 [2m0:00:23 â€¢ 0:02:32[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[0.2253, 0.7747]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:23 â€¢ 0:02:32[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[0.1900, 0.8100]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:23 â€¢ 0:02:32[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[0.1558, 0.8442]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:23 â€¢ 0:02:32[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[0.1242, 0.8758]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:23 â€¢ 0:02:32[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[0.0986, 0.9014]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:23 â€¢ 0:02:32[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[0.0785, 0.9215]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:23 â€¢ 0:02:32[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[0.0630, 0.9370]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:23 â€¢ 0:02:32[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[0.0511, 0.9489]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:23 â€¢ 0:02:32[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[0.0420, 0.9580]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 27/203 [2m0:00:23 â€¢ 0:02:32[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[0.5522, 0.4478]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 27/203 [2m0:00:23 â€¢ 0:02:32[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[0.0770, 0.9230],m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 27/203 [2m0:00:23 â€¢ 0:02:32[0m [2;4m1.16it/s[0m  
        [0.2133, 0.7867],
        [0.1466, 0.8534],
        [0.0349, 0.9651],
        [0.0852, 0.9148],
        [0.1192, 0.8808],
        [0.1167, 0.8833],
        [0.2121, 0.7879],
        [0.1001, 0.8999],
[2K        [0.0906, 0.9094]], device='cuda:0')0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 27/203 [2m0:00:23 â€¢ 0:02:32[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[0.5586, 0.4414],;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 28/203 [2m0:00:24 â€¢ 0:02:31[0m [2;4m1.16it/s[0m  
        [0.5542, 0.4458],
        [0.5550, 0.4450],
        [0.5573, 0.4427],
        [0.5563, 0.4437],
        [0.5535, 0.4465],
        [0.5528, 0.4472],
        [0.5580, 0.4420],
        [0.5584, 0.4416],
[2K        [0.5559, 0.4441]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 28/203 [2m0:00:24 â€¢ 0:02:31[0m [2;4m1.16it/s[0m  
[2KClass label: CleanAndJerk[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 28/203 [2m0:00:24 â€¢ 0:02:31[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[0.5635, 0.4365]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:24 â€¢ 0:02:31[0m [2;4m1.16it/s[0m  
[2KStep 0 - TTA Loss: 0.8708[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 28/203 [2m0:00:24 â€¢ 0:02:31[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[0.7387, 0.2613]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:24 â€¢ 0:02:31[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[0.8358, 0.1642]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:24 â€¢ 0:02:31[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[9.9909e-01, 9.1463e-04]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 28/203 [2m0:00:24 â€¢ 0:02:31[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[0.9930, 0.0070]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:24 â€¢ 0:02:31[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[0.0269, 0.9731]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 28/203 [2m0:00:24 â€¢ 0:02:31[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[2.1443e-05, 9.9998e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 28/203 [2m0:00:24 â€¢ 0:02:31[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[5.6978e-08, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 28/203 [2m0:00:24 â€¢ 0:02:31[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[4.5596e-10, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 28/203 [2m0:00:24 â€¢ 0:02:31[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[9.4928e-12, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 28/203 [2m0:00:24 â€¢ 0:02:31[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[4.4101e-13, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 28/203 [2m0:00:24 â€¢ 0:02:31[0m [2;4m1.16it/s[0m  
[2KStep 10 - TTA Loss: 1.2067[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 28/203 [2m0:00:24 â€¢ 0:02:31[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[4.0558e-14, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 28/203 [2m0:00:24 â€¢ 0:02:31[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[6.6230e-15, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 28/203 [2m0:00:24 â€¢ 0:02:31[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[1.7210e-15, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 28/203 [2m0:00:24 â€¢ 0:02:31[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[6.5507e-16, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 28/203 [2m0:00:24 â€¢ 0:02:31[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[3.3870e-16, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 28/203 [2m0:00:24 â€¢ 0:02:31[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[2.2104e-16, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 28/203 [2m0:00:24 â€¢ 0:02:31[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[1.7189e-16, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 28/203 [2m0:00:24 â€¢ 0:02:31[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[1.5077e-16, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 28/203 [2m0:00:24 â€¢ 0:02:31[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[1.4296e-16, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 28/203 [2m0:00:24 â€¢ 0:02:31[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[0.5148, 0.4852]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 28/203 [2m0:00:24 â€¢ 0:02:31[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[3.1006e-06, 1.0000e+00],38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 28/203 [2m0:00:24 â€¢ 0:02:31[0m [2;4m1.16it/s[0m  
        [1.4977e-05, 9.9998e-01],
        [1.3794e-16, 1.0000e+00],
        [3.2693e-07, 1.0000e+00],
        [1.8318e-09, 1.0000e+00],
        [7.4823e-09, 1.0000e+00],
        [2.6503e-08, 1.0000e+00],
        [3.6387e-05, 9.9996e-01],
        [6.7485e-07, 1.0000e+00],
[2K        [8.7539e-09, 1.0000e+00]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 28/203 [2m0:00:24 â€¢ 0:02:31[0m [2;4m1.16it/s[0m  
[2KAttention weights: tensor([[0.5586, 0.4414],;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 29/203 [2m0:00:25 â€¢ 0:02:29[0m [2;4m1.17it/s[0m  
        [0.5542, 0.4458],
        [0.5551, 0.4449],
        [0.5573, 0.4427],
        [0.5564, 0.4436],
        [0.5535, 0.4465],
        [0.5528, 0.4472],
        [0.5580, 0.4420],
        [0.5585, 0.4415],
[2K        [0.5559, 0.4441]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 29/203 [2m0:00:25 â€¢ 0:02:29[0m [2;4m1.17it/s[0m  
[2KClass label: Billiardsâ”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 29/203 [2m0:00:25 â€¢ 0:02:29[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.5838, 0.4162]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:25 â€¢ 0:02:29[0m [2;4m1.17it/s[0m  
[2KStep 0 - TTA Loss: 0.5602[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 29/203 [2m0:00:25 â€¢ 0:02:29[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.5838, 0.4162]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:25 â€¢ 0:02:29[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.5841, 0.4159]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:25 â€¢ 0:02:29[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.5871, 0.4129]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:25 â€¢ 0:02:29[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.5956, 0.4044]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:25 â€¢ 0:02:29[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.6051, 0.3949]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:25 â€¢ 0:02:29[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.6304, 0.3696]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:25 â€¢ 0:02:29[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.7559, 0.2441]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:25 â€¢ 0:02:29[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.9406, 0.0594]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 29/203 [2m0:00:25 â€¢ 0:02:29[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[9.9957e-01, 4.2941e-04]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 29/203 [2m0:00:25 â€¢ 0:02:29[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[9.9996e-01, 3.6261e-05]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 29/203 [2m0:00:25 â€¢ 0:02:29[0m [2;4m1.17it/s[0m  
[2KStep 10 - TTA Loss: 1.5522[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 29/203 [2m0:00:25 â€¢ 0:02:29[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.7280e-06]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 29/203 [2m0:00:25 â€¢ 0:02:29[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.7095e-07]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 29/203 [2m0:00:25 â€¢ 0:02:29[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.5899e-08]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 29/203 [2m0:00:25 â€¢ 0:02:29[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.5150e-09]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 29/203 [2m0:00:25 â€¢ 0:02:29[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.6185e-10]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 29/203 [2m0:00:25 â€¢ 0:02:29[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.0478e-11]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 29/203 [2m0:00:25 â€¢ 0:02:29[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 5.0544e-12]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 29/203 [2m0:00:25 â€¢ 0:02:29[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 7.1932e-13]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 29/203 [2m0:00:25 â€¢ 0:02:29[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.1844e-13]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 29/203 [2m0:00:25 â€¢ 0:02:29[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.5764, 0.4236]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 29/203 [2m0:00:25 â€¢ 0:02:29[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[9.9964e-01, 3.5532e-04],38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 29/203 [2m0:00:25 â€¢ 0:02:29[0m [2;4m1.17it/s[0m  
        [1.0000e+00, 2.2339e-14],
        [9.8159e-01, 1.8412e-02],
        [9.9940e-01, 6.0284e-04],
        [9.9911e-01, 8.8790e-04],
        [9.9970e-01, 2.9887e-04],
        [9.9942e-01, 5.7596e-04],
        [9.9997e-01, 2.6653e-05],
        [9.9996e-01, 3.7998e-05],
[2K        [9.9946e-01, 5.4173e-04]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 29/203 [2m0:00:25 â€¢ 0:02:29[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.5587, 0.4413],;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 30/203 [2m0:00:26 â€¢ 0:02:28[0m [2;4m1.17it/s[0m  
        [0.5545, 0.4455],
        [0.5552, 0.4448],
        [0.5574, 0.4426],
        [0.5565, 0.4435],
        [0.5536, 0.4464],
        [0.5529, 0.4471],
        [0.5581, 0.4419],
        [0.5586, 0.4414],
[2K        [0.5560, 0.4440]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 30/203 [2m0:00:26 â€¢ 0:02:28[0m [2;4m1.17it/s[0m  
[2KClass label: CleanAndJerk[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 30/203 [2m0:00:26 â€¢ 0:02:28[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.5727, 0.4273]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:26 â€¢ 0:02:28[0m [2;4m1.17it/s[0m  
[2KStep 0 - TTA Loss: 0.5836[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 30/203 [2m0:00:26 â€¢ 0:02:28[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.5450, 0.4550]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:26 â€¢ 0:02:28[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.4644, 0.5356]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:26 â€¢ 0:02:28[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.6286, 0.3714]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:26 â€¢ 0:02:28[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.8081, 0.1919]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:26 â€¢ 0:02:28[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.9097, 0.0903]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:26 â€¢ 0:02:28[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.9614, 0.0386]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:26 â€¢ 0:02:28[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.9620, 0.0380]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:26 â€¢ 0:02:28[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.9417, 0.0583]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:26 â€¢ 0:02:28[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.8910, 0.1090]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:26 â€¢ 0:02:28[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.8399, 0.1601]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:26 â€¢ 0:02:28[0m [2;4m1.17it/s[0m  
[2KStep 10 - TTA Loss: 0.7921[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 30/203 [2m0:00:26 â€¢ 0:02:28[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.7811, 0.2189]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:26 â€¢ 0:02:28[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.7252, 0.2748]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:26 â€¢ 0:02:28[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.6779, 0.3221]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:26 â€¢ 0:02:28[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.6417, 0.3583]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:26 â€¢ 0:02:28[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.6184, 0.3816]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:26 â€¢ 0:02:28[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.6031, 0.3969]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:26 â€¢ 0:02:28[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.5942, 0.4058]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:26 â€¢ 0:02:28[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.5902, 0.4098]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:26 â€¢ 0:02:28[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.5887, 0.4113]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 30/203 [2m0:00:26 â€¢ 0:02:28[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.5550, 0.4450]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 30/203 [2m0:00:26 â€¢ 0:02:28[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[9.2627e-01, 7.3727e-02],38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 30/203 [2m0:00:26 â€¢ 0:02:28[0m [2;4m1.17it/s[0m  
        [9.9995e-01, 4.6983e-05],
        [5.8838e-01, 4.1162e-01],
        [9.1141e-01, 8.8588e-02],
        [8.7117e-01, 1.2883e-01],
        [9.1297e-01, 8.7033e-02],
        [8.8535e-01, 1.1465e-01],
        [9.7017e-01, 2.9829e-02],
        [9.6463e-01, 3.5369e-02],
[2K        [9.0579e-01, 9.4214e-02]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 30/203 [2m0:00:26 â€¢ 0:02:28[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.5588, 0.4412],mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 31/203 [2m0:00:26 â€¢ 0:02:27[0m [2;4m1.17it/s[0m   
        [0.5546, 0.4454],
        [0.5553, 0.4447],
        [0.5575, 0.4425],
        [0.5566, 0.4434],
        [0.5537, 0.4463],
        [0.5530, 0.4470],
        [0.5582, 0.4418],
        [0.5587, 0.4413],
[2K        [0.5561, 0.4439]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 31/203 [2m0:00:26 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
[2KClass label: CleanAndJerkâ”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 31/203 [2m0:00:26 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.5677, 0.4323]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:26 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
[2KStep 0 - TTA Loss: 0.6141â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 31/203 [2m0:00:26 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.5677, 0.4323]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:26 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.5692, 0.4308]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:26 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.5780, 0.4220]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:26 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.6017, 0.3983]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:26 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.6501, 0.3499]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:26 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.7117, 0.2883]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:26 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.7676, 0.2324]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:26 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.8272, 0.1728]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:26 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.9790, 0.0210]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 31/203 [2m0:00:26 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.2654e-08]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:26 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
[2KStep 10 - TTA Loss: 1.5898[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 31/203 [2m0:00:26 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.9455e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:26 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 6.1592e-21]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:26 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.4015e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:26 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.0488e-34]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:26 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 8.2202e-41]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:26 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 31/203 [2m0:00:26 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 31/203 [2m0:00:26 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 31/203 [2m0:00:26 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”[0m 31/203 [2m0:00:26 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.6221, 0.3779]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 31/203 [2m0:00:26 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.3186e-26],;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 31/203 [2m0:00:26 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
        [1.0000e+00, 3.8854e-23],
        [1.0000e+00, 0.0000e+00],
        [1.0000e+00, 2.5284e-30],
        [1.0000e+00, 4.8373e-40],
        [1.0000e+00, 9.4611e-38],
        [1.0000e+00, 5.4779e-35],
        [1.0000e+00, 9.6167e-23],
        [1.0000e+00, 1.8207e-29],
[2K        [1.0000e+00, 1.4613e-37]], device='cuda:0')5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 31/203 [2m0:00:26 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.5588, 0.4412],mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 32/203 [2m0:00:27 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
        [0.5547, 0.4453],
        [0.5554, 0.4446],
        [0.5576, 0.4424],
        [0.5567, 0.4433],
        [0.5538, 0.4462],
        [0.5531, 0.4469],
        [0.5583, 0.4417],
        [0.5588, 0.4412],
[2K        [0.5562, 0.4438]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 32/203 [2m0:00:27 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
[2KClass label: TennisSwingâ”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 32/203 [2m0:00:27 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.6462, 0.3538]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:27 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
[2KStep 0 - TTA Loss: 0.6446â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 32/203 [2m0:00:27 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.9729, 0.0271]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 32/203 [2m0:00:27 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[5.7930e-06, 9.9999e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:27 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1.1608e-11, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:27 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1.3920e-16, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:27 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[8.1710e-21, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:27 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[2.0500e-24, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:27 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1.9345e-27, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:27 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[6.0144e-30, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:27 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[5.4140e-32, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:27 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1.2399e-33, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:27 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
[2KStep 10 - TTA Loss: 0.9409[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 32/203 [2m0:00:27 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[6.3844e-35, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:27 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[6.5641e-36, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:27 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1.2051e-36, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:27 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[3.5595e-37, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:27 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1.5383e-37, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:27 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[8.9340e-38, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:27 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[6.4659e-38, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:27 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[5.4631e-38, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:27 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[5.0979e-38, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:27 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.4984, 0.5016]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 32/203 [2m0:00:27 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1.8735e-19, 1.0000e+00],;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 32/203 [2m0:00:27 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
        [2.6246e-14, 1.0000e+00],
        [1.0000e+00, 6.6537e-08],
        [6.3477e-20, 1.0000e+00],
        [1.8230e-11, 1.0000e+00],
        [4.3279e-14, 1.0000e+00],
        [1.9838e-06, 1.0000e+00],
        [6.8839e-14, 1.0000e+00],
        [4.9116e-38, 1.0000e+00],
[2K        [8.6251e-16, 1.0000e+00]], device='cuda:0')5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 32/203 [2m0:00:27 â€¢ 0:02:27[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.5588, 0.4412],6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 33/203 [2m0:00:28 â€¢ 0:02:26[0m [2;4m1.17it/s[0m  
        [0.5547, 0.4453],
        [0.5555, 0.4445],
        [0.5576, 0.4424],
        [0.5567, 0.4433],
        [0.5538, 0.4462],
        [0.5532, 0.4468],
        [0.5583, 0.4417],
        [0.5588, 0.4412],
[2K        [0.5562, 0.4438]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 33/203 [2m0:00:28 â€¢ 0:02:26[0m [2;4m1.17it/s[0m  
[2KClass label: CleanAndJerkâ”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 33/203 [2m0:00:28 â€¢ 0:02:26[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.5673, 0.4327]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:28 â€¢ 0:02:26[0m [2;4m1.17it/s[0m  
[2KStep 0 - TTA Loss: 0.5298â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 33/203 [2m0:00:28 â€¢ 0:02:26[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.5673, 0.4327]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:28 â€¢ 0:02:26[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.5695, 0.4305]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:28 â€¢ 0:02:26[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.5814, 0.4186]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:28 â€¢ 0:02:26[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.6152, 0.3848]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:28 â€¢ 0:02:26[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.6802, 0.3198]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:28 â€¢ 0:02:26[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.7729, 0.2271]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:28 â€¢ 0:02:26[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.8571, 0.1429]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:28 â€¢ 0:02:26[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.9533, 0.0467]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 33/203 [2m0:00:28 â€¢ 0:02:26[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[9.9911e-01, 8.9385e-04]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 33/203 [2m0:00:28 â€¢ 0:02:26[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[9.9999e-01, 1.3017e-05]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 33/203 [2m0:00:28 â€¢ 0:02:26[0m [2;4m1.17it/s[0m  
[2KStep 10 - TTA Loss: 1.3941[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 33/203 [2m0:00:28 â€¢ 0:02:26[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.4342e-07]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 33/203 [2m0:00:28 â€¢ 0:02:26[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.3123e-09]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 33/203 [2m0:00:28 â€¢ 0:02:26[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.0977e-11]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 33/203 [2m0:00:28 â€¢ 0:02:26[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 9.1794e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 33/203 [2m0:00:28 â€¢ 0:02:26[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 8.3193e-16]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 33/203 [2m0:00:28 â€¢ 0:02:26[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 8.7640e-18]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 33/203 [2m0:00:28 â€¢ 0:02:26[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.1378e-19]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 33/203 [2m0:00:28 â€¢ 0:02:26[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.9069e-21]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 33/203 [2m0:00:28 â€¢ 0:02:26[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.2721e-23]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 33/203 [2m0:00:28 â€¢ 0:02:26[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.5940, 0.4060]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 33/203 [2m0:00:28 â€¢ 0:02:26[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.5120e-08],[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 33/203 [2m0:00:28 â€¢ 0:02:26[0m [2;4m1.17it/s[0m  
        [1.0000e+00, 2.0383e-07],
        [1.0000e+00, 1.2651e-24],
        [1.0000e+00, 9.6959e-10],
        [1.0000e+00, 1.3372e-13],
        [1.0000e+00, 1.3518e-12],
        [1.0000e+00, 4.4641e-12],
        [1.0000e+00, 1.7869e-07],
        [1.0000e+00, 3.2404e-08],
[2K        [1.0000e+00, 1.6458e-12]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 33/203 [2m0:00:28 â€¢ 0:02:26[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.5589, 0.4411],6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 34/203 [2m0:00:29 â€¢ 0:02:25[0m [2;4m1.17it/s[0m  
        [0.5547, 0.4453],
        [0.5555, 0.4445],
        [0.5576, 0.4424],
        [0.5567, 0.4433],
        [0.5538, 0.4462],
        [0.5532, 0.4468],
        [0.5583, 0.4417],
        [0.5588, 0.4412],
[2K        [0.5562, 0.4438]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 34/203 [2m0:00:29 â€¢ 0:02:25[0m [2;4m1.17it/s[0m  
[2KClass label: HammerThrowâ”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 34/203 [2m0:00:29 â€¢ 0:02:25[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.5845, 0.4155]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:29 â€¢ 0:02:25[0m [2;4m1.17it/s[0m  
[2KStep 0 - TTA Loss: 0.6214â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 34/203 [2m0:00:29 â€¢ 0:02:25[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.8689, 0.1311]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:29 â€¢ 0:02:25[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.9390, 0.0610]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 34/203 [2m0:00:29 â€¢ 0:02:25[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 6.9647e-09]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 34/203 [2m0:00:29 â€¢ 0:02:25[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 5.9110e-15]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 34/203 [2m0:00:29 â€¢ 0:02:25[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.5474e-20]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 34/203 [2m0:00:29 â€¢ 0:02:25[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.3135e-24]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 34/203 [2m0:00:29 â€¢ 0:02:25[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.5742e-28]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 34/203 [2m0:00:29 â€¢ 0:02:25[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.2677e-31]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 34/203 [2m0:00:29 â€¢ 0:02:25[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 7.5894e-34]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 34/203 [2m0:00:29 â€¢ 0:02:25[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 8.1601e-36]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 34/203 [2m0:00:29 â€¢ 0:02:25[0m [2;4m1.17it/s[0m  
[2KStep 10 - TTA Loss: 1.5045[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 34/203 [2m0:00:29 â€¢ 0:02:25[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.3941e-37]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 34/203 [2m0:00:29 â€¢ 0:02:25[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.6407e-38]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 34/203 [2m0:00:29 â€¢ 0:02:25[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.2715e-39]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 34/203 [2m0:00:29 â€¢ 0:02:25[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 5.5613e-40]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 34/203 [2m0:00:29 â€¢ 0:02:25[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.1360e-40]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 34/203 [2m0:00:29 â€¢ 0:02:25[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.1577e-40]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 34/203 [2m0:00:29 â€¢ 0:02:25[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 8.0792e-41]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 34/203 [2m0:00:29 â€¢ 0:02:25[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 6.7150e-41]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 34/203 [2m0:00:29 â€¢ 0:02:25[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 6.2309e-41]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 34/203 [2m0:00:29 â€¢ 0:02:25[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.6308, 0.3692]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 34/203 [2m0:00:29 â€¢ 0:02:25[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.1419e-29],[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 34/203 [2m0:00:29 â€¢ 0:02:25[0m [2;4m1.17it/s[0m  
        [1.0000e+00, 6.1041e-17],
        [1.0000e+00, 1.0836e-30],
        [1.0000e+00, 1.1127e-26],
        [1.0000e+00, 5.7309e-41],
        [1.0000e+00, 5.4242e-32],
        [1.0000e+00, 3.2779e-30],
        [1.0000e+00, 5.9885e-25],
        [1.0000e+00, 2.2473e-23],
[2K        [1.0000e+00, 1.1797e-34]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 34/203 [2m0:00:29 â€¢ 0:02:25[0m [2;4m1.17it/s[0m  
[2KAttention weights: tensor([[0.5588, 0.4412],6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 35/203 [2m0:00:30 â€¢ 0:02:23[0m [2;4m1.18it/s[0m  
        [0.5547, 0.4453],
        [0.5555, 0.4445],
        [0.5575, 0.4425],
        [0.5566, 0.4434],
        [0.5538, 0.4462],
        [0.5531, 0.4469],
        [0.5583, 0.4417],
        [0.5587, 0.4413],
[2K        [0.5562, 0.4438]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 35/203 [2m0:00:30 â€¢ 0:02:23[0m [2;4m1.18it/s[0m  
[2KClass label: GolfSwingâ”â”â”â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 35/203 [2m0:00:30 â€¢ 0:02:23[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.6305, 0.3695]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:30 â€¢ 0:02:23[0m [2;4m1.18it/s[0m  
[2KStep 0 - TTA Loss: 1.0392â”[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 35/203 [2m0:00:30 â€¢ 0:02:23[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.6305, 0.3695]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:30 â€¢ 0:02:23[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.6323, 0.3677]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:30 â€¢ 0:02:23[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.6441, 0.3559]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:30 â€¢ 0:02:23[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.6734, 0.3266]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:30 â€¢ 0:02:23[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.7230, 0.2770]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:30 â€¢ 0:02:23[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.7535, 0.2465]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:30 â€¢ 0:02:23[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.7622, 0.2378]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:30 â€¢ 0:02:23[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.8491, 0.1509]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:30 â€¢ 0:02:23[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.9802, 0.0198]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:30 â€¢ 0:02:23[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.9979, 0.0021]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:30 â€¢ 0:02:23[0m [2;4m1.18it/s[0m  
[2KStep 10 - TTA Loss: 1.4904[0m[38;2;98;6;224mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 35/203 [2m0:00:30 â€¢ 0:02:23[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.5918, 0.4082]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:30 â€¢ 0:02:23[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.0059, 0.9941]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”[0m 35/203 [2m0:00:30 â€¢ 0:02:23[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[2.4374e-05, 9.9998e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 35/203 [2m0:00:30 â€¢ 0:02:23[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[1.0975e-07, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 35/203 [2m0:00:30 â€¢ 0:02:23[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[5.7402e-10, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 35/203 [2m0:00:30 â€¢ 0:02:23[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[3.6939e-12, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 35/203 [2m0:00:30 â€¢ 0:02:23[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[3.0687e-14, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 35/203 [2m0:00:30 â€¢ 0:02:23[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[3.4209e-16, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 35/203 [2m0:00:30 â€¢ 0:02:23[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[5.2781e-18, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)m 35/203 [2m0:00:30 â€¢ 0:02:23[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.5251, 0.4749]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 35/203 [2m0:00:30 â€¢ 0:02:23[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[4.3677e-15, 1.0000e+00],[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 35/203 [2m0:00:30 â€¢ 0:02:23[0m [2;4m1.18it/s[0m  
        [6.8916e-08, 1.0000e+00],
        [5.6065e-09, 1.0000e+00],
        [1.1059e-19, 1.0000e+00],
        [1.4268e-12, 1.0000e+00],
        [3.6686e-11, 1.0000e+00],
        [1.1103e-10, 1.0000e+00],
        [2.6488e-09, 1.0000e+00],
        [6.3924e-14, 1.0000e+00],
[2K        [8.2354e-14, 1.0000e+00]], device='cuda:0')[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 35/203 [2m0:00:30 â€¢ 0:02:23[0m [2;4m1.18it/s[0m  
[2KAttention weights: tensor([[0.5588, 0.4412],7mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 36/203 [2m0:00:30 â€¢ 0:02:21[0m [2;4m1.19it/s[0m   
        [0.5547, 0.4453],
        [0.5554, 0.4446],
        [0.5575, 0.4425],
        [0.5566, 0.4434],
        [0.5538, 0.4462],
        [0.5531, 0.4469],
        [0.5582, 0.4418],
        [0.5587, 0.4413],
[2K        [0.5561, 0.4439]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 36/203 [2m0:00:30 â€¢ 0:02:21[0m [2;4m1.19it/s[0m  
[2KClass label: TennisSwingâ”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 36/203 [2m0:00:30 â€¢ 0:02:21[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[0.6476, 0.3524]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:30 â€¢ 0:02:21[0m [2;4m1.19it/s[0m  
[2KStep 0 - TTA Loss: 0.7029â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 36/203 [2m0:00:30 â€¢ 0:02:21[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[0.1086, 0.8914]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:30 â€¢ 0:02:21[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[0.0104, 0.9896]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:30 â€¢ 0:02:21[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[0.0012, 0.9988]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 36/203 [2m0:00:30 â€¢ 0:02:21[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[1.4659e-04, 9.9985e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:30 â€¢ 0:02:21[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[2.5987e-05, 9.9997e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:30 â€¢ 0:02:21[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[5.7950e-06, 9.9999e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:30 â€¢ 0:02:21[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[1.6815e-06, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:30 â€¢ 0:02:21[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[6.0076e-07, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:30 â€¢ 0:02:21[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[2.6482e-07, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:30 â€¢ 0:02:21[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[1.3791e-07, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:30 â€¢ 0:02:21[0m [2;4m1.19it/s[0m  
[2KStep 10 - TTA Loss: 1.3084â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 36/203 [2m0:00:30 â€¢ 0:02:21[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[8.3168e-08, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:30 â€¢ 0:02:21[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[5.6411e-08, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:30 â€¢ 0:02:21[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[4.2455e-08, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:30 â€¢ 0:02:21[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[3.4386e-08, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:30 â€¢ 0:02:21[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[2.9791e-08, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:30 â€¢ 0:02:21[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[2.7248e-08, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:30 â€¢ 0:02:21[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[2.5822e-08, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:30 â€¢ 0:02:21[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[2.5146e-08, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:30 â€¢ 0:02:21[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[2.4871e-08, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:30 â€¢ 0:02:21[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[0.5341, 0.4659]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 36/203 [2m0:00:30 â€¢ 0:02:21[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[8.8916e-09, 1.0000e+00],5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 36/203 [2m0:00:30 â€¢ 0:02:21[0m [2;4m1.19it/s[0m  
        [6.3184e-05, 9.9994e-01],
        [1.4088e-05, 9.9999e-01],
        [4.4325e-11, 1.0000e+00],
        [1.2639e-07, 1.0000e+00],
        [8.1601e-07, 1.0000e+00],
        [1.8253e-06, 1.0000e+00],
        [1.1473e-05, 9.9999e-01],
        [2.4704e-08, 1.0000e+00],
[2K        [3.6690e-08, 1.0000e+00]], device='cuda:0');5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 36/203 [2m0:00:30 â€¢ 0:02:21[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[0.5587, 0.4413],7mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:31 â€¢ 0:02:20[0m [2;4m1.19it/s[0m  
        [0.5546, 0.4454],
        [0.5554, 0.4446],
        [0.5574, 0.4426],
        [0.5565, 0.4435],
        [0.5537, 0.4463],
        [0.5531, 0.4469],
        [0.5582, 0.4418],
        [0.5586, 0.4414],
[2K        [0.5561, 0.4439]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:31 â€¢ 0:02:20[0m [2;4m1.19it/s[0m  
[2KClass label: SoccerPenaltyâ”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:31 â€¢ 0:02:20[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[0.6362, 0.3638]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:00:31 â€¢ 0:02:20[0m [2;4m1.19it/s[0m  
[2KStep 0 - TTA Loss: 0.9450â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:31 â€¢ 0:02:20[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[0.6362, 0.3638]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:00:31 â€¢ 0:02:20[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[0.6407, 0.3593]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:00:31 â€¢ 0:02:20[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[0.6643, 0.3357]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:00:31 â€¢ 0:02:20[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[0.7243, 0.2757]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:00:31 â€¢ 0:02:20[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[0.8268, 0.1732]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:00:31 â€¢ 0:02:20[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[0.9188, 0.0812]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:00:31 â€¢ 0:02:20[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[0.9642, 0.0358]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 37/203 [2m0:00:31 â€¢ 0:02:20[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[9.9994e-01, 5.6717e-05]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:31 â€¢ 0:02:20[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.5076e-08]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:31 â€¢ 0:02:20[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.7219e-11]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:31 â€¢ 0:02:20[0m [2;4m1.19it/s[0m  
[2KStep 10 - TTA Loss: 1.7316â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:31 â€¢ 0:02:20[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 3.7629e-15]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:31 â€¢ 0:02:20[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 5.6321e-19]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:31 â€¢ 0:02:20[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 6.8929e-23]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:31 â€¢ 0:02:20[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 8.1575e-27]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:31 â€¢ 0:02:20[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.0881e-30]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:31 â€¢ 0:02:20[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.8727e-34]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:31 â€¢ 0:02:20[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 4.6638e-38]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:31 â€¢ 0:02:20[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.8451e-41]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:31 â€¢ 0:02:20[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 1.2612e-44]], device='cuda:0', grad_fn=<SoftmaxBackward0>)203 [2m0:00:31 â€¢ 0:02:20[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[0.6059, 0.3941]], device='cuda:0')â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:31 â€¢ 0:02:20[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[1.0000e+00, 2.6306e-21],5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:31 â€¢ 0:02:20[0m [2;4m1.19it/s[0m  
        [1.0000e+00, 6.5558e-18],
        [1.0000e+00, 1.3222e-12],
        [1.0000e+00, 9.8417e-17],
        [1.0000e+00, 2.1630e-22],
        [1.0000e+00, 3.2976e-23],
        [1.0000e+00, 6.0002e-22],
        [1.0000e+00, 0.0000e+00],
        [1.0000e+00, 2.2616e-17],
[2K        [1.0000e+00, 9.3472e-20]], device='cuda:0');5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 37/203 [2m0:00:31 â€¢ 0:02:20[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[0.5589, 0.4411],7mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 38/203 [2m0:00:32 â€¢ 0:02:19[0m [2;4m1.19it/s[0m  
        [0.5548, 0.4452],
        [0.5555, 0.4445],
        [0.5576, 0.4424],
        [0.5567, 0.4433],
        [0.5539, 0.4461],
        [0.5532, 0.4468],
        [0.5585, 0.4415],
        [0.5588, 0.4412],
[2K        [0.5562, 0.4438]], device='cuda:0', grad_fn=<SoftmaxBackward0>)â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 38/203 [2m0:00:32 â€¢ 0:02:19[0m [2;4m1.19it/s[0m  
[2KClass label: ThrowDiscusâ”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 38/203 [2m0:00:32 â€¢ 0:02:19[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[0.6348, 0.3652]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:00:32 â€¢ 0:02:19[0m [2;4m1.19it/s[0m  
[2KStep 0 - TTA Loss: 0.8780â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 38/203 [2m0:00:32 â€¢ 0:02:19[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[0.9831, 0.0169]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:00:32 â€¢ 0:02:19[0m [2;4m1.19it/s[0m  
[2KAttention weights: tensor([[0.9958, 0.0042]], device='cuda:0', grad_fn=<SoftmaxBackward0>)[0m 38/203 [2m0:00:32 â€¢ 0:02:19[0m [2;4m1.19it/s[0m  
[2KTesting [38;2;98;6;224mâ”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m 38/203 [2m0:00:32 â€¢ 0:02:19[0m [2;4m1.19it/s[0m

Detected KeyboardInterrupt, attempting graceful shutdown ...
[?25h[[36m2025-02-14 00:43:44,099[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/def/fewshot/logs/train/runs/2025-02-14_00-43-01[0m
[[36m2025-02-14 00:43:44,100[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
